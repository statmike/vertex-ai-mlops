{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a52aa270",
   "metadata": {},
   "source": [
    "# 05d - Vertex AI > Training > Training Pipelines - With Python Source Distribution\n",
    "\n",
    "**Training Jobs Overview:**  \n",
    "Where a model gets trained is where it consumes computing resources.  With Vertex AI, you have choices for configuring the computing resources available at training.  This notebook is an example of an execution environment.  When it was set up there were choices for machine type and accelerators (GPUs).  \n",
    "\n",
    "In the 04 series of demonstrations, the model training happened directly in the notebook.  The models were then imported to Vertex AI and deployed to an endpoint for online predictions. \n",
    "\n",
    "In this 05 series of demonstrations, the same model is trained using managed computing resources in Vertex AI as custom training jobs.  These jobs will be demonstrated as:\n",
    "\n",
    "-  Custom Job from a python file and python source distribution\n",
    "-  Training Pipeline that trains and saves models from a python file and python source distribution\n",
    "-  Hyperparameter Tuning Jobs from a python source distribution\n",
    "\n",
    "**This Notebook: An extension of 05b**  \n",
    "This notebook trains the same Tensorflow Keras model from 04a by first modifying and saving the training code to a python script.  Then a Python source distribution is built containing the script.  While this example fits nicely in a single script, larger examples will benefit from the flexibility offered by source distributions and this job gives an example of making the shift.  \n",
    "\n",
    "The source distribution is then used as an input for a Vertex AI Training Job.  The client used here is `aiplatform.CustomPythonPackageTrainingJob(python_package_gcs_uri=)`. The functional difference from the `aiplatform.CustomJob` version in 05b is that this method automatically uploads the final saved model to Vertex AI > Models.  Running the job this way first triggers a job in Vertex AI > Training > Training Pipeline.  This Training Pipeline triggers a Custom Job in Vertex AI > Training > Custom Jobs.  If The Custom Job completes successfully then the final saved model is registered in Vertex AI > Models.\n",
    "\n",
    "The training can be reviewed with Vertex AI's managed Tensorboard under Experiments > Experiments, or by clicking on the `05d...` custom job under Training > Custom Jobs and then clicking the 'Open Tensorboard' link. \n",
    "\n",
    "**Prerequisites:**\n",
    "\n",
    "-  01 - BigQuery - Table Data Source\n",
    "-  05 - Vertex AI > Experiments - Managed Tensorboard\n",
    "-  Understanding:\n",
    "   -  04a - Vertex AI > Notebooks - Models Built in Notebooks with Tensorflow\n",
    "      -  Contains a more granular review of the Tensorflow model training\n",
    "\n",
    "**Overview:**\n",
    "\n",
    "-  Setup\n",
    "-  Connect to Tensorboard instance from 05\n",
    "-  Create a `train.py` Python script that recreates the local training in 04a\n",
    "-  Build a Python source distribution that contains the `train.py` script\n",
    "-  Use Python Client google.cloud.aiplatform for Vertex AI\n",
    "   -  Custom training job with aiplatform.CustomPythonPackageTrainingJob\n",
    "      -  Run job with .run\n",
    "   -  Create Endpoint with Vertex AI with aiplatform.Endpoint.create\n",
    "      -  Deploy model to endpoint with .deploy \n",
    "-  Online Prediction demonstrated using Vertex AI Endpoint with deployed model\n",
    "   -  Get records to score from BigQuery table\n",
    "   -  Prediction with aiplatform.Endpoint.predict\n",
    "   -  Prediction with REST\n",
    "   -  Prediction with gcloud (CLI)\n",
    "\n",
    "**Resources:**\n",
    "\n",
    "-  [BigQuery Tensorflow Reader](https://www.tensorflow.org/io/tutorials/bigquery)\n",
    "-  [Keras Sequential](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential)\n",
    "   -  [Keras API](https://www.tensorflow.org/api_docs/python/tf/keras)\n",
    "-  [Python Client For Google BigQuery](https://googleapis.dev/python/bigquery/latest/index.html)\n",
    "-  [Tensorflow Python Client](https://www.tensorflow.org/api_docs/python/tf)\n",
    "-  [Tensorflow I/O Python Client](https://www.tensorflow.org/io/api_docs/python/tfio/bigquery)\n",
    "-  [Python Client for Vertex AI](https://googleapis.dev/python/aiplatform/latest/aiplatform.html)\n",
    "-  [Create a Python source distribution](https://cloud.google.com/vertex-ai/docs/training/create-python-pre-built-container) for a Vertex AI custom training job\n",
    "-  Containers for training (Pre-Built)\n",
    "   -  [Overview](https://cloud.google.com/vertex-ai/docs/training/create-python-pre-built-container)\n",
    "   -  [List](https://cloud.google.com/vertex-ai/docs/training/pre-built-containers)\n",
    "\n",
    "**Related Training:**\n",
    "\n",
    "-  todo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e105cf",
   "metadata": {},
   "source": [
    "---\n",
    "## Vertex AI - Conceptual Flow\n",
    "\n",
    "<img src=\"architectures/slides/slide_27.png\">\n",
    "\n",
    "---\n",
    "## Vertex AI - Workflow\n",
    "\n",
    "<img src=\"architectures/slides/slide_28.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f24fdb",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4a3117",
   "metadata": {},
   "source": [
    "inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e18a5582",
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION = 'us-central1'\n",
    "PROJECT_ID='statmike-mlops'\n",
    "DATANAME = 'digits'\n",
    "NOTEBOOK = '05d'\n",
    "\n",
    "# Resources\n",
    "#TRAIN_IMAGE = 'us-docker.pkg.dev/cloud-aiplatform/training/tf-cpu.2-4:latest'\n",
    "TRAIN_IMAGE = 'us-docker.pkg.dev/vertex-ai/training/tf-cpu.2-3:latest'\n",
    "DEPLOY_IMAGE ='us-docker.pkg.dev/cloud-aiplatform/prediction/tf2-cpu.2-3:latest'\n",
    "TRAIN_COMPUTE = 'n1-standard-4'\n",
    "DEPLOY_COMPUTE = 'n1-standard-4'\n",
    "\n",
    "# Model Training\n",
    "VAR_TARGET = 'target'\n",
    "VAR_OMIT = 'target_OE' # add more variables to the string with space delimiters\n",
    "EPOCHS = 25\n",
    "BATCH_SIZE = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420b70e0",
   "metadata": {},
   "source": [
    "packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f845b287",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "from datetime import datetime\n",
    "\n",
    "from google.cloud import bigquery\n",
    "from google.protobuf import json_format\n",
    "from google.protobuf.struct_pb2 import Value\n",
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fb3700",
   "metadata": {},
   "source": [
    "clients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7165b817",
   "metadata": {},
   "outputs": [],
   "source": [
    "aiplatform.init(project=PROJECT_ID, location=REGION)\n",
    "bigquery = bigquery.Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f5318b",
   "metadata": {},
   "source": [
    "parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17dc8625",
   "metadata": {},
   "outputs": [],
   "source": [
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "BUCKET = PROJECT_ID\n",
    "URI = f\"gs://{BUCKET}/{DATANAME}/models/{NOTEBOOK}\"\n",
    "DIR = f\"temp/{NOTEBOOK}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "830ba403",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'691911073727-compute@developer.gserviceaccount.com'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Give service account roles/storage.objectAdmin permissions\n",
    "# Console > IMA > Select Account <projectnumber>-compute@developer.gserviceaccount.com > edit - give role\n",
    "SERVICE_ACCOUNT = !gcloud config list --format='value(core.account)' \n",
    "SERVICE_ACCOUNT = SERVICE_ACCOUNT[0]\n",
    "SERVICE_ACCOUNT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0178216",
   "metadata": {},
   "source": [
    "environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32266c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf {DIR}\n",
    "!mkdir -p {DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb5f46b",
   "metadata": {},
   "source": [
    "---\n",
    "## Get Tensorboard Instance Name\n",
    "The training job will show up as an experiment for the Tensorboard instance and have the same name as the training job ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d16967c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tb = aiplatform.Tensorboard.list(filter=f'display_name={DATANAME}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab89060a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'projects/691911073727/locations/us-central1/tensorboards/8640050331994030080'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tb[0].resource_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52825b89",
   "metadata": {},
   "source": [
    "---\n",
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c24a099",
   "metadata": {},
   "source": [
    "### Assemble Python File for Training\n",
    "\n",
    "Create the main python trainer file as `/train.py`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87dbb080",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p {DIR}/source/trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa43333e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing temp/05d/source/trainer/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {DIR}/source/trainer/train.py\n",
    "\n",
    "# package import\n",
    "from tensorflow_io.bigquery import BigQueryClient\n",
    "from tensorflow.python.framework import dtypes\n",
    "import tensorflow as tf\n",
    "from google.cloud import bigquery\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# import argument to local variables\n",
    "parser = argparse.ArgumentParser()\n",
    "# the passed param, dest: a name for the param, default: if absent fetch this param from the OS, type: type to convert to, help: description of argument\n",
    "parser.add_argument('--epochs', dest = 'epochs', default = 10, type = int, help = 'Number of Epochs')\n",
    "parser.add_argument('--batch_size', dest = 'batch_size', default = 32, type = int, help = 'Batch Size')\n",
    "parser.add_argument('--var_target', dest = 'var_target', type=str)\n",
    "parser.add_argument('--var_omit', dest = 'var_omit', type=str, nargs='*')\n",
    "parser.add_argument('--project_id', dest = 'project_id', type=str)\n",
    "parser.add_argument('--dataname', dest = 'dataname', type=str)\n",
    "parser.add_argument('--region', dest = 'region', type=str)\n",
    "parser.add_argument('--notebook', dest = 'notebook', type=str)\n",
    "args = parser.parse_args()\n",
    "\n",
    "# built in parameters for data source:\n",
    "PROJECT_ID = args.project_id\n",
    "DATANAME = args.dataname\n",
    "REGION = args.region\n",
    "NOTEBOOK = args.notebook\n",
    "\n",
    "# clients\n",
    "bigquery = bigquery.Client(project = PROJECT_ID)\n",
    "\n",
    "# get schema from bigquery source\n",
    "query = f\"SELECT * FROM {DATANAME}.INFORMATION_SCHEMA.COLUMNS WHERE TABLE_NAME = '{DATANAME}_prepped'\"\n",
    "schema = bigquery.query(query).to_dataframe()\n",
    "\n",
    "# prepare inputs for tensorflow training\n",
    "OMIT = args.var_omit + ['splits']\n",
    "\n",
    "selected_fields = schema[~schema.column_name.isin(OMIT)].column_name.tolist()\n",
    "\n",
    "feature_columns = []\n",
    "feature_layer_inputs = {}\n",
    "for header in selected_fields:\n",
    "    if header != args.var_target:\n",
    "        feature_columns.append(tf.feature_column.numeric_column(header))\n",
    "        feature_layer_inputs[header] = tf.keras.Input(shape=(1,),name=header)\n",
    "\n",
    "# all the columns in this data source are either float64 or int64\n",
    "output_types = schema[~schema.column_name.isin(OMIT)].data_type.tolist()\n",
    "output_types = [dtypes.float64 if x=='FLOAT64' else dtypes.int64 for x in output_types]\n",
    "\n",
    "# remap input data to Tensorflow inputs of features and target\n",
    "def transTable(row_dict):\n",
    "    target=row_dict.pop(args.var_target)\n",
    "    target = tf.one_hot(tf.cast(target,tf.int64),10)\n",
    "    target = tf.cast(target,tf.float32)\n",
    "    return(row_dict,target)\n",
    "\n",
    "# function to setup a bigquery reader with Tensorflow I/O\n",
    "def bq_reader(split):\n",
    "    reader = BigQueryClient()\n",
    "\n",
    "    training = reader.read_session(\n",
    "        parent = f\"projects/{PROJECT_ID}\",\n",
    "        project_id = PROJECT_ID,\n",
    "        table_id = f\"{DATANAME}_prepped\",\n",
    "        dataset_id = DATANAME,\n",
    "        selected_fields = selected_fields,\n",
    "        output_types = output_types,\n",
    "        row_restriction = f\"splits='{split}'\",\n",
    "        requested_streams = 3\n",
    "    )\n",
    "    \n",
    "    return training\n",
    "\n",
    "train = bq_reader('TRAIN').parallel_read_rows().map(transTable).shuffle(args.batch_size*3).batch(args.batch_size)\n",
    "validate = bq_reader('VALIDATE').parallel_read_rows().map(transTable).batch(args.batch_size)\n",
    "test = bq_reader('TEST').parallel_read_rows().map(transTable).batch(args.batch_size)\n",
    "\n",
    "# define model and compile\n",
    "feature_layer = tf.keras.layers.DenseFeatures(feature_columns)\n",
    "feature_layer_outputs = feature_layer(feature_layer_inputs)\n",
    "model = tf.keras.Model(\n",
    "    inputs = [v for v in feature_layer_inputs.values()],\n",
    "    outputs = tf.keras.layers.Dense(10, activation = tf.nn.softmax)(feature_layer_outputs)\n",
    ")\n",
    "model.compile(\n",
    "    optimizer = 'sgd',\n",
    "    loss = 'categorical_crossentropy',\n",
    "    metrics = ['accuracy']\n",
    ")\n",
    "#tf.keras.utils.plot_model(model, show_shapes = True, show_dtype = True)\n",
    "\n",
    "\n",
    "# setup tensorboard logs and train\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=os.environ['AIP_TENSORBOARD_LOG_DIR'], histogram_freq=1)\n",
    "history = model.fit(train, epochs = args.epochs, callbacks = [tensorboard_callback], validation_data = validate)\n",
    "\n",
    "# output the model save files\n",
    "model.save(os.getenv(\"AIP_MODEL_DIR\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f767e37e",
   "metadata": {},
   "source": [
    "### Assemble Python Source Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a20d562",
   "metadata": {},
   "source": [
    "create `setup.py` file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e429bd4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing temp/05d/source/setup.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {DIR}/source/setup.py\n",
    "from setuptools import setup\n",
    "from setuptools import find_packages\n",
    "\n",
    "REQUIRED_PACKAGES = ['tensorflow_io']\n",
    "\n",
    "setup(\n",
    "    name = 'trainer',\n",
    "    version = '0.1',\n",
    "    install_requires = REQUIRED_PACKAGES, \n",
    "    packages = find_packages(),\n",
    "    include_package_data = True,\n",
    "    description='Training Package'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad54b372",
   "metadata": {},
   "source": [
    "add `__init__.py` file to the trainer modules folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "36da17c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!touch {DIR}/source/trainer/__init__.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c491ed3",
   "metadata": {},
   "source": [
    "Create the source distribution and copy it to the projects storage bucket:\n",
    "- change to the local direcory with the source folder\n",
    "- remove any previous distributions\n",
    "- tar and gzip the source folder\n",
    "- copy the distribution to the project folder on GCS\n",
    "- change back to the local project directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d9f07810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/vertex-ai-mlops/temp/05d\n",
      "source/\n",
      "source/trainer/\n",
      "source/trainer/train.py\n",
      "source/trainer/__init__.py\n",
      "source/setup.py\n",
      "Copying file://source.tar.gz [Content-Type=application/x-tar]...\n",
      "/ [1 files][  1.9 KiB/  1.9 KiB]                                                \n",
      "Operation completed over 1 objects/1.9 KiB.                                      \n",
      "/home/jupyter/vertex-ai-mlops\n"
     ]
    }
   ],
   "source": [
    "%cd {DIR}\n",
    "\n",
    "!rm -f source.tar source.tar.gz\n",
    "!tar cvf source.tar source\n",
    "!gzip source.tar\n",
    "!gsutil cp source.tar.gz {URI}/{TIMESTAMP}/source.tar.gz\n",
    "\n",
    "temp = '../'*(DIR.count('/')+1)\n",
    "%cd {temp}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd9529f",
   "metadata": {},
   "source": [
    "### Setup Training Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8ad8c826",
   "metadata": {},
   "outputs": [],
   "source": [
    "CMDARGS = [\n",
    "    \"--epochs=\" + str(EPOCHS),\n",
    "    \"--batch_size=\" + str(BATCH_SIZE),\n",
    "    \"--var_target=\" + VAR_TARGET,\n",
    "    \"--var_omit=\" + VAR_OMIT,\n",
    "    \"--project_id=\" + PROJECT_ID,\n",
    "    \"--dataname=\" + DATANAME,\n",
    "    \"--region=\" + REGION,\n",
    "    \"--notebook=\" + NOTEBOOK\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9859db9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "customJob = aiplatform.CustomPythonPackageTrainingJob(\n",
    "    display_name = f'{NOTEBOOK}_{DATANAME}_{TIMESTAMP}',\n",
    "    python_package_gcs_uri = f\"{URI}/{TIMESTAMP}/source.tar.gz\",\n",
    "    python_module_name = \"trainer.train\",\n",
    "    container_uri = TRAIN_IMAGE,\n",
    "    model_serving_container_image_uri = DEPLOY_IMAGE,\n",
    "    staging_bucket = f\"{URI}/{TIMESTAMP}\",\n",
    "    labels = {'notebook':f'{NOTEBOOK}'}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf751a1",
   "metadata": {},
   "source": [
    "### Run Training Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3b293b92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.training_jobs:Training Output directory:\n",
      "gs://statmike-mlops/digits/models/05d/20210919123929 \n",
      "INFO:google.cloud.aiplatform.training_jobs:View Training:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/training/521135526216990720?project=691911073727\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomPythonPackageTrainingJob projects/691911073727/locations/us-central1/trainingPipelines/521135526216990720 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:View backing custom job:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/training/7852503138366914560?project=691911073727\n",
      "INFO:google.cloud.aiplatform.training_jobs:View tensorboard:\n",
      "https://us-central1.tensorboard.googleusercontent.com/experiment/projects+691911073727+locations+us-central1+tensorboards+8640050331994030080+experiments+7852503138366914560\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomPythonPackageTrainingJob projects/691911073727/locations/us-central1/trainingPipelines/521135526216990720 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomPythonPackageTrainingJob projects/691911073727/locations/us-central1/trainingPipelines/521135526216990720 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomPythonPackageTrainingJob projects/691911073727/locations/us-central1/trainingPipelines/521135526216990720 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomPythonPackageTrainingJob projects/691911073727/locations/us-central1/trainingPipelines/521135526216990720 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomPythonPackageTrainingJob projects/691911073727/locations/us-central1/trainingPipelines/521135526216990720 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomPythonPackageTrainingJob run completed. Resource name: projects/691911073727/locations/us-central1/trainingPipelines/521135526216990720\n",
      "INFO:google.cloud.aiplatform.training_jobs:Model available at projects/691911073727/locations/us-central1/models/5215652153611255808\n"
     ]
    }
   ],
   "source": [
    "model = customJob.run(\n",
    "    base_output_dir = f\"{URI}/{TIMESTAMP}\",\n",
    "    service_account = SERVICE_ACCOUNT,\n",
    "    args = CMDARGS,\n",
    "    replica_count = 1,\n",
    "    machine_type = TRAIN_COMPUTE,\n",
    "    accelerator_count = 0,\n",
    "    tensorboard = tb[0].resource_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bbd6a92d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'05d_digits_20210919123929-model'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.display_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0bf05fe",
   "metadata": {},
   "source": [
    "---\n",
    "## Serving"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b30499",
   "metadata": {},
   "source": [
    "### Create An Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fc7d95d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.models:Creating Endpoint\n",
      "INFO:google.cloud.aiplatform.models:Create Endpoint backing LRO: projects/691911073727/locations/us-central1/endpoints/5743479307594891264/operations/6641585419051859968\n",
      "INFO:google.cloud.aiplatform.models:Endpoint created. Resource name: projects/691911073727/locations/us-central1/endpoints/5743479307594891264\n",
      "INFO:google.cloud.aiplatform.models:To use this Endpoint in another session:\n",
      "INFO:google.cloud.aiplatform.models:endpoint = aiplatform.Endpoint('projects/691911073727/locations/us-central1/endpoints/5743479307594891264')\n"
     ]
    }
   ],
   "source": [
    "endpoint = aiplatform.Endpoint.create(\n",
    "    display_name = f'{NOTEBOOK}_{DATANAME}_{TIMESTAMP}',\n",
    "    labels = {'notebook':f'{NOTEBOOK}'}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "246cc37f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'05d_digits_20210919123929'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "endpoint.display_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c6326c",
   "metadata": {},
   "source": [
    "### Deploy Model To Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "962c5134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.models:Deploying Model projects/691911073727/locations/us-central1/models/5215652153611255808 to Endpoint : projects/691911073727/locations/us-central1/endpoints/5743479307594891264\n",
      "INFO:google.cloud.aiplatform.models:Deploy Endpoint model backing LRO: projects/691911073727/locations/us-central1/endpoints/5743479307594891264/operations/4335742409838166016\n",
      "INFO:google.cloud.aiplatform.models:Endpoint model deployed. Resource name: projects/691911073727/locations/us-central1/endpoints/5743479307594891264\n"
     ]
    }
   ],
   "source": [
    "endpoint.deploy(\n",
    "    model = model,\n",
    "    deployed_model_display_name = f'{NOTEBOOK}_{DATANAME}_{TIMESTAMP}',\n",
    "    traffic_percentage = 100,\n",
    "    machine_type = DEPLOY_COMPUTE,\n",
    "    min_replica_count = 1,\n",
    "    max_replica_count = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309072d0",
   "metadata": {},
   "source": [
    "---\n",
    "## Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d185973e",
   "metadata": {},
   "source": [
    "### Prepare a record for prediction: instance and parameters lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2b6ce61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = bigquery.query(query = f\"SELECT * FROM {DATANAME}.{DATANAME} LIMIT 10\").to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b9a7a5b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p0</th>\n",
       "      <th>p1</th>\n",
       "      <th>p2</th>\n",
       "      <th>p3</th>\n",
       "      <th>p4</th>\n",
       "      <th>p5</th>\n",
       "      <th>p6</th>\n",
       "      <th>p7</th>\n",
       "      <th>p8</th>\n",
       "      <th>p9</th>\n",
       "      <th>...</th>\n",
       "      <th>p56</th>\n",
       "      <th>p57</th>\n",
       "      <th>p58</th>\n",
       "      <th>p59</th>\n",
       "      <th>p60</th>\n",
       "      <th>p61</th>\n",
       "      <th>p62</th>\n",
       "      <th>p63</th>\n",
       "      <th>target</th>\n",
       "      <th>target_OE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>Even</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>Even</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>Even</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>Even</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows Ã— 66 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    p0   p1    p2    p3   p4   p5   p6   p7   p8    p9  ...  p56  p57   p58  \\\n",
       "0  0.0  5.0  16.0  15.0  5.0  0.0  0.0  0.0  0.0   2.0  ...  0.0  6.0  16.0   \n",
       "1  0.0  5.0  16.0  12.0  1.0  0.0  0.0  0.0  0.0   5.0  ...  0.0  8.0  16.0   \n",
       "2  0.0  5.0  15.0  16.0  6.0  0.0  0.0  0.0  0.0  11.0  ...  0.0  6.0  16.0   \n",
       "3  0.0  4.0  15.0  15.0  8.0  0.0  0.0  0.0  0.0   8.0  ...  0.0  7.0  14.0   \n",
       "\n",
       "    p59   p60   p61  p62  p63  target  target_OE  \n",
       "0  16.0  16.0  16.0  7.0  0.0       2       Even  \n",
       "1  16.0  16.0  16.0  4.0  0.0       2       Even  \n",
       "2  16.0  16.0  13.0  3.0  0.0       2       Even  \n",
       "3  11.0   0.0   0.0  0.0  0.0       2       Even  \n",
       "\n",
       "[4 rows x 66 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c16ed1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "newob = pred[pred.columns[~pred.columns.isin(VAR_OMIT.split()+[VAR_TARGET])]].to_dict(orient='records')[0]\n",
    "#newob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "97b776e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "instances = [json_format.ParseDict(newob, Value())]\n",
    "parameters = json_format.ParseDict({}, Value())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573f07b3",
   "metadata": {},
   "source": [
    "### Get Predictions: Python Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2518a3f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction(predictions=[[7.6975839e-09, 0.0398400389, 0.958279788, 0.000480729243, 5.47196793e-12, 7.45280886e-06, 0.00124983524, 4.61849536e-09, 0.00014210376, 3.12039781e-08]], deployed_model_id='4237403464239415296', explanations=None)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = endpoint.predict(instances=instances, parameters=parameters)\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5c3c19dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7.6975839e-09,\n",
       " 0.0398400389,\n",
       " 0.958279788,\n",
       " 0.000480729243,\n",
       " 5.47196793e-12,\n",
       " 7.45280886e-06,\n",
       " 0.00124983524,\n",
       " 4.61849536e-09,\n",
       " 0.00014210376,\n",
       " 3.12039781e-08]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction.predictions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "adb8e7c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(prediction.predictions[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b94b2d",
   "metadata": {},
   "source": [
    "### Get Predictions: REST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "da7292d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{DIR}/request.json','w') as file:\n",
    "    file.write(json.dumps({\"instances\": [newob]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ff119d5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"predictions\": [\n",
      "    [\n",
      "      7.6975839e-09,\n",
      "      0.0398400389,\n",
      "      0.958279788,\n",
      "      0.000480729243,\n",
      "      5.47196793e-12,\n",
      "      7.45280886e-06,\n",
      "      0.00124983524,\n",
      "      4.61849536e-09,\n",
      "      0.00014210376,\n",
      "      3.12039781e-08\n",
      "    ]\n",
      "  ],\n",
      "  \"deployedModelId\": \"4237403464239415296\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "!curl -X POST \\\n",
    "-H \"Authorization: Bearer \"$(gcloud auth application-default print-access-token) \\\n",
    "-H \"Content-Type: application/json; charset=utf-8\" \\\n",
    "-d @{DIR}/request.json \\\n",
    "https://{REGION}-aiplatform.googleapis.com/v1/{endpoint.resource_name}:predict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a89a08",
   "metadata": {},
   "source": [
    "### Get Predictions: gcloud (CLI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bd556dea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using endpoint [https://us-central1-prediction-aiplatform.googleapis.com/]\n",
      "[[7.6975839e-09, 0.0398400389, 0.958279788, 0.000480729243, 5.47196793e-12, 7.45280886e-06, 0.00124983524, 4.61849536e-09, 0.00014210376, 3.12039781e-08]]\n"
     ]
    }
   ],
   "source": [
    "!gcloud beta ai endpoints predict {endpoint.name.rsplit('/',1)[-1]} --region={REGION} --json-request={DIR}/request.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eafeeff",
   "metadata": {},
   "source": [
    "---\n",
    "## Remove Resources\n",
    "see notebook \"XX - Cleanup\""
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-3.m79",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-3:m79"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
