{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b45fbf72",
   "metadata": {},
   "source": [
    "![tracker](https://us-central1-vertex-ai-mlops-369716.cloudfunctions.net/pixel-tracking?path=statmike%2Fvertex-ai-mlops%2F05+-+TensorFlow&file=05c+-+Vertex+AI+Custom+Model+-+TensorFlow+-+Custom+Job+With+Custom+Container.ipynb)\n",
    "<!--- header table --->\n",
    "<table align=\"left\">\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://colab.research.google.com/github/statmike/vertex-ai-mlops/blob/main/05%20-%20TensorFlow/05c%20-%20Vertex%20AI%20Custom%20Model%20-%20TensorFlow%20-%20Custom%20Job%20With%20Custom%20Container.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Google Colaboratory logo\">\n",
    "      <br>Run in<br>Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https%3A%2F%2Fraw.githubusercontent.com%2Fstatmike%2Fvertex-ai-mlops%2Fmain%2F05%2520-%2520TensorFlow%2F05c%2520-%2520Vertex%2520AI%2520Custom%2520Model%2520-%2520TensorFlow%2520-%2520Custom%2520Job%2520With%2520Custom%2520Container.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\">\n",
    "      <br>Run in<br>Colab Enterprise\n",
    "    </a>\n",
    "  </td>      \n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://github.com/statmike/vertex-ai-mlops/blob/main/05%20-%20TensorFlow/05c%20-%20Vertex%20AI%20Custom%20Model%20-%20TensorFlow%20-%20Custom%20Job%20With%20Custom%20Container.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      <br>View on<br>GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/statmike/vertex-ai-mlops/main/05%20-%20TensorFlow/05c%20-%20Vertex%20AI%20Custom%20Model%20-%20TensorFlow%20-%20Custom%20Job%20With%20Custom%20Container.ipynb\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
    "      <br>Open in<br>Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "**Changes In Progress**\n",
    "\n",
    "Every attempt is being made to ensure the public version of this notebook runs without error while the follow enhancements are being made:\n",
    "- The workflow of the notebook is being adapted to a Kubeflow Pipeline running on Vertex AI Pipelines\n",
    "- Including Evaluation data within Vertex AI Model Registry\n",
    "- Update the Vertex AI Experiments integration which has been great simplified within the API over the past few months\n",
    "- add client library reference links to each section\n",
    "\n",
    "Order: notebook 05 and 05a will be updated first.  Then 05b-05i will follow quickly.\n",
    "\n",
    "This note will be removed once these changes are complete.\n",
    "\n",
    "---\n",
    "---\n",
    "\n",
    "# 05c - Vertex AI > Training > Custom Jobs - With Custom Container\n",
    "\n",
    "**05 Series Overview**\n",
    "\n",
    ">**NOTE:** The notebooks in the `05 - TensorFlow` series demonstrate training, serving and operations for TensorFlow models and take advantage of [Vertex AI TensorBoard](https://cloud.google.com/vertex-ai/docs/experiments/tensorboard-overview) to track training across experiments.  Running these notebooks will create a Vertex AI TensorBoard instance which previously (before August 2023) had a subscription cost but is now priced based on storage of which this notebook will create minimal size (<2MB). - [Vertex AI Pricing](https://cloud.google.com/vertex-ai/pricing#tensorboard).\n",
    "\n",
    "Where a model gets trained is where it consumes computing resources.  With Vertex AI, you have choices for configuring the computing resources available at training.  This notebook is an example of an execution environment.  When it was set up there were choices for machine type and accelerators (GPUs).  \n",
    "\n",
    "In the [05 - Vertex AI Custom Model - TensorFlow - in Notebook](./05%20-%20Vertex%20AI%20Custom%20Model%20-%20TensorFlow%20-%20in%20Notebook.ipynb) notebook, the model training happened directly in the notebook.  The model was then imported to Vertex AI and deployed to an endpoint for online predictions. \n",
    "\n",
    "In this `05a-05i` series of demonstrations, the same model is trained using managed computing resources in Vertex AI Training as managed jobs.  These jobs will be demonstrated as:\n",
    "\n",
    "-  [Custom Job](https://cloud.google.com/vertex-ai/docs/training/create-custom-job) that trains and saves (to GCS) a model from a python script (`05a`), python source distribution (`05b`), and custom container (`05c`)\n",
    "-  [Training Pipeline](https://cloud.google.com/vertex-ai/docs/training/hyperparameter-tuning-overview) that trains and registers a model from a python script (`05d`), python source distribution (`05e`), and custom container (`05f`)\n",
    "-  [Hyperparameter Tuning Jobs](https://cloud.google.com/vertex-ai/docs/training/create-training-pipeline) from a python script (`05g`), python source distribution (`05h`), and custom container (`05i`)\n",
    "\n",
    "**This Notebook (`05c`): An extension of `05a` that saves the Python script in a custom container.**\n",
    "\n",
    "This notebook trains the same Tensorflow Keras model from [05 - Vertex AI Custom Model - TensorFlow - in Notebook](./05%20-%20Vertex%20AI%20Custom%20Model%20-%20TensorFlow%20-%20in%20Notebook.ipynb) by first modifying and saving the training code to a Python script as shown in [05 - Vertex AI Custom Model - TensorFlow - Notebook to Script](./05%20-%20Vertex%20AI%20Custom%20Model%20-%20TensorFlow%20-%20Notebook%20to%20Script.ipynb). \n",
    "\n",
    "A custom container is built that contains the script and required Python libaries.  For more guidance on this process visit the tip notebook [Python Custom Containers](../Tips/Python%20Custom%20Containers.ipynb).  The script, along with a `requirement.txt` and `Dockerfile` are stored in GCS and then used by Cloud Build to extend the pre-built Vertex AI container and store the resulting image in Artifact Registry.  \n",
    "\n",
    "This job is launched using the Vertex AI client library:\n",
    "- [Python Cloud Client Libraries](https://cloud.google.com/python/docs/reference)\n",
    "    - [google-cloud-aiplatform](https://cloud.google.com/python/docs/reference/aiplatform/latest)\n",
    "        - [`aiplatform` package](https://cloud.google.com/python/docs/reference/aiplatform/latest/google.cloud.aiplatform)\n",
    "            - [`aiplatform.CustomJob()`](https://cloud.google.com/python/docs/reference/aiplatform/latest/google.cloud.aiplatform.CustomJob#google_cloud_aiplatform_CustomJob)\n",
    "                - with [worker_pool_specs](https://cloud.google.com/python/docs/reference/aiplatform/latest/google.cloud.aiplatform_v1.types.WorkerPoolSpec) using `container_spec`\n",
    "\n",
    "|Custom Job|Workflow With Custom Job|\n",
    "|:---:|:---:|\n",
    "|![](../architectures/architectures/images/custom%20training/c_job.png)|![](../architectures/architectures/images/custom%20training/c_workflow.png)|\n",
    "\n",
    "**Vertex AI Training**\n",
    "\n",
    "In Vertex AI Training you can run your training code as a job where you specify the compute resources to use. For tips on preparing code, running training jobs, and workflows for building custom containers with software and training code combined please visit these [tips notebooks](../Tips/readme.md) in this repository:\n",
    "- [Python Packages](../Tips/Python%20Packages.ipynb)\n",
    "- [Python Custom Containers](../Tips/Python%20Custom%20Containers.ipynb)\n",
    "- [Python Training](../Tips/Python%20Training.ipynb)\n",
    "- [Python Job Parameters](../Tips/Python%20Job%20Parameters.ipybnd)\n",
    "\n",
    "<p align=\"center\" width=\"100%\">\n",
    "    <img src=\"../architectures/overview/training.png\" width=\"45%\">\n",
    "    &nbsp; &nbsp; &nbsp; &nbsp;\n",
    "    <img src=\"../architectures/overview/training2.png\" width=\"45%\">\n",
    "</p>\n",
    "\n",
    "**Prerequisites:**\n",
    "-  [01 - BigQuery - Table Data Source](../01%20-%20Data%20Sources/01%20-%20BigQuery%20-%20Table%20Data%20Source.ipynb)\n",
    "-  Understanding:\n",
    "    -  Model overview in [05 - Vertex AI Custom Model - TensorFlow - in Notebook](./05%20-%20Vertex%20AI%20Custom%20Model%20-%20TensorFlow%20-%20in%20Notebook.ipynb)\n",
    "    -  Convert notebook code to Python Script in [05 - Vertex AI Custom Model - TensorFlow - Notebook to Script](./05%20-%20Vertex%20AI%20Custom%20Model%20-%20TensorFlow%20-%20Notebook%20to%20Script.ipynb)\n",
    "\n",
    "**Resources:**\n",
    "- [Vertex AI Custom Container For Training](https://cloud.google.com/vertex-ai/docs/training/containers-overview)\n",
    "\n",
    "<!--\n",
    "**Conceptual Flow & Workflow**\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img alt=\"Conceptual Flow\" src=\"../architectures/slides/05c_arch.png\" width=\"45%\">\n",
    "&nbsp; &nbsp; &nbsp; &nbsp;\n",
    "  <img alt=\"Workflow\" src=\"../architectures/slides/05c_console.png\" width=\"45%\">\n",
    "</p>\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "od_UkDpvRmgD",
    "tags": []
   },
   "source": [
    "---\n",
    "## Colab Setup\n",
    "\n",
    "To run this notebook in Colab click [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/statmike/vertex-ai-mlops/blob/main/05%20-%20TensorFlow/05c%20-%20Vertex%20AI%20Custom%20Model%20-%20TensorFlow%20-%20Custom%20Job%20With%20Custom%20Container.ipynb) and run the cells in this section.  Otherwise, skip this section.\n",
    "\n",
    "This cell will authenticate to GCP (follow prompts in the popup)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 195,
     "status": "ok",
     "timestamp": 1683726184843,
     "user": {
      "displayName": "Mike Henderson",
      "userId": "07691629187611687318"
     },
     "user_tz": 240
    },
    "id": "8UO9FnqyKBlF",
    "tags": []
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = 'statmike-mlops-349915' # replace with project ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 68869,
     "status": "ok",
     "timestamp": 1683726253709,
     "user": {
      "displayName": "Mike Henderson",
      "userId": "07691629187611687318"
     },
     "user_tz": 240
    },
    "id": "N98-KK7LRkjm",
    "outputId": "09ec5008-0def-4e1a-c349-c598ee752f78",
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "    from google.colab import auth\n",
    "    auth.authenticate_user()\n",
    "    !gcloud config set project {PROJECT_ID}\n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Installs\n",
    "\n",
    "The list `packages` contains tuples of package import names and install names.  If the import name is not found then the install name is used to install quitely for the current user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# tuples of (import name, install name)\n",
    "packages = [\n",
    "    ('google.cloud.aiplatform', 'google-cloud-aiplatform'),\n",
    "    ('kfp', 'kfp'),\n",
    "    ('google_cloud_pipeline_components', 'google-cloud-pipeline-components'),\n",
    "    ('google.cloud.devtools', 'google-cloud-build'),\n",
    "    ('google.cloud.artifactregistry_v1', 'google-cloud-artifact-registry')\n",
    "]\n",
    "\n",
    "import importlib\n",
    "install = False\n",
    "for package in packages:\n",
    "    if not importlib.util.find_spec(package[0]):\n",
    "        print(f'installing package {package[1]}')\n",
    "        install = True\n",
    "        !pip install {package[1]} -U -q --user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Enable APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!gcloud services enable artifactregistry.googleapis.com\n",
    "!gcloud services enable cloudbuild.googleapis.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restart Kernel (If Installs Occured)\n",
    "\n",
    "After a kernel restart the code submission can start with the next cell after this one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if install:\n",
    "    import IPython\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'statmike-mlops-349915'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project = !gcloud config get-value project\n",
    "PROJECT_ID = project[0]\n",
    "PROJECT_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "REGION = 'us-central1'\n",
    "EXPERIMENT = '05c'\n",
    "SERIES = '05'\n",
    "\n",
    "# source data\n",
    "BQ_PROJECT = PROJECT_ID\n",
    "BQ_DATASET = 'fraud'\n",
    "BQ_TABLE = 'fraud_prepped'\n",
    "\n",
    "# specify a GCS Bucket\n",
    "GCS_BUCKET = PROJECT_ID\n",
    "\n",
    "# Model Training\n",
    "VAR_TARGET = 'Class'\n",
    "VAR_OMIT = 'transaction_id,splits' # add more variables to the string with comma delimiters\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "from datetime import datetime\n",
    "import importlib\n",
    "from IPython.display import Markdown as md\n",
    "from google.cloud.devtools import cloudbuild_v1\n",
    "from google.cloud import artifactregistry_v1\n",
    "from google.cloud import storage\n",
    "from google.cloud import bigquery\n",
    "#from google.protobuf import json_format\n",
    "#from google.protobuf.struct_pb2 import Value\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "clients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "aiplatform.init(project = PROJECT_ID, location = REGION)\n",
    "bq = bigquery.Client(project = PROJECT_ID)\n",
    "gcs = storage.Client(project = PROJECT_ID)\n",
    "ar_client = artifactregistry_v1.ArtifactRegistryClient()\n",
    "cb_client = cloudbuild_v1.CloudBuildClient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "URI = f\"gs://{GCS_BUCKET}/{SERIES}/{EXPERIMENT}\"\n",
    "DIR = f\"temp/{EXPERIMENT}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1026793852137-compute@developer.gserviceaccount.com'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SERVICE_ACCOUNT = !gcloud config list --format='value(core.account)' \n",
    "SERVICE_ACCOUNT = SERVICE_ACCOUNT[0]\n",
    "SERVICE_ACCOUNT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List the service accounts current roles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROLE\n",
      "roles/bigquery.admin\n",
      "roles/owner\n",
      "roles/run.admin\n",
      "roles/secretmanager.secretAccessor\n",
      "roles/storage.objectAdmin\n"
     ]
    }
   ],
   "source": [
    "!gcloud projects get-iam-policy $PROJECT_ID --filter=\"bindings.members:$SERVICE_ACCOUNT\" --format='table(bindings.role)' --flatten=\"bindings[].members\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Note: If the resulting list is missing [roles/storage.objectAdmin](https://cloud.google.com/storage/docs/access-control/iam-roles) then [revisit the setup notebook](../00%20-%20Setup/00%20-%20Environment%20Setup.ipynb#permissions) and add this permission to the service account with the provided instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(DIR):\n",
    "    os.makedirs(DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment Tracking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "FRAMEWORK = 'tf'\n",
    "TASK = 'classification'\n",
    "MODEL_TYPE = 'dnn'\n",
    "EXPERIMENT_NAME = f'experiment-{SERIES}-{EXPERIMENT}-{FRAMEWORK}-{TASK}-{MODEL_TYPE}'\n",
    "RUN_NAME = f'run-{TIMESTAMP}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## Get Vertex AI Experiments Tensorboard Instance Name\n",
    "[Vertex AI Experiments](https://cloud.google.com/vertex-ai/docs/experiments/tensorboard-overview) has managed [Tensorboard](https://www.tensorflow.org/tensorboard) instances that you can track Tensorboard Experiments (a training run or hyperparameter tuning sweep).  \n",
    "\n",
    "The training job will show up as an experiment for the Tensorboard instance and have the same name as the training job ID.\n",
    "\n",
    "This code checks to see if a Tensorboard Instance has been created in the project, retrieves it if so, creates it otherwise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tb = aiplatform.Tensorboard.list(filter=f\"labels.series={SERIES}\")\n",
    "if tb:\n",
    "    tb = tb[0]\n",
    "else: \n",
    "    tb = aiplatform.Tensorboard.create(display_name = SERIES, labels = {'series' : f'{SERIES}'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'projects/1026793852137/locations/us-central1/tensorboards/7876136041294331904'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tb.resource_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup Vertex AI Experiments\n",
    "\n",
    "The code in this section initializes the experiment and starts a run that represents this notebook.  Throughout the notebook sections for model training and evaluation information will be logged to the experiment using:\n",
    "- [.log_params](https://cloud.google.com/python/docs/reference/aiplatform/latest/google.cloud.aiplatform#google_cloud_aiplatform_log_params)\n",
    "- [.log_metrics](https://cloud.google.com/python/docs/reference/aiplatform/latest/google.cloud.aiplatform#google_cloud_aiplatform_log_metrics)\n",
    "- [.log_time_series_metrics](https://cloud.google.com/python/docs/reference/aiplatform/latest/google.cloud.aiplatform#google_cloud_aiplatform_log_time_series_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "aiplatform.init(experiment = EXPERIMENT_NAME, experiment_tensorboard = tb.resource_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python File for Training\n",
    "\n",
    "This notebook trains the same Tensorflow Keras model from [05 - Vertex AI Custom Model - TensorFlow - in Notebook](./05%20-%20Vertex%20AI%20Custom%20Model%20-%20TensorFlow%20-%20in%20Notebook.ipynb) by first modifying and saving the training code to a python script as shown in [05 - Vertex AI Custom Model - TensorFlow - Notebook to Script](./05%20-%20Vertex%20AI%20Custom%20Model%20-%20TensorFlow%20-%20Notebook%20to%20Script.ipynb) which stores the script in [`./code/train.py`](./code/train.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve the training script (if not already included in a clone of this repository):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document Found at `./code/train.py`\n"
     ]
    }
   ],
   "source": [
    "file = \"./code/train.py\"\n",
    "if not os.path.exists(file):\n",
    "    print('Retrieving document...')\n",
    "    if not os.path.exists(os.path.dirname(file)):\n",
    "      os.makedirs(os.path.dirname(file))\n",
    "    import requests, urllib.parse\n",
    "    r = requests.get(f'https://raw.githubusercontent.com/statmike/vertex-ai-mlops/main/05%20-%20TensorFlow/{urllib.parse.quote(file[2:])}')\n",
    "    open(file, 'wb').write(r.content)\n",
    "    print(f'Document now at `{file}`')\n",
    "else:\n",
    "    print(f'Document Found at `{file}`')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Review the script:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "\n",
       "\n",
       "# package import\n",
       "from tensorflow.python.framework import dtypes\n",
       "from tensorflow_io.bigquery import BigQueryClient\n",
       "import tensorflow as tf\n",
       "from google.cloud import bigquery\n",
       "from google.cloud import aiplatform\n",
       "import argparse\n",
       "import os\n",
       "\n",
       "# import argument to local variables\n",
       "parser = argparse.ArgumentParser()\n",
       "# the passed param, dest: a name for the param, default: if absent fetch this param from the OS, type: type to convert to, help: description of argument\n",
       "parser.add_argument('--epochs', dest = 'epochs', default = 10, type = int, help = 'Number of Epochs')\n",
       "parser.add_argument('--batch_size', dest = 'batch_size', default = 32, type = int, help = 'Batch Size')\n",
       "parser.add_argument('--var_target', dest = 'var_target', type=str)\n",
       "parser.add_argument('--var_omit', dest = 'var_omit', type=str)#, nargs='*')\n",
       "parser.add_argument('--project_id', dest = 'project_id', type=str)\n",
       "parser.add_argument('--bq_project', dest = 'bq_project', type=str)\n",
       "parser.add_argument('--bq_dataset', dest = 'bq_dataset', type=str)\n",
       "parser.add_argument('--bq_table', dest = 'bq_table', type=str)\n",
       "parser.add_argument('--region', dest = 'region', type=str)\n",
       "parser.add_argument('--experiment', dest = 'experiment', type=str)\n",
       "parser.add_argument('--series', dest = 'series', type=str)\n",
       "parser.add_argument('--experiment_name', dest = 'experiment_name', type=str)\n",
       "parser.add_argument('--run_name', dest = 'run_name', type=str)\n",
       "args = parser.parse_args()\n",
       "\n",
       "# clients\n",
       "bq = bigquery.Client(project = args.project_id)\n",
       "aiplatform.init(project = args.project_id, location = args.region)\n",
       "\n",
       "# Vertex AI Experiment\n",
       "if args.run_name in [run.name for run in aiplatform.ExperimentRun.list(experiment = args.experiment_name)]:\n",
       "    expRun = aiplatform.ExperimentRun(run_name = args.run_name, experiment = args.experiment_name)\n",
       "else:\n",
       "    expRun = aiplatform.ExperimentRun.create(run_name = args.run_name, experiment = args.experiment_name)\n",
       "expRun.log_params({'experiment': args.experiment, 'series': args.series, 'project_id': args.project_id})\n",
       "\n",
       "# get schema from bigquery source\n",
       "query = f\"SELECT * FROM {args.bq_project}.{args.bq_dataset}.INFORMATION_SCHEMA.COLUMNS WHERE TABLE_NAME = '{args.bq_table}'\"\n",
       "schema = bq.query(query).to_dataframe()\n",
       "\n",
       "# get number of classes from bigquery source\n",
       "nclasses = bq.query(query = f'SELECT DISTINCT {args.var_target} FROM {args.bq_project}.{args.bq_dataset}.{args.bq_table} WHERE {args.var_target} is not null').to_dataframe()\n",
       "nclasses = nclasses.shape[0]\n",
       "expRun.log_params({'data_source': f'bq://{args.bq_project}.{args.bq_dataset}.{args.bq_table}', 'nclasses': nclasses, 'var_split': 'splits', 'var_target': args.var_target})\n",
       "\n",
       "# Make a list of columns to omit\n",
       "OMIT = [x for x in args.var_omit.split(',') if x != '']\n",
       "\n",
       "# use schema to prepare a list of columns to read from BigQuery\n",
       "selected_fields = schema[~schema.column_name.isin(OMIT)].column_name.tolist()\n",
       "\n",
       "# all the columns in this data source are either float64 or int64\n",
       "output_types = [dtypes.float64 if x=='FLOAT64' else dtypes.int64 for x in schema[~schema.column_name.isin(OMIT)].data_type.tolist()]\n",
       "\n",
       "# remap input data to Tensorflow inputs of features and target\n",
       "def transTable(row_dict):\n",
       "    target = row_dict.pop(args.var_target)\n",
       "    target = tf.one_hot(tf.cast(target, tf.int64), nclasses)\n",
       "    target = tf.cast(target, tf.float32)\n",
       "    return(row_dict, target)\n",
       "\n",
       "# function to setup a bigquery reader with Tensorflow I/O\n",
       "def bq_reader(split):\n",
       "    reader = BigQueryClient()\n",
       "\n",
       "    training = reader.read_session(\n",
       "        parent = f\"projects/{args.project_id}\",\n",
       "        project_id = args.bq_project,\n",
       "        table_id = args.bq_table,\n",
       "        dataset_id = args.bq_dataset,\n",
       "        selected_fields = selected_fields,\n",
       "        output_types = output_types,\n",
       "        row_restriction = f\"splits='{split}'\",\n",
       "        requested_streams = 3\n",
       "    )\n",
       "    \n",
       "    return training\n",
       "\n",
       "# setup feed for train, validate and test\n",
       "train = bq_reader('TRAIN').parallel_read_rows().prefetch(1).map(transTable).shuffle(args.batch_size*10).batch(args.batch_size)\n",
       "validate = bq_reader('VALIDATE').parallel_read_rows().prefetch(1).map(transTable).batch(args.batch_size)\n",
       "test = bq_reader('TEST').parallel_read_rows().prefetch(1).map(transTable).batch(args.batch_size)\n",
       "expRun.log_params({'training.batch_size': args.batch_size, 'training.shuffle': 10*args.batch_size, 'training.prefetch': 1})\n",
       "# Logistic Regression\n",
       "\n",
       "# feature list\n",
       "numeric_features = [feature for feature in schema[~schema.column_name.isin(OMIT + [args.var_target])]['column_name'].to_list()]\n",
       "\n",
       "# feature inputs\n",
       "features = [tf.keras.Input(shape = (1,), dtype = dtypes.float64, name = feature) for feature in numeric_features]\n",
       "\n",
       "# normalize features - before training\n",
       "#normalized_features = []\n",
       "#for feature in features:\n",
       "#    normalizer = tf.keras.layers.Normalization(axis = None, name = feature.name + '_normalized')\n",
       "#    feature_data = train.map(lambda x, y: x[feature.name])\n",
       "#    normalizer.adapt(feature_data)\n",
       "#    normalized_features.append(normalizer(feature))\n",
       "\n",
       "# concatenate features\n",
       "all_features = tf.keras.layers.Concatenate(name = 'feature_layer')(features)\n",
       "#all_features = tf.keras.layers.Concatenate(name = 'feature_layer')(normalized_features) # (features)\n",
       "\n",
       "# batch normalization of inputs - during training\n",
       "all_features = tf.keras.layers.BatchNormalization(name = 'batch_normalization_layer')(all_features)\n",
       "\n",
       "# logistic - using softmax activation to nclasses\n",
       "logistic = tf.keras.layers.Dense(nclasses, activation = tf.nn.softmax, name = 'logistic')(all_features)\n",
       "\n",
       "# the model\n",
       "model = tf.keras.Model(\n",
       "    inputs = features,\n",
       "    outputs = logistic,\n",
       "    name = args.experiment\n",
       ")\n",
       "\n",
       "# compile the model\n",
       "model.compile(\n",
       "    optimizer = tf.keras.optimizers.SGD(), #SGD or Adam\n",
       "    loss = tf.keras.losses.CategoricalCrossentropy(),\n",
       "    metrics = ['accuracy', tf.keras.metrics.AUC(curve = 'PR', name = 'auprc')]\n",
       ")\n",
       "\n",
       "# setup tensorboard logs and train\n",
       "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=os.environ['AIP_TENSORBOARD_LOG_DIR'], histogram_freq=1)\n",
       "history = model.fit(train, epochs = args.epochs, callbacks = [tensorboard_callback], validation_data = validate)\n",
       "expRun.log_params({'training.epochs': history.params['epochs']})\n",
       "for e in range(0, history.params['epochs']):\n",
       "    expRun.log_time_series_metrics(\n",
       "        {\n",
       "            'train_loss': history.history['loss'][e],\n",
       "            'train_accuracy': history.history['accuracy'][e],\n",
       "            'train_auprc': history.history['auprc'][e],\n",
       "            'val_loss': history.history['val_loss'][e],\n",
       "            'val_accuracy': history.history['val_accuracy'][e],\n",
       "            'val_auprc': history.history['val_auprc'][e]\n",
       "        }\n",
       "    )\n",
       "\n",
       "# test evaluations:\n",
       "loss, accuracy, auprc = model.evaluate(test)\n",
       "expRun.log_metrics({'test_loss': loss, 'test_accuracy': accuracy, 'test_auprc': auprc})\n",
       "\n",
       "# val evaluations:\n",
       "loss, accuracy, auprc = model.evaluate(validate)\n",
       "expRun.log_metrics({'val_loss': loss, 'val_accuracy': accuracy, 'val_auprc': auprc})\n",
       "\n",
       "# training evaluations:\n",
       "loss, accuracy, auprc = model.evaluate(train)\n",
       "expRun.log_metrics({'train_loss': loss, 'train_accuracy': accuracy, 'train_auprc': auprc})\n",
       "\n",
       "# output the model save files\n",
       "model.save(os.getenv(\"AIP_MODEL_DIR\"))\n",
       "expRun.log_params({'model.save': os.getenv(\"AIP_MODEL_DIR\")})\n",
       "expRun.end_run()\n",
       "\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SCRIPT_PATH = './code/train.py'\n",
    "\n",
    "with open(SCRIPT_PATH, 'r') as file:\n",
    "    data = file.read()\n",
    "md(f\"```python\\n\\n{data}\\n```\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose Computing Environment\n",
    "\n",
    "When using [custom training on Vertex AI](https://cloud.google.com/vertex-ai/docs/training/custom-training-methods) the compute environment is specified as parameters.  At a minimum this will include the [compute resources](https://cloud.google.com/vertex-ai/docs/training/configure-compute) and [container](https://cloud.google.com/vertex-ai/docs/training/configure-container-settings) URIs.\n",
    "\n",
    "This example uses minimal compute with a single node and no accelerators (GPU).\n",
    "\n",
    "For a container, a [pre-built custome training container](https://cloud.google.com/vertex-ai/docs/training/pre-built-containers) for TensorFlow is being used and additional packages are being specified in the jobs parameters. This leads to two important considerations:\n",
    "- What is the Python version on the container? \n",
    "    - This will have an impact when additional packages are being installed and the packages may have version requirments.\n",
    "- What is the framework version on the container?\n",
    "    - Make sure the options you need from a framework are included in the version you pick."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resources\n",
    "TRAIN_COMPUTE = 'n1-standard-4'\n",
    "DEPLOY_COMPUTE = 'n1-standard-4'\n",
    "TRAIN_IMAGE = 'us-docker.pkg.dev/vertex-ai/training/tf-cpu.2-12.py310:latest'\n",
    "DEPLOY_IMAGE ='us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-12:latest'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Custom Container with Cloud Build\n",
    "\n",
    "Cloud Build creates and manages the build on GCP.  The API creates a build by providing:\n",
    "- location of the source\n",
    "- instructions\n",
    "- location to store the built artifacts\n",
    "\n",
    "The instruction part of Cloud Build has options:\n",
    "- Dockerfile\n",
    "- Build Config file (YAML or JSON)\n",
    "- Cloud Native Buildpacks\n",
    "\n",
    "This notebook uses the approach of using the Python Client for Cloud Build and not referencing any local files.  For that reason, the first step is creating a Dockerfile for the workflow and storing it in GCS. The next step is running Cloud Build and using the client to specify the Build config rather than a config file.  The steps of the build config start with getting the code (git clone, or copy from GCS) and copying the Dockerfile.  \n",
    "\n",
    "There are many workflows for creating containers with ML training code.  Many of the most common ones are explored in the tips notebook [Python Custom Containers](../Tips/Python%20Custom%20Containers.ipynb).  The method used here is the simplest - copy the training code directly into the container.  The other methods include packaging the training code as a Python Distribution and using `pip install` in from GCS, GitHub and even Artifact Registry as a private repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Store Resources in Cloud Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bucket = gcs.lookup_bucket(GCS_BUCKET)\n",
    "SOURCEPATH = f'{SERIES}/{EXPERIMENT}/training'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Copy Training Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "blob = bucket.blob(f'{SOURCEPATH}/{EXPERIMENT}_trainer/train.py')\n",
    "blob.upload_from_filename(SCRIPT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Requirements.txt File for Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "requirements = f\"\"\"tensorflow_io\n",
    "google-cloud-aiplatform>={aiplatform.__version__}\n",
    "db-dtypes\n",
    "protobuf>={importlib.metadata.version('protobuf')}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "blob = bucket.blob(f'{SOURCEPATH}/requirements.txt')\n",
    "blob.upload_from_string(requirements)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the Dockerfile\n",
    "A basic dockerfile thats take the base image and copies the code in and define an entrypoint - what python script to run first in this case.  Add RUN entries to pip install additional packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dockerfile = f\"\"\"\n",
    "FROM {TRAIN_IMAGE}\n",
    "WORKDIR /training\n",
    "# copy requirements and install them\n",
    "COPY requirements.txt ./\n",
    "RUN pip install --no-cache-dir --upgrade pip \\\n",
    "  && pip install --no-cache-dir -r requirements.txt\n",
    "## Copies the trainer code to the docker image\n",
    "COPY {EXPERIMENT}_trainer/* ./{EXPERIMENT}_trainer/\n",
    "## Sets up the entry point to invoke the trainer\n",
    "ENTRYPOINT [\"python\", \"-m\", \"{EXPERIMENT}_trainer.train\"]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "blob = bucket.blob(f'{SOURCEPATH}/Dockerfile')\n",
    "blob.upload_from_string(dockerfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup Artifact Registry\n",
    "\n",
    "Artifact registry organizes artifacts with repositories.  Each repository contains packages and is designated to hold a partifcular format of package: Docker images, Python Packages and [others](https://cloud.google.com/artifact-registry/docs/supported-formats#package)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### List Repositories\n",
    "\n",
    "This may be empty if no repositories have been created for this project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "projects/statmike-mlops-349915/locations/us-central1/repositories/statmike-mlops-349915\n",
      "projects/statmike-mlops-349915/locations/us-central1/repositories/statmike-mlops-349915-docker\n",
      "projects/statmike-mlops-349915/locations/us-central1/repositories/statmike-mlops-349915-python\n"
     ]
    }
   ],
   "source": [
    "for repo in ar_client.list_repositories(parent = f'projects/{PROJECT_ID}/locations/{REGION}'):\n",
    "    print(repo.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Docker Image Repository\n",
    "\n",
    "Create an Artifact Registry Repository to hold Docker Images created by this notebook.  First, check to see if it is already created by a previous run and retrieve it if it has.  Otherwise, create!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved existing repo: projects/statmike-mlops-349915/locations/us-central1/repositories/statmike-mlops-349915\n"
     ]
    }
   ],
   "source": [
    "docker_repo = None\n",
    "for repo in ar_client.list_repositories(parent = f'projects/{PROJECT_ID}/locations/{REGION}'):\n",
    "    if f'{PROJECT_ID}' == repo.name.split('/')[-1]:\n",
    "        docker_repo = repo\n",
    "        print(f'Retrieved existing repo: {docker_repo.name}')\n",
    "\n",
    "if not docker_repo:\n",
    "    operation = ar_client.create_repository(\n",
    "        request = artifactregistry_v1.CreateRepositoryRequest(\n",
    "            parent = f'projects/{PROJECT_ID}/locations/{REGION}',\n",
    "            repository_id = f'{PROJECT_ID}',\n",
    "            repository = artifactregistry_v1.Repository(\n",
    "                description = f'A repository for the {SERIES} series that holds docker images.',\n",
    "                name = f'{PROJECT_ID}',\n",
    "                format_ = artifactregistry_v1.Repository.Format.DOCKER,\n",
    "                labels = {'series': SERIES}\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    print('Creating Repository ...')\n",
    "    docker_repo = operation.result()\n",
    "    print(f'Completed creating repo: {docker_repo.name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('projects/statmike-mlops-349915/locations/us-central1/repositories/statmike-mlops-349915',\n",
       " 'DOCKER')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docker_repo.name, docker_repo.format_.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "REPOSITORY = f\"{REGION}-docker.pkg.dev/{PROJECT_ID}/{docker_repo.name.split('/')[-1]}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Custom Container\n",
    "Use the Cloud Build client to construct and run the build instructions.  Here the files collected in GCS are copied to the build instance, then the Docker build is run in the folder with the `Dockerfile`.  The resulting image is pushed to Artifact Registry (setup above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# setup the build config with empty list of steps - these will be added sequentially\n",
    "build = cloudbuild_v1.Build(\n",
    "    steps = []\n",
    ")\n",
    "# retrieve the source\n",
    "build.steps.append(\n",
    "    {\n",
    "        'name': 'gcr.io/cloud-builders/gsutil',\n",
    "        'args': ['cp', '-r', f'gs://{GCS_BUCKET}/{SOURCEPATH}/*', '/workspace']\n",
    "    }\n",
    ")\n",
    "# docker build\n",
    "build.steps.append(\n",
    "    {\n",
    "        'name': 'gcr.io/cloud-builders/docker',\n",
    "        'args': ['build', '-t', f'{REPOSITORY}/{EXPERIMENT}_trainer', '/workspace']\n",
    "    }    \n",
    ")\n",
    "# docker push\n",
    "build.images = [f\"{REPOSITORY}/{EXPERIMENT}_trainer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "steps {\n",
       "  name: \"gcr.io/cloud-builders/gsutil\"\n",
       "  args: \"cp\"\n",
       "  args: \"-r\"\n",
       "  args: \"gs://statmike-mlops-349915/05/05c/training/*\"\n",
       "  args: \"/workspace\"\n",
       "}\n",
       "steps {\n",
       "  name: \"gcr.io/cloud-builders/docker\"\n",
       "  args: \"build\"\n",
       "  args: \"-t\"\n",
       "  args: \"us-central1-docker.pkg.dev/statmike-mlops-349915/statmike-mlops-349915/05c_trainer\"\n",
       "  args: \"/workspace\"\n",
       "}\n",
       "images: \"us-central1-docker.pkg.dev/statmike-mlops-349915/statmike-mlops-349915/05c_trainer\""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "operation = cb_client.create_build(\n",
    "    project_id = PROJECT_ID,\n",
    "    build = build\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<Status.SUCCESS: 3>,\n",
       " images: \"us-central1-docker.pkg.dev/statmike-mlops-349915/statmike-mlops-349915/05c_trainer\")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = operation.result()\n",
    "response.status, response.artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review the Custom Container with Artifact Registry in the Google Cloud Console:\n",
      "https://console.cloud.google.com/artifacts/docker/statmike-mlops-349915/us-central1/statmike-mlops-349915?project=statmike-mlops-349915\n"
     ]
    }
   ],
   "source": [
    "print(f\"Review the Custom Container with Artifact Registry in the Google Cloud Console:\\nhttps://console.cloud.google.com/artifacts/docker/{PROJECT_ID}/{REGION}/{PROJECT_ID}?project={PROJECT_ID}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Training Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CMDARGS = [\n",
    "    \"--epochs=\" + str(EPOCHS),\n",
    "    \"--batch_size=\" + str(BATCH_SIZE),\n",
    "    \"--var_target=\" + VAR_TARGET,\n",
    "    \"--var_omit=\" + VAR_OMIT,\n",
    "    \"--project_id=\" + PROJECT_ID,\n",
    "    \"--bq_project=\" + BQ_PROJECT,\n",
    "    \"--bq_dataset=\" + BQ_DATASET,\n",
    "    \"--bq_table=\" + BQ_TABLE,\n",
    "    \"--region=\" + REGION,\n",
    "    \"--experiment=\" + EXPERIMENT,\n",
    "    \"--series=\" + SERIES,\n",
    "    \"--experiment_name=\" + EXPERIMENT_NAME,\n",
    "    \"--run_name=\" + RUN_NAME\n",
    "]\n",
    "\n",
    "MACHINE_SPEC = {\n",
    "    \"machine_type\": TRAIN_COMPUTE,\n",
    "    \"accelerator_count\": 0\n",
    "}\n",
    "\n",
    "WORKER_POOL_SPEC = [\n",
    "    {\n",
    "        \"replica_count\": 1,\n",
    "        \"machine_spec\": MACHINE_SPEC,\n",
    "        \"container_spec\": {\n",
    "            \"image_uri\": f\"{REPOSITORY}/{EXPERIMENT}_trainer\",\n",
    "            \"command\": [],\n",
    "            \"args\": CMDARGS\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "customJob = aiplatform.CustomJob(\n",
    "    display_name = f'{SERIES}_{EXPERIMENT}_{TIMESTAMP}',\n",
    "    worker_pool_specs = WORKER_POOL_SPEC,\n",
    "    base_output_dir = f\"{URI}/models/{TIMESTAMP}\",\n",
    "    staging_bucket = f\"{URI}/models/{TIMESTAMP}\",\n",
    "    labels = {'series' : f'{SERIES}', 'experiment' : f'{EXPERIMENT}', 'experiment_name' : f'{EXPERIMENT_NAME}', 'run_name' : f'{RUN_NAME}'}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Training Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating CustomJob\n",
      "CustomJob created. Resource name: projects/1026793852137/locations/us-central1/customJobs/5315233478629916672\n",
      "To use this CustomJob in another session:\n",
      "custom_job = aiplatform.CustomJob.get('projects/1026793852137/locations/us-central1/customJobs/5315233478629916672')\n",
      "View Custom Job:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/training/5315233478629916672?project=1026793852137\n",
      "View Tensorboard:\n",
      "https://us-central1.tensorboard.googleusercontent.com/experiment/projects+1026793852137+locations+us-central1+tensorboards+7876136041294331904+experiments+5315233478629916672\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/5315233478629916672 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/5315233478629916672 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/5315233478629916672 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/5315233478629916672 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/5315233478629916672 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/5315233478629916672 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/5315233478629916672 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/5315233478629916672 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/5315233478629916672 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/5315233478629916672 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/5315233478629916672 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/5315233478629916672 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/5315233478629916672 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/5315233478629916672 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/5315233478629916672 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/5315233478629916672 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/5315233478629916672 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/5315233478629916672 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/5315233478629916672 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/5315233478629916672 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/5315233478629916672 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/5315233478629916672 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/5315233478629916672 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/5315233478629916672 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/5315233478629916672 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/5315233478629916672 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/5315233478629916672 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/5315233478629916672 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/5315233478629916672 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/5315233478629916672 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/5315233478629916672 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/5315233478629916672 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/5315233478629916672 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/5315233478629916672 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/5315233478629916672 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/5315233478629916672 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/5315233478629916672 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/5315233478629916672 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/5315233478629916672 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/5315233478629916672 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/5315233478629916672 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/5315233478629916672 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/5315233478629916672 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/5315233478629916672 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/5315233478629916672 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/5315233478629916672 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/5315233478629916672 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/5315233478629916672 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/5315233478629916672 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/5315233478629916672 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/5315233478629916672 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/5315233478629916672 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/5315233478629916672 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/5315233478629916672 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/5315233478629916672 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/5315233478629916672 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/5315233478629916672 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/5315233478629916672 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/5315233478629916672 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/5315233478629916672 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/5315233478629916672 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/5315233478629916672 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/5315233478629916672 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/5315233478629916672 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/5315233478629916672 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/5315233478629916672 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/5315233478629916672 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/5315233478629916672 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/5315233478629916672 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/5315233478629916672 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/5315233478629916672 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/5315233478629916672 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/5315233478629916672 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/5315233478629916672 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/5315233478629916672 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/5315233478629916672 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/5315233478629916672 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/5315233478629916672 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/5315233478629916672 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/5315233478629916672 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/5315233478629916672 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/5315233478629916672 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/5315233478629916672 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/5315233478629916672 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/5315233478629916672 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/5315233478629916672 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/5315233478629916672 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/5315233478629916672 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/5315233478629916672 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/5315233478629916672 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/5315233478629916672 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/5315233478629916672 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/5315233478629916672 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/5315233478629916672 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/5315233478629916672 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/5315233478629916672 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/5315233478629916672 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/5315233478629916672 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/5315233478629916672 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/5315233478629916672 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/5315233478629916672 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/5315233478629916672 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/5315233478629916672 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/5315233478629916672 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/5315233478629916672 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/5315233478629916672 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/5315233478629916672 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/5315233478629916672 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/5315233478629916672 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/5315233478629916672 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/5315233478629916672 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/5315233478629916672 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/5315233478629916672 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/5315233478629916672 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/5315233478629916672 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/5315233478629916672 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/5315233478629916672 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/5315233478629916672 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/5315233478629916672 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/5315233478629916672 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/5315233478629916672 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/5315233478629916672 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/5315233478629916672 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/5315233478629916672 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/5315233478629916672 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/5315233478629916672 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/5315233478629916672 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/5315233478629916672 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/5315233478629916672 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/5315233478629916672 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/5315233478629916672 current state:\n",
      "JobState.JOB_STATE_SUCCEEDED\n",
      "CustomJob run completed. Resource name: projects/1026793852137/locations/us-central1/customJobs/5315233478629916672\n"
     ]
    }
   ],
   "source": [
    "customJob.run(\n",
    "    service_account = SERVICE_ACCOUNT,\n",
    "    tensorboard = tb.resource_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'05_05c_20231221185424'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customJob.display_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'projects/1026793852137/locations/us-central1/customJobs/5315233478629916672'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customJob.resource_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create hyperlinks to job and tensorboard here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review the Custom Job here:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/training/5315233478629916672/cpu?cloudshell=false&project=statmike-mlops-349915\n",
      "Review the TensorBoard From the Job here:\n",
      "https://us-central1.tensorboard.googleusercontent.com/experiment/projects+1026793852137+locations+us-central1+tensorboards+7876136041294331904+experiments+5315233478629916672\n"
     ]
    }
   ],
   "source": [
    "job_link = f\"https://console.cloud.google.com/vertex-ai/locations/{REGION}/training/{customJob.resource_name.split('/')[-1]}/cpu?cloudshell=false&project={PROJECT_ID}\"\n",
    "board_link = f\"https://{REGION}.tensorboard.googleusercontent.com/experiment/{tb.resource_name.replace('/', '+')}+experiments+{customJob.resource_name.split('/')[-1]}\"\n",
    "\n",
    "print(f'Review the Custom Job here:\\n{job_link}')\n",
    "print(f'Review the TensorBoard From the Job here:\\n{board_link}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Serving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Already in Registry:\n",
      "Loading model as new default version.\n",
      "Creating Model\n",
      "Create Model backing LRO: projects/1026793852137/locations/us-central1/models/model_05_05c/operations/9122763355658911744\n",
      "Model created. Resource name: projects/1026793852137/locations/us-central1/models/model_05_05c@6\n",
      "To use this Model in another session:\n",
      "model = aiplatform.Model('projects/1026793852137/locations/us-central1/models/model_05_05c@6')\n"
     ]
    }
   ],
   "source": [
    "modelmatch = aiplatform.Model.list(filter = f'display_name={SERIES}_{EXPERIMENT} AND labels.series={SERIES} AND labels.experiment={EXPERIMENT}')\n",
    "\n",
    "upload_model = True\n",
    "if modelmatch:\n",
    "    print(\"Model Already in Registry:\")\n",
    "    if RUN_NAME in modelmatch[0].version_aliases:\n",
    "        print(\"This version already loaded, no action taken.\")\n",
    "        upload_model = False\n",
    "        model = aiplatform.Model(model_name = modelmatch[0].resource_name)\n",
    "    else:\n",
    "        print('Loading model as new default version.')\n",
    "        parent_model = modelmatch[0].resource_name\n",
    "\n",
    "else:\n",
    "    print('This is a new model, creating in model registry')\n",
    "    parent_model = ''\n",
    "\n",
    "if upload_model:\n",
    "    model = aiplatform.Model.upload(\n",
    "        display_name = f'{SERIES}_{EXPERIMENT}',\n",
    "        model_id = f'model_{SERIES}_{EXPERIMENT}',\n",
    "        parent_model =  parent_model,\n",
    "        serving_container_image_uri = DEPLOY_IMAGE,\n",
    "        artifact_uri = f\"{URI}/models/{TIMESTAMP}/model\",\n",
    "        is_default_version = True,\n",
    "        version_aliases = [RUN_NAME],\n",
    "        version_description = RUN_NAME,\n",
    "        labels = {'series' : f'{SERIES}', 'experiment' : f'{EXPERIMENT}', 'experiment_name' : f'{EXPERIMENT_NAME}', 'run_name' : f'{RUN_NAME}'}        \n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Note** on Version Aliases:\n",
    ">Expectation is a name starting with `a-z` that can include `[a-zA-Z0-9-]`\n",
    ">\n",
    ">**Retrieve a Model Resource**\n",
    ">[aiplatform.Model()](https://cloud.google.com/python/docs/reference/aiplatform/latest/google.cloud.aiplatform.Model)\n",
    ">```Python\n",
    "model = aiplatform.Model(model_name = f'model_{SERIES}_{EXPERIMENT}') # retrieves default version\n",
    "model = aiplatform.Model(model_name = f'model_{SERIES}_{EXPERIMENT}@time-{TIMESTAMP}') # retrieves specific version\n",
    "model = aiplatform.Model(model_name = f'model_{SERIES}_{EXPERIMENT}', version = f'time-{TIMESTAMP}') # retrieves specific version\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review the model in the Vertex AI Model Registry:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/models/model_05_05c?project=statmike-mlops-349915\n"
     ]
    }
   ],
   "source": [
    "print(f'Review the model in the Vertex AI Model Registry:\\nhttps://console.cloud.google.com/vertex-ai/locations/{REGION}/models/{model.name}?project={PROJECT_ID}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vertex AI Experiment Update and Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "expRun = aiplatform.ExperimentRun(run_name = RUN_NAME, experiment = EXPERIMENT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "expRun.log_params({\n",
    "    'model.uri': model.uri,\n",
    "    'model.display_name': model.display_name,\n",
    "    'model.name': model.name,\n",
    "    'model.resource_name': model.resource_name,\n",
    "    'model.version_id': model.version_id,\n",
    "    'model.versioned_resource_name': model.versioned_resource_name,\n",
    "    'customJobs.display_name': customJob.display_name,\n",
    "    'customJobs.resource_name': customJob.resource_name,\n",
    "    'customJobs.link': job_link,\n",
    "    'customJobs.tensorboard': board_link\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the experiment run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "expRun.update_state(state = aiplatform.gapic.Execution.State.COMPLETE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve the experiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "exp = aiplatform.Experiment(experiment_name = EXPERIMENT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>experiment_name</th>\n",
       "      <th>run_name</th>\n",
       "      <th>run_type</th>\n",
       "      <th>state</th>\n",
       "      <th>param.training.epochs</th>\n",
       "      <th>param.training.batch_size</th>\n",
       "      <th>param.customJobs.resource_name</th>\n",
       "      <th>param.model.version_id</th>\n",
       "      <th>param.customJobs.tensorboard</th>\n",
       "      <th>param.project_id</th>\n",
       "      <th>...</th>\n",
       "      <th>metric.train_loss</th>\n",
       "      <th>metric.val_loss</th>\n",
       "      <th>metric.train_auprc</th>\n",
       "      <th>metric.val_auprc</th>\n",
       "      <th>time_series_metric.train_accuracy</th>\n",
       "      <th>time_series_metric.train_loss</th>\n",
       "      <th>time_series_metric.val_accuracy</th>\n",
       "      <th>time_series_metric.val_auprc</th>\n",
       "      <th>time_series_metric.train_auprc</th>\n",
       "      <th>time_series_metric.val_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>experiment-05-05c-tf-classification-dnn</td>\n",
       "      <td>run-20231221185424</td>\n",
       "      <td>system.ExperimentRun</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>10.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>projects/1026793852137/locations/us-central1/c...</td>\n",
       "      <td>6</td>\n",
       "      <td>https://us-central1.tensorboard.googleusercont...</td>\n",
       "      <td>statmike-mlops-349915</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006411</td>\n",
       "      <td>0.007693</td>\n",
       "      <td>0.999382</td>\n",
       "      <td>0.999341</td>\n",
       "      <td>0.999263</td>\n",
       "      <td>0.004525</td>\n",
       "      <td>0.999044</td>\n",
       "      <td>0.999341</td>\n",
       "      <td>0.999527</td>\n",
       "      <td>0.007693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>experiment-05-05c-tf-classification-dnn</td>\n",
       "      <td>run-20231221015345</td>\n",
       "      <td>system.ExperimentRun</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>10.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>projects/1026793852137/locations/us-central1/c...</td>\n",
       "      <td>5</td>\n",
       "      <td>https://us-central1.tensorboard.googleusercont...</td>\n",
       "      <td>statmike-mlops-349915</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006249</td>\n",
       "      <td>0.005858</td>\n",
       "      <td>0.999459</td>\n",
       "      <td>0.999478</td>\n",
       "      <td>0.999167</td>\n",
       "      <td>0.004829</td>\n",
       "      <td>0.998902</td>\n",
       "      <td>0.999478</td>\n",
       "      <td>0.999518</td>\n",
       "      <td>0.005858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>experiment-05-05c-tf-classification-dnn</td>\n",
       "      <td>run-20231220172443</td>\n",
       "      <td>system.ExperimentRun</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>10.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>projects/1026793852137/locations/us-central1/c...</td>\n",
       "      <td>4</td>\n",
       "      <td>https://us-central1.tensorboard.googleusercont...</td>\n",
       "      <td>statmike-mlops-349915</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007485</td>\n",
       "      <td>0.007404</td>\n",
       "      <td>0.999304</td>\n",
       "      <td>0.999243</td>\n",
       "      <td>0.999162</td>\n",
       "      <td>0.005418</td>\n",
       "      <td>0.998938</td>\n",
       "      <td>0.999243</td>\n",
       "      <td>0.999413</td>\n",
       "      <td>0.007404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>experiment-05-05c-tf-classification-dnn</td>\n",
       "      <td>run-20230210130701</td>\n",
       "      <td>system.ExperimentRun</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>10.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>projects/1026793852137/locations/us-central1/c...</td>\n",
       "      <td>3</td>\n",
       "      <td>https://us-central1.tensorboard.googleusercont...</td>\n",
       "      <td>statmike-mlops-349915</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005805</td>\n",
       "      <td>0.005957</td>\n",
       "      <td>0.999481</td>\n",
       "      <td>0.999573</td>\n",
       "      <td>0.999228</td>\n",
       "      <td>0.004220</td>\n",
       "      <td>0.998973</td>\n",
       "      <td>0.999573</td>\n",
       "      <td>0.999614</td>\n",
       "      <td>0.005957</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows  41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           experiment_name            run_name  \\\n",
       "0  experiment-05-05c-tf-classification-dnn  run-20231221185424   \n",
       "1  experiment-05-05c-tf-classification-dnn  run-20231221015345   \n",
       "2  experiment-05-05c-tf-classification-dnn  run-20231220172443   \n",
       "3  experiment-05-05c-tf-classification-dnn  run-20230210130701   \n",
       "\n",
       "               run_type     state  param.training.epochs  \\\n",
       "0  system.ExperimentRun  COMPLETE                   10.0   \n",
       "1  system.ExperimentRun  COMPLETE                   10.0   \n",
       "2  system.ExperimentRun  COMPLETE                   10.0   \n",
       "3  system.ExperimentRun  COMPLETE                   10.0   \n",
       "\n",
       "   param.training.batch_size  \\\n",
       "0                      100.0   \n",
       "1                      100.0   \n",
       "2                      100.0   \n",
       "3                      100.0   \n",
       "\n",
       "                      param.customJobs.resource_name param.model.version_id  \\\n",
       "0  projects/1026793852137/locations/us-central1/c...                      6   \n",
       "1  projects/1026793852137/locations/us-central1/c...                      5   \n",
       "2  projects/1026793852137/locations/us-central1/c...                      4   \n",
       "3  projects/1026793852137/locations/us-central1/c...                      3   \n",
       "\n",
       "                        param.customJobs.tensorboard       param.project_id  \\\n",
       "0  https://us-central1.tensorboard.googleusercont...  statmike-mlops-349915   \n",
       "1  https://us-central1.tensorboard.googleusercont...  statmike-mlops-349915   \n",
       "2  https://us-central1.tensorboard.googleusercont...  statmike-mlops-349915   \n",
       "3  https://us-central1.tensorboard.googleusercont...  statmike-mlops-349915   \n",
       "\n",
       "   ...  metric.train_loss  metric.val_loss metric.train_auprc  \\\n",
       "0  ...           0.006411         0.007693           0.999382   \n",
       "1  ...           0.006249         0.005858           0.999459   \n",
       "2  ...           0.007485         0.007404           0.999304   \n",
       "3  ...           0.005805         0.005957           0.999481   \n",
       "\n",
       "  metric.val_auprc time_series_metric.train_accuracy  \\\n",
       "0         0.999341                          0.999263   \n",
       "1         0.999478                          0.999167   \n",
       "2         0.999243                          0.999162   \n",
       "3         0.999573                          0.999228   \n",
       "\n",
       "  time_series_metric.train_loss time_series_metric.val_accuracy  \\\n",
       "0                      0.004525                        0.999044   \n",
       "1                      0.004829                        0.998902   \n",
       "2                      0.005418                        0.998938   \n",
       "3                      0.004220                        0.998973   \n",
       "\n",
       "  time_series_metric.val_auprc time_series_metric.train_auprc  \\\n",
       "0                     0.999341                       0.999527   \n",
       "1                     0.999478                       0.999518   \n",
       "2                     0.999243                       0.999413   \n",
       "3                     0.999573                       0.999614   \n",
       "\n",
       "  time_series_metric.val_loss  \n",
       "0                    0.007693  \n",
       "1                    0.005858  \n",
       "2                    0.007404  \n",
       "3                    0.005957  \n",
       "\n",
       "[4 rows x 41 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp.get_data_frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review the Experiments TensorBoard to compare runs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Experiment TensorBoard Link:\n",
      "https://us-central1.tensorboard.googleusercontent.com/experiment/projects+1026793852137+locations+us-central1+tensorboards+7876136041294331904+experiments+experiment-05-05c-tf-classification-dnn\n"
     ]
    }
   ],
   "source": [
    "print(f\"The Experiment TensorBoard Link:\\nhttps://{REGION}.tensorboard.googleusercontent.com/experiment/{tb.resource_name.replace('/', '+')}+experiments+{exp.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>step</th>\n",
       "      <th>wall_time</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>val_accuracy</th>\n",
       "      <th>val_auprc</th>\n",
       "      <th>train_auprc</th>\n",
       "      <th>val_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2023-12-21 19:06:30.544000+00:00</td>\n",
       "      <td>0.980733</td>\n",
       "      <td>0.076621</td>\n",
       "      <td>0.997557</td>\n",
       "      <td>0.998416</td>\n",
       "      <td>0.995228</td>\n",
       "      <td>0.020917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2023-12-21 19:06:30.662000+00:00</td>\n",
       "      <td>0.998566</td>\n",
       "      <td>0.010685</td>\n",
       "      <td>0.998548</td>\n",
       "      <td>0.999083</td>\n",
       "      <td>0.999425</td>\n",
       "      <td>0.013051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2023-12-21 19:06:30.757000+00:00</td>\n",
       "      <td>0.998939</td>\n",
       "      <td>0.007428</td>\n",
       "      <td>0.998725</td>\n",
       "      <td>0.999135</td>\n",
       "      <td>0.999448</td>\n",
       "      <td>0.010850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2023-12-21 19:06:30.853000+00:00</td>\n",
       "      <td>0.999101</td>\n",
       "      <td>0.006368</td>\n",
       "      <td>0.998832</td>\n",
       "      <td>0.999167</td>\n",
       "      <td>0.999443</td>\n",
       "      <td>0.009893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2023-12-21 19:06:30.939000+00:00</td>\n",
       "      <td>0.999114</td>\n",
       "      <td>0.005785</td>\n",
       "      <td>0.998902</td>\n",
       "      <td>0.999183</td>\n",
       "      <td>0.999454</td>\n",
       "      <td>0.009324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>2023-12-21 19:06:31.036000+00:00</td>\n",
       "      <td>0.999141</td>\n",
       "      <td>0.005447</td>\n",
       "      <td>0.998938</td>\n",
       "      <td>0.999192</td>\n",
       "      <td>0.999487</td>\n",
       "      <td>0.008762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>2023-12-21 19:06:31.134000+00:00</td>\n",
       "      <td>0.999184</td>\n",
       "      <td>0.005098</td>\n",
       "      <td>0.999009</td>\n",
       "      <td>0.999289</td>\n",
       "      <td>0.999479</td>\n",
       "      <td>0.008399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>2023-12-21 19:06:31.230000+00:00</td>\n",
       "      <td>0.999189</td>\n",
       "      <td>0.004902</td>\n",
       "      <td>0.999009</td>\n",
       "      <td>0.999292</td>\n",
       "      <td>0.999471</td>\n",
       "      <td>0.008133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>2023-12-21 19:06:31.316000+00:00</td>\n",
       "      <td>0.999189</td>\n",
       "      <td>0.004688</td>\n",
       "      <td>0.999009</td>\n",
       "      <td>0.999293</td>\n",
       "      <td>0.999548</td>\n",
       "      <td>0.007908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>2023-12-21 19:06:31.417000+00:00</td>\n",
       "      <td>0.999263</td>\n",
       "      <td>0.004525</td>\n",
       "      <td>0.999044</td>\n",
       "      <td>0.999341</td>\n",
       "      <td>0.999527</td>\n",
       "      <td>0.007693</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   step                        wall_time  train_accuracy  train_loss  \\\n",
       "0     1 2023-12-21 19:06:30.544000+00:00        0.980733    0.076621   \n",
       "1     2 2023-12-21 19:06:30.662000+00:00        0.998566    0.010685   \n",
       "2     3 2023-12-21 19:06:30.757000+00:00        0.998939    0.007428   \n",
       "3     4 2023-12-21 19:06:30.853000+00:00        0.999101    0.006368   \n",
       "4     5 2023-12-21 19:06:30.939000+00:00        0.999114    0.005785   \n",
       "5     6 2023-12-21 19:06:31.036000+00:00        0.999141    0.005447   \n",
       "6     7 2023-12-21 19:06:31.134000+00:00        0.999184    0.005098   \n",
       "7     8 2023-12-21 19:06:31.230000+00:00        0.999189    0.004902   \n",
       "8     9 2023-12-21 19:06:31.316000+00:00        0.999189    0.004688   \n",
       "9    10 2023-12-21 19:06:31.417000+00:00        0.999263    0.004525   \n",
       "\n",
       "   val_accuracy  val_auprc  train_auprc  val_loss  \n",
       "0      0.997557   0.998416     0.995228  0.020917  \n",
       "1      0.998548   0.999083     0.999425  0.013051  \n",
       "2      0.998725   0.999135     0.999448  0.010850  \n",
       "3      0.998832   0.999167     0.999443  0.009893  \n",
       "4      0.998902   0.999183     0.999454  0.009324  \n",
       "5      0.998938   0.999192     0.999487  0.008762  \n",
       "6      0.999009   0.999289     0.999479  0.008399  \n",
       "7      0.999009   0.999292     0.999471  0.008133  \n",
       "8      0.999009   0.999293     0.999548  0.007908  \n",
       "9      0.999044   0.999341     0.999527  0.007693  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expRun.get_time_series_data_frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review Experiment and Run in Console"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review The Experiment in the Console:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/experiments/experiment-05-05c-tf-classification-dnn?project=statmike-mlops-349915\n"
     ]
    }
   ],
   "source": [
    "print(f'Review The Experiment in the Console:\\nhttps://console.cloud.google.com/vertex-ai/locations/{REGION}/experiments/{EXPERIMENT_NAME}?project={PROJECT_ID}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review The Experiment Run in the Console:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/experiments/experiment-05-05c-tf-classification-dnn/runs/experiment-05-05c-tf-classification-dnn-run-20231221185424?project=statmike-mlops-349915\n"
     ]
    }
   ],
   "source": [
    "print(f'Review The Experiment Run in the Console:\\nhttps://console.cloud.google.com/vertex-ai/locations/{REGION}/experiments/{EXPERIMENT_NAME}/runs/{EXPERIMENT_NAME}-{RUN_NAME}?project={PROJECT_ID}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare This Run Using Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Get a list of all experiments in this project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "experiments = aiplatform.Experiment.list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove experiments not in the SERIES:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "experiments = [e for e in experiments if e.name.split('-')[0:2] == ['experiment', SERIES]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine the runs from all experiments in SERIES into a single dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "experiment-05-05f-tf-classification-dnn\n",
      "experiment-05-05i-tf-classification-dnn\n",
      "experiment-05-05h-tf-classification-dnn\n",
      "experiment-05-05g-tf-classification-dnn\n",
      "experiment-05-05e-tf-classification-dnn\n",
      "experiment-05-05d-tf-classification-dnn\n",
      "experiment-05-05c-tf-classification-dnn\n",
      "experiment-05-05b-tf-classification-dnn\n",
      "experiment-05-05a-tf-classification-dnn\n",
      "experiment-05-05-tf-classification-dnn\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for experiment in experiments:\n",
    "        results.append(experiment.get_data_frame())\n",
    "        print(experiment.name)\n",
    "results = pd.concat(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create ranks for models within experiment and across the entire SERIES:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>experiment_name</th>\n",
       "      <th>run_name</th>\n",
       "      <th>param.model.display_name</th>\n",
       "      <th>param.model.version_id</th>\n",
       "      <th>metric.test_auprc</th>\n",
       "      <th>series_rank</th>\n",
       "      <th>experiment_rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>experiment-05-05-tf-classification-dnn</td>\n",
       "      <td>run-20230210115433</td>\n",
       "      <td>05_05</td>\n",
       "      <td>6</td>\n",
       "      <td>0.999130</td>\n",
       "      <td>132.0</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>experiment-05-05-tf-classification-dnn</td>\n",
       "      <td>run-20230213133609</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.999309</td>\n",
       "      <td>115.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>experiment-05-05-tf-classification-dnn</td>\n",
       "      <td>run-20230308225745</td>\n",
       "      <td>05_05</td>\n",
       "      <td>7</td>\n",
       "      <td>0.999469</td>\n",
       "      <td>77.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>experiment-05-05-tf-classification-dnn</td>\n",
       "      <td>run-20230324103811</td>\n",
       "      <td>05_05</td>\n",
       "      <td>8</td>\n",
       "      <td>0.999299</td>\n",
       "      <td>117.0</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>experiment-05-05-tf-classification-dnn</td>\n",
       "      <td>run-20230324104933</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.999469</td>\n",
       "      <td>78.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>experiment-05-05i-tf-classification-dnn</td>\n",
       "      <td>run-20230211221928-5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.999584</td>\n",
       "      <td>13.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>experiment-05-05i-tf-classification-dnn</td>\n",
       "      <td>run-20230211221928-6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.999538</td>\n",
       "      <td>36.0</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>experiment-05-05i-tf-classification-dnn</td>\n",
       "      <td>run-20230211221928-7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.999538</td>\n",
       "      <td>37.0</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>experiment-05-05i-tf-classification-dnn</td>\n",
       "      <td>run-20230211221928-8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.999584</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>experiment-05-05i-tf-classification-dnn</td>\n",
       "      <td>run-20230211221928-9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.999583</td>\n",
       "      <td>16.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>159 rows  7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             experiment_name              run_name  \\\n",
       "158   experiment-05-05-tf-classification-dnn    run-20230210115433   \n",
       "157   experiment-05-05-tf-classification-dnn    run-20230213133609   \n",
       "156   experiment-05-05-tf-classification-dnn    run-20230308225745   \n",
       "155   experiment-05-05-tf-classification-dnn    run-20230324103811   \n",
       "154   experiment-05-05-tf-classification-dnn    run-20230324104933   \n",
       "..                                       ...                   ...   \n",
       "59   experiment-05-05i-tf-classification-dnn  run-20230211221928-5   \n",
       "58   experiment-05-05i-tf-classification-dnn  run-20230211221928-6   \n",
       "57   experiment-05-05i-tf-classification-dnn  run-20230211221928-7   \n",
       "56   experiment-05-05i-tf-classification-dnn  run-20230211221928-8   \n",
       "55   experiment-05-05i-tf-classification-dnn  run-20230211221928-9   \n",
       "\n",
       "    param.model.display_name param.model.version_id  metric.test_auprc  \\\n",
       "158                    05_05                      6           0.999130   \n",
       "157                      NaN                    NaN           0.999309   \n",
       "156                    05_05                      7           0.999469   \n",
       "155                    05_05                      8           0.999299   \n",
       "154                      NaN                    NaN           0.999469   \n",
       "..                       ...                    ...                ...   \n",
       "59                       NaN                    NaN           0.999584   \n",
       "58                       NaN                    NaN           0.999538   \n",
       "57                       NaN                    NaN           0.999538   \n",
       "56                       NaN                    NaN           0.999584   \n",
       "55                       NaN                    NaN           0.999583   \n",
       "\n",
       "     series_rank  experiment_rank  \n",
       "158        132.0             16.0  \n",
       "157        115.0             11.0  \n",
       "156         77.0              4.0  \n",
       "155        117.0             12.0  \n",
       "154         78.0              5.0  \n",
       "..           ...              ...  \n",
       "59          13.0              7.0  \n",
       "58          36.0             13.0  \n",
       "57          37.0             14.0  \n",
       "56           8.0              3.0  \n",
       "55          16.0              9.0  \n",
       "\n",
       "[159 rows x 7 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ranker(metric = 'metric.test_auprc'):\n",
    "    ranks = results[['experiment_name', 'run_name', 'param.model.display_name', 'param.model.version_id', metric]].copy().reset_index(drop = True)\n",
    "    ranks['series_rank'] = ranks[metric].rank(method = 'dense', ascending = False)\n",
    "    ranks['experiment_rank'] = ranks.groupby('experiment_name')[metric].rank(method = 'dense', ascending = False)\n",
    "    return ranks.sort_values(by = ['experiment_name', 'run_name'])\n",
    "    \n",
    "ranks = ranker('metric.test_auprc')\n",
    "ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>experiment_name</th>\n",
       "      <th>run_name</th>\n",
       "      <th>param.model.display_name</th>\n",
       "      <th>param.model.version_id</th>\n",
       "      <th>metric.test_auprc</th>\n",
       "      <th>series_rank</th>\n",
       "      <th>experiment_rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>experiment-05-05c-tf-classification-dnn</td>\n",
       "      <td>run-20231221185424</td>\n",
       "      <td>05_05c</td>\n",
       "      <td>6</td>\n",
       "      <td>0.999393</td>\n",
       "      <td>98.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            experiment_name            run_name  \\\n",
       "96  experiment-05-05c-tf-classification-dnn  run-20231221185424   \n",
       "\n",
       "   param.model.display_name param.model.version_id  metric.test_auprc  \\\n",
       "96                   05_05c                      6           0.999393   \n",
       "\n",
       "    series_rank  experiment_rank  \n",
       "96         98.0              3.0  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_rank = ranks.loc[(ranks['param.model.display_name'] == model.display_name) & (ranks['param.model.version_id'] == model.version_id)]\n",
    "current_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current model is ranked 3.0 within this experiment and 98.0 across this series.\n"
     ]
    }
   ],
   "source": [
    "print(f\"The current model is ranked {current_rank['experiment_rank'].iloc[0]} within this experiment and {current_rank['series_rank'].iloc[0]} across this series.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create/Retrieve The Endpoint For This Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint Exists: projects/1026793852137/locations/us-central1/endpoints/725723853820526592\n",
      "Review the Endpoint in the Console:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/endpoints/725723853820526592?project=statmike-mlops-349915\n"
     ]
    }
   ],
   "source": [
    "endpoints = aiplatform.Endpoint.list(filter = f\"labels.series={SERIES} AND display_name={SERIES}\")\n",
    "if endpoints:\n",
    "    endpoint = endpoints[0]\n",
    "    print(f\"Endpoint Exists: {endpoints[0].resource_name}\")\n",
    "else:\n",
    "    endpoint = aiplatform.Endpoint.create(\n",
    "        display_name = f\"{SERIES}\",\n",
    "        labels = {'series' : f\"{SERIES}\"}    \n",
    "    )\n",
    "    print(f\"Endpoint Created: {endpoint.resource_name}\")\n",
    "    \n",
    "print(f'Review the Endpoint in the Console:\\nhttps://console.cloud.google.com/vertex-ai/locations/{REGION}/endpoints/{endpoint.name}?project={PROJECT_ID}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'05'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "endpoint.display_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'2423682068408958976': 100}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "endpoint.traffic_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "deployed_models = endpoint.list_models()\n",
    "#deployed_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Should This Model Be Deployed?\n",
    "Is it better than the model already deployed on the endpoint?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current model is ranked worse (98.0) than a currently deployed model (3.0)\n"
     ]
    }
   ],
   "source": [
    "deploy = False\n",
    "if deployed_models:\n",
    "    for deployed_model in deployed_models:\n",
    "        deployed_rank = ranks.loc[(ranks['param.model.display_name'] == deployed_model.display_name) & (ranks['param.model.version_id'] == deployed_model.model_version_id)]['series_rank'].iloc[0]\n",
    "        model_rank = current_rank['series_rank'].iloc[0]\n",
    "        if deployed_model.display_name == model.display_name and deployed_model.model_version_id == model.version_id:\n",
    "            print(f'The current model/version is already deployed.')\n",
    "            break\n",
    "        elif model_rank <= deployed_rank:\n",
    "            deploy = True\n",
    "            print(f'The current model is ranked better ({model_rank}) than a currently deployed model ({deployed_rank}).')\n",
    "            break\n",
    "    if deploy == False: print(f'The current model is ranked worse ({model_rank}) than a currently deployed model ({deployed_rank})')\n",
    "else: \n",
    "    deploy = True\n",
    "    print('No models currently deployed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy Model To Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not deploying - current model is worse (98.0) than the currently deployed model (3.0)\n"
     ]
    }
   ],
   "source": [
    "if deploy:\n",
    "    print(f'Deploying model with 100% of traffic...')\n",
    "    endpoint.deploy(\n",
    "        model = model,\n",
    "        deployed_model_display_name = model.display_name,\n",
    "        traffic_percentage = 100,\n",
    "        machine_type = DEPLOY_COMPUTE,\n",
    "        min_replica_count = 1,\n",
    "        max_replica_count = 1\n",
    "    )\n",
    "else: print(f'Not deploying - current model is worse ({model_rank}) than the currently deployed model ({deployed_rank})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Deployed Models without Traffic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 05_05 with version 18 has traffic = 100\n"
     ]
    }
   ],
   "source": [
    "for deployed_model in endpoint.list_models():\n",
    "    if deployed_model.id in endpoint.traffic_split:\n",
    "        print(f\"Model {deployed_model.display_name} with version {deployed_model.model_version_id} has traffic = {endpoint.traffic_split[deployed_model.id]}\")\n",
    "    else:\n",
    "        endpoint.undeploy(deployed_model_id = deployed_model.id)\n",
    "        print(f\"Undeploying {deployed_model.display_name} with version {deployed_model.model_version_id} because it has no traffic.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'2423682068408958976': 100}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "endpoint.traffic_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#endpoint.list_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Prediction\n",
    "\n",
    "See many more details on requesting predictions in the [05Tools - Prediction](./05Tools%20-%20Prediction.ipynb) notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare a record for prediction: instance and parameters lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n = 10\n",
    "# Make a list of columns to omit\n",
    "OMIT = [x for x in VAR_OMIT.split(',') if x != '']\n",
    "pred = bq.query(\n",
    "    query = f\"\"\"\n",
    "        SELECT * EXCEPT({','.join([VAR_TARGET] + OMIT)})\n",
    "        FROM {BQ_PROJECT}.{BQ_DATASET}.{BQ_TABLE}\n",
    "        WHERE splits='TEST'\n",
    "        LIMIT {n}\n",
    "        \"\"\"\n",
    ").to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>35337</td>\n",
       "      <td>1.092844</td>\n",
       "      <td>-0.013230</td>\n",
       "      <td>1.359829</td>\n",
       "      <td>2.731537</td>\n",
       "      <td>-0.707357</td>\n",
       "      <td>0.873837</td>\n",
       "      <td>-0.796130</td>\n",
       "      <td>0.437707</td>\n",
       "      <td>0.396770</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.240428</td>\n",
       "      <td>0.037603</td>\n",
       "      <td>0.380026</td>\n",
       "      <td>-0.167647</td>\n",
       "      <td>0.027557</td>\n",
       "      <td>0.592115</td>\n",
       "      <td>0.219695</td>\n",
       "      <td>0.036970</td>\n",
       "      <td>0.010984</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>60481</td>\n",
       "      <td>1.238973</td>\n",
       "      <td>0.035226</td>\n",
       "      <td>0.063003</td>\n",
       "      <td>0.641406</td>\n",
       "      <td>-0.260893</td>\n",
       "      <td>-0.580097</td>\n",
       "      <td>0.049938</td>\n",
       "      <td>-0.034733</td>\n",
       "      <td>0.405932</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.265080</td>\n",
       "      <td>-0.060003</td>\n",
       "      <td>-0.053585</td>\n",
       "      <td>-0.057718</td>\n",
       "      <td>0.104983</td>\n",
       "      <td>0.537987</td>\n",
       "      <td>0.589563</td>\n",
       "      <td>-0.046207</td>\n",
       "      <td>-0.006212</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>139587</td>\n",
       "      <td>1.870539</td>\n",
       "      <td>0.211079</td>\n",
       "      <td>0.224457</td>\n",
       "      <td>3.889486</td>\n",
       "      <td>-0.380177</td>\n",
       "      <td>0.249799</td>\n",
       "      <td>-0.577133</td>\n",
       "      <td>0.179189</td>\n",
       "      <td>-0.120462</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.374356</td>\n",
       "      <td>0.196006</td>\n",
       "      <td>0.656552</td>\n",
       "      <td>0.180776</td>\n",
       "      <td>-0.060226</td>\n",
       "      <td>-0.228979</td>\n",
       "      <td>0.080827</td>\n",
       "      <td>0.009868</td>\n",
       "      <td>-0.036997</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>162908</td>\n",
       "      <td>-3.368339</td>\n",
       "      <td>-1.980442</td>\n",
       "      <td>0.153645</td>\n",
       "      <td>-0.159795</td>\n",
       "      <td>3.847169</td>\n",
       "      <td>-3.516873</td>\n",
       "      <td>-1.209398</td>\n",
       "      <td>-0.292122</td>\n",
       "      <td>0.760543</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.923275</td>\n",
       "      <td>-0.545992</td>\n",
       "      <td>-0.252324</td>\n",
       "      <td>-1.171627</td>\n",
       "      <td>0.214333</td>\n",
       "      <td>-0.159652</td>\n",
       "      <td>-0.060883</td>\n",
       "      <td>1.294977</td>\n",
       "      <td>0.120503</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>165236</td>\n",
       "      <td>2.180149</td>\n",
       "      <td>0.218732</td>\n",
       "      <td>-2.637726</td>\n",
       "      <td>0.348776</td>\n",
       "      <td>1.063546</td>\n",
       "      <td>-1.249197</td>\n",
       "      <td>0.942021</td>\n",
       "      <td>-0.547652</td>\n",
       "      <td>-0.087823</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.250653</td>\n",
       "      <td>0.234502</td>\n",
       "      <td>0.825237</td>\n",
       "      <td>-0.176957</td>\n",
       "      <td>0.563779</td>\n",
       "      <td>0.730183</td>\n",
       "      <td>0.707494</td>\n",
       "      <td>-0.131066</td>\n",
       "      <td>-0.090428</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>62606</td>\n",
       "      <td>1.199408</td>\n",
       "      <td>0.352007</td>\n",
       "      <td>0.379645</td>\n",
       "      <td>1.372017</td>\n",
       "      <td>0.291347</td>\n",
       "      <td>0.524919</td>\n",
       "      <td>-0.117555</td>\n",
       "      <td>0.132907</td>\n",
       "      <td>-0.935169</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.042979</td>\n",
       "      <td>-0.050291</td>\n",
       "      <td>-0.126609</td>\n",
       "      <td>-0.022218</td>\n",
       "      <td>-0.599026</td>\n",
       "      <td>0.258188</td>\n",
       "      <td>0.928721</td>\n",
       "      <td>-0.058988</td>\n",
       "      <td>-0.008856</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>90719</td>\n",
       "      <td>1.937447</td>\n",
       "      <td>0.337882</td>\n",
       "      <td>-0.000630</td>\n",
       "      <td>3.816486</td>\n",
       "      <td>0.276515</td>\n",
       "      <td>1.079842</td>\n",
       "      <td>-0.730626</td>\n",
       "      <td>0.197353</td>\n",
       "      <td>1.137566</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.315667</td>\n",
       "      <td>-0.038376</td>\n",
       "      <td>0.208914</td>\n",
       "      <td>0.160189</td>\n",
       "      <td>-0.015145</td>\n",
       "      <td>-0.162678</td>\n",
       "      <td>-0.000843</td>\n",
       "      <td>-0.018178</td>\n",
       "      <td>-0.039339</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>113350</td>\n",
       "      <td>1.891900</td>\n",
       "      <td>0.401086</td>\n",
       "      <td>-0.119983</td>\n",
       "      <td>4.047500</td>\n",
       "      <td>0.049952</td>\n",
       "      <td>0.192793</td>\n",
       "      <td>-0.108512</td>\n",
       "      <td>-0.040400</td>\n",
       "      <td>-0.390391</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.267639</td>\n",
       "      <td>0.094177</td>\n",
       "      <td>0.613712</td>\n",
       "      <td>0.070986</td>\n",
       "      <td>0.079543</td>\n",
       "      <td>0.135219</td>\n",
       "      <td>0.128961</td>\n",
       "      <td>0.003667</td>\n",
       "      <td>-0.045079</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>156499</td>\n",
       "      <td>0.060003</td>\n",
       "      <td>1.461355</td>\n",
       "      <td>0.378915</td>\n",
       "      <td>2.835455</td>\n",
       "      <td>1.626526</td>\n",
       "      <td>-0.164732</td>\n",
       "      <td>1.551858</td>\n",
       "      <td>-0.412927</td>\n",
       "      <td>-1.735264</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.175275</td>\n",
       "      <td>0.042293</td>\n",
       "      <td>0.277536</td>\n",
       "      <td>-0.123379</td>\n",
       "      <td>1.081552</td>\n",
       "      <td>-0.053079</td>\n",
       "      <td>-0.149809</td>\n",
       "      <td>-0.314438</td>\n",
       "      <td>-0.216539</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>73902</td>\n",
       "      <td>-1.859260</td>\n",
       "      <td>2.158799</td>\n",
       "      <td>1.085671</td>\n",
       "      <td>2.615483</td>\n",
       "      <td>0.246660</td>\n",
       "      <td>2.133925</td>\n",
       "      <td>-1.569015</td>\n",
       "      <td>-2.612353</td>\n",
       "      <td>-1.312509</td>\n",
       "      <td>...</td>\n",
       "      <td>0.590142</td>\n",
       "      <td>-0.867178</td>\n",
       "      <td>-0.700479</td>\n",
       "      <td>0.231972</td>\n",
       "      <td>-1.374527</td>\n",
       "      <td>0.140285</td>\n",
       "      <td>0.128806</td>\n",
       "      <td>0.153606</td>\n",
       "      <td>0.092042</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows  30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Time        V1        V2        V3        V4        V5        V6  \\\n",
       "0   35337  1.092844 -0.013230  1.359829  2.731537 -0.707357  0.873837   \n",
       "1   60481  1.238973  0.035226  0.063003  0.641406 -0.260893 -0.580097   \n",
       "2  139587  1.870539  0.211079  0.224457  3.889486 -0.380177  0.249799   \n",
       "3  162908 -3.368339 -1.980442  0.153645 -0.159795  3.847169 -3.516873   \n",
       "4  165236  2.180149  0.218732 -2.637726  0.348776  1.063546 -1.249197   \n",
       "5   62606  1.199408  0.352007  0.379645  1.372017  0.291347  0.524919   \n",
       "6   90719  1.937447  0.337882 -0.000630  3.816486  0.276515  1.079842   \n",
       "7  113350  1.891900  0.401086 -0.119983  4.047500  0.049952  0.192793   \n",
       "8  156499  0.060003  1.461355  0.378915  2.835455  1.626526 -0.164732   \n",
       "9   73902 -1.859260  2.158799  1.085671  2.615483  0.246660  2.133925   \n",
       "\n",
       "         V7        V8        V9  ...       V20       V21       V22       V23  \\\n",
       "0 -0.796130  0.437707  0.396770  ... -0.240428  0.037603  0.380026 -0.167647   \n",
       "1  0.049938 -0.034733  0.405932  ... -0.265080 -0.060003 -0.053585 -0.057718   \n",
       "2 -0.577133  0.179189 -0.120462  ... -0.374356  0.196006  0.656552  0.180776   \n",
       "3 -1.209398 -0.292122  0.760543  ... -0.923275 -0.545992 -0.252324 -1.171627   \n",
       "4  0.942021 -0.547652 -0.087823  ... -0.250653  0.234502  0.825237 -0.176957   \n",
       "5 -0.117555  0.132907 -0.935169  ... -0.042979 -0.050291 -0.126609 -0.022218   \n",
       "6 -0.730626  0.197353  1.137566  ... -0.315667 -0.038376  0.208914  0.160189   \n",
       "7 -0.108512 -0.040400 -0.390391  ... -0.267639  0.094177  0.613712  0.070986   \n",
       "8  1.551858 -0.412927 -1.735264  ... -0.175275  0.042293  0.277536 -0.123379   \n",
       "9 -1.569015 -2.612353 -1.312509  ...  0.590142 -0.867178 -0.700479  0.231972   \n",
       "\n",
       "        V24       V25       V26       V27       V28  Amount  \n",
       "0  0.027557  0.592115  0.219695  0.036970  0.010984     0.0  \n",
       "1  0.104983  0.537987  0.589563 -0.046207 -0.006212     0.0  \n",
       "2 -0.060226 -0.228979  0.080827  0.009868 -0.036997     0.0  \n",
       "3  0.214333 -0.159652 -0.060883  1.294977  0.120503     0.0  \n",
       "4  0.563779  0.730183  0.707494 -0.131066 -0.090428     0.0  \n",
       "5 -0.599026  0.258188  0.928721 -0.058988 -0.008856     0.0  \n",
       "6 -0.015145 -0.162678 -0.000843 -0.018178 -0.039339     0.0  \n",
       "7  0.079543  0.135219  0.128961  0.003667 -0.045079     0.0  \n",
       "8  1.081552 -0.053079 -0.149809 -0.314438 -0.216539     0.0  \n",
       "9 -1.374527  0.140285  0.128806  0.153606  0.092042     0.0  \n",
       "\n",
       "[10 rows x 30 columns]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "newobs = pred.to_dict(orient = 'records')\n",
    "#newobs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "newobs = [{k:[v] for k,v in newob.items()} for newob in newobs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Time': [35337],\n",
       " 'V1': [1.0928441854981998],\n",
       " 'V2': [-0.0132303486713432],\n",
       " 'V3': [1.35982868199426],\n",
       " 'V4': [2.7315370965921004],\n",
       " 'V5': [-0.707357349219652],\n",
       " 'V6': [0.8738370029866129],\n",
       " 'V7': [-0.7961301510622031],\n",
       " 'V8': [0.437706509544851],\n",
       " 'V9': [0.39676985012996396],\n",
       " 'V10': [0.587438102569443],\n",
       " 'V11': [-0.14979756231827498],\n",
       " 'V12': [0.29514781622888103],\n",
       " 'V13': [-1.30382621882143],\n",
       " 'V14': [-0.31782283120234495],\n",
       " 'V15': [-2.03673231037199],\n",
       " 'V16': [0.376090905274179],\n",
       " 'V17': [-0.30040350116459497],\n",
       " 'V18': [0.433799615590844],\n",
       " 'V19': [-0.145082264348681],\n",
       " 'V20': [-0.240427548108996],\n",
       " 'V21': [0.0376030733329398],\n",
       " 'V22': [0.38002620963091405],\n",
       " 'V23': [-0.16764742731151097],\n",
       " 'V24': [0.0275573495476881],\n",
       " 'V25': [0.59211469704354],\n",
       " 'V26': [0.219695164116351],\n",
       " 'V27': [0.0369695108704894],\n",
       " 'V28': [0.010984441006191],\n",
       " 'Amount': [0.0]}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newobs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#instances = [json_format.ParseDict(newobs[0], Value())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Predictions: Python Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction(predictions=[[0.999759495, 0.000240550202]], deployed_model_id='2423682068408958976', model_version_id='18', model_resource_name='projects/1026793852137/locations/us-central1/models/model_05_05', explanations=None)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = endpoint.predict(instances = newobs[0:1])\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction(predictions=[[0.999759436, 0.000240550187], [0.999389887, 0.000610154064], [0.999763489, 0.000236547974], [0.999877691, 0.000122421377], [0.999873161, 0.000126962157], [0.999579489, 0.000420496828], [0.999929368, 7.053015e-05], [0.999938548, 6.13750744e-05], [0.999892116, 0.00010785809], [0.999943733, 5.62981877e-05]], deployed_model_id='2423682068408958976', model_version_id='18', model_resource_name='projects/1026793852137/locations/us-central1/models/model_05_05', explanations=None)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = endpoint.predict(instances = newobs)\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.999759436, 0.000240550187]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction.predictions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(prediction.predictions[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Predictions: REST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(f'{DIR}/request.json','w') as file:\n",
    "    file.write(json.dumps({\"instances\": newobs[0:1]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"predictions\": [\n",
      "    [\n",
      "      0.999759495,\n",
      "      0.000240550202\n",
      "    ]\n",
      "  ],\n",
      "  \"deployedModelId\": \"2423682068408958976\",\n",
      "  \"model\": \"projects/1026793852137/locations/us-central1/models/model_05_05\",\n",
      "  \"modelDisplayName\": \"05_05\",\n",
      "  \"modelVersionId\": \"18\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "!curl -X POST \\\n",
    "-H \"Authorization: Bearer \"$(gcloud auth application-default print-access-token) \\\n",
    "-H \"Content-Type: application/json; charset=utf-8\" \\\n",
    "-d @{DIR}/request.json \\\n",
    "https://{REGION}-aiplatform.googleapis.com/v1/{endpoint.resource_name}:predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Predictions: gcloud (CLI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using endpoint [https://us-central1-prediction-aiplatform.googleapis.com/]\n",
      "[[0.999759495, 0.000240550202]]\n"
     ]
    }
   ],
   "source": [
    "!gcloud ai endpoints predict {endpoint.name.rsplit('/',1)[-1]} --region={REGION} --json-request={DIR}/request.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Remove Resources\n",
    "see notebook \"99 - Cleanup\""
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-root-py",
   "name": "workbench-notebooks.m113",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/workbench-notebooks:m113"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
