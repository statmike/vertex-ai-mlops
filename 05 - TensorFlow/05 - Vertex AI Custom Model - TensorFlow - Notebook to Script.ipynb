{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07399992",
   "metadata": {},
   "source": [
    "![tracker](https://us-central1-vertex-ai-mlops-369716.cloudfunctions.net/pixel-tracking?path=statmike%2Fvertex-ai-mlops%2F05+-+TensorFlow&file=05+-+Vertex+AI+Custom+Model+-+TensorFlow+-+Notebook+to+Script.ipynb)\n",
    "<!--- header table --->\n",
    "<table align=\"left\">\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://colab.research.google.com/github/statmike/vertex-ai-mlops/blob/main/05%20-%20TensorFlow/05%20-%20Vertex%20AI%20Custom%20Model%20-%20TensorFlow%20-%20Notebook%20to%20Script.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Google Colaboratory logo\">\n",
    "      <br>Run in<br>Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https%3A%2F%2Fraw.githubusercontent.com%2Fstatmike%2Fvertex-ai-mlops%2Fmain%2F05%2520-%2520TensorFlow%2F05%2520-%2520Vertex%2520AI%2520Custom%2520Model%2520-%2520TensorFlow%2520-%2520Notebook%2520to%2520Script.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\">\n",
    "      <br>Run in<br>Colab Enterprise\n",
    "    </a>\n",
    "  </td>      \n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://github.com/statmike/vertex-ai-mlops/blob/main/05%20-%20TensorFlow/05%20-%20Vertex%20AI%20Custom%20Model%20-%20TensorFlow%20-%20Notebook%20to%20Script.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      <br>View on<br>GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/statmike/vertex-ai-mlops/main/05%20-%20TensorFlow/05%20-%20Vertex%20AI%20Custom%20Model%20-%20TensorFlow%20-%20Notebook%20to%20Script.ipynb\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
    "      <br>Open in<br>Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1387e49e-a52e-4aeb-9e0d-7fdf9aa72c48",
   "metadata": {},
   "source": [
    "# 05 - Vertex AI Custom Model - TensorFlow - From Notebook to Script\n",
    "\n",
    "In the notebook [05 - Vertex AI Custom Model - TensorFlow - in Notebook](./05%20-%20Vertex%20AI%20Custom%20Model%20-%20TensorFlow%20-%20in%20Notebook.ipynb) code was developed ad-hoc and run piece by piece in cells of the notebook.  As the data scales and training scales it is tempting to increase the size of compute backing the notebook environment with more CPU, Memory and/or GPU.  A better practice for scaling is running the ML training code in training jobs with specified compute.  The notebooks `05a` through `05i` show the ways to run training jobs for various purposes.  This notebook builds the script that is used by notebooks `05a` through `05f` and is the basis for the hyperparameter tuning version built in [05 - Vertex AI Custom Model - TensorFlow - Notebook to Hyperparameter Tuning Script](./05%20-%20Vertex%20AI%20Custom%20Model%20-%20TensorFlow%20-%20Notebook%20to%20Hyperparameter%20Tuning%20Script.ipynb) and used in `05g` through `05i`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d85eeb-94f2-4297-964c-4c340f3ac0f4",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bc801b-7da7-4d18-9085-3c260bc68bcf",
   "metadata": {},
   "source": [
    "inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cd6da559-f60d-4a5b-9c5f-5c0af64e610f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SCRIPT_PATH = './code/train.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12dca265-c411-438a-ac35-f4f16317d529",
   "metadata": {},
   "source": [
    "packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d209b693-223a-401f-a52d-00ddb08757de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from IPython.display import Markdown as md"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7087e9dc-76d6-4280-ad8e-5d3e6b257985",
   "metadata": {},
   "source": [
    "environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0115f764-a321-4572-b5bb-1cdd9c69119e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The code directory alredy exists\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists('code'):\n",
    "    print('The code directory alredy exists')\n",
    "else:\n",
    "    print('Creating the code directory')\n",
    "    os.makedirs('code')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77da047a-673b-4cd3-ae69-386b6b47d125",
   "metadata": {},
   "source": [
    "---\n",
    "## Build the ML Training Code\n",
    "\n",
    "Use the cell magic `%%writefile` to create a single ML training code file named `train.py`.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9a29fb-c56e-496f-b81d-3859ac90459e",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "49bae688-6852-4e19-bb55-8248255df29a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./code/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {SCRIPT_PATH}\n",
    "\n",
    "# package import\n",
    "from tensorflow.python.framework import dtypes\n",
    "from tensorflow_io.bigquery import BigQueryClient\n",
    "import tensorflow as tf\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import aiplatform\n",
    "import argparse\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8bf4fcc-7ecb-40c2-9363-19414e45940d",
   "metadata": {},
   "source": [
    "### Parse inputs and parameters\n",
    "This script uses the `argparse` package to parse input parameters.  To see more details on this method and alternatives like `docopt` and `click` as well as methods for using files to import parameters (YAML, JSON, pickle) check on this tips notebook: [Python Job Parameters](../Tips/Python%20Job%20Parameters.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "914f37bd-4db8-4084-a024-a6950a796697",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to ./code/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a {SCRIPT_PATH}\n",
    "\n",
    "# import argument to local variables\n",
    "parser = argparse.ArgumentParser()\n",
    "# the passed param, dest: a name for the param, default: if absent fetch this param from the OS, type: type to convert to, help: description of argument\n",
    "parser.add_argument('--epochs', dest = 'epochs', default = 10, type = int, help = 'Number of Epochs')\n",
    "parser.add_argument('--batch_size', dest = 'batch_size', default = 32, type = int, help = 'Batch Size')\n",
    "parser.add_argument('--var_target', dest = 'var_target', type=str)\n",
    "parser.add_argument('--var_omit', dest = 'var_omit', type=str)#, nargs='*')\n",
    "parser.add_argument('--project_id', dest = 'project_id', type=str)\n",
    "parser.add_argument('--bq_project', dest = 'bq_project', type=str)\n",
    "parser.add_argument('--bq_dataset', dest = 'bq_dataset', type=str)\n",
    "parser.add_argument('--bq_table', dest = 'bq_table', type=str)\n",
    "parser.add_argument('--region', dest = 'region', type=str)\n",
    "parser.add_argument('--experiment', dest = 'experiment', type=str)\n",
    "parser.add_argument('--series', dest = 'series', type=str)\n",
    "parser.add_argument('--experiment_name', dest = 'experiment_name', type=str)\n",
    "parser.add_argument('--run_name', dest = 'run_name', type=str)\n",
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c8c592-7b2c-4580-834d-5f14d9261fb8",
   "metadata": {},
   "source": [
    "### Define Clients for Services\n",
    "This script with use clients for BigQuery and Vertex AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e1e1aead-898d-4265-aca5-dbd558278719",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to ./code/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a {SCRIPT_PATH}\n",
    "\n",
    "# clients\n",
    "bq = bigquery.Client(project = args.project_id)\n",
    "aiplatform.init(project = args.project_id, location = args.region)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e905551e-903e-4ace-b70b-b64b97eb9df4",
   "metadata": {},
   "source": [
    "### Setup Vertex AI Experiments\n",
    "\n",
    "Use the Vertex AI client to setup a run of the experiment (input)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9c8c5108-e3e1-4688-8f45-c713b3a5d893",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to ./code/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a {SCRIPT_PATH}\n",
    "\n",
    "# Vertex AI Experiment\n",
    "if args.run_name in [run.name for run in aiplatform.ExperimentRun.list(experiment = args.experiment_name)]:\n",
    "    expRun = aiplatform.ExperimentRun(run_name = args.run_name, experiment = args.experiment_name)\n",
    "else:\n",
    "    expRun = aiplatform.ExperimentRun.create(run_name = args.run_name, experiment = args.experiment_name)\n",
    "expRun.log_params({'experiment': args.experiment, 'series': args.series, 'project_id': args.project_id})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48e3053-0d5f-45ba-82c8-4a492f32a4fd",
   "metadata": {},
   "source": [
    "### Training Data\n",
    "\n",
    "Use the BigQuery client to read column information and target variable information for the source table in BigQuery.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5b45b2a6-e689-49cf-9b90-366cd1570e35",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to ./code/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a {SCRIPT_PATH}\n",
    "\n",
    "# get schema from bigquery source\n",
    "query = f\"SELECT * FROM {args.bq_project}.{args.bq_dataset}.INFORMATION_SCHEMA.COLUMNS WHERE TABLE_NAME = '{args.bq_table}'\"\n",
    "schema = bq.query(query).to_dataframe()\n",
    "\n",
    "# get number of classes from bigquery source\n",
    "nclasses = bq.query(query = f'SELECT DISTINCT {args.var_target} FROM {args.bq_project}.{args.bq_dataset}.{args.bq_table} WHERE {args.var_target} is not null').to_dataframe()\n",
    "nclasses = nclasses.shape[0]\n",
    "expRun.log_params({'data_source': f'bq://{args.bq_project}.{args.bq_dataset}.{args.bq_table}', 'nclasses': nclasses, 'var_split': 'splits', 'var_target': args.var_target})\n",
    "\n",
    "# Make a list of columns to omit\n",
    "OMIT = [x for x in args.var_omit.split(',') if x != '']\n",
    "\n",
    "# use schema to prepare a list of columns to read from BigQuery\n",
    "selected_fields = schema[~schema.column_name.isin(OMIT)].column_name.tolist()\n",
    "\n",
    "# all the columns in this data source are either float64 or int64\n",
    "output_types = [dtypes.float64 if x=='FLOAT64' else dtypes.int64 for x in schema[~schema.column_name.isin(OMIT)].data_type.tolist()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2decc46-ee6d-47e4-96fd-158e59645712",
   "metadata": {},
   "source": [
    "### Read From BigQuery Using TensorFlow I/O \n",
    "\n",
    "Define function for reading from BigQuery using TFIO and for transforming the input data into features and a target variable that is one-hot encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "68dca0c8-0624-48b5-8a35-c3305a6034f4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to ./code/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a {SCRIPT_PATH}\n",
    "\n",
    "# remap input data to Tensorflow inputs of features and target\n",
    "def transTable(row_dict):\n",
    "    target = row_dict.pop(args.var_target)\n",
    "    target = tf.one_hot(tf.cast(target, tf.int64), nclasses)\n",
    "    target = tf.cast(target, tf.float32)\n",
    "    return(row_dict, target)\n",
    "\n",
    "# function to setup a bigquery reader with Tensorflow I/O\n",
    "def bq_reader(split):\n",
    "    reader = BigQueryClient()\n",
    "\n",
    "    training = reader.read_session(\n",
    "        parent = f\"projects/{args.project_id}\",\n",
    "        project_id = args.bq_project,\n",
    "        table_id = args.bq_table,\n",
    "        dataset_id = args.bq_dataset,\n",
    "        selected_fields = selected_fields,\n",
    "        output_types = output_types,\n",
    "        row_restriction = f\"splits='{split}'\",\n",
    "        requested_streams = 3\n",
    "    )\n",
    "    \n",
    "    return training\n",
    "\n",
    "# setup feed for train, validate and test\n",
    "train = bq_reader('TRAIN').parallel_read_rows().prefetch(1).map(transTable).shuffle(args.batch_size*10).batch(args.batch_size)\n",
    "validate = bq_reader('VALIDATE').parallel_read_rows().prefetch(1).map(transTable).batch(args.batch_size)\n",
    "test = bq_reader('TEST').parallel_read_rows().prefetch(1).map(transTable).batch(args.batch_size)\n",
    "expRun.log_params({'training.batch_size': args.batch_size, 'training.shuffle': 10*args.batch_size, 'training.prefetch': 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b10b92-8ad4-4a06-acd7-6b21e66089fe",
   "metadata": {},
   "source": [
    "### Define The Model\n",
    "\n",
    "Define a DNN version of Logistic Regression using Keras sequential layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4cc799d1-e41f-4a92-aa06-4550c1ca38fb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to ./code/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a {SCRIPT_PATH}\n",
    "# Logistic Regression\n",
    "\n",
    "# feature list\n",
    "numeric_features = [feature for feature in schema[~schema.column_name.isin(OMIT + [args.var_target])]['column_name'].to_list()]\n",
    "\n",
    "# feature inputs\n",
    "features = [tf.keras.Input(shape = (1,), dtype = dtypes.float64, name = feature) for feature in numeric_features]\n",
    "\n",
    "# normalize features - before training\n",
    "#normalized_features = []\n",
    "#for feature in features:\n",
    "#    normalizer = tf.keras.layers.Normalization(axis = None, name = feature.name + '_normalized')\n",
    "#    feature_data = train.map(lambda x, y: x[feature.name])\n",
    "#    normalizer.adapt(feature_data)\n",
    "#    normalized_features.append(normalizer(feature))\n",
    "\n",
    "# concatenate features\n",
    "all_features = tf.keras.layers.Concatenate(name = 'feature_layer')(features)\n",
    "#all_features = tf.keras.layers.Concatenate(name = 'feature_layer')(normalized_features) # (features)\n",
    "\n",
    "# batch normalization of inputs - during training\n",
    "all_features = tf.keras.layers.BatchNormalization(name = 'batch_normalization_layer')(all_features)\n",
    "\n",
    "# logistic - using softmax activation to nclasses\n",
    "logistic = tf.keras.layers.Dense(nclasses, activation = tf.nn.softmax, name = 'logistic')(all_features)\n",
    "\n",
    "# the model\n",
    "model = tf.keras.Model(\n",
    "    inputs = features,\n",
    "    outputs = logistic,\n",
    "    name = args.experiment\n",
    ")\n",
    "\n",
    "# compile the model\n",
    "model.compile(\n",
    "    optimizer = tf.keras.optimizers.SGD(), #SGD or Adam\n",
    "    loss = tf.keras.losses.CategoricalCrossentropy(),\n",
    "    metrics = ['accuracy', tf.keras.metrics.AUC(curve = 'PR', name = 'auprc')]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f3e08d-90ce-4aa3-bb17-e19cdff08550",
   "metadata": {},
   "source": [
    "### Train The Model\n",
    "Initialize the model fit or training and log the training information to Vertex AI Tensorboard using the predefinied environment variable `AIP_TENSORBOARD_LOG_DIR`.  When done, also log the training history to Vertex AI Experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f11dec9e-ca0a-406d-bb65-ed1b88422328",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to ./code/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a {SCRIPT_PATH}\n",
    "\n",
    "# setup tensorboard logs and train\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=os.environ['AIP_TENSORBOARD_LOG_DIR'], histogram_freq=1)\n",
    "history = model.fit(train, epochs = args.epochs, callbacks = [tensorboard_callback], validation_data = validate)\n",
    "expRun.log_params({'training.epochs': history.params['epochs']})\n",
    "for e in range(0, history.params['epochs']):\n",
    "    expRun.log_time_series_metrics(\n",
    "        {\n",
    "            'train_loss': history.history['loss'][e],\n",
    "            'train_accuracy': history.history['accuracy'][e],\n",
    "            'train_auprc': history.history['auprc'][e],\n",
    "            'val_loss': history.history['val_loss'][e],\n",
    "            'val_accuracy': history.history['val_accuracy'][e],\n",
    "            'val_auprc': history.history['val_auprc'][e]\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0600e41-576e-4848-a87a-1dbfe38362e8",
   "metadata": {},
   "source": [
    "### Evaluate The Model\n",
    "Gather metrics for the trained model on each split of the data: train, validate, and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cdaf079e-95ae-4963-a743-e57eaa4de582",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to ./code/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a {SCRIPT_PATH}\n",
    "\n",
    "# test evaluations:\n",
    "loss, accuracy, auprc = model.evaluate(test)\n",
    "expRun.log_metrics({'test_loss': loss, 'test_accuracy': accuracy, 'test_auprc': auprc})\n",
    "\n",
    "# val evaluations:\n",
    "loss, accuracy, auprc = model.evaluate(validate)\n",
    "expRun.log_metrics({'val_loss': loss, 'val_accuracy': accuracy, 'val_auprc': auprc})\n",
    "\n",
    "# training evaluations:\n",
    "loss, accuracy, auprc = model.evaluate(train)\n",
    "expRun.log_metrics({'train_loss': loss, 'train_accuracy': accuracy, 'train_auprc': auprc})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91cb2214-0b18-4529-9185-7a2c6bb30541",
   "metadata": {},
   "source": [
    "### Save The Model\n",
    "Write the model save file to the GCS using the predefined environment variable `AIP_MODEL_DIR`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f63b3687-8d33-4ac8-a581-f07280089c61",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to ./code/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a {SCRIPT_PATH}\n",
    "\n",
    "# output the model save files\n",
    "model.save(os.getenv(\"AIP_MODEL_DIR\"))\n",
    "expRun.log_params({'model.save': os.getenv(\"AIP_MODEL_DIR\")})\n",
    "expRun.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7965f285-42ca-4734-851b-acb2e57a34d9",
   "metadata": {},
   "source": [
    "---\n",
    "## Review The Training Code\n",
    "Read in the completed script and print it out for review below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5bd6cd2d-7d5f-4c08-861b-637f04d3861a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "\n",
       "\n",
       "# package import\n",
       "from tensorflow.python.framework import dtypes\n",
       "from tensorflow_io.bigquery import BigQueryClient\n",
       "import tensorflow as tf\n",
       "from google.cloud import bigquery\n",
       "from google.cloud import aiplatform\n",
       "import argparse\n",
       "import os\n",
       "\n",
       "# import argument to local variables\n",
       "parser = argparse.ArgumentParser()\n",
       "# the passed param, dest: a name for the param, default: if absent fetch this param from the OS, type: type to convert to, help: description of argument\n",
       "parser.add_argument('--epochs', dest = 'epochs', default = 10, type = int, help = 'Number of Epochs')\n",
       "parser.add_argument('--batch_size', dest = 'batch_size', default = 32, type = int, help = 'Batch Size')\n",
       "parser.add_argument('--var_target', dest = 'var_target', type=str)\n",
       "parser.add_argument('--var_omit', dest = 'var_omit', type=str)#, nargs='*')\n",
       "parser.add_argument('--project_id', dest = 'project_id', type=str)\n",
       "parser.add_argument('--bq_project', dest = 'bq_project', type=str)\n",
       "parser.add_argument('--bq_dataset', dest = 'bq_dataset', type=str)\n",
       "parser.add_argument('--bq_table', dest = 'bq_table', type=str)\n",
       "parser.add_argument('--region', dest = 'region', type=str)\n",
       "parser.add_argument('--experiment', dest = 'experiment', type=str)\n",
       "parser.add_argument('--series', dest = 'series', type=str)\n",
       "parser.add_argument('--experiment_name', dest = 'experiment_name', type=str)\n",
       "parser.add_argument('--run_name', dest = 'run_name', type=str)\n",
       "args = parser.parse_args()\n",
       "\n",
       "# clients\n",
       "bq = bigquery.Client(project = args.project_id)\n",
       "aiplatform.init(project = args.project_id, location = args.region)\n",
       "\n",
       "# Vertex AI Experiment\n",
       "if args.run_name in [run.name for run in aiplatform.ExperimentRun.list(experiment = args.experiment_name)]:\n",
       "    expRun = aiplatform.ExperimentRun(run_name = args.run_name, experiment = args.experiment_name)\n",
       "else:\n",
       "    expRun = aiplatform.ExperimentRun.create(run_name = args.run_name, experiment = args.experiment_name)\n",
       "expRun.log_params({'experiment': args.experiment, 'series': args.series, 'project_id': args.project_id})\n",
       "\n",
       "# get schema from bigquery source\n",
       "query = f\"SELECT * FROM {args.bq_project}.{args.bq_dataset}.INFORMATION_SCHEMA.COLUMNS WHERE TABLE_NAME = '{args.bq_table}'\"\n",
       "schema = bq.query(query).to_dataframe()\n",
       "\n",
       "# get number of classes from bigquery source\n",
       "nclasses = bq.query(query = f'SELECT DISTINCT {args.var_target} FROM {args.bq_project}.{args.bq_dataset}.{args.bq_table} WHERE {args.var_target} is not null').to_dataframe()\n",
       "nclasses = nclasses.shape[0]\n",
       "expRun.log_params({'data_source': f'bq://{args.bq_project}.{args.bq_dataset}.{args.bq_table}', 'nclasses': nclasses, 'var_split': 'splits', 'var_target': args.var_target})\n",
       "\n",
       "# Make a list of columns to omit\n",
       "OMIT = [x for x in args.var_omit.split(',') if x != '']\n",
       "\n",
       "# use schema to prepare a list of columns to read from BigQuery\n",
       "selected_fields = schema[~schema.column_name.isin(OMIT)].column_name.tolist()\n",
       "\n",
       "# all the columns in this data source are either float64 or int64\n",
       "output_types = [dtypes.float64 if x=='FLOAT64' else dtypes.int64 for x in schema[~schema.column_name.isin(OMIT)].data_type.tolist()]\n",
       "\n",
       "# remap input data to Tensorflow inputs of features and target\n",
       "def transTable(row_dict):\n",
       "    target = row_dict.pop(args.var_target)\n",
       "    target = tf.one_hot(tf.cast(target, tf.int64), nclasses)\n",
       "    target = tf.cast(target, tf.float32)\n",
       "    return(row_dict, target)\n",
       "\n",
       "# function to setup a bigquery reader with Tensorflow I/O\n",
       "def bq_reader(split):\n",
       "    reader = BigQueryClient()\n",
       "\n",
       "    training = reader.read_session(\n",
       "        parent = f\"projects/{args.project_id}\",\n",
       "        project_id = args.bq_project,\n",
       "        table_id = args.bq_table,\n",
       "        dataset_id = args.bq_dataset,\n",
       "        selected_fields = selected_fields,\n",
       "        output_types = output_types,\n",
       "        row_restriction = f\"splits='{split}'\",\n",
       "        requested_streams = 3\n",
       "    )\n",
       "    \n",
       "    return training\n",
       "\n",
       "# setup feed for train, validate and test\n",
       "train = bq_reader('TRAIN').parallel_read_rows().prefetch(1).map(transTable).shuffle(args.batch_size*10).batch(args.batch_size)\n",
       "validate = bq_reader('VALIDATE').parallel_read_rows().prefetch(1).map(transTable).batch(args.batch_size)\n",
       "test = bq_reader('TEST').parallel_read_rows().prefetch(1).map(transTable).batch(args.batch_size)\n",
       "expRun.log_params({'training.batch_size': args.batch_size, 'training.shuffle': 10*args.batch_size, 'training.prefetch': 1})\n",
       "# Logistic Regression\n",
       "\n",
       "# feature list\n",
       "numeric_features = [feature for feature in schema[~schema.column_name.isin(OMIT + [args.var_target])]['column_name'].to_list()]\n",
       "\n",
       "# feature inputs\n",
       "features = [tf.keras.Input(shape = (1,), dtype = dtypes.float64, name = feature) for feature in numeric_features]\n",
       "\n",
       "# normalize features - before training\n",
       "#normalized_features = []\n",
       "#for feature in features:\n",
       "#    normalizer = tf.keras.layers.Normalization(axis = None, name = feature.name + '_normalized')\n",
       "#    feature_data = train.map(lambda x, y: x[feature.name])\n",
       "#    normalizer.adapt(feature_data)\n",
       "#    normalized_features.append(normalizer(feature))\n",
       "\n",
       "# concatenate features\n",
       "all_features = tf.keras.layers.Concatenate(name = 'feature_layer')(features)\n",
       "#all_features = tf.keras.layers.Concatenate(name = 'feature_layer')(normalized_features) # (features)\n",
       "\n",
       "# batch normalization of inputs - during training\n",
       "all_features = tf.keras.layers.BatchNormalization(name = 'batch_normalization_layer')(all_features)\n",
       "\n",
       "# logistic - using softmax activation to nclasses\n",
       "logistic = tf.keras.layers.Dense(nclasses, activation = tf.nn.softmax, name = 'logistic')(all_features)\n",
       "\n",
       "# the model\n",
       "model = tf.keras.Model(\n",
       "    inputs = features,\n",
       "    outputs = logistic,\n",
       "    name = args.experiment\n",
       ")\n",
       "\n",
       "# compile the model\n",
       "model.compile(\n",
       "    optimizer = tf.keras.optimizers.SGD(), #SGD or Adam\n",
       "    loss = tf.keras.losses.CategoricalCrossentropy(),\n",
       "    metrics = ['accuracy', tf.keras.metrics.AUC(curve = 'PR', name = 'auprc')]\n",
       ")\n",
       "\n",
       "# setup tensorboard logs and train\n",
       "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=os.environ['AIP_TENSORBOARD_LOG_DIR'], histogram_freq=1)\n",
       "history = model.fit(train, epochs = args.epochs, callbacks = [tensorboard_callback], validation_data = validate)\n",
       "expRun.log_params({'training.epochs': history.params['epochs']})\n",
       "for e in range(0, history.params['epochs']):\n",
       "    expRun.log_time_series_metrics(\n",
       "        {\n",
       "            'train_loss': history.history['loss'][e],\n",
       "            'train_accuracy': history.history['accuracy'][e],\n",
       "            'train_auprc': history.history['auprc'][e],\n",
       "            'val_loss': history.history['val_loss'][e],\n",
       "            'val_accuracy': history.history['val_accuracy'][e],\n",
       "            'val_auprc': history.history['val_auprc'][e]\n",
       "        }\n",
       "    )\n",
       "\n",
       "# test evaluations:\n",
       "loss, accuracy, auprc = model.evaluate(test)\n",
       "expRun.log_metrics({'test_loss': loss, 'test_accuracy': accuracy, 'test_auprc': auprc})\n",
       "\n",
       "# val evaluations:\n",
       "loss, accuracy, auprc = model.evaluate(validate)\n",
       "expRun.log_metrics({'val_loss': loss, 'val_accuracy': accuracy, 'val_auprc': auprc})\n",
       "\n",
       "# training evaluations:\n",
       "loss, accuracy, auprc = model.evaluate(train)\n",
       "expRun.log_metrics({'train_loss': loss, 'train_accuracy': accuracy, 'train_auprc': auprc})\n",
       "\n",
       "# output the model save files\n",
       "model.save(os.getenv(\"AIP_MODEL_DIR\"))\n",
       "expRun.log_params({'model.save': os.getenv(\"AIP_MODEL_DIR\")})\n",
       "expRun.end_run()\n",
       "\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(SCRIPT_PATH, 'r') as file:\n",
    "    data = file.read()\n",
    "md(f\"```python\\n\\n{data}\\n```\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104711a7-b279-4e9c-92c8-9b34ec000ea1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488ca8d8-653a-43cc-8350-0b664870f363",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-root-py",
   "name": "workbench-notebooks.m113",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/workbench-notebooks:m113"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
