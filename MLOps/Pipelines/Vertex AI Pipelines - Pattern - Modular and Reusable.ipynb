{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11e738e4",
   "metadata": {},
   "source": [
    "![tracker](https://us-central1-vertex-ai-mlops-369716.cloudfunctions.net/pixel-tracking?path=statmike%2Fvertex-ai-mlops%2FMLOps%2FPipelines&file=Vertex+AI+Pipelines+-+Pattern+-+Modular+and+Reusable.ipynb)\n",
    "<!--- header table --->\n",
    "<table align=\"left\">\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://colab.research.google.com/github/statmike/vertex-ai-mlops/blob/main/MLOps/Pipelines/Vertex%20AI%20Pipelines%20-%20Pattern%20-%20Modular%20and%20Reusable.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Google Colaboratory logo\">\n",
    "      <br>Run in<br>Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https%3A%2F%2Fraw.githubusercontent.com%2Fstatmike%2Fvertex-ai-mlops%2Fmain%2FMLOps%2FPipelines%2FVertex%2520AI%2520Pipelines%2520-%2520Pattern%2520-%2520Modular%2520and%2520Reusable.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\">\n",
    "      <br>Run in<br>Colab Enterprise\n",
    "    </a>\n",
    "  </td>      \n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://github.com/statmike/vertex-ai-mlops/blob/main/MLOps/Pipelines/Vertex%20AI%20Pipelines%20-%20Pattern%20-%20Modular%20and%20Reusable.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      <br>View on<br>GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/statmike/vertex-ai-mlops/main/MLOps/Pipelines/Vertex%20AI%20Pipelines%20-%20Pattern%20-%20Modular%20and%20Reusable.ipynb\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
    "      <br>Open in<br>Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac15428-70c8-403f-b3f4-d4871f190cc8",
   "metadata": {},
   "source": [
    "---\n",
    "This notebook present a pattern, or common workflow with **pipelines**.  The concept used in the notebook are introduced and explained in more detail in this [series of notebook based workflows](./readme.md) that teach all the ways to use pipelines within Vertex AI. The suggested order and description/reason is:\n",
    "\n",
    "|Notebook Workflow|Description|\n",
    "|---|---|\n",
    "|[Vertex AI Pipelines - Start Here](./Vertex%20AI%20Pipelines%20-%20Start%20Here.ipynb)|What are pipelines? Start here to go from code to pipeline and see it in action.|\n",
    "|[Vertex AI Pipelines - Introduction](./Vertex%20AI%20Pipelines%20-%20Introduction.ipynb)|Introduction to pipelines with the console and Vertex AI SDK|\n",
    "|[Vertex AI Pipelines - Components](./Vertex%20AI%20Pipelines%20-%20Components.ipynb)|An introduction to all the ways to create pipeline components from your code|\n",
    "|[Vertex AI Pipelines - IO](./Vertex%20AI%20Pipelines%20-%20IO.ipynb)|An overview of all the type of inputs and outputs for pipeline components|\n",
    "|[Vertex AI Pipelines - Control](./Vertex%20AI%20Pipelines%20-%20Control.ipynb)|An overview of controlling the flow of exectution for pipelines|\n",
    "|[Vertex AI Pipelines - Secret Manager](./Vertex%20AI%20Pipelines%20-%20Secret%20Manager.ipynb)|How to pass sensitive information to pipelines and components|\n",
    "|[Vertex AI Pipelines - GCS Read and Write](./Vertex%20AI%20Pipelines%20-%20GCS%20Read%20and%20Write.ipynb)|How to read/write to GCS from components, including container components.|\n",
    "|[Vertex AI Pipelines - Scheduling](./Vertex%20AI%20Pipelines%20-%20Scheduling.ipynb)|How to schedule pipeline execution|\n",
    "|[Vertex AI Pipelines - Notifications](./Vertex%20AI%20Pipelines%20-%20Notifications.ipynb)|How to send email notification of pipeline status.|\n",
    "|[Vertex AI Pipelines - Management](./Vertex%20AI%20Pipelines%20-%20Management.ipynb)|Managing, Reusing, and Storing pipelines and components|\n",
    "|[Vertex AI Pipelines - Testing](./Vertex%20AI%20Pipelines%20-%20Testing.ipynb)|Strategies for testing components and pipeliens locally and remotely to aide development.|\n",
    "|[Vertex AI Pipelines - Managing Pipeline Jobs](./Vertex%20AI%20Pipelines%20-%20Managing%20Pipeline%20Jobs.ipynb)|Manage runs of pipelines in an environment: list, check status, filtered list, cancel and delete jobs.|\n",
    "\n",
    "\n",
    "To discover these notebooks as part of an introduction to MLOps orchestration [start here](./readme.md).  To read more about MLOps also check out [the parent folder](../readme.md).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62610a4-4383-4975-8035-da2cd6da74c4",
   "metadata": {},
   "source": [
    "# Vertex AI Pipelines - Pattern - Modular and Reusable\n",
    "\n",
    "The patterns makes use of modular components and pipelines while also show that pipelines can be used within pipelines for ultimate modularity.  The components and pipelines are managed and stored in artifact registry for easy recall and reloading.\n",
    "\n",
    "This notebook walks through:\n",
    "- Example: A example pipeline, completely local build/compile\n",
    "- Example 1: Save the pipeline to Artifact Registry and run directly on Vertex AI Pipelines\n",
    "- Example 2: Save a component to Artifact Registry and recall it (download and import) for use in a new pipeline\n",
    "- Example 3: Creating a sub-pipeline of multiple components, saving to Artifact Registry.  Then, recall the pipeline and use it as a component in a new pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2d0450-eea1-4212-a052-b6721762e0de",
   "metadata": {
    "id": "od_UkDpvRmgD",
    "tags": []
   },
   "source": [
    "---\n",
    "## Colab Setup\n",
    "\n",
    "To run this notebook in Colab run the cells in this section.  Otherwise, skip this section.\n",
    "\n",
    "This cell will authenticate to GCP (follow prompts in the popup)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e49ba7fb-0211-43a5-939f-71e6213bf2b7",
   "metadata": {
    "executionInfo": {
     "elapsed": 195,
     "status": "ok",
     "timestamp": 1683726184843,
     "user": {
      "displayName": "Mike Henderson",
      "userId": "07691629187611687318"
     },
     "user_tz": 240
    },
    "id": "8UO9FnqyKBlF",
    "tags": []
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = 'statmike-mlops-349915' # replace with project ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abdd5820-a8d3-44d0-aaa1-a9722ec4ef73",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 68869,
     "status": "ok",
     "timestamp": 1683726253709,
     "user": {
      "displayName": "Mike Henderson",
      "userId": "07691629187611687318"
     },
     "user_tz": 240
    },
    "id": "N98-KK7LRkjm",
    "outputId": "09ec5008-0def-4e1a-c349-c598ee752f78",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not a Colab Environment\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from google.colab import auth\n",
    "    auth.authenticate_user()\n",
    "    !gcloud config set project {PROJECT_ID}\n",
    "    print('Colab authorized to GCP')\n",
    "except Exception:\n",
    "    print('Not a Colab Environment')\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c463c2fb-2dab-4b34-8c15-a6e2e9b8aa06",
   "metadata": {},
   "source": [
    "---\n",
    "## Installs\n",
    "\n",
    "The list `packages` contains tuples of package import names and install names.  If the import name is not found then the install name is used to install quitely for the current user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "24e83026-67c5-4da9-a330-4c5ea0f0c600",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# tuples of (import name, install name, min_version)\n",
    "packages = [\n",
    "    ('google.cloud.aiplatform', 'google-cloud-aiplatform'),\n",
    "    ('google.cloud.artifactregistry_v1', 'google-cloud-artifact-registry'),\n",
    "    ('kfp', 'kfp'),\n",
    "    ('google_cloud_pipeline_components', 'google-cloud-pipeline-components'),\n",
    "]\n",
    "\n",
    "import importlib\n",
    "install = False\n",
    "for package in packages:\n",
    "    if not importlib.util.find_spec(package[0]):\n",
    "        print(f'installing package {package[1]}')\n",
    "        install = True\n",
    "        !pip install {package[1]} -U -q --user\n",
    "    elif len(package) == 3:\n",
    "        if importlib.metadata.version(package[0]) < package[2]:\n",
    "            print(f'updating package {package[1]}')\n",
    "            install = True\n",
    "            !pip install {package[1]} -U -q --user"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b262ab7-8494-456a-8e3b-4a9ec7c29d85",
   "metadata": {},
   "source": [
    "### API Enablement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9421840-c96b-42b9-9b74-96945b01145d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!gcloud services enable aiplatform.googleapis.com\n",
    "!gcloud services enable artifactregistry.googleapis.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c14d3ef-7383-441f-9245-b2c032247b02",
   "metadata": {},
   "source": [
    "### Restart Kernel (If Installs Occured)\n",
    "\n",
    "After a kernel restart the code submission can start with the next cell after this one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86d6aa93-57b4-4439-be1a-e9ba7eda9cc9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if install:\n",
    "    import IPython\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)\n",
    "    IPython.display.display(IPython.display.Markdown(\"\"\"<div class=\\\"alert alert-block alert-warning\\\">\n",
    "        <b>⚠️ The kernel is going to restart. Please wait until it is finished before continuing to the next step. The previous cells do not need to be run again⚠️</b>\n",
    "        </div>\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7533e99d-c663-4ad0-9046-d9c130772c45",
   "metadata": {
    "id": "appt8-yVRtJ1"
   },
   "source": [
    "---\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9a0054-777f-40b9-bc3b-49bc4f81cc9c",
   "metadata": {
    "id": "63mx2EozRxFP"
   },
   "source": [
    "Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa524ea9-cc5a-4b6e-927e-de5328f8d9fb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 2124,
     "status": "ok",
     "timestamp": 1683726390544,
     "user": {
      "displayName": "Mike Henderson",
      "userId": "07691629187611687318"
     },
     "user_tz": 240
    },
    "id": "xzcoXjM5Rky5",
    "outputId": "b3bdcbc1-70d5-472e-aea2-42c74a42efde",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'statmike-mlops-349915'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project = !gcloud config get-value project\n",
    "PROJECT_ID = project[0]\n",
    "PROJECT_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "790823c7-8a0e-491a-80c4-b6ab75455a85",
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1683726390712,
     "user": {
      "displayName": "Mike Henderson",
      "userId": "07691629187611687318"
     },
     "user_tz": 240
    },
    "id": "IxWrFtqYMfku",
    "tags": []
   },
   "outputs": [],
   "source": [
    "REGION = 'us-central1'\n",
    "EXPERIMENT = 'pipeline-pattern-modular'\n",
    "SERIES = 'mlops'\n",
    "\n",
    "# gcs bucket\n",
    "GCS_BUCKET = PROJECT_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ec0207-283e-4d11-961f-c0bfad343a49",
   "metadata": {
    "id": "LuajVwCiO6Yg"
   },
   "source": [
    "Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bfde5110-d793-469e-bd23-75c2e297413b",
   "metadata": {
    "executionInfo": {
     "elapsed": 17761,
     "status": "ok",
     "timestamp": 1683726409304,
     "user": {
      "displayName": "Mike Henderson",
      "userId": "07691629187611687318"
     },
     "user_tz": 240
    },
    "id": "LVC7zzSLRk2C",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import time\n",
    "import importlib\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud import artifactregistry_v1\n",
    "import kfp\n",
    "from google_cloud_pipeline_components.types import artifact_types\n",
    "from typing import NamedTuple\n",
    "from IPython.display import Markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7bb4c50-ecc0-4626-a284-c9b5bdb41dc1",
   "metadata": {
    "id": "EyAVFG9TO9H-"
   },
   "source": [
    "Clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "87d563d9-8939-44bd-ac29-d6842a01aba2",
   "metadata": {
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1683726409306,
     "user": {
      "displayName": "Mike Henderson",
      "userId": "07691629187611687318"
     },
     "user_tz": 240
    },
    "id": "L0RPE13LOZce",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# vertex ai clients\n",
    "aiplatform.init(project = PROJECT_ID, location = REGION)\n",
    "\n",
    "# artifact registry client\n",
    "ar_client = artifactregistry_v1.ArtifactRegistryClient()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2747308a-b4b8-40ed-a73a-09e24c4d59c7",
   "metadata": {},
   "source": [
    "parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "07ae7434-7dd7-4d5f-a92f-8469ad00307f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DIR = f\"temp/{SERIES}-{EXPERIMENT}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f01cfc8f-5131-4057-adf3-2593cef9c28f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1026793852137-compute@developer.gserviceaccount.com'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SERVICE_ACCOUNT = !gcloud config list --format='value(core.account)' \n",
    "SERVICE_ACCOUNT = SERVICE_ACCOUNT[0]\n",
    "SERVICE_ACCOUNT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8e4928-e064-4603-8f08-f8a06aa8b765",
   "metadata": {},
   "source": [
    "environment:\n",
    "- make a local folder for temporary storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "50f248d3-915a-431b-ae3d-a473819f6582",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(DIR):\n",
    "    os.makedirs(DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8767d23-3327-4db3-8e75-ce50b0bde7d9",
   "metadata": {},
   "source": [
    "---\n",
    "## Example Pipeline: Components and Pipeline are Local\n",
    "\n",
    "> The pipeline used here is also used and explained in the workflow: [Vertex AI Pipelines - IO](./Vertex%20AI%20Pipelines%20-%20IO.ipynb).\n",
    "\n",
    "This is an example pipeline that makes use of:\n",
    "- all 8 `kfp` artifact types and multiple Google Cloud Artifact Types\n",
    "- passing artifacts as outputs and intput between components\n",
    "- returning multiple artifacts from components\n",
    "- saving content for multiple artifacts with the same component\n",
    "\n",
    "It has components that do the following:\n",
    "- `data_source` Creates a BigQuery Table artifact from the metadata (project, dataset, table name)\n",
    "- `data_prep` From a BigQuery Table artifact it retrieves, splits and saves the prepared data while defining dataset artifacts that point to train and test data.  It also create a generic artifact that contains lists for feature names and classificaiton labels in the datasets.\n",
    "- `model_gb` Retrieves training data and feature information to use with Scikit-Learn with a pipeline that imputes, scales, encodes and then fits a gradient boosting classifier that is then saved as a model artifact.\n",
    "- `model_rf` Retrieves training data and feature information to use with Scikit-Learn in a pipeline that imputes, scales, encodes and then fits a random forest classifier that is then saved as a model artifact.\n",
    "- `metrics` Retrieves data, feature info, and model files to compute evalaution metrics that are saved as artifacts.\n",
    "- `overview` Compiles metrics results from both models on train and test data and then saves as HTML and Markdown artifacts for easy review."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54e983f-051d-4105-a244-196d1437bf69",
   "metadata": {},
   "source": [
    "### Create Pipeline Components\n",
    "\n",
    "These are simple Python components, specifically lightweight Python components.  For more details on the types of components check out this workflow in the same repository:\n",
    "- [Vertex AI Pipelines - Components](./Vertex%20AI%20Pipelines%20-%20Components.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd75f82e-8b2d-41ef-b725-ffc2ada33ae9",
   "metadata": {},
   "source": [
    "#### Component: `data_source`\n",
    "\n",
    "This component defines an artifact that points to the data source in place, in BigQuery.  It uses the Google Cloud Artifact Type for BigQuery Tables: [`google_cloud_pipeline_components.types.artifact_types.BQTable()`](https://google-cloud-pipeline-components.readthedocs.io/en/google-cloud-pipeline-components-2.14.0/api/artifact_types.html#google_cloud_pipeline_components.types.artifact_types.BQTable).\n",
    "\n",
    ">**NOTE:** This could be done with an importer component `kfp.dsl.importer` - see [Vertex AI Pipelines - Components](./Vertex%20AI%20Pipelines%20-%20Components.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6cac9243-99ce-4c50-85f3-f6dac5ea9ca7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@kfp.dsl.component(\n",
    "    base_image = \"python:3.11\",\n",
    "    packages_to_install = [\"google-cloud-pipeline-components\"]\n",
    ")\n",
    "def data_source(\n",
    "    bq_project: str,\n",
    "    bq_dataset: str,\n",
    "    bq_table: str,\n",
    "    bq_table_artifact: kfp.dsl.Output[artifact_types.BQTable]\n",
    "):\n",
    "    \n",
    "    bq_table_artifact.uri = f'https://www.googleapis.com/bigquery/v2/projects/{bq_project}/datasets/{bq_dataset}/tables/{bq_table}'\n",
    "    bq_table_artifact.metadata['projectId'] = bq_project\n",
    "    bq_table_artifact.metadata['datasetId'] = bq_dataset\n",
    "    bq_table_artifact.metadata['tableId'] = bq_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8923a8-91c3-4082-9919-5e32f664f62e",
   "metadata": {},
   "source": [
    "#### Component: `data_prep`\n",
    "\n",
    "A lightweight Python component that:\n",
    "- read data from BigQuery Table using Input Artifact for BigQuery Table\n",
    "- split data in the train, eval, text\n",
    "- create output artifacts (`kfp.dsl.Dataset`) for each split of the data\n",
    "- create output artifact (`kfp.dsl.Artifact`) with feature information from the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "127edcf1-afa4-4e1e-8519-8012627303ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@kfp.dsl.component(\n",
    "    base_image = \"python:3.11\",\n",
    "    packages_to_install = [\"google-cloud-pipeline-components\", \"bigframes\", \"scikit-learn\"]\n",
    ")\n",
    "def data_prep(\n",
    "    project_id: str,\n",
    "    bq_source: kfp.dsl.Input[artifact_types.BQTable],\n",
    ") -> NamedTuple(\n",
    "        'output',\n",
    "        train=kfp.dsl.Dataset,\n",
    "        test=kfp.dsl.Dataset,\n",
    "        features=kfp.dsl.Artifact\n",
    "):\n",
    "    from typing import NamedTuple\n",
    "    outputs = NamedTuple(\n",
    "            'output',\n",
    "            train=kfp.dsl.Dataset,\n",
    "            test=kfp.dsl.Dataset,\n",
    "            features=kfp.dsl.Artifact\n",
    "    )\n",
    "    \n",
    "    # connect to BigQuery table, ELT, read to local\n",
    "    import bigframes.pandas as bpd\n",
    "    bpd.options.bigquery.project = project_id\n",
    "    bpd.options.bigquery.location = 'us'\n",
    "    ds = bpd.read_gbq(f\"{bq_source.metadata['projectId']}.{bq_source.metadata['datasetId']}.{bq_source.metadata['tableId']}\")\n",
    "    # fix data quality issue\n",
    "    ds['sex'] = ds['sex'].replace('.', None)\n",
    "    full_ds = ds.to_pandas()\n",
    "    \n",
    "    # split data into train/test\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    train_ds, test_ds = train_test_split(full_ds, test_size = 0.25)\n",
    "    \n",
    "    # write test and train to Dataset artifacts - with specific subfolders\n",
    "    import os\n",
    "    #train\n",
    "    train = kfp.dsl.Dataset(\n",
    "        uri = kfp.dsl.get_uri(suffix = 'train'),\n",
    "        metadata = dict(\n",
    "            samples = train_ds.shape[0],\n",
    "            filename = 'data.txt'\n",
    "        )\n",
    "    )\n",
    "    path = train.path + '/data.txt'\n",
    "    os.makedirs(os.path.dirname(path), exist_ok = True)\n",
    "    with open(path, 'w') as f:\n",
    "        f.write(train_ds.to_json(orient='records'))\n",
    "    # test    \n",
    "    test = kfp.dsl.Dataset(\n",
    "        uri = kfp.dsl.get_uri(suffix = 'test'),\n",
    "        metadata = dict(\n",
    "            samples = test_ds.shape[0],\n",
    "            filename = 'data.txt'\n",
    "        )\n",
    "    )\n",
    "    path = test.path + '/data.txt'\n",
    "    os.makedirs(os.path.dirname(path), exist_ok = True)\n",
    "    with open(path, 'w') as f:\n",
    "        f.write(test_ds.to_json(orient='records'))\n",
    "    \n",
    "    # add feature info the feature Artifact\n",
    "    features = kfp.dsl.Artifact(\n",
    "        metadata = dict(\n",
    "            label_col = 'species',\n",
    "            label_values = ds['species'].unique().to_list(),\n",
    "            train_n = train_ds.shape[0],\n",
    "            test_n = test_ds.shape[0],\n",
    "            features = [x for x in ds.columns.to_list() if x != 'species']\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    return outputs(train, test, features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614c971a-6499-4d18-b53e-36eec44712c5",
   "metadata": {},
   "source": [
    "#### Component: `model_gb`\n",
    "\n",
    "A lightweight Python component that:\n",
    "- inputs artifacts for training data as well as feature information created by the `data_prep` component\n",
    "- creates a model with `sklearn.ensemble.GradientBoostingClassifier`\n",
    "- output artifact for the model (`kfp.dsl.Model`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ae037c19-6153-4bd8-9b58-0ee465895d37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@kfp.dsl.component(\n",
    "    base_image = \"python:3.11\",\n",
    "    packages_to_install = [\"pandas\", \"scikit-learn\"]\n",
    ")\n",
    "def model_gb(\n",
    "    train: kfp.dsl.Dataset,\n",
    "    features: kfp.dsl.Artifact\n",
    ") -> kfp.dsl.Model:\n",
    "    \n",
    "    # import data\n",
    "    import pandas as pd\n",
    "    from io import StringIO\n",
    "    with open(train.path + f\"/{train.metadata['filename']}\", 'r') as f:\n",
    "        train_ds = f.read()\n",
    "    train_ds = pd.read_json(StringIO(train_ds), orient='records')\n",
    "    \n",
    "    # prepare data for training: split the features (x) and label (y)\n",
    "    train_x = train_ds[features.metadata['features']]\n",
    "    train_y = train_ds[features.metadata['label_col']] \n",
    "    \n",
    "    # create pipeline with preprocessing and training\n",
    "    import sklearn.ensemble\n",
    "    import sklearn.impute\n",
    "    import sklearn.pipeline\n",
    "    import sklearn.preprocessing\n",
    "    import sklearn.compose\n",
    "    import numpy as np\n",
    "    numerical_transformer = sklearn.pipeline.Pipeline([\n",
    "        ('imputer', sklearn.impute.SimpleImputer(strategy = 'mean')),\n",
    "        ('scaler', sklearn.preprocessing.MinMaxScaler()),\n",
    "    ])\n",
    "    categorical_transformer = sklearn.pipeline.Pipeline([\n",
    "        ('imputer', sklearn.impute.SimpleImputer(strategy = 'most_frequent', add_indicator = True)),\n",
    "        ('encoder', sklearn.preprocessing.OrdinalEncoder()),\n",
    "    ])\n",
    "    preprocessor = sklearn.compose.ColumnTransformer(\n",
    "        transformers = [\n",
    "            ('numerical', numerical_transformer, [c for c in train_x.columns if train_x[c].isna().any() and train_x[c].dtypes == 'float64']),\n",
    "            ('categorical', categorical_transformer, [c for c in train_x.columns if train_x[c].isna().any() and train_x[c].dtypes == 'object'])\n",
    "        ]\n",
    "    )\n",
    "    pipeline = sklearn.pipeline.Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', sklearn.ensemble.GradientBoostingClassifier(n_estimators = 100, learning_rate = 0.125, max_depth = 3)),\n",
    "    ])\n",
    "    \n",
    "    # fit/train model\n",
    "    pipeline.fit(train_x, train_y)\n",
    "    \n",
    "    # save model and create artifact\n",
    "    import pickle, os\n",
    "    model = kfp.dsl.Model(\n",
    "        uri = kfp.dsl.get_uri(),\n",
    "        metadata = dict(\n",
    "            accuracy = pipeline.score(train_x, train_y)\n",
    "        )\n",
    "    )\n",
    "    path = model.path + '/model.pkl'\n",
    "    os.makedirs(os.path.dirname(path), exist_ok = True)\n",
    "    with open(path, 'wb') as f:\n",
    "        pickle.dump(pipeline, f)\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23439fc-ad3f-408f-b421-cfec5d900332",
   "metadata": {},
   "source": [
    "#### Component: `model_rf`\n",
    "\n",
    "A lightweight Python component that:\n",
    "- inputs artifacts for training data as well as feature information created by the `data_prep` component\n",
    "- creates a model with `sklearn.ensemble.RandomForestClassifier`\n",
    "- output artifacts for the model (`kfp.dsl.Model`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1994e8cc-a7f7-410b-a187-98651b18d95b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@kfp.dsl.component(\n",
    "    base_image = \"python:3.11\",\n",
    "    packages_to_install = [\"pandas\", \"scikit-learn\"]\n",
    ")\n",
    "def model_rf(\n",
    "    train: kfp.dsl.Dataset,\n",
    "    features: kfp.dsl.Artifact\n",
    ") -> kfp.dsl.Model:\n",
    "    \n",
    "    # import data\n",
    "    import pandas as pd\n",
    "    from io import StringIO\n",
    "    with open(train.path + f\"/{train.metadata['filename']}\", 'r') as f:\n",
    "        train_ds = f.read()\n",
    "    train_ds = pd.read_json(StringIO(train_ds), orient='records')\n",
    "    \n",
    "    # prepare data for training: split the features (x) and label (y)\n",
    "    train_x = train_ds[features.metadata['features']]\n",
    "    train_y = train_ds[features.metadata['label_col']] \n",
    "    \n",
    "    # create pipeline with preprocessing and training\n",
    "    import sklearn.ensemble\n",
    "    import sklearn.impute\n",
    "    import sklearn.pipeline\n",
    "    import sklearn.preprocessing\n",
    "    import sklearn.compose\n",
    "    import numpy as np\n",
    "    numerical_transformer = sklearn.pipeline.Pipeline([\n",
    "        ('imputer', sklearn.impute.SimpleImputer(strategy = 'mean')),\n",
    "        ('scaler', sklearn.preprocessing.MinMaxScaler()),\n",
    "    ])\n",
    "    categorical_transformer = sklearn.pipeline.Pipeline([\n",
    "        ('imputer', sklearn.impute.SimpleImputer(strategy = 'most_frequent', add_indicator = True)),\n",
    "        ('encoder', sklearn.preprocessing.OrdinalEncoder()),\n",
    "    ])\n",
    "    preprocessor = sklearn.compose.ColumnTransformer(\n",
    "        transformers = [\n",
    "            ('numerical', numerical_transformer, [c for c in train_x.columns if train_x[c].isna().any() and train_x[c].dtypes == 'float64']),\n",
    "            ('categorical', categorical_transformer, [c for c in train_x.columns if train_x[c].isna().any() and train_x[c].dtypes == 'object'])\n",
    "        ]\n",
    "    )\n",
    "    pipeline = sklearn.pipeline.Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', sklearn.ensemble.RandomForestClassifier(n_estimators = 200, max_depth = 3)),\n",
    "    ])\n",
    "    \n",
    "    # fit/train model\n",
    "    pipeline.fit(train_x, train_y)\n",
    "    \n",
    "    # save model and create artifact\n",
    "    import pickle, os\n",
    "    model = kfp.dsl.Model(\n",
    "        uri = kfp.dsl.get_uri(),\n",
    "        metadata = dict(\n",
    "            accuracy = pipeline.score(train_x, train_y)\n",
    "        )\n",
    "    )\n",
    "    path = model.path + '/model.pkl'\n",
    "    os.makedirs(os.path.dirname(path), exist_ok = True)\n",
    "    with open(path, 'wb') as f:\n",
    "        pickle.dump(pipeline, f)\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3551a7d3-e57b-4d33-a6ce-eeab11a5fd38",
   "metadata": {},
   "source": [
    "#### Component: `metrics`\n",
    "\n",
    "A lightweight Python component that:\n",
    "- inputs artifacts for a dataset and a model\n",
    "- create artifacts for:\n",
    "    - Metrics with `kfp.dsl.Metrics`\n",
    "    - Classification metrics with `kfp.dsl.ClassificationMetrics`\n",
    "    - Sliced Classification metrics wtih `kfp.dsl.SlicedClassificationMetrics`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c32df4db-96a5-4dcb-97a1-193f70b134d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@kfp.dsl.component(\n",
    "    base_image = \"python:3.11\",\n",
    "    packages_to_install = [\"pandas\", \"numpy\", \"scikit-learn\"]\n",
    ")\n",
    "def metrics(\n",
    "    data: kfp.dsl.Dataset,\n",
    "    features: kfp.dsl.Artifact,\n",
    "    model: kfp.dsl.Model\n",
    ") -> NamedTuple(\n",
    "        'output',\n",
    "        metrics=kfp.dsl.Metrics,\n",
    "        class_metrics=kfp.dsl.ClassificationMetrics,\n",
    "        #slice_class_metrics=kfp.dsl.SlicedClassificationMetrics\n",
    "):\n",
    "    from typing import NamedTuple\n",
    "    outputs = NamedTuple(\n",
    "            'output',\n",
    "            metrics=kfp.dsl.Metrics,\n",
    "            class_metrics=kfp.dsl.ClassificationMetrics,\n",
    "            #slice_class_metrics=kfp.dsl.SlicedClassificationMetrics\n",
    "    )\n",
    "    \n",
    "    # import data\n",
    "    import pandas as pd\n",
    "    from io import StringIO\n",
    "    with open(data.path + f\"/{data.metadata['filename']}\", 'r') as f:\n",
    "        ds = f.read()\n",
    "    ds = pd.read_json(StringIO(ds), orient='records')\n",
    "    \n",
    "    # get the ground truth\n",
    "    x = ds[features.metadata['features']]\n",
    "    y = ds[features.metadata['label_col']]\n",
    "    \n",
    "    # import model\n",
    "    import pickle\n",
    "    with open(model.path+'/model.pkl', 'rb') as f:\n",
    "        classifier = pickle.load(f)\n",
    "    pred = classifier.predict(x)\n",
    "    proba = classifier.predict_proba(x)\n",
    "    \n",
    "    # metrics artifact\n",
    "    import sklearn.metrics\n",
    "    metrics = kfp.dsl.Metrics()\n",
    "    metrics.log_metric('accuracy', classifier.score(x, y))\n",
    "    if len(features.metadata['label_values'])>2:\n",
    "        metrics.log_metric('precision', sklearn.metrics.precision_score(y, pred, average='macro'))\n",
    "        metrics.log_metric('recall', sklearn.metrics.recall_score(y, pred, average='macro'))\n",
    "        metrics.log_metric('f1', sklearn.metrics.f1_score(y, pred, average='macro'))\n",
    "        metrics.log_metric('average_precision', sklearn.metrics.average_precision_score(y, proba, average='macro'))\n",
    "    else:\n",
    "        metrics.log_metric('precision', sklearn.metrics.precision_score(y, pred, average='binary'))\n",
    "        metrics.log_metric('recall', sklearn.metrics.recall_score(y, pred, average='binary'))\n",
    "        metrics.log_metric('f1', sklearn.metrics.f1_score(y, pred, average='binary'))\n",
    "        metrics.log_metric('average_precision', sklearn.metrics.average_precision_score(y, proba, average='binary'))\n",
    "    \n",
    "    # classification metrics artifact\n",
    "    class_metrics = kfp.dsl.ClassificationMetrics()\n",
    "    class_metrics.log_confusion_matrix(\n",
    "        categories = classifier.classes_,\n",
    "        matrix = sklearn.metrics.confusion_matrix(y, pred).tolist()\n",
    "    )\n",
    "    \n",
    "    # sliced classification metrics artifact\n",
    "    #import numpy as np\n",
    "    #import sklearn.preprocessing\n",
    "    #slice_class_metrics = kfp.dsl.SlicedClassificationMetrics()\n",
    "    #labeler = sklearn.preprocessing.LabelBinarizer().fit(y)\n",
    "    #for c in classifier.classes_:\n",
    "    #    i = np.where(labeler.transform([c]) == 1)[0][0]\n",
    "    #    fpr, tpr, thresholds = sklearn.metrics.roc_curve(\n",
    "    #        y_true = labeler.transform(y)[:, i],\n",
    "    #        y_score = classifier.predict_proba(x)[:, i]\n",
    "    #    )\n",
    "    #    infs = [t==np.inf for t in thresholds.tolist()]\n",
    "    #    \n",
    "    #    slice_class_metrics.load_roc_readings(\n",
    "    #        c,\n",
    "    #        [\n",
    "    #            [t for i,t in enumerate(thresholds.tolist()) if infs[i]==True],\n",
    "    #            [t for i,t in enumerate(tpr.tolist()) if infs[i]==True],\n",
    "    #            [t for i,t in enumerate(fpr.tolist()) if infs[i]==True]\n",
    "    #        ]\n",
    "    #    )\n",
    "        \n",
    "                       \n",
    "    return outputs(metrics, class_metrics) #, slice_class_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4c63d8-600d-424d-8e31-674aa9ec6404",
   "metadata": {},
   "source": [
    "#### Component: `overview`\n",
    "\n",
    "A lightweight Python component that:\n",
    "- inputs a list of metric artifacts\n",
    "- create a `kfp.dsl.HTML` artifact\n",
    "- creates a `kfp.dsl.Markdown` artifact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6e0cec3c-7f5d-4954-b4fa-d54111f4ffdb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@kfp.dsl.component(\n",
    "    base_image = \"python:3.11\",\n",
    "    packages_to_install = [\"pandas\", \"tabulate\"]\n",
    ")\n",
    "def overview(\n",
    "    metrics_0: kfp.dsl.Metrics,\n",
    "    metrics_1: kfp.dsl.Metrics,\n",
    "    metrics_2: kfp.dsl.Metrics,\n",
    "    metrics_3: kfp.dsl.Metrics,\n",
    "    models: list,\n",
    "    data: list\n",
    ") -> NamedTuple(\n",
    "        'output',\n",
    "        html=kfp.dsl.HTML,\n",
    "        md=kfp.dsl.Markdown\n",
    "):\n",
    "    from typing import NamedTuple\n",
    "    outputs = NamedTuple(\n",
    "            'output',\n",
    "            html=kfp.dsl.HTML,\n",
    "            md=kfp.dsl.Markdown\n",
    "    )\n",
    "    \n",
    "    # construct dataframe\n",
    "    import pandas as pd\n",
    "    metrics = [metrics_0.metadata, metrics_1.metadata, metrics_2.metadata, metrics_3.metadata]\n",
    "    records = []\n",
    "    for m, metric in enumerate(metrics):\n",
    "        records.append(\n",
    "            dict(\n",
    "                model = models[m],\n",
    "                data = data[m]\n",
    "            )|metrics[m]\n",
    "        )\n",
    "    df = pd.DataFrame(records)\n",
    " \n",
    "    import os\n",
    "    # html artifact\n",
    "    html = kfp.dsl.HTML(uri = kfp.dsl.get_uri('html.html'))\n",
    "    os.makedirs(os.path.dirname(html.path), exist_ok = True)\n",
    "    with open(html.path, 'w') as f:\n",
    "        f.write(df.to_html(index = False))\n",
    "    \n",
    "    \n",
    "    # markdown artifact\n",
    "    md = kfp.dsl.Markdown(uri = kfp.dsl.get_uri('md.md'))\n",
    "    os.makedirs(os.path.dirname(md.path), exist_ok = True)\n",
    "    with open(md.path, 'w') as f:\n",
    "        f.write(df.to_markdown(index = False))\n",
    "    \n",
    "    return outputs(html, md)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece30b47-0259-4f2e-b6c6-941d8e26aaa3",
   "metadata": {},
   "source": [
    "### Create Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ff1188e2-64b4-401d-9e04-20b6d1000b70",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipeline_name = f'{SERIES}-{EXPERIMENT}-preview'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "797488e2-ea3a-459d-acb0-7bbefdf424ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@kfp.dsl.pipeline(\n",
    "    name = pipeline_name,\n",
    "    description = 'A simple pipeline for testing',\n",
    "    pipeline_root = f'gs://{GCS_BUCKET}/{SERIES}/{EXPERIMENT}/pipeline_root'\n",
    ")\n",
    "def pipeline(\n",
    "    project_id: str,\n",
    "    bq_project: str,\n",
    "    bq_dataset: str,\n",
    "    bq_table: str\n",
    "):\n",
    "    \n",
    "    bq_source = data_source(\n",
    "        bq_project = bq_project,\n",
    "        bq_dataset = bq_dataset,\n",
    "        bq_table = bq_table\n",
    "    )\n",
    "    train_data = data_prep(\n",
    "        project_id = project_id,\n",
    "        bq_source = bq_source.output\n",
    "    )\n",
    "    model_1 = model_gb(\n",
    "        train = train_data.outputs['train'],\n",
    "        features = train_data.outputs['features']\n",
    "    )\n",
    "    model_2 = model_rf(\n",
    "        train = train_data.outputs['train'],\n",
    "        features = train_data.outputs['features']\n",
    "    )\n",
    "    metrics_1_train = metrics(\n",
    "        data = train_data.outputs['train'],\n",
    "        features = train_data.outputs['features'],\n",
    "        model = model_1.output,\n",
    "    ).set_display_name('Metrics: Training Data')\n",
    "    metrics_1_test = metrics(\n",
    "        data = train_data.outputs['test'],\n",
    "        features = train_data.outputs['features'],\n",
    "        model = model_1.output,\n",
    "    ).set_display_name('Metrics: Test Data')\n",
    "    metrics_2_train = metrics(\n",
    "        data = train_data.outputs['train'],\n",
    "        features = train_data.outputs['features'],\n",
    "        model = model_2.output,\n",
    "    ).set_display_name('Metrics: Training Data')\n",
    "    metrics_2_test = metrics(\n",
    "        data = train_data.outputs['test'],\n",
    "        features = train_data.outputs['features'],\n",
    "        model = model_2.output,\n",
    "    ).set_display_name('Metrics: Test Data')\n",
    "    \n",
    "    review = overview(\n",
    "        metrics_0 = metrics_1_train.outputs['metrics'],\n",
    "        metrics_1 = metrics_1_test.outputs['metrics'],\n",
    "        metrics_2 = metrics_2_train.outputs['metrics'],\n",
    "        metrics_3 = metrics_2_test.outputs['metrics'],\n",
    "        models = ['GB', 'GB', 'RF', 'RF'],\n",
    "        data = ['Train', 'Test', 'Train', 'Test']\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e7f30a-3bfe-46b4-86b2-b23eaafdb358",
   "metadata": {},
   "source": [
    "### Compile Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ee6aaae3-ff62-42d9-9925-30f0b3dd05dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "kfp.compiler.Compiler().compile(\n",
    "    pipeline_func = pipeline,\n",
    "    package_path = f'{DIR}/{pipeline_name}.yaml'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e1049f-0a1e-46af-a9c9-2377c9f7e899",
   "metadata": {},
   "source": [
    "### Create Pipeline Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9ea07375-b0ca-4c6d-ad92-58b94fe11c6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "parameters = dict(\n",
    "    project_id = PROJECT_ID,\n",
    "    bq_project = 'bigquery-public-data',\n",
    "    bq_dataset = 'ml_datasets',\n",
    "    bq_table = 'penguins'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "70647781-88ad-42b6-8175-1e5bedb1c0fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipeline_job = aiplatform.PipelineJob(\n",
    "    display_name = pipeline_name,\n",
    "    template_path = f\"{DIR}/{pipeline_name}.yaml\",\n",
    "    parameter_values = parameters,\n",
    "    pipeline_root = f'gs://{GCS_BUCKET}/{SERIES}/{EXPERIMENT}/pipeline_root',\n",
    "    enable_caching = None # True (enabled), False (disable), None (defer to component level caching) \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c4cf0b-12e8-4909-9211-ef454a801845",
   "metadata": {},
   "source": [
    "### Submit Pipeline Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9364f31d-562c-4c46-a3f6-4233ef547273",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/1026793852137/locations/us-central1/pipelineJobs/mlops-pipeline-pattern-modular-preview-20240801123852\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/1026793852137/locations/us-central1/pipelineJobs/mlops-pipeline-pattern-modular-preview-20240801123852')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/mlops-pipeline-pattern-modular-preview-20240801123852?project=1026793852137\n"
     ]
    }
   ],
   "source": [
    "response = pipeline_job.submit(\n",
    "    service_account = SERVICE_ACCOUNT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2599a54d-59e2-4af8-b6ac-f6bb18d60cbb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Dashboard can be viewed here:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/mlops-pipeline-pattern-modular-preview-20240801123852?project=1026793852137\n"
     ]
    }
   ],
   "source": [
    "print(f'The Dashboard can be viewed here:\\n{pipeline_job._dashboard_uri()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "321b9c0e-e9f6-43a4-8717-f5b2fa931ad6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PipelineJob projects/1026793852137/locations/us-central1/pipelineJobs/mlops-pipeline-pattern-modular-preview-20240801123852 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/1026793852137/locations/us-central1/pipelineJobs/mlops-pipeline-pattern-modular-preview-20240801123852 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/1026793852137/locations/us-central1/pipelineJobs/mlops-pipeline-pattern-modular-preview-20240801123852 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/1026793852137/locations/us-central1/pipelineJobs/mlops-pipeline-pattern-modular-preview-20240801123852 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/1026793852137/locations/us-central1/pipelineJobs/mlops-pipeline-pattern-modular-preview-20240801123852 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/1026793852137/locations/us-central1/pipelineJobs/mlops-pipeline-pattern-modular-preview-20240801123852 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/1026793852137/locations/us-central1/pipelineJobs/mlops-pipeline-pattern-modular-preview-20240801123852 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob run completed. Resource name: projects/1026793852137/locations/us-central1/pipelineJobs/mlops-pipeline-pattern-modular-preview-20240801123852\n"
     ]
    }
   ],
   "source": [
    "pipeline_job.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0241d2f6-a917-4021-89fe-6f1aae86153c",
   "metadata": {},
   "source": [
    "### Retrieve Pipeline Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4f901101-a2e1-435f-9d5e-0c8122373516",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pipeline_name</th>\n",
       "      <th>run_name</th>\n",
       "      <th>param.input:project_id</th>\n",
       "      <th>param.vertex-ai-pipelines-artifact-argument-binding</th>\n",
       "      <th>param.input:bq_project</th>\n",
       "      <th>param.input:bq_table</th>\n",
       "      <th>param.vmlmd_lineage_integration</th>\n",
       "      <th>param.input:bq_dataset</th>\n",
       "      <th>metric.precision</th>\n",
       "      <th>metric.average_precision</th>\n",
       "      <th>metric.f1</th>\n",
       "      <th>metric.accuracy</th>\n",
       "      <th>metric.recall</th>\n",
       "      <th>metric.confusionMatrix</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mlops-pipeline-pattern-modular-preview</td>\n",
       "      <td>mlops-pipeline-pattern-modular-preview-2024080...</td>\n",
       "      <td>statmike-mlops-349915</td>\n",
       "      <td>{'output:metrics-class_metrics': ['projects/10...</td>\n",
       "      <td>bigquery-public-data</td>\n",
       "      <td>penguins</td>\n",
       "      <td>{'pipeline_run_component': {'parent_task_names...</td>\n",
       "      <td>ml_datasets</td>\n",
       "      <td>0.997199</td>\n",
       "      <td>0.99994</td>\n",
       "      <td>0.996848</td>\n",
       "      <td>0.996124</td>\n",
       "      <td>0.996528</td>\n",
       "      <td>{'annotationSpecs': [{'displayName': 'Adelie P...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            pipeline_name  \\\n",
       "0  mlops-pipeline-pattern-modular-preview   \n",
       "\n",
       "                                            run_name param.input:project_id  \\\n",
       "0  mlops-pipeline-pattern-modular-preview-2024080...  statmike-mlops-349915   \n",
       "\n",
       "  param.vertex-ai-pipelines-artifact-argument-binding param.input:bq_project  \\\n",
       "0  {'output:metrics-class_metrics': ['projects/10...    bigquery-public-data   \n",
       "\n",
       "  param.input:bq_table                    param.vmlmd_lineage_integration  \\\n",
       "0             penguins  {'pipeline_run_component': {'parent_task_names...   \n",
       "\n",
       "  param.input:bq_dataset  metric.precision  metric.average_precision  \\\n",
       "0            ml_datasets          0.997199                   0.99994   \n",
       "\n",
       "   metric.f1  metric.accuracy  metric.recall  \\\n",
       "0   0.996848         0.996124       0.996528   \n",
       "\n",
       "                              metric.confusionMatrix  \n",
       "0  {'annotationSpecs': [{'displayName': 'Adelie P...  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aiplatform.get_pipeline_df(pipeline = f'{pipeline_name}')[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d54d3d4-834c-4750-9735-487c34b1f5d2",
   "metadata": {},
   "source": [
    "---\n",
    "## Manage Pipelines With Artifact Registry\n",
    "\n",
    "The YAML file created above is a complete specification of a workflow with input parameters and a staging bucket.  This could be very useful to save, share, and reuse.  This section covers using Artifact Registry do this with a combination of:\n",
    "- Kubeflow Pipelines SDK and the included [`kfp.registry.RegistryClient`](https://kubeflow-pipelines.readthedocs.io/en/latest/source/registry.html)\n",
    "- Google Cloud [Artifact Registry](https://cloud.google.com/artifact-registry/docs/overview) with native format for [Kubeflow pipeline templates](https://cloud.google.com/artifact-registry/docs/kfp)\n",
    "- [Integration with Vertex AI](https://cloud.google.com/vertex-ai/docs/pipelines/create-pipeline-template#kubeflow-pipelines-sdk-client) for creating, uploading and using pipeline templates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5154f828-c5b8-41d9-a31a-2460ac3bc86a",
   "metadata": {},
   "source": [
    "### Setup Artifact Registry For KFP Repository\n",
    "\n",
    "[Artifact registry](https://cloud.google.com/artifact-registry/docs) organizes artifacts with repositories.  Each repository contains packages and is designated to hold a partifcular format of package: Docker images, Python Packages and [others](https://cloud.google.com/artifact-registry/docs/supported-formats#package).  There is even a registry type specifically for [Kubeflow pipeline templates](https://cloud.google.com/artifact-registry/docs/kfp?hl=en) which is the focus in this notebooks workflow. It specifically stores and managed `kfp` pipelines files (compiled pipeliens are YAML files)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42f85a8-32cf-4cc6-8652-137c87bb5a3a",
   "metadata": {},
   "source": [
    "#### List All Repositories In Project/Region\n",
    "\n",
    "This may be empty if no repositories have been created for this project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "010d37f9-e08f-482b-88ba-ace8d6d3ea5a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "projects/statmike-mlops-349915/locations/us-central1/repositories/gcf-artifacts\n",
      "projects/statmike-mlops-349915/locations/us-central1/repositories/mlops\n",
      "projects/statmike-mlops-349915/locations/us-central1/repositories/statmike-mlops-349915\n",
      "projects/statmike-mlops-349915/locations/us-central1/repositories/statmike-mlops-349915-docker\n",
      "projects/statmike-mlops-349915/locations/us-central1/repositories/statmike-mlops-349915-python\n"
     ]
    }
   ],
   "source": [
    "for repo in ar_client.list_repositories(parent = f'projects/{PROJECT_ID}/locations/{REGION}'):\n",
    "    print(repo.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca782f2-0b74-4bc9-87d2-682d4bc32481",
   "metadata": {},
   "source": [
    "#### Create/Retrieve KFP Repository\n",
    "\n",
    "Create an Artifact Registry Repository to hold Docker Images created by this notebook.  First, check to see if it is already created by a previous run and retrieve it if it has.  Otherwise, create one named for this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d3eba1c7-8b78-45c6-b980-c8ed39e8c353",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    kfp_repo = ar_client.get_repository(name = f'projects/{PROJECT_ID}/locations/{REGION}/repositories/{SERIES}')\n",
    "except Exception:\n",
    "    operation = ar_client.create_repository(\n",
    "        parent = f'projects/{PROJECT_ID}/locations/{REGION}',\n",
    "        repository_id = SERIES,\n",
    "        repository = artifactregistry_v1.Repository(\n",
    "            name = SERIES,\n",
    "            format = artifactregistry_v1.Repository.Format.KFP\n",
    "        )\n",
    "    )\n",
    "    kfp_repo = operation.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "aa985395-4037-4e52-b773-ed4a90a7e984",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('projects/statmike-mlops-349915/locations/us-central1/repositories/mlops',\n",
       " 'KFP')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kfp_repo.name, kfp_repo.format_.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "94639fa8-46b8-411f-bba8-eeaa9e363ed6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "REPOSITORY = f\"https://{REGION}-{kfp_repo.format_.name}.pkg.dev/{PROJECT_ID}/{kfp_repo.name.split('/')[-1]}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d599dfaa-a04d-4c02-a148-cf4f5f86df75",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://us-central1-KFP.pkg.dev/statmike-mlops-349915/mlops'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "REPOSITORY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ad734d-fdf2-4e25-8109-4bdb66146b6e",
   "metadata": {},
   "source": [
    "---\n",
    "## Example 1: Full Pipeline Saved To Artifact Registry\n",
    "\n",
    "> For more details on using the KFP artifact registry to manage pipelines see the workflow: [Vertex AI Pipelines - Management](./Vertex%20AI%20Pipelines%20-%20Management.ipynb)\n",
    "\n",
    "This example upload a complete pipeline to the registry and then create runs directly referencing the repository."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c75cc53-edfe-4268-ac70-6bfab1b85a6a",
   "metadata": {},
   "source": [
    "### Upload Pipeline To Registry\n",
    "\n",
    "Loading the pipelines compiled YAML file can be done in several ways:\n",
    "- Directly in the [Console For Vertex AI Pipelines](https://cloud.google.com/vertex-ai/docs/pipelines/create-pipeline-template#upload-the-template)\n",
    "- With code using the KFP SDK [`kfp.registry`](https://kubeflow-pipelines.readthedocs.io/en/latest/source/registry.html)\n",
    "\n",
    "This section uses code to manage the upload."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5623db-dfa3-4629-8876-a9a783ca3189",
   "metadata": {},
   "source": [
    "#### Setup KFP Registry Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b56354ac-b3d8-4193-8e07-a9555af5c76c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import kfp.registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "b54beacd-add4-4382-b995-c51a73df2f65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "token = !gcloud auth application-default print-access-token\n",
    "kfp_registry = kfp.registry.RegistryClient(\n",
    "    host = REPOSITORY,\n",
    "    auth = kfp.registry.ApiAuth(token[0])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cdfe839-d87e-4d77-9a7f-c54bca0129a0",
   "metadata": {},
   "source": [
    "#### Upload YAML File To Registry\n",
    "\n",
    "Like other repository types there are two types of references for artifacts: tags and version.  The versions are managed by the repository and the naming of versions is possible by supplying tags.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fb0eb209-b45d-4fbf-a574-f38a6dee0619",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mlops-pipeline-pattern-modular-preview.yaml\n"
     ]
    }
   ],
   "source": [
    "!ls {DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "aaff0df2-bbce-4c81-82ce-ce50c0c95355",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mlops-pipeline-pattern-modular-preview'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9d7fced9-e571-413f-958f-25df161fe87b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "template, version = kfp_registry.upload_pipeline(\n",
    "    file_name = f\"{DIR}/{pipeline_name}.yaml\",\n",
    "    tags = ['v1', 'new'],\n",
    "    extra_headers = dict(description = 'Full Pipeline Template', note = 'This is an example for the full pipeline.')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8441b29e-4a6d-42f8-9466-c0f50fc6fbce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('mlops-pipeline-pattern-modular-preview',\n",
       " 'sha256:320595151ec9f4e0e884a61cb2093634c717871dcb272cfe0705e44dd40d0d58')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template, version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4de0eb7-5101-412e-b790-508ae22bcc8e",
   "metadata": {},
   "source": [
    "### Pipeline Runs From The Registry\n",
    "\n",
    "The pipeline could be download to local first but it is also possible to directly reference the pipeline in the repository when creating a run.  More methods, including no code runs from the console, are covered in the workflow: [Vertex AI Pipelines - Management](./Vertex%20AI%20Pipelines%20-%20Management.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af0ebed-6975-45bb-800d-3a82afafca56",
   "metadata": {},
   "source": [
    "#### Remote: Vertex AI SDK\n",
    "\n",
    "This is actually idential to the normal use of the Verex AI Pipeline SDK and just the `template_path` needs updating to point to the version in the artifact registry.\n",
    "- [`aiplatform.PipelineJob()`](https://cloud.google.com/python/docs/reference/aiplatform/latest/google.cloud.aiplatform.PipelineJob)\n",
    "\n",
    "The `template_path` can refer directly to a version (`@<version>`) or any tag ('/<tag>'):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "661c30fd-b74a-41fd-837b-37c263f4ea12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "parameters = dict(\n",
    "    project_id = PROJECT_ID,\n",
    "    bq_project = 'bigquery-public-data',\n",
    "    bq_dataset = 'ml_datasets',\n",
    "    bq_table = 'penguins'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "29853ba2-e025-4647-bb53-628a48f80d07",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipeline_job = aiplatform.PipelineJob(\n",
    "    display_name = pipeline_name,\n",
    "    template_path = f\"{REPOSITORY.lower()}/{pipeline_name}/{version}\",\n",
    "    parameter_values = parameters,\n",
    "    pipeline_root = f'gs://{GCS_BUCKET}/{SERIES}/{EXPERIMENT}/pipeline_root',\n",
    "    enable_caching = False # True (enabled), False (disable), None (defer to component level caching) \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "cb29a125-b12d-46de-9735-dd0f678e1e0e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/1026793852137/locations/us-central1/pipelineJobs/mlops-pipeline-pattern-modular-preview-20240801142330\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/1026793852137/locations/us-central1/pipelineJobs/mlops-pipeline-pattern-modular-preview-20240801142330')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/mlops-pipeline-pattern-modular-preview-20240801142330?project=1026793852137\n"
     ]
    }
   ],
   "source": [
    "response = pipeline_job.submit(\n",
    "    service_account = SERVICE_ACCOUNT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ea4ce15d-55a1-46d2-b4d2-105adb840bc2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PipelineJob projects/1026793852137/locations/us-central1/pipelineJobs/mlops-pipeline-pattern-modular-preview-20240801142330 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/1026793852137/locations/us-central1/pipelineJobs/mlops-pipeline-pattern-modular-preview-20240801142330 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/1026793852137/locations/us-central1/pipelineJobs/mlops-pipeline-pattern-modular-preview-20240801142330 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/1026793852137/locations/us-central1/pipelineJobs/mlops-pipeline-pattern-modular-preview-20240801142330 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/1026793852137/locations/us-central1/pipelineJobs/mlops-pipeline-pattern-modular-preview-20240801142330 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/1026793852137/locations/us-central1/pipelineJobs/mlops-pipeline-pattern-modular-preview-20240801142330 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/1026793852137/locations/us-central1/pipelineJobs/mlops-pipeline-pattern-modular-preview-20240801142330 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob run completed. Resource name: projects/1026793852137/locations/us-central1/pipelineJobs/mlops-pipeline-pattern-modular-preview-20240801142330\n"
     ]
    }
   ],
   "source": [
    "pipeline_job.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f9800a07-93b7-4121-a5dd-4536baca80dc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pipeline_name</th>\n",
       "      <th>run_name</th>\n",
       "      <th>param.input:bq_project</th>\n",
       "      <th>param.input:bq_table</th>\n",
       "      <th>param.vmlmd_lineage_integration</th>\n",
       "      <th>param.input:project_id</th>\n",
       "      <th>param.input:bq_dataset</th>\n",
       "      <th>param.vertex-ai-pipelines-artifact-argument-binding</th>\n",
       "      <th>metric.f1</th>\n",
       "      <th>metric.average_precision</th>\n",
       "      <th>metric.precision</th>\n",
       "      <th>metric.accuracy</th>\n",
       "      <th>metric.recall</th>\n",
       "      <th>metric.confusionMatrix</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mlops-pipeline-pattern-modular-preview</td>\n",
       "      <td>mlops-pipeline-pattern-modular-preview-2024080...</td>\n",
       "      <td>bigquery-public-data</td>\n",
       "      <td>penguins</td>\n",
       "      <td>{'pipeline_template_component': {'version_sha2...</td>\n",
       "      <td>statmike-mlops-349915</td>\n",
       "      <td>ml_datasets</td>\n",
       "      <td>{'output:metrics-class_metrics': ['projects/10...</td>\n",
       "      <td>0.986425</td>\n",
       "      <td>0.999493</td>\n",
       "      <td>0.990991</td>\n",
       "      <td>0.988372</td>\n",
       "      <td>0.982456</td>\n",
       "      <td>{'rows': [{'row': [114.0, 2.0, 0.0]}, {'row': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mlops-pipeline-pattern-modular-preview</td>\n",
       "      <td>mlops-pipeline-pattern-modular-preview-2024080...</td>\n",
       "      <td>bigquery-public-data</td>\n",
       "      <td>penguins</td>\n",
       "      <td>{'pipeline_template_component': {'template_id'...</td>\n",
       "      <td>statmike-mlops-349915</td>\n",
       "      <td>ml_datasets</td>\n",
       "      <td>{'output:metrics-2-class_metrics': ['projects/...</td>\n",
       "      <td>0.964231</td>\n",
       "      <td>0.995937</td>\n",
       "      <td>0.966460</td>\n",
       "      <td>0.965116</td>\n",
       "      <td>0.962418</td>\n",
       "      <td>{'rows': [{'row': [33.0, 1.0, 0.0]}, {'row': [...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mlops-pipeline-pattern-modular-preview</td>\n",
       "      <td>mlops-pipeline-pattern-modular-preview-2024080...</td>\n",
       "      <td>bigquery-public-data</td>\n",
       "      <td>penguins</td>\n",
       "      <td>{'pipeline_run_component': {'location_id': 'us...</td>\n",
       "      <td>statmike-mlops-349915</td>\n",
       "      <td>ml_datasets</td>\n",
       "      <td>{'output:metrics-3-class_metrics': ['projects/...</td>\n",
       "      <td>0.996848</td>\n",
       "      <td>0.999940</td>\n",
       "      <td>0.997199</td>\n",
       "      <td>0.996124</td>\n",
       "      <td>0.996528</td>\n",
       "      <td>{'annotationSpecs': [{'displayName': 'Adelie P...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            pipeline_name  \\\n",
       "0  mlops-pipeline-pattern-modular-preview   \n",
       "1  mlops-pipeline-pattern-modular-preview   \n",
       "2  mlops-pipeline-pattern-modular-preview   \n",
       "\n",
       "                                            run_name param.input:bq_project  \\\n",
       "0  mlops-pipeline-pattern-modular-preview-2024080...   bigquery-public-data   \n",
       "1  mlops-pipeline-pattern-modular-preview-2024080...   bigquery-public-data   \n",
       "2  mlops-pipeline-pattern-modular-preview-2024080...   bigquery-public-data   \n",
       "\n",
       "  param.input:bq_table                    param.vmlmd_lineage_integration  \\\n",
       "0             penguins  {'pipeline_template_component': {'version_sha2...   \n",
       "1             penguins  {'pipeline_template_component': {'template_id'...   \n",
       "2             penguins  {'pipeline_run_component': {'location_id': 'us...   \n",
       "\n",
       "  param.input:project_id param.input:bq_dataset  \\\n",
       "0  statmike-mlops-349915            ml_datasets   \n",
       "1  statmike-mlops-349915            ml_datasets   \n",
       "2  statmike-mlops-349915            ml_datasets   \n",
       "\n",
       "  param.vertex-ai-pipelines-artifact-argument-binding  metric.f1  \\\n",
       "0  {'output:metrics-class_metrics': ['projects/10...    0.986425   \n",
       "1  {'output:metrics-2-class_metrics': ['projects/...    0.964231   \n",
       "2  {'output:metrics-3-class_metrics': ['projects/...    0.996848   \n",
       "\n",
       "   metric.average_precision  metric.precision  metric.accuracy  metric.recall  \\\n",
       "0                  0.999493          0.990991         0.988372       0.982456   \n",
       "1                  0.995937          0.966460         0.965116       0.962418   \n",
       "2                  0.999940          0.997199         0.996124       0.996528   \n",
       "\n",
       "                              metric.confusionMatrix  \n",
       "0  {'rows': [{'row': [114.0, 2.0, 0.0]}, {'row': ...  \n",
       "1  {'rows': [{'row': [33.0, 1.0, 0.0]}, {'row': [...  \n",
       "2  {'annotationSpecs': [{'displayName': 'Adelie P...  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aiplatform.get_pipeline_df(pipeline = f'{pipeline_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5415334b-44f5-4ee5-8428-53f1f6b4f7ed",
   "metadata": {},
   "source": [
    "---\n",
    "## Example 2: Local Pipeline From Modular Components Saved in Artifact Registry\n",
    "\n",
    "Components can be compiled and the resulting YAML can be treated as source and recalled easily from local directories as files, from URLs, and from strings. \n",
    "\n",
    "> Learn more about modular components and their structure in the workflow: [Vertex AI Pipelines - Management](./Vertex%20AI%20Pipelines%20-%20Management.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6a66d2-ce60-4f47-9c97-4325aba67ff8",
   "metadata": {},
   "source": [
    "### Compile Components\n",
    "\n",
    "Just as pipelines can be compiled into YAML, so can components.  The following does this compilation for both components created above.\n",
    "- [`kfp.compiler`](https://kubeflow-pipelines.readthedocs.io/en/latest/source/compiler.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8a6785f9-9ed0-4028-a457-867dc28c91fd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "kfp.dsl.python_component.PythonComponent"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "03e32acb-0c1e-4d91-89a5-041c9e00cccb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "kfp.compiler.Compiler().compile(\n",
    "    data_source,\n",
    "    package_path = f'{DIR}/component_data_source.yaml'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2e41dba7-bf42-44cf-b992-d93c63be7331",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "component_data_source.yaml  mlops-pipeline-pattern-modular-preview.yaml\n"
     ]
    }
   ],
   "source": [
    "!ls {DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e954df-12f5-4fab-9cb6-777b18de1267",
   "metadata": {},
   "source": [
    "### Loading A Compiled Component\n",
    "\n",
    "Managing (saving, reusing, and sharing) a compiled component file is completed by being able to load the component directly for future pipelines.  This is accomplished using the [`kfp.components`](https://kubeflow-pipelines.readthedocs.io/en/latest/source/components.html) module which offers three functions for loading components compiled as YAML from either a file, a url or directly from a string:\n",
    "- [`kfp.components.load_component_from_file`](https://kubeflow-pipelines.readthedocs.io/en/latest/source/components.html#kfp.components.load_component_from_file)\n",
    "- [`kfp.components.load_component_from_url`](https://kubeflow-pipelines.readthedocs.io/en/latest/source/components.html#kfp.components.load_component_from_url)\n",
    "- [`kfp.components.load_component_from_text`](https://kubeflow-pipelines.readthedocs.io/en/latest/source/components.html#kfp.components.load_component_from_text)\n",
    "\n",
    "The following cell load the `example_parameters` component from file and creates a new local component named `imported_example_parameters`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3d4b6ce8-6d74-44a5-8201-306bed149c3e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import_data_source = kfp.components.load_component_from_file(f'{DIR}/component_data_source.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f95c6041-e5e5-48ae-a100-b5018f271ac4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "kfp.dsl.yaml_component.YamlComponent"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(import_data_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a57cf124-5b04-405e-bd43-af0d762f3e0d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "kfp.dsl.python_component.PythonComponent"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data_source)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c18f773-3351-41ac-a78e-1d55603e88b3",
   "metadata": {},
   "source": [
    "### Save A Component To Artifact Registry\n",
    "\n",
    "The compiled YAML file could also be saved in the KFP artifact registry similar to a complete pipeline as show before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "80afd889-ad1a-4ae0-95c0-c68430c5155e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "template, version = kfp_registry.upload_pipeline(\n",
    "    file_name = f\"{DIR}/component_data_source.yaml\",\n",
    "    tags = ['v1', 'new'],\n",
    "    extra_headers = dict(description = 'A single component: data_source', note = 'This is an example for a single component.')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3faba4cc-7ba7-47fc-beb1-f03c27a8d91e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('data-source',\n",
       " 'sha256:e0ea97bcbe73b2b2f8b50cd1633f863562922eb38ff762e6957191bc7996d1a1')"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template, version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d0cdb5-237c-4377-bae6-ba6e2c135920",
   "metadata": {},
   "source": [
    "### Download And Import Component From Artifact Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "bb8dbe40-0ae6-4122-86e6-d8a66c0edc86",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'temp/mlops-pipeline-pattern-modular/downloaded_component_data_source.yaml'"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kfp_registry.download_pipeline(\n",
    "    package_name = template,\n",
    "    version = version,\n",
    "    file_name = f'{DIR}/downloaded_component_data_source.yaml'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "12ca9603-ba81-4cf9-99c2-f74f8b7a47d2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "component_data_source.yaml\n",
      "downloaded_component_data_source.yaml\n",
      "mlops-pipeline-pattern-modular-preview.yaml\n"
     ]
    }
   ],
   "source": [
    "!ls {DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "9ce4be2b-a527-489b-a945-e1cc7ce91bf5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import_data_source = kfp.components.load_component_from_file(f'{DIR}/downloaded_component_data_source.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "39b8b9fd-1044-4b68-bbd3-07600cbee45d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "kfp.dsl.yaml_component.YamlComponent"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(import_data_source)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d881d5-1e01-4ccb-94a4-32b404b57012",
   "metadata": {},
   "source": [
    "### Create \n",
    "\n",
    "The same pipeline specification as above but using the downloaded and imported component from artifact registry: `data_source`, now named `import_data_source`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "13fda239-6fe5-481d-ba80-af271d103e1f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipeline_name = f'{SERIES}-{EXPERIMENT}-example-2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "b800cb00-6f54-43e9-b92a-c4c8a4fc07c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@kfp.dsl.pipeline(\n",
    "    name = pipeline_name,\n",
    "    description = 'A simple pipeline for testing',\n",
    "    pipeline_root = f'gs://{GCS_BUCKET}/{SERIES}/{EXPERIMENT}/pipeline_root'\n",
    ")\n",
    "def pipeline(\n",
    "    project_id: str,\n",
    "    bq_project: str,\n",
    "    bq_dataset: str,\n",
    "    bq_table: str\n",
    "):\n",
    "    # use the downloaded/imported component for data_source here\n",
    "    bq_source = import_data_source(\n",
    "        bq_project = bq_project,\n",
    "        bq_dataset = bq_dataset,\n",
    "        bq_table = bq_table\n",
    "    )\n",
    "    train_data = data_prep(\n",
    "        project_id = project_id,\n",
    "        bq_source = bq_source.output\n",
    "    )\n",
    "    model_1 = model_gb(\n",
    "        train = train_data.outputs['train'],\n",
    "        features = train_data.outputs['features']\n",
    "    )\n",
    "    model_2 = model_rf(\n",
    "        train = train_data.outputs['train'],\n",
    "        features = train_data.outputs['features']\n",
    "    )\n",
    "    metrics_1_train = metrics(\n",
    "        data = train_data.outputs['train'],\n",
    "        features = train_data.outputs['features'],\n",
    "        model = model_1.output,\n",
    "    ).set_display_name('Metrics: Training Data')\n",
    "    metrics_1_test = metrics(\n",
    "        data = train_data.outputs['test'],\n",
    "        features = train_data.outputs['features'],\n",
    "        model = model_1.output,\n",
    "    ).set_display_name('Metrics: Test Data')\n",
    "    metrics_2_train = metrics(\n",
    "        data = train_data.outputs['train'],\n",
    "        features = train_data.outputs['features'],\n",
    "        model = model_2.output,\n",
    "    ).set_display_name('Metrics: Training Data')\n",
    "    metrics_2_test = metrics(\n",
    "        data = train_data.outputs['test'],\n",
    "        features = train_data.outputs['features'],\n",
    "        model = model_2.output,\n",
    "    ).set_display_name('Metrics: Test Data')\n",
    "    \n",
    "    review = overview(\n",
    "        metrics_0 = metrics_1_train.outputs['metrics'],\n",
    "        metrics_1 = metrics_1_test.outputs['metrics'],\n",
    "        metrics_2 = metrics_2_train.outputs['metrics'],\n",
    "        metrics_3 = metrics_2_test.outputs['metrics'],\n",
    "        models = ['GB', 'GB', 'RF', 'RF'],\n",
    "        data = ['Train', 'Test', 'Train', 'Test']\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42d2859-4300-40c8-90ff-74d7bad5f0c2",
   "metadata": {},
   "source": [
    "### Compile Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "81fbf80c-41c0-4127-a6e4-b529fdceb61f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "kfp.compiler.Compiler().compile(\n",
    "    pipeline_func = pipeline,\n",
    "    package_path = f'{DIR}/{pipeline_name}.yaml'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2450055-3394-43eb-ab52-047e7c0a4c72",
   "metadata": {},
   "source": [
    "### Create Pipeline Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "0f860ed8-4db1-4016-a563-7365a74cb698",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "parameters = dict(\n",
    "    project_id = PROJECT_ID,\n",
    "    bq_project = 'bigquery-public-data',\n",
    "    bq_dataset = 'ml_datasets',\n",
    "    bq_table = 'penguins'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "e7ed76fd-dd4e-43a5-9f92-aba24d638d27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipeline_job = aiplatform.PipelineJob(\n",
    "    display_name = pipeline_name,\n",
    "    template_path = f\"{DIR}/{pipeline_name}.yaml\",\n",
    "    parameter_values = parameters,\n",
    "    pipeline_root = f'gs://{GCS_BUCKET}/{SERIES}/{EXPERIMENT}/pipeline_root',\n",
    "    enable_caching = None # True (enabled), False (disable), None (defer to component level caching) \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b64a659-0fe0-41c8-8cd0-85769645f82e",
   "metadata": {},
   "source": [
    "### Submit Pipeline Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "2ca2eee0-0673-4260-aa6e-d3e4b6ba6008",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/1026793852137/locations/us-central1/pipelineJobs/mlops-pipeline-pattern-modular-example-2-20240801170641\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/1026793852137/locations/us-central1/pipelineJobs/mlops-pipeline-pattern-modular-example-2-20240801170641')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/mlops-pipeline-pattern-modular-example-2-20240801170641?project=1026793852137\n"
     ]
    }
   ],
   "source": [
    "response = pipeline_job.submit(\n",
    "    service_account = SERVICE_ACCOUNT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "beb3a1c2-2e7c-43e3-a9a7-9b0f2c2190f8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Dashboard can be viewed here:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/mlops-pipeline-pattern-modular-example-2-20240801170641?project=1026793852137\n"
     ]
    }
   ],
   "source": [
    "print(f'The Dashboard can be viewed here:\\n{pipeline_job._dashboard_uri()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "d0c71085-be36-48ec-bd13-699ddf726808",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PipelineJob projects/1026793852137/locations/us-central1/pipelineJobs/mlops-pipeline-pattern-modular-example-2-20240801170641 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/1026793852137/locations/us-central1/pipelineJobs/mlops-pipeline-pattern-modular-example-2-20240801170641 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/1026793852137/locations/us-central1/pipelineJobs/mlops-pipeline-pattern-modular-example-2-20240801170641 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/1026793852137/locations/us-central1/pipelineJobs/mlops-pipeline-pattern-modular-example-2-20240801170641 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/1026793852137/locations/us-central1/pipelineJobs/mlops-pipeline-pattern-modular-example-2-20240801170641 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/1026793852137/locations/us-central1/pipelineJobs/mlops-pipeline-pattern-modular-example-2-20240801170641 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/1026793852137/locations/us-central1/pipelineJobs/mlops-pipeline-pattern-modular-example-2-20240801170641 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob run completed. Resource name: projects/1026793852137/locations/us-central1/pipelineJobs/mlops-pipeline-pattern-modular-example-2-20240801170641\n"
     ]
    }
   ],
   "source": [
    "pipeline_job.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f108e5e-d1a4-41a0-8056-a4c85665adf4",
   "metadata": {},
   "source": [
    "### Retrieve Pipeline Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "327f6950-3a79-4f96-9679-ed8f226a4b71",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pipeline_name</th>\n",
       "      <th>run_name</th>\n",
       "      <th>param.input:bq_table</th>\n",
       "      <th>param.input:bq_project</th>\n",
       "      <th>param.input:project_id</th>\n",
       "      <th>param.vertex-ai-pipelines-artifact-argument-binding</th>\n",
       "      <th>param.vmlmd_lineage_integration</th>\n",
       "      <th>param.input:bq_dataset</th>\n",
       "      <th>metric.precision</th>\n",
       "      <th>metric.f1</th>\n",
       "      <th>metric.accuracy</th>\n",
       "      <th>metric.average_precision</th>\n",
       "      <th>metric.recall</th>\n",
       "      <th>metric.confusionMatrix</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mlops-pipeline-pattern-modular-example-2</td>\n",
       "      <td>mlops-pipeline-pattern-modular-example-2-20240...</td>\n",
       "      <td>penguins</td>\n",
       "      <td>bigquery-public-data</td>\n",
       "      <td>statmike-mlops-349915</td>\n",
       "      <td>{'output:metrics-3-class_metrics': ['projects/...</td>\n",
       "      <td>{'pipeline_run_component': {'location_id': 'us...</td>\n",
       "      <td>ml_datasets</td>\n",
       "      <td>0.975334</td>\n",
       "      <td>0.973245</td>\n",
       "      <td>0.976744</td>\n",
       "      <td>0.997809</td>\n",
       "      <td>0.971285</td>\n",
       "      <td>{'rows': [{'row': [39.0, 0.0, 0.0]}, {'row': [...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              pipeline_name  \\\n",
       "0  mlops-pipeline-pattern-modular-example-2   \n",
       "\n",
       "                                            run_name param.input:bq_table  \\\n",
       "0  mlops-pipeline-pattern-modular-example-2-20240...             penguins   \n",
       "\n",
       "  param.input:bq_project param.input:project_id  \\\n",
       "0   bigquery-public-data  statmike-mlops-349915   \n",
       "\n",
       "  param.vertex-ai-pipelines-artifact-argument-binding  \\\n",
       "0  {'output:metrics-3-class_metrics': ['projects/...    \n",
       "\n",
       "                     param.vmlmd_lineage_integration param.input:bq_dataset  \\\n",
       "0  {'pipeline_run_component': {'location_id': 'us...            ml_datasets   \n",
       "\n",
       "   metric.precision  metric.f1  metric.accuracy  metric.average_precision  \\\n",
       "0          0.975334   0.973245         0.976744                  0.997809   \n",
       "\n",
       "   metric.recall                             metric.confusionMatrix  \n",
       "0       0.971285  {'rows': [{'row': [39.0, 0.0, 0.0]}, {'row': [...  "
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aiplatform.get_pipeline_df(pipeline = f'{pipeline_name}')[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9202d1b7-94d8-4b7f-9876-f18bb0777d41",
   "metadata": {},
   "source": [
    "---\n",
    "## Example 3: Pipelines As Components\n",
    "\n",
    "This example will combine several components into a pipeline.  Then it will save this pipelines and recall it (download and import) as a component is a new pipeline.  The new pipeline will be saved to artifact registry and used in a pipeline run."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60725a04-8eff-4fa6-9630-046bb965c24c",
   "metadata": {},
   "source": [
    "### Pipeline: Data Preparation\n",
    "\n",
    "This pipeline will combine the `data_source` and `data_prep` component portion of the pipeline into a new pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "e8dc0c2d-c529-4e8c-ab78-ca159fdfe5a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipeline_name = f'{SERIES}-{EXPERIMENT}-example-3-data-preparation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "eb7c15ec-5fd4-4bb6-8c05-a0b85ae4f5b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@kfp.dsl.pipeline(\n",
    "    name = pipeline_name,\n",
    "    description = 'A simple pipeline for testing',\n",
    "    pipeline_root = f'gs://{GCS_BUCKET}/{SERIES}/{EXPERIMENT}/pipeline_root'\n",
    ")\n",
    "def pipeline(\n",
    "    project_id: str,\n",
    "    bq_project: str,\n",
    "    bq_dataset: str,\n",
    "    bq_table: str\n",
    ") -> NamedTuple(\n",
    "        'output',\n",
    "        train=kfp.dsl.Dataset,\n",
    "        test=kfp.dsl.Dataset,\n",
    "        features=kfp.dsl.Artifact\n",
    "):\n",
    "    \n",
    "    # use the downloaded/imported component for data_source here\n",
    "    bq_source = import_data_source(\n",
    "        bq_project = bq_project,\n",
    "        bq_dataset = bq_dataset,\n",
    "        bq_table = bq_table\n",
    "    )\n",
    "    train_data = data_prep(\n",
    "        project_id = project_id,\n",
    "        bq_source = bq_source.output\n",
    "    )\n",
    "    \n",
    "\n",
    "    from typing import NamedTuple\n",
    "    outputs = NamedTuple(\n",
    "            'output',\n",
    "            train=kfp.dsl.Dataset,\n",
    "            test=kfp.dsl.Dataset,\n",
    "            features=kfp.dsl.Artifact\n",
    "    )\n",
    "    \n",
    "    return outputs(\n",
    "        train_data.outputs['train'],\n",
    "        train_data.outputs['test'],\n",
    "        train_data.outputs['features']\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959908ef-4408-435f-be6c-4376af61f994",
   "metadata": {},
   "source": [
    "#### Compile Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "b9a768ee-2c6a-4fd4-927b-be97ff941702",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "kfp.compiler.Compiler().compile(\n",
    "    pipeline_func = pipeline,\n",
    "    package_path = f'{DIR}/{pipeline_name}.yaml'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a789d4a-1194-459c-8641-1d14d85c2b85",
   "metadata": {},
   "source": [
    "#### Upload Pipeline To Registry\n",
    "\n",
    "Like other repository types there are two types of references for artifacts: tags and version.  The versions are managed by the repository and the naming of versions is possible by supplying tags.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "ddeba253-305e-4942-8cae-e58daffe4460",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "component_data_source.yaml\n",
      "downloaded_component_data_source.yaml\n",
      "mlops-pipeline-pattern-modular-example-2.yaml\n",
      "mlops-pipeline-pattern-modular-example-3-data-preparation.yaml\n",
      "mlops-pipeline-pattern-modular-example-3-data_preparation.yaml\n",
      "mlops-pipeline-pattern-modular-preview.yaml\n"
     ]
    }
   ],
   "source": [
    "!ls {DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "0ed73976-795a-4c28-a0e4-63d658af9137",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mlops-pipeline-pattern-modular-example-3-data-preparation'"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "7bde60a6-6b56-4213-ba4f-6f8f9de027bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "template, version = kfp_registry.upload_pipeline(\n",
    "    file_name = f\"{DIR}/{pipeline_name}.yaml\",\n",
    "    tags = ['v1', 'new'],\n",
    "    extra_headers = dict(description = 'Data Prep Pipeline Template', note = 'This is an example for the data prep pipeline.')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "c67ed685-f29c-40be-befd-c5049da9b964",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('mlops-pipeline-pattern-modular-example-3-data-preparation',\n",
       " 'sha256:b36f6b0b6dc7df584a669f600ef36dacbda23ff23244216b8aa1700691df4a0f')"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template, version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a58361-505a-4d89-846f-561732e69dfe",
   "metadata": {},
   "source": [
    "#### Pipeline Runs From The Registry\n",
    "\n",
    "The pipeline could be download to local first but it is also possible to directly reference the pipeline in the repository when creating a run.  More methods, including no code runs from the console, are covered in the workflow: [Vertex AI Pipelines - Management](./Vertex%20AI%20Pipelines%20-%20Management.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "225dbbc0-5971-44bb-906c-fddf5cb685b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "parameters = dict(\n",
    "    project_id = PROJECT_ID,\n",
    "    bq_project = 'bigquery-public-data',\n",
    "    bq_dataset = 'ml_datasets',\n",
    "    bq_table = 'penguins'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "5203e1ce-839e-43d8-a240-b48b08d8c70e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipeline_job = aiplatform.PipelineJob(\n",
    "    display_name = pipeline_name,\n",
    "    template_path = f\"{REPOSITORY.lower()}/{pipeline_name}/{version}\",\n",
    "    parameter_values = parameters,\n",
    "    pipeline_root = f'gs://{GCS_BUCKET}/{SERIES}/{EXPERIMENT}/pipeline_root',\n",
    "    enable_caching = False # True (enabled), False (disable), None (defer to component level caching) \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "9b88a874-f0cf-46e3-8ff8-8453b5baf2bf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/1026793852137/locations/us-central1/pipelineJobs/mlops-pipeline-pattern-modular-example-3-data-preparation-20240801173254\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/1026793852137/locations/us-central1/pipelineJobs/mlops-pipeline-pattern-modular-example-3-data-preparation-20240801173254')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/mlops-pipeline-pattern-modular-example-3-data-preparation-20240801173254?project=1026793852137\n"
     ]
    }
   ],
   "source": [
    "response = pipeline_job.submit(\n",
    "    service_account = SERVICE_ACCOUNT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "9fbf7206-368c-40f4-b617-7eabec6d9f6f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PipelineJob projects/1026793852137/locations/us-central1/pipelineJobs/mlops-pipeline-pattern-modular-example-3-data-preparation-20240801173254 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/1026793852137/locations/us-central1/pipelineJobs/mlops-pipeline-pattern-modular-example-3-data-preparation-20240801173254 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/1026793852137/locations/us-central1/pipelineJobs/mlops-pipeline-pattern-modular-example-3-data-preparation-20240801173254 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/1026793852137/locations/us-central1/pipelineJobs/mlops-pipeline-pattern-modular-example-3-data-preparation-20240801173254 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/1026793852137/locations/us-central1/pipelineJobs/mlops-pipeline-pattern-modular-example-3-data-preparation-20240801173254 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob run completed. Resource name: projects/1026793852137/locations/us-central1/pipelineJobs/mlops-pipeline-pattern-modular-example-3-data-preparation-20240801173254\n"
     ]
    }
   ],
   "source": [
    "pipeline_job.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "9833e964-cb2b-46a5-81ab-b11f03341f8c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pipeline_name</th>\n",
       "      <th>run_name</th>\n",
       "      <th>param.input:bq_project</th>\n",
       "      <th>param.vertex-ai-pipelines-artifact-argument-binding</th>\n",
       "      <th>param.input:bq_table</th>\n",
       "      <th>param.input:bq_dataset</th>\n",
       "      <th>param.input:project_id</th>\n",
       "      <th>param.vmlmd_lineage_integration</th>\n",
       "      <th>metric.filename</th>\n",
       "      <th>metric.samples</th>\n",
       "      <th>metric.train_n</th>\n",
       "      <th>metric.label_col</th>\n",
       "      <th>metric.features</th>\n",
       "      <th>metric.label_values</th>\n",
       "      <th>metric.test_n</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mlops-pipeline-pattern-modular-example-3-data-...</td>\n",
       "      <td>mlops-pipeline-pattern-modular-example-3-data-...</td>\n",
       "      <td>bigquery-public-data</td>\n",
       "      <td>{'output:train': ['projects/1026793852137/loca...</td>\n",
       "      <td>penguins</td>\n",
       "      <td>ml_datasets</td>\n",
       "      <td>statmike-mlops-349915</td>\n",
       "      <td>{'pipeline_template_component': {'task_name': ...</td>\n",
       "      <td>data.txt</td>\n",
       "      <td>258.0</td>\n",
       "      <td>258.0</td>\n",
       "      <td>species</td>\n",
       "      <td>[island, culmen_length_mm, culmen_depth_mm, fl...</td>\n",
       "      <td>[Gentoo penguin (Pygoscelis papua), Adelie Pen...</td>\n",
       "      <td>86.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       pipeline_name  \\\n",
       "0  mlops-pipeline-pattern-modular-example-3-data-...   \n",
       "\n",
       "                                            run_name param.input:bq_project  \\\n",
       "0  mlops-pipeline-pattern-modular-example-3-data-...   bigquery-public-data   \n",
       "\n",
       "  param.vertex-ai-pipelines-artifact-argument-binding param.input:bq_table  \\\n",
       "0  {'output:train': ['projects/1026793852137/loca...              penguins   \n",
       "\n",
       "  param.input:bq_dataset param.input:project_id  \\\n",
       "0            ml_datasets  statmike-mlops-349915   \n",
       "\n",
       "                     param.vmlmd_lineage_integration metric.filename  \\\n",
       "0  {'pipeline_template_component': {'task_name': ...        data.txt   \n",
       "\n",
       "   metric.samples  metric.train_n metric.label_col  \\\n",
       "0           258.0           258.0          species   \n",
       "\n",
       "                                     metric.features  \\\n",
       "0  [island, culmen_length_mm, culmen_depth_mm, fl...   \n",
       "\n",
       "                                 metric.label_values  metric.test_n  \n",
       "0  [Gentoo penguin (Pygoscelis papua), Adelie Pen...           86.0  "
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aiplatform.get_pipeline_df(pipeline = f'{pipeline_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b787766c-6431-405c-8fbf-4c1a4245c745",
   "metadata": {},
   "source": [
    "### Pipeline: Full Pipeline With Data Prep Pipeline as Component\n",
    "\n",
    "Reconstruct the full pipeline using the downloaded data prep pipeline as a component"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808beac0-ad33-48eb-867b-5e4794c49707",
   "metadata": {},
   "source": [
    "#### Download and Import Data Prep Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "0c076177-9045-408e-8375-cc01cd35b4cb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'temp/mlops-pipeline-pattern-modular/downloaded_data_prep_pipeline.yaml'"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kfp_registry.download_pipeline(\n",
    "    package_name = template,\n",
    "    version = version,\n",
    "    file_name = f'{DIR}/downloaded_data_prep_pipeline.yaml'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "a3c15e06-5236-4148-b40d-bf85c688e5bc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "component_data_source.yaml\n",
      "downloaded_component_data_source.yaml\n",
      "downloaded_data_prep_pipeline.yaml\n",
      "mlops-pipeline-pattern-modular-example-2.yaml\n",
      "mlops-pipeline-pattern-modular-example-3-data-preparation.yaml\n",
      "mlops-pipeline-pattern-modular-example-3-data_preparation.yaml\n",
      "mlops-pipeline-pattern-modular-preview.yaml\n"
     ]
    }
   ],
   "source": [
    "!ls {DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "1af2931f-1220-4a00-b19f-87978e05126b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_prep_pipeline = kfp.components.load_component_from_file(f'{DIR}/downloaded_data_prep_pipeline.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "6066278c-b1a8-406c-b913-666778db2bc9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "kfp.dsl.yaml_component.YamlComponent"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data_prep_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8c5313-2f5a-4351-a307-2ce5d62de578",
   "metadata": {},
   "source": [
    "#### Build Pipeline Using Sub-Pipeline As Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "d3a2b994-1075-4d46-927a-1551e58608cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipeline_name = f'{SERIES}-{EXPERIMENT}-example-3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "6959e264-ed77-4520-b636-311d42a3c188",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@kfp.dsl.pipeline(\n",
    "    name = pipeline_name,\n",
    "    description = 'A simple pipeline for testing',\n",
    "    pipeline_root = f'gs://{GCS_BUCKET}/{SERIES}/{EXPERIMENT}/pipeline_root'\n",
    ")\n",
    "def pipeline(\n",
    "    project_id: str,\n",
    "    bq_project: str,\n",
    "    bq_dataset: str,\n",
    "    bq_table: str\n",
    "):\n",
    "\n",
    "    # use the pipeline as a component:\n",
    "    train_data = data_prep_pipeline(\n",
    "        project_id = project_id,\n",
    "        bq_project = bq_project,\n",
    "        bq_dataset = bq_dataset,\n",
    "        bq_table = bq_table\n",
    "    )\n",
    "    model_1 = model_gb(\n",
    "        train = train_data.outputs['train'],\n",
    "        features = train_data.outputs['features']\n",
    "    )\n",
    "    model_2 = model_rf(\n",
    "        train = train_data.outputs['train'],\n",
    "        features = train_data.outputs['features']\n",
    "    )\n",
    "    metrics_1_train = metrics(\n",
    "        data = train_data.outputs['train'],\n",
    "        features = train_data.outputs['features'],\n",
    "        model = model_1.output,\n",
    "    ).set_display_name('Metrics: Training Data')\n",
    "    metrics_1_test = metrics(\n",
    "        data = train_data.outputs['test'],\n",
    "        features = train_data.outputs['features'],\n",
    "        model = model_1.output,\n",
    "    ).set_display_name('Metrics: Test Data')\n",
    "    metrics_2_train = metrics(\n",
    "        data = train_data.outputs['train'],\n",
    "        features = train_data.outputs['features'],\n",
    "        model = model_2.output,\n",
    "    ).set_display_name('Metrics: Training Data')\n",
    "    metrics_2_test = metrics(\n",
    "        data = train_data.outputs['test'],\n",
    "        features = train_data.outputs['features'],\n",
    "        model = model_2.output,\n",
    "    ).set_display_name('Metrics: Test Data')\n",
    "    \n",
    "    review = overview(\n",
    "        metrics_0 = metrics_1_train.outputs['metrics'],\n",
    "        metrics_1 = metrics_1_test.outputs['metrics'],\n",
    "        metrics_2 = metrics_2_train.outputs['metrics'],\n",
    "        metrics_3 = metrics_2_test.outputs['metrics'],\n",
    "        models = ['GB', 'GB', 'RF', 'RF'],\n",
    "        data = ['Train', 'Test', 'Train', 'Test']\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6436248-ea90-42e5-b922-0eceab2b7f67",
   "metadata": {},
   "source": [
    "#### Compile Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "2e77522e-769c-478c-8289-a43bf1dba46a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "kfp.compiler.Compiler().compile(\n",
    "    pipeline_func = pipeline,\n",
    "    package_path = f'{DIR}/{pipeline_name}.yaml'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec731d4-332d-44e5-9bc0-5bcfa606eb73",
   "metadata": {},
   "source": [
    "#### Upload Pipeline To Registry\n",
    "\n",
    "Like other repository types there are two types of references for artifacts: tags and version.  The versions are managed by the repository and the naming of versions is possible by supplying tags.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "959eea15-2110-4866-8446-0a31c145bc07",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "component_data_source.yaml\n",
      "downloaded_component_data_source.yaml\n",
      "downloaded_data_prep_pipeline.yaml\n",
      "mlops-pipeline-pattern-modular-example-2.yaml\n",
      "mlops-pipeline-pattern-modular-example-3-data-preparation.yaml\n",
      "mlops-pipeline-pattern-modular-example-3-data_preparation.yaml\n",
      "mlops-pipeline-pattern-modular-example-3.yaml\n",
      "mlops-pipeline-pattern-modular-preview.yaml\n"
     ]
    }
   ],
   "source": [
    "!ls {DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "e47fa1da-bdcb-4c75-8a6b-07faaa1b95c4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mlops-pipeline-pattern-modular-example-3'"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "64949a7c-f614-4281-a934-eb1dbc0e8c1b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "template, version = kfp_registry.upload_pipeline(\n",
    "    file_name = f\"{DIR}/{pipeline_name}.yaml\",\n",
    "    tags = ['v1', 'new'],\n",
    "    extra_headers = dict(description = 'Example 3 pipeline built from pipelines', note = 'This is an example of a modular pipeline construction.')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "daa20516-ceb5-41b1-b9c9-4ad868a183d9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('mlops-pipeline-pattern-modular-example-3',\n",
       " 'sha256:299e61e951dfc528ae9ab16d9251a1df9971bfbed409d924a3fdb41c5fc05775')"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template, version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80d4bd5-e4d7-4b80-b5bb-52333e9e48e6",
   "metadata": {},
   "source": [
    "#### Pipeline Runs From The Registry\n",
    "\n",
    "The pipeline could be download to local first but it is also possible to directly reference the pipeline in the repository when creating a run.  More methods, including no code runs from the console, are covered in the workflow: [Vertex AI Pipelines - Management](./Vertex%20AI%20Pipelines%20-%20Management.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "6e2f24b5-b46d-49a5-a6c8-1765a4a3bd42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "parameters = dict(\n",
    "    project_id = PROJECT_ID,\n",
    "    bq_project = 'bigquery-public-data',\n",
    "    bq_dataset = 'ml_datasets',\n",
    "    bq_table = 'penguins'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "ee4ab654-fcf9-4dd4-81fa-c2eacf512499",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipeline_job = aiplatform.PipelineJob(\n",
    "    display_name = pipeline_name,\n",
    "    template_path = f\"{REPOSITORY.lower()}/{pipeline_name}/{version}\",\n",
    "    parameter_values = parameters,\n",
    "    pipeline_root = f'gs://{GCS_BUCKET}/{SERIES}/{EXPERIMENT}/pipeline_root',\n",
    "    enable_caching = False # True (enabled), False (disable), None (defer to component level caching) \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "5eda4dd8-057b-42bf-9493-67bde90ad400",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/1026793852137/locations/us-central1/pipelineJobs/mlops-pipeline-pattern-modular-example-3-20240801174507\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/1026793852137/locations/us-central1/pipelineJobs/mlops-pipeline-pattern-modular-example-3-20240801174507')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/mlops-pipeline-pattern-modular-example-3-20240801174507?project=1026793852137\n"
     ]
    }
   ],
   "source": [
    "response = pipeline_job.submit(\n",
    "    service_account = SERVICE_ACCOUNT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "78c59c5f-dc2e-40cb-81b2-3b27d6b7a628",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PipelineJob projects/1026793852137/locations/us-central1/pipelineJobs/mlops-pipeline-pattern-modular-example-3-20240801174507 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/1026793852137/locations/us-central1/pipelineJobs/mlops-pipeline-pattern-modular-example-3-20240801174507 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/1026793852137/locations/us-central1/pipelineJobs/mlops-pipeline-pattern-modular-example-3-20240801174507 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/1026793852137/locations/us-central1/pipelineJobs/mlops-pipeline-pattern-modular-example-3-20240801174507 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/1026793852137/locations/us-central1/pipelineJobs/mlops-pipeline-pattern-modular-example-3-20240801174507 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/1026793852137/locations/us-central1/pipelineJobs/mlops-pipeline-pattern-modular-example-3-20240801174507 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob run completed. Resource name: projects/1026793852137/locations/us-central1/pipelineJobs/mlops-pipeline-pattern-modular-example-3-20240801174507\n"
     ]
    }
   ],
   "source": [
    "pipeline_job.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "2e4aa52c-ed71-494f-9f47-d8188e9bdbee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pipeline_name</th>\n",
       "      <th>run_name</th>\n",
       "      <th>param.vertex-ai-pipelines-artifact-argument-binding</th>\n",
       "      <th>param.vmlmd_lineage_integration</th>\n",
       "      <th>param.input:project_id</th>\n",
       "      <th>param.input:bq_table</th>\n",
       "      <th>param.input:bq_project</th>\n",
       "      <th>param.input:bq_dataset</th>\n",
       "      <th>metric.confusionMatrix</th>\n",
       "      <th>metric.precision</th>\n",
       "      <th>metric.f1</th>\n",
       "      <th>metric.average_precision</th>\n",
       "      <th>metric.accuracy</th>\n",
       "      <th>metric.recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mlops-pipeline-pattern-modular-example-3</td>\n",
       "      <td>mlops-pipeline-pattern-modular-example-3-20240...</td>\n",
       "      <td>{'output:metrics-2-metrics': ['projects/102679...</td>\n",
       "      <td>{'pipeline_template_component': {'template_id'...</td>\n",
       "      <td>statmike-mlops-349915</td>\n",
       "      <td>penguins</td>\n",
       "      <td>bigquery-public-data</td>\n",
       "      <td>ml_datasets</td>\n",
       "      <td>{'rows': [{'row': [118.0, 0.0, 0.0]}, {'row': ...</td>\n",
       "      <td>0.152455</td>\n",
       "      <td>0.20922</td>\n",
       "      <td>0.345348</td>\n",
       "      <td>0.457364</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              pipeline_name  \\\n",
       "0  mlops-pipeline-pattern-modular-example-3   \n",
       "\n",
       "                                            run_name  \\\n",
       "0  mlops-pipeline-pattern-modular-example-3-20240...   \n",
       "\n",
       "  param.vertex-ai-pipelines-artifact-argument-binding  \\\n",
       "0  {'output:metrics-2-metrics': ['projects/102679...    \n",
       "\n",
       "                     param.vmlmd_lineage_integration param.input:project_id  \\\n",
       "0  {'pipeline_template_component': {'template_id'...  statmike-mlops-349915   \n",
       "\n",
       "  param.input:bq_table param.input:bq_project param.input:bq_dataset  \\\n",
       "0             penguins   bigquery-public-data            ml_datasets   \n",
       "\n",
       "                              metric.confusionMatrix  metric.precision  \\\n",
       "0  {'rows': [{'row': [118.0, 0.0, 0.0]}, {'row': ...          0.152455   \n",
       "\n",
       "   metric.f1  metric.average_precision  metric.accuracy  metric.recall  \n",
       "0    0.20922                  0.345348         0.457364       0.333333  "
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aiplatform.get_pipeline_df(pipeline = f'{pipeline_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21c7bec-b12f-497a-89ab-ad122d9fee92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8049681f-fd60-4f5f-8761-a0e6812a32dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db615a67-062a-461b-9930-9a811072732e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7824470-8e93-4974-8775-de1f27ea229e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b7df22-3e38-4aec-8110-35db62e39611",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15450a3a-fcb1-4ee4-bd4a-5dace24de85a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e093a4-8347-410c-8ef7-67ba2df7a49e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m128",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m128"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
