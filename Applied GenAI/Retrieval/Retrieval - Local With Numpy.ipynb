{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f2f048b",
   "metadata": {},
   "source": [
    "![tracker](https://us-central1-vertex-ai-mlops-369716.cloudfunctions.net/pixel-tracking?path=statmike%2Fvertex-ai-mlops%2FApplied+GenAI%2FRetrieval&file=Retrieval+-+Local+With+Numpy.ipynb)\n",
    "<!--- header table --->\n",
    "<table align=\"left\">\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://colab.research.google.com/github/statmike/vertex-ai-mlops/blob/main/Applied%20GenAI/Retrieval/Retrieval%20-%20Local%20With%20Numpy.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Google Colaboratory logo\">\n",
    "      <br>Run in<br>Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https%3A%2F%2Fraw.githubusercontent.com%2Fstatmike%2Fvertex-ai-mlops%2Fmain%2FApplied%2520GenAI%2FRetrieval%2FRetrieval%2520-%2520Local%2520With%2520Numpy.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\">\n",
    "      <br>Run in<br>Colab Enterprise\n",
    "    </a>\n",
    "  </td>      \n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://github.com/statmike/vertex-ai-mlops/blob/main/Applied%20GenAI/Retrieval/Retrieval%20-%20Local%20With%20Numpy.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      <br>View on<br>GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/statmike/vertex-ai-mlops/main/Applied%20GenAI/Retrieval/Retrieval%20-%20Local%20With%20Numpy.ipynb\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
    "      <br>Open in<br>Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a698e894-1333-42b9-aaac-d85f393a8898",
   "metadata": {},
   "source": [
    "# Retrieval - Local With NumPy\n",
    "\n",
    "In prior workflows, a series of documents was [processed into chunks](../Chunking/readme.md), and for each chunk, [embeddings](../Embeddings/readme.md) were created:\n",
    "\n",
    "- Process: [Large Document Processing - Document AI Layout Parser](../Chunking/Large%20Document%20Processing%20-%20Document%20AI%20Layout%20Parser.ipynb)\n",
    "- Embed: [Vertex AI Text Embeddings API](../Embeddings/Vertex%20AI%20Text%20Embeddings%20API.ipynb)\n",
    "\n",
    "Retrieving chunks for a query involves calculating the embedding for the query and then using similarity metrics to find relevant chunks. A thorough review of similarity matching can be found in [The Math of Similarity](../Embeddings/The%20Math%20of%20Similarity.ipynb) - use dot product! As development moves from experiment to application, the process of storing and computing similarity is migrated to a [retrieval](./readme.md) system. This workflow is part of a [series of workflows exploring many retrieval systems](./readme.md).  \n",
    "\n",
    "A detailed [comparison of many retrieval systems](./readme.md#comparison-of-vector-database-solutions) can be found in the readme as well.\n",
    "\n",
    "---\n",
    "\n",
    "**NumPy For Local Search (and indexing)**\n",
    "\n",
    "This workflow builds a retrieval system locally using [NumPy](https://numpy.org/)! NumPy is a powerful Python library for numerical computation and provides an easy-to-implement local solution for similarity search. This workflow also extends NumPy to approximate nearest neighbor search by building an Inverted File (IVF) index using [k-means](https://en.wikipedia.org/wiki/K-means_clustering) clustering with the [scikit-learn](https://scikit-learn.org/stable/) package.\n",
    "\n",
    "---\n",
    "\n",
    "**Use Case Data**\n",
    "\n",
    "Buying a home usually involves borrowing money from a lending institution, typically through a mortgage secured by the home's value. But how do these institutions manage the risks associated with such large loans, and how are lending standards established?\n",
    "\n",
    "In the United States, two government-sponsored enterprises (GSEs) play a vital role in the housing market:\n",
    "\n",
    "- Federal National Mortgage Association ([Fannie Mae](https://www.fanniemae.com/))\n",
    "- Federal Home Loan Mortgage Corporation ([Freddie Mac](https://www.freddiemac.com/))\n",
    "\n",
    "These GSEs purchase mortgages from lenders, enabling those lenders to offer more loans. This process also allows Fannie Mae and Freddie Mac to set standards for mortgages, ensuring they are responsible and borrowers are more likely to repay them. This system makes homeownership more affordable and stabilizes the housing market by maintaining a steady flow of liquidity for lenders and keeping interest rates controlled.\n",
    "\n",
    "However, navigating the complexities of these GSEs and their extensive servicing guides can be challenging.\n",
    "\n",
    "**Approaches**\n",
    "\n",
    "[This series](../readme.md) covers many generative AI workflows. These documents are used directly as long context for Gemini in the workflow [Long Context Retrieval With The Vertex AI Gemini API](../Generate/Long%20Context%20Retrieval%20With%20The%20Vertex%20AI%20Gemini%20API.ipynb). The workflow below uses a [retrieval](./readme.md) approach with the already generated chunks and embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72ea5c4-15a9-47d6-be99-55e527c9fa47",
   "metadata": {
    "id": "od_UkDpvRmgD"
   },
   "source": [
    "---\n",
    "## Colab Setup\n",
    "\n",
    "When running this notebook in [Colab](https://colab.google/) or [Colab Enterprise](https://cloud.google.com/colab/docs/introduction), this section will authenticate to GCP (follow prompts in the popup) and set the current project for the session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae4fc4b5-ded9-4998-91c5-3a739f93abe1",
   "metadata": {
    "executionInfo": {
     "elapsed": 195,
     "status": "ok",
     "timestamp": 1683726184843,
     "user": {
      "displayName": "Mike Henderson",
      "userId": "07691629187611687318"
     },
     "user_tz": 240
    },
    "id": "8UO9FnqyKBlF",
    "tags": []
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = 'statmike-mlops-349915' # replace with project ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e8556f8-aba8-48e8-bb1e-78a7f8d5e292",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 68869,
     "status": "ok",
     "timestamp": 1683726253709,
     "user": {
      "displayName": "Mike Henderson",
      "userId": "07691629187611687318"
     },
     "user_tz": 240
    },
    "id": "N98-KK7LRkjm",
    "outputId": "09ec5008-0def-4e1a-c349-c598ee752f78",
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    from google.colab import auth\n",
    "    auth.authenticate_user()\n",
    "    !gcloud config set project {PROJECT_ID}\n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4314aff0-869c-470d-a7df-accc3d0c8f87",
   "metadata": {},
   "source": [
    "---\n",
    "## Installs and API Enablement\n",
    "\n",
    "The clients packages may need installing in this environment. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40d7541-201a-4b15-843f-10ceaf40026d",
   "metadata": {},
   "source": [
    "### Installs (If Needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63e2119e-e87d-4e1c-8443-7db5a118c379",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# tuples of (import name, install name, min_version)\n",
    "packages = [\n",
    "    ('google.cloud.aiplatform', 'google-cloud-aiplatform', '1.69.0'),\n",
    "    ('numpy', 'numpy'),\n",
    "    ('sklearn', 'scikit-learn'),\n",
    "    ('psutil', 'psutil'),\n",
    "    ('GPUtil', 'GPUtil')\n",
    "]\n",
    "\n",
    "import importlib\n",
    "install = False\n",
    "for package in packages:\n",
    "    if not importlib.util.find_spec(package[0]):\n",
    "        print(f'installing package {package[1]}')\n",
    "        install = True\n",
    "        !pip install {package[1]} -U -q --user\n",
    "    elif len(package) == 3:\n",
    "        if importlib.metadata.version(package[0]) < package[2]:\n",
    "            print(f'updating package {package[1]}')\n",
    "            install = True\n",
    "            !pip install {package[1]} -U -q --user"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d797c9-2cb6-4324-8bd4-22d36e71eef8",
   "metadata": {},
   "source": [
    "### API Enablement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae98c1e2-5e2f-4435-8404-7b5a387943ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!gcloud services enable aiplatform.googleapis.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c670aa3f-2855-46f4-92fa-102f3602768c",
   "metadata": {},
   "source": [
    "### Restart Kernel (If Installs Occured)\n",
    "\n",
    "After a kernel restart the code submission can start with the next cell after this one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f30908a-8c84-4e1b-8376-7b4da7738d6f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if install:\n",
    "    import IPython\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)\n",
    "    IPython.display.display(IPython.display.Markdown(\"\"\"<div class=\\\"alert alert-block alert-warning\\\">\n",
    "        <b>⚠️ The kernel is going to restart. Please wait until it is finished before continuing to the next step. The previous cells do not need to be run again⚠️</b>\n",
    "        </div>\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4dc5ac-5948-4712-a63a-a13653a4753e",
   "metadata": {
    "id": "appt8-yVRtJ1"
   },
   "source": [
    "---\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300f5d4f-f85e-454d-bbf6-36c369d5afdb",
   "metadata": {
    "id": "63mx2EozRxFP"
   },
   "source": [
    "Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "258676f3-d059-4cab-9d98-1a9502b1c87b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 2124,
     "status": "ok",
     "timestamp": 1683726390544,
     "user": {
      "displayName": "Mike Henderson",
      "userId": "07691629187611687318"
     },
     "user_tz": 240
    },
    "id": "xzcoXjM5Rky5",
    "outputId": "b3bdcbc1-70d5-472e-aea2-42c74a42efde",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'statmike-mlops-349915'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project = !gcloud config get-value project\n",
    "PROJECT_ID = project[0]\n",
    "PROJECT_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b8b99e0-801b-4615-aa34-4338f102e54e",
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1683726390712,
     "user": {
      "displayName": "Mike Henderson",
      "userId": "07691629187611687318"
     },
     "user_tz": 240
    },
    "id": "IxWrFtqYMfku",
    "tags": []
   },
   "outputs": [],
   "source": [
    "REGION = 'us-central1'\n",
    "SERIES = 'applied-genai'\n",
    "EXPERIMENT = 'retrieval-numpy'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79513291-1fa9-4793-865b-a04b4a97d3aa",
   "metadata": {
    "id": "LuajVwCiO6Yg"
   },
   "source": [
    "Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91d2d7be-0aa0-4d8a-8bb6-85486fbd241d",
   "metadata": {
    "executionInfo": {
     "elapsed": 17761,
     "status": "ok",
     "timestamp": 1683726409304,
     "user": {
      "displayName": "Mike Henderson",
      "userId": "07691629187611687318"
     },
     "user_tz": 240
    },
    "id": "LVC7zzSLRk2C",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os, json, sys, time, glob\n",
    "\n",
    "import numpy as np\n",
    "import sklearn.cluster\n",
    "import psutil\n",
    "import GPUtil\n",
    "\n",
    "# Vertex AI\n",
    "from google.cloud import aiplatform\n",
    "import vertexai.language_models # for embeddings API\n",
    "import vertexai.generative_models # for Gemini Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ec644e4-2d42-4e15-9773-6c08b4c7b018",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.71.0'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aiplatform.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21071181-1138-45ee-828c-5a98bf0df696",
   "metadata": {
    "id": "EyAVFG9TO9H-"
   },
   "source": [
    "Clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3011a2d7-e026-4015-a4af-5eac8420bf84",
   "metadata": {
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1683726409306,
     "user": {
      "displayName": "Mike Henderson",
      "userId": "07691629187611687318"
     },
     "user_tz": 240
    },
    "id": "L0RPE13LOZce",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# vertex ai clients\n",
    "vertexai.init(project = PROJECT_ID, location = REGION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c806df85-71ff-440e-8a23-c1806e689a2d",
   "metadata": {},
   "source": [
    "---\n",
    "## Text & Embeddings For Examples\n",
    "\n",
    "This repository contains a [section for document processing (chunking)](../Chunking/readme.md) that includes an example of processing mulitple large pdfs (over 1000 pages) into chunks: [Large Document Processing - Document AI Layout Parser](../Chunking/Large%20Document%20Processing%20-%20Document%20AI%20Layout%20Parser.ipynb).  The chunks of text from that workflow are stored with this repository and loaded by another companion workflow that augments the chunks with text embeddings: [Vertex AI Text Embeddings API](../Embeddings/Vertex%20AI%20Text%20Embeddings%20API.ipynb).\n",
    "\n",
    "The following code will load the version of the chunks that includes text embeddings and prepare it for a local example of retrival augmented generation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f0538b-1b96-4a2f-92c3-4ffc01c9d8a5",
   "metadata": {},
   "source": [
    "### Get The Documents\n",
    "\n",
    "If you are working from a clone of this notebooks [repository](https://github.com/statmike/vertex-ai-mlops) then the documents are already present. The following cell checks for the documents folder and if it is missing gets it (`git clone`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "93d660e6-c6d7-4a74-829f-6aeb8157c8ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "local_dir = '../Embeddings/files/embeddings-api'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4e94bcb5-c705-4524-8e22-afe4a879f4d8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents Found in folder `../Embeddings/files/embeddings-api`\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(local_dir):\n",
    "    print('Retrieving documents...')\n",
    "    parent_dir = os.path.dirname(local_dir)\n",
    "    temp_dir = os.path.join(parent_dir, 'temp')\n",
    "    if not os.path.exists(temp_dir):\n",
    "        os.makedirs(temp_dir)\n",
    "    !git clone https://www.github.com/statmike/vertex-ai-mlops {temp_dir}/vertex-ai-mlops\n",
    "    shutil.copytree(f'{temp_dir}/vertex-ai-mlops/Applied GenAI/Embeddings/files/embeddings-api', local_dir)\n",
    "    shutil.rmtree(temp_dir)\n",
    "    print(f'Documents are now in folder `{local_dir}`')\n",
    "else:\n",
    "    print(f'Documents Found in folder `{local_dir}`')             "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4336a06f-ad9a-48d3-9e0b-7bc0ceec181a",
   "metadata": {},
   "source": [
    "### Load The Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5fe60266-1ac7-494b-9f70-e7b510ea7d9d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../Embeddings/files/embeddings-api/large-files-chunk-embeddings-0000.jsonl',\n",
       " '../Embeddings/files/embeddings-api/large-files-chunk-embeddings-0001.jsonl',\n",
       " '../Embeddings/files/embeddings-api/large-files-chunk-embeddings-0002.jsonl',\n",
       " '../Embeddings/files/embeddings-api/large-files-chunk-embeddings-0003.jsonl',\n",
       " '../Embeddings/files/embeddings-api/large-files-chunk-embeddings-0004.jsonl',\n",
       " '../Embeddings/files/embeddings-api/large-files-chunk-embeddings-0005.jsonl',\n",
       " '../Embeddings/files/embeddings-api/large-files-chunk-embeddings-0006.jsonl',\n",
       " '../Embeddings/files/embeddings-api/large-files-chunk-embeddings-0007.jsonl',\n",
       " '../Embeddings/files/embeddings-api/large-files-chunk-embeddings-0008.jsonl',\n",
       " '../Embeddings/files/embeddings-api/large-files-chunk-embeddings-0009.jsonl']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jsonl_files = glob.glob(f\"{local_dir}/large-files*.jsonl\")\n",
    "jsonl_files.sort()\n",
    "jsonl_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b27565e7-603f-441a-bda3-576208f17f12",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9040"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks = []\n",
    "for file in jsonl_files:\n",
    "    with open(file, 'r') as f:\n",
    "        chunks.extend([json.loads(line) for line in f])\n",
    "len(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d79406f-53be-43df-944a-fe3d5674376e",
   "metadata": {},
   "source": [
    "### Review A Chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7d50e4de-f7ba-4ee9-ad64-c360cfb2029a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['instance', 'predictions', 'status'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "37fd0807-9c88-420d-a816-723164a2caef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fannie_part_0_c17'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[0]['instance']['chunk_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "428bd5a2-1f38-4b57-b538-b4e11d6a1744",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Selling Guide Fannie Mae Single Family\n",
      "\n",
      "## Fannie Mae Copyright Notice\n",
      "\n",
      "### Fannie Mae Copyright Notice\n",
      "\n",
      "|-|\n",
      "| Section B3-4.2, Verification of Depository Assets 402 |\n",
      "| B3-4.2-01, Verification of Deposits and Assets (05/04/2022) 403 |\n",
      "| B3-4.2-02, Depository Accounts (12/14/2022) 405 |\n",
      "| B3-4.2-03, Individual Development Accounts (02/06/2019) 408 |\n",
      "| B3-4.2-04, Pooled Savings (Community Savings Funds) (04/01/2009) 411 |\n",
      "| B3-4.2-05, Foreign Assets (05/04/2022) 411 |\n",
      "| Section B3-4.3, Verification of Non-Depository Assets 412 |\n",
      "| B3-4.3-01, Stocks, Stock Options, Bonds, and Mutual Funds (06/30/2015) 412 |\n",
      "| B3-4.3-02, Trust Accounts (04/01/2009) 413 |\n",
      "| B3-4.3-03, Retirement Accounts (06/30/2015) 414 |\n",
      "| B3-4.3-04, Personal Gifts (09/06/2023) 415 |\n",
      "| B3-4.3-05, Gifts of Equity (10/07/2020) 418 |\n",
      "| B3-4.3-06, Grants and Lender Contributions (12/14/2022) 419 |\n",
      "| B3-4.3-07, Disaster Relief Grants or Loans (04/01/2009) 423 |\n",
      "| B3-4.3-08, Employer Assistance (09/29/2015) 423 |\n",
      "| B3-4.3-09, Earnest Money Deposit (05/04/2022) 425 |\n",
      "| B3-4.3-10, Anticipated Sales Proceeds (02/23/2016) B3-4.3-11, Trade Equity (12/16/2020) 426 428 |\n",
      "| B3-4.3-12, Rent-Related Credits (08/07/2024) 429 |\n",
      "| B3-4.3-13, Sweat Equity (04/15/2014) 430 |\n",
      "| B3-4.3-14, Bridge/Swing Loans (04/01/2009) 431 |\n",
      "| B3-4.3-15, Borrowed Funds Secured by an Asset (10/30/2009) 431 |\n",
      "|  |\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(chunks[0]['instance']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cd0e3b12-3aed-4f37-bcdc-82b052358408",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.031277116388082504,\n",
       " 0.03056905046105385,\n",
       " 0.010865348391234875,\n",
       " 0.0623614676296711,\n",
       " 0.03228681534528732,\n",
       " 0.05066155269742012,\n",
       " 0.046544693410396576,\n",
       " 0.05509665608406067,\n",
       " -0.014074751175940037,\n",
       " 0.008380400016903877]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[0]['predictions'][0]['embeddings']['values'][0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406c7dcb-a1e4-48fd-a398-3d2b323aa856",
   "metadata": {},
   "source": [
    "### Prepare Chunk Structure\n",
    "\n",
    "Make a list of dictionaries with information for each chunk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "20600613-1d98-4548-8db3-4f0f9bec299f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "content_chunks = [\n",
    "    dict(\n",
    "        gse = chunk['instance']['gse'],\n",
    "        chunk_id = chunk['instance']['chunk_id'],\n",
    "        content = chunk['instance']['content'],\n",
    "        embedding = chunk['predictions'][0]['embeddings']['values']\n",
    "    ) for chunk in chunks\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d437a257-dd0f-4495-93ba-70f301d35955",
   "metadata": {},
   "source": [
    "### Query Embedding\n",
    "\n",
    "Create a query, or prompt, and get the embedding for it:\n",
    "\n",
    "Connect to models for text embeddings. Learn more about the model API:\n",
    "- [Vertex AI Text Embeddings API](../Embeddings/Vertex%20AI%20Text%20Embeddings%20API.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "312537eb-af4f-4934-80e0-fcb557f27ed5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "question = \"Does a lender have to perform servicing functions directly?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "90b098f7-10cd-42ba-9321-1e53e129f507",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "embedder = vertexai.language_models.TextEmbeddingModel.from_pretrained('text-embedding-004')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "121e503f-1acd-4bbd-bdfb-22d9c6fb8fe8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.0005117303808219731,\n",
       " 0.009651427157223225,\n",
       " 0.01768726110458374,\n",
       " 0.014538003131747246,\n",
       " -0.01829824410378933,\n",
       " 0.027877431362867355,\n",
       " -0.021124685183167458,\n",
       " 0.008830446749925613,\n",
       " -0.02669006586074829,\n",
       " 0.06414774805307388]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_embedding = embedder.get_embeddings([question])[0].values\n",
    "question_embedding[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4026f205-239d-4183-9163-1fd62d4b2feb",
   "metadata": {},
   "source": [
    "---\n",
    "## Numpy For Vector Similarity Search\n",
    "\n",
    "Embeddings can be used with math to measure similarity.  For deeper details into this checkout the companion workflow here: [The Math of Similarity](../Embeddings/The%20Math%20of%20Similarity.ipynb).  Retrieval systems handle the storage and math of similarity as a service.  For an overview of Google Cloud based solutions for retrieval check out [this companion series](../Retrieval/readme.md).\n",
    "\n",
    "The content below motivates retrieval with the embeddings that accompany the text chunks using a local vector database with brute force matching using [Numpy](https://numpy.org/)!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d640fa-f582-43c7-8842-3efe3a3e5de8",
   "metadata": {},
   "source": [
    "### Vector DB With Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f7661a33-241a-48bd-ad0f-5dbf54f3f859",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vector_db = [\n",
    "    [\n",
    "        chunk['instance']['chunk_id'],\n",
    "        chunk['predictions'][0]['embeddings']['values'],\n",
    "    ]\n",
    "    for chunk in chunks\n",
    "]\n",
    "vector_index = np.array([row[1] for row in vector_db])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "91a4106e-c9ff-47cb-8b20-83e92b6d1373",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9040"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vector_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4eeff585-e82f-47b9-a3fa-8dcf6a015cca",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9040, 768)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_index.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a58bf6b-e822-440d-acfc-8eecf26bf39d",
   "metadata": {},
   "source": [
    "### Matching With Numpy\n",
    "\n",
    "Use dot product to calculate similarity and find matches for a query embedding.  Why dot product?  Check out the companion workflow: [The Math of Similarity](../Embeddings/The%20Math%20of%20Similarity.ipynb)\n",
    "\n",
    "> **NOTE:**  This will calculate the similarity for all embeddings vectors stored in the local vector db which is just a Numpy array here.  This is very fast because there are <10000 embeddings vectors.  As this scales it would be better to consider a solution that searches a subset of embeddings.  More details on retrieval solutions can be found in [Retrieval](../Retrieval/readme.md).  One method is to partion the embeddings and only search partition near the query embedding.  This is covered as an example later in this workflow (below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "364954a9-76fa-404a-a891-736a9018b6b2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(141, 0.7099842015202706),\n",
       " (6673, 0.6805260859043876),\n",
       " (7246, 0.6753296984114661),\n",
       " (698, 0.6723706814818046),\n",
       " (327, 0.6683496311110356)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity = np.dot(question_embedding, vector_index.T)\n",
    "matches = np.argsort(similarity)[-5:].tolist()\n",
    "matches.reverse()\n",
    "matches = [(match, similarity[match]) for match in matches]\n",
    "matches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4bffd2-4858-4c26-bbd0-6eb2f94d4250",
   "metadata": {},
   "source": [
    "### Get Text For Matches\n",
    "\n",
    "Make a dictionary for each lookup of chunk content by chunk id:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ffcdb0a4-2c20-4157-9caa-f23e8b26e040",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chunk_lookup = {}\n",
    "for chunk in chunks:\n",
    "    chunk_lookup[chunk['instance']['chunk_id']] = chunk['instance']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bf69ede4-5ce0-4b43-b08e-9973cb3f6355",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Selling Guide Fannie Mae Single Family\n",
      "\n",
      "## Fannie Mae Copyright Notice\n",
      "\n",
      "### Fannie Mae Copyright Notice\n",
      "\n",
      "|-|\n",
      "| Section B3-4.2, Verification of Depository Assets 402 |\n",
      "| B3-4.2-01, Verification of Deposits and Assets (05/04/2022) 403 |\n",
      "| B3-4.2-02, Depository Accounts (12/14/2022) 405 |\n",
      "| B3-4.2-03, Individual Development Accounts (02/06/2019) 408 |\n",
      "| B3-4.2-04, Pooled Savings (Community Savings Funds) (04/01/2009) 411 |\n",
      "| B3-4.2-05, Foreign Assets (05/04/2022) 411 |\n",
      "| Section B3-4.3, Verification of Non-Depository Assets 412 |\n",
      "| B3-4.3-01, Stocks, Stock Options, Bonds, and Mutual Funds (06/30/2015) 412 |\n",
      "| B3-4.3-02, Trust Accounts (04/01/2009) 413 |\n",
      "| B3-4.3-03, Retirement Accounts (06/30/2015) 414 |\n",
      "| B3-4.3-04, Personal Gifts (09/06/2023) 415 |\n",
      "| B3-4.3-05, Gifts of Equity (10/07/2020) 418 |\n",
      "| B3-4.3-06, Grants and Lender Contributions (12/14/2022) 419 |\n",
      "| B3-4.3-07, Disaster Relief Grants or Loans (04/01/2009) 423 |\n",
      "| B3-4.3-08, Employer Assistance (09/29/2015) 423 |\n",
      "| B3-4.3-09, Earnest Money Deposit (05/04/2022) 425 |\n",
      "| B3-4.3-10, Anticipated Sales Proceeds (02/23/2016) B3-4.3-11, Trade Equity (12/16/2020) 426 428 |\n",
      "| B3-4.3-12, Rent-Related Credits (08/07/2024) 429 |\n",
      "| B3-4.3-13, Sweat Equity (04/15/2014) 430 |\n",
      "| B3-4.3-14, Bridge/Swing Loans (04/01/2009) 431 |\n",
      "| B3-4.3-15, Borrowed Funds Secured by an Asset (10/30/2009) 431 |\n",
      "|  |\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(chunk_lookup['fannie_part_0_c17'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8212b3a5-279c-4666-856f-41797972744b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match 1 (0.71) is chunk fannie_part_0_c352:\n",
      "# A3-3-03, Other Servicing Arrangements (12/15/2015)\n",
      "\n",
      "Introduction This topic provides an overview of other servicing arrangements, including: • Subservicing • General Requirements for Subservicing Arrangements • Pledge of Servicing Rights and Transfer of Interest in Servicing Income\n",
      "\n",
      "## Subservicing\n",
      "\n",
      "A lender may use other organizations to perform some or all of its servicing functions. Fannie Mae refers to these arrangements as “subservicing” arrangements, meaning that a servicer (the “subservicer”) other than the contractually responsible servicer (the “master” servicer) is performing the servicing functions. The following are not considered to be subservicing arrangements: • when a computer service bureau is used to perform accounting and reporting functions; • when the originating lender sells and assigns servicing to another lender, unless the originating lender continues to be the contractually responsible servicer.\n",
      "###################################################\n",
      "Match 2 (0.68) is chunk freddie_part_4_c509:\n",
      "# (1) Notice requirements\n",
      "\n",
      "The notice must advise the Borrower of the following: 1. The date the new Servicing Agent or Master Servicer undertakes the performance of the Servicing obligations 2. The name and address of the Servicer undertaking the performance of the Servicing obligations 3. The names and telephone numbers of the contact persons or departments where the Borrowers' inquiries relating to the transfer should be directed. (If toll-free numbers are not available, the letter must indicate that collect calls will be accepted.) Such names and telephone numbers must be provided for the party previously performing the Servicing obligations as well as the new Servicing Agent or Master Servicer undertaking the performance of the Servicing obligations. 4. The date when the party previously performing the Servicing obligation will no longer collect the Borrowers' payments and when the new Servicing Agent or Master Servicer undertaking the performance of the Servicing obligations will begin to collect them 5. Procedures for maintenance of automatic draft payments, if applicable. Every effort must be made to continue, without interruption, electronic payments on the Borrower's Mortgage, to the extent permitted by applicable law.\n",
      "###################################################\n",
      "Match 3 (0.68) is chunk freddie_part_4_c510:\n",
      "# (1) Notice requirements\n",
      "\n",
      "The notice may not amend the terms of a Mortgage other than those relating to where to send payments. The Master Servicer and any Servicing Agent must ensure that their staff and facilities are adequately prepared to process Servicing and accounting transactions and to respond to Borrower inquiries during the transfer transition period. The Servicing Agent or Master Servicer undertaking the performance of the Servicing obligations must assume responsibility for responding to Borrower inquiries received after the date of such undertaking. If any Servicing or accounting problem cannot be resolved without the involvement of the Servicer that was performing the Servicing obligations, the new Servicing Agent or Master Servicer undertaking the performance of the Servicing obligations, and not the Borrower, should initiate the contact with the prior Servicer.\n",
      "###################################################\n",
      "Match 4 (0.67) is chunk fannie_part_0_c353:\n",
      "# A3-3-03, Other Servicing Arrangements (12/15/2015)\n",
      "\n",
      "## General Requirements for Subservicing Arrangements\n",
      "\n",
      "A servicer may use a subservicer only if it will not interfere with the servicer's ability to meet Fannie Mae's remitting and reporting requirements. A master servicer may not enter into new subservicing arrangements—or extend existing arrangements to include newly originated mortgages-unless both the master servicer and the subservicer are Fannie Mae- approved servicers in good standing who are able to perform the duties associated with the master servicer/subservicer arrangement. The master servicer must ensure that its written agreement with the subservicer acknowledges Fannie Mae's right to rescind its recognition of the subservicing arrangement if Fannie Mae decides to transfer the master servicer's portfolio for any reason. 114 The master servicer must confirm its existing subservicing arrangements when it submits the Lender Record Information (Form 582) each year. For additional information concerning subservicer and master servicer duties, responsibilities, and other requirements, see the Servicing Guide on Fannie Mae's website.\n",
      "###################################################\n",
      "Match 5 (0.67) is chunk fannie_part_0_c326:\n",
      "# Chapter A3-3, Third-Party Lending Functions and Servicing Arrangements\n",
      "\n",
      "Introduction This chapter explains Fannie Mae's requirements regarding the outsourcing of mortgage origination and servicing functions. It also addresses other servicing arrangements as well as the requirements related to document custody and document custodians. 105\n",
      "###################################################\n"
     ]
    }
   ],
   "source": [
    "for m, match in enumerate(matches):\n",
    "    print(f\"Match {m+1} ({match[1]:.2f}) is chunk {vector_db[match[0]][0]}:\\n{chunk_lookup[vector_db[match[0]][0]]}\\n###################################################\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba3aaec-1792-4b07-8e61-dba638b59bc6",
   "metadata": {},
   "source": [
    "---\n",
    "## Retrieval Augmented Generation (RAG)\n",
    "\n",
    "Build a simple retrieval augmented generation process that enhances a query by retrieving context.  This is done here by constructing three functions for the stages:\n",
    "- `retrieve` - a function that uses an embedding to search for matching context parts, pieces of texts\n",
    "    - this uses the system built earlier in this workflow!\n",
    "- `augment` - prepare chunks into a prompt\n",
    "- `generate` - make the llm request with the augmented prompt\n",
    "\n",
    "A final function is used to execute the workflow of rag:\n",
    "- `rag` - a function that receives the query an orchestrates the workflow through `retrieve` > `augment` > `generate`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc89e60-f9b3-4004-97ac-a260198643e7",
   "metadata": {},
   "source": [
    "### Clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "595e6fd2-ffce-4b40-b74f-1490ec9bfd50",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "embedder = vertexai.language_models.TextEmbeddingModel.from_pretrained('text-embedding-004')\n",
    "llm = vertexai.generative_models.GenerativeModel(\"gemini-1.5-flash-002\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32d40f1-b971-4f23-82f4-33cb4ad5010a",
   "metadata": {},
   "source": [
    "### Retrieve Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5a50c21d-f552-49e1-8a01-01450026fd81",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def retrieve_numpy(query_embedding, n_matches = 5):\n",
    "    \n",
    "    similarity = np.dot(query_embedding, vector_index.T)\n",
    "    bf_matches = np.argsort(similarity)[-(n_matches):].tolist()\n",
    "    bf_matches.reverse()\n",
    "    bf_matches = [(match, similarity[match]) for match in bf_matches]\n",
    "    \n",
    "    matches = []\n",
    "    for m, match in enumerate(bf_matches):\n",
    "        matches.append(dict(\n",
    "            chunk_id = vector_db[match[0]][0],\n",
    "            content = chunk_lookup[vector_db[match[0]][0]]\n",
    "        ))\n",
    "    \n",
    "    return matches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a46494-4748-4652-bae1-740a6eeed220",
   "metadata": {},
   "source": [
    "### Augment Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "551b23eb-0aa7-481f-90e9-2d3749d753ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def augment(matches):\n",
    "\n",
    "    prompt = ''\n",
    "    for m, match in enumerate(matches):\n",
    "        prompt += f\"Context {m+1}:\\n{match['content']}\\n\\n\"\n",
    "    prompt += f'Answer the following question using the provided contexts:\\n'\n",
    "\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48518dc5-7ba6-431f-9538-89876c6a3e03",
   "metadata": {},
   "source": [
    "### Generate Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ae88428f-81ec-4332-ad5d-22ca008e0b07",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate(prompt):\n",
    "\n",
    "    result = llm.generate_content(prompt)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efcc31c7-bc2e-4379-83f9-5d7d483a7458",
   "metadata": {},
   "source": [
    "### RAG Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8518f047-c82d-4529-a9e1-b1bfeffab00e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rag(query):\n",
    "    \n",
    "    query_embedding = embedder.get_embeddings([query])[0].values\n",
    "    matches = retrieve_numpy(query_embedding)\n",
    "    prompt = augment(matches) + query\n",
    "    result = generate(prompt)\n",
    "    \n",
    "    return result.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9e1514-740b-4b1f-b8ba-5c78ed579c34",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Example In Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cb3e2673-4211-4832-85be-3fc3dca53953",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Does a lender have to perform servicing functions directly?'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a2ad612a-3f4b-4148-bda6-a9a8195e0f1f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No, a lender does not have to perform servicing functions directly.  Context 1 explicitly states that a lender \"may use other organizations to perform some or all of its servicing functions,\" referring to this as \"subservicing.\"  This involves a \"master servicer\" and a \"subservicer,\" where the subservicer performs functions on behalf of the master servicer.  The contexts also describe procedures and regulations surrounding these arrangements.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(rag(question))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4cbd79-3bcc-4831-aecb-63f93c1971bd",
   "metadata": {},
   "source": [
    "---\n",
    "### Profiling Performance\n",
    "\n",
    "Profile the timing of each step in the RAG function for sequential calls. The environment choosen for this workflow is a minimal testing enviornment so load testing (simoultaneous requests) would not be helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8a1eb655-e037-4ee9-9bb1-050b8622e108",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "profile = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c4a1a7cf-b415-4fd5-bded-d235106b3eaa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rag(query, profile = profile):\n",
    "    \n",
    "    timings = {}\n",
    "    start_time = time.time()\n",
    "    \n",
    "    \n",
    "    # 1. Get embeddings\n",
    "    embedding_start = time.time()\n",
    "    query_embedding = embedder.get_embeddings([query])[0].values\n",
    "    timings['embedding'] = time.time() - embedding_start\n",
    "\n",
    "    # 2. Retrieve from Bigtable\n",
    "    retrieval_start = time.time()\n",
    "    matches = retrieve_numpy(query_embedding)\n",
    "    timings['retrieve_numpy'] = time.time() - retrieval_start\n",
    "\n",
    "    # 3. Augment the prompt\n",
    "    augment_start = time.time()\n",
    "    prompt = augment(matches) + query\n",
    "    timings['augment'] = time.time() - augment_start\n",
    "\n",
    "    # 4. Generate text\n",
    "    generate_start = time.time()\n",
    "    result = generate(prompt)\n",
    "    timings['generate'] = time.time() - generate_start\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    timings['total'] = total_time\n",
    "    \n",
    "    profile.append(timings)\n",
    "    \n",
    "    return result.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5a467c2e-c140-46da-8ced-720e0d848314",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No, a lender does not have to perform servicing functions directly.  Context 1 explicitly states that a lender may use other organizations to perform some or all of its servicing functions, referring to this as \"subservicing.\"  This involves a \"master servicer\" and a \"subservicer,\" where the subservicer performs the servicing functions on behalf of the master servicer.  However,  the master servicer remains contractually responsible.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(rag(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5028414d-925c-4b2e-bf4a-8c6e9845dff1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'embedding': 0.10621380805969238,\n",
       "  'retrieve_numpy': 0.0040607452392578125,\n",
       "  'augment': 3.981590270996094e-05,\n",
       "  'generate': 0.8290009498596191,\n",
       "  'total': 0.9393248558044434}]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6f9cb49d-6c87-4ca5-8c1d-37f88dad417f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    response = rag(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c46ae0-f256-4a24-9a60-da95cbb77975",
   "metadata": {},
   "source": [
    "### Report From Profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "89247391-2b1b-4a0f-8870-36f32a363007",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_timings = {}\n",
    "for timings in profile:\n",
    "    for key, value in timings.items():\n",
    "        if key not in all_timings:\n",
    "            all_timings[key] = []\n",
    "        all_timings[key].append(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1d8443f4-e98c-46b9-98b2-ff0ea682e286",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics for 'embedding':\n",
      "  Min: 0.0462 seconds\n",
      "  Max: 0.3017 seconds\n",
      "  Mean: 0.0602 seconds\n",
      "  Median: 0.0528 seconds\n",
      "  Std Dev: 0.0332 seconds\n",
      "  P95: 0.0904 seconds\n",
      "  P99: 0.2560 seconds\n",
      "\n",
      "Statistics for 'retrieve_numpy':\n",
      "  Min: 0.0038 seconds\n",
      "  Max: 0.0129 seconds\n",
      "  Mean: 0.0042 seconds\n",
      "  Median: 0.0040 seconds\n",
      "  Std Dev: 0.0011 seconds\n",
      "  P95: 0.0054 seconds\n",
      "  P99: 0.0079 seconds\n",
      "\n",
      "Statistics for 'augment':\n",
      "  Min: 0.0000 seconds\n",
      "  Max: 0.0001 seconds\n",
      "  Mean: 0.0000 seconds\n",
      "  Median: 0.0000 seconds\n",
      "  Std Dev: 0.0000 seconds\n",
      "  P95: 0.0001 seconds\n",
      "  P99: 0.0001 seconds\n",
      "\n",
      "Statistics for 'generate':\n",
      "  Min: 0.5788 seconds\n",
      "  Max: 1.2178 seconds\n",
      "  Mean: 0.7623 seconds\n",
      "  Median: 0.7355 seconds\n",
      "  Std Dev: 0.1136 seconds\n",
      "  P95: 0.9955 seconds\n",
      "  P99: 1.0743 seconds\n",
      "\n",
      "Statistics for 'total':\n",
      "  Min: 0.6290 seconds\n",
      "  Max: 1.2720 seconds\n",
      "  Mean: 0.8267 seconds\n",
      "  Median: 0.7922 seconds\n",
      "  Std Dev: 0.1181 seconds\n",
      "  P95: 1.0569 seconds\n",
      "  P99: 1.1332 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for key, values in all_timings.items():\n",
    "    arr = np.array(values)\n",
    "    print(f\"Statistics for '{key}':\")\n",
    "    print(f\"  Min: {np.min(arr):.4f} seconds\")\n",
    "    print(f\"  Max: {np.max(arr):.4f} seconds\")\n",
    "    print(f\"  Mean: {np.mean(arr):.4f} seconds\")\n",
    "    print(f\"  Median: {np.median(arr):.4f} seconds\")\n",
    "    print(f\"  Std Dev: {np.std(arr):.4f} seconds\")\n",
    "    print(f\"  P95: {np.percentile(arr, 95):.4f} seconds\")\n",
    "    print(f\"  P99: {np.percentile(arr, 99):.4f} seconds\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8d6c82-6166-4065-8a60-1e74d08e5bf0",
   "metadata": {},
   "source": [
    "---\n",
    "## Deeper Profiling\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa5c5d2-6280-43bf-8433-56d4e396dfb7",
   "metadata": {},
   "source": [
    "### Size Of Objects\n",
    "\n",
    "The design above involves three objects:\n",
    "- `vector_db` - a Python list of list objects that each contain a chunk_id and the embedding vector for the chunk\n",
    "- `vector_index` - a numpy array of rows for each embedding vector\n",
    "- `content_chunks` - a Python dict that has keys for each chunk_id and values are the text of the chunk\n",
    "\n",
    "These are used by finding the index of matching embeddings from the `vector_index` and then looking up the cooresponding chunk_id in `vector_db` before finally retrieving the text of the chunk from `content_chunks`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce6c60b-b23e-41ef-aedc-ede695b6e06d",
   "metadata": {},
   "source": [
    "#### Object: `vector_db`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "43a86991-af81-434f-a6d4-4acdb9c18145",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(vector_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1eedc2f8-6a4f-4335-96b1-022ec55aa4f9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9040"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vector_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "116c4524-963c-4b88-a0bd-86da9ab7a9d1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75672"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# size in bytes\n",
    "sys.getsizeof(vector_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0472d48f-5538-48bb-bf4a-43b5d4ee2fbf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07216644287109375"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# size in megabytes\n",
    "sys.getsizeof(vector_db)/ (1024*1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4355f6-7cad-46ae-b5f8-4ee21dc4e9ab",
   "metadata": {},
   "source": [
    "#### Object: `vector_index`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c5a6e98f-ff33-43b0-9708-1eb324438527",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(vector_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a2a964af-6d36-42a0-a11f-61e7fa019452",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9040, 768)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_index.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "48bb5d32-8b45-4a4a-86b2-3a5137a66f32",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55541888"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# size in bytes\n",
    "sys.getsizeof(vector_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "85862542-f5e2-4047-8e9c-98f1b99df17c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52.9688720703125"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# size in megabytes\n",
    "sys.getsizeof(vector_index)/ (1024*1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62cc570-507d-4854-9f40-45e1756c36ab",
   "metadata": {},
   "source": [
    "#### Object: `content_chunks`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "76d42354-57c3-4236-96f9-bbdbc64e945e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(chunk_lookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9c2b451a-4df9-4d1e-b758-c8a9c4d094cd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9040"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunk_lookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "eec6ff5c-fda8-483a-844e-6d8145ed3e32",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "295000"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# size in bytes\n",
    "sys.getsizeof(chunk_lookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d2ec63cb-3ff7-4d76-83a9-32fcd0c882d7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.28133392333984375"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# size in megabytes\n",
    "sys.getsizeof(chunk_lookup)/ (1024*1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0ac782-3788-43a9-8ef4-bab23c89bc67",
   "metadata": {},
   "source": [
    "### Local Compute Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ee08e2ce-1a32-4f54-9253-a85c38bf4d3c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Count: 4\n",
      "CPU Frequency: 2199.998 MHz\n"
     ]
    }
   ],
   "source": [
    "# Get CPU count\n",
    "cpu_count = psutil.cpu_count(logical=True)  # Includes logical cores (hyperthreading)\n",
    "print(f\"CPU Count: {cpu_count}\")\n",
    "\n",
    "# Get CPU frequency\n",
    "cpu_freq = psutil.cpu_freq()\n",
    "print(f\"CPU Frequency: {cpu_freq.current} MHz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1b94670a-cdeb-4f65-97fd-8af2ab4e288c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Memory: 31.36 GB\n",
      "Used Memory: 14.82 GB\n",
      "Available Memory: 16.11 GB\n",
      "Memory Percentage Used: 48.6%\n"
     ]
    }
   ],
   "source": [
    "mem = psutil.virtual_memory()\n",
    "print(f\"Total Memory: {mem.total / (1024**3):.2f} GB\")\n",
    "print(f\"Used Memory: {mem.used / (1024**3):.2f} GB\")\n",
    "print(f\"Available Memory: {mem.available / (1024**3):.2f} GB\")\n",
    "print(f\"Memory Percentage Used: {mem.percent}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "66d9c272-6100-48f4-8821-3e17736d92fb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPUs found.\n"
     ]
    }
   ],
   "source": [
    "# Get all available GPUs\n",
    "gpus = GPUtil.getGPUs()\n",
    "\n",
    "if gpus:\n",
    "    for gpu in gpus:\n",
    "        print(f\"GPU Name: {gpu.name}\")\n",
    "        print(f\"GPU Memory Total: {gpu.memoryTotal} MB\")\n",
    "        print(f\"GPU Memory Used: {gpu.memoryUsed} MB\")\n",
    "        print(f\"GPU Memory Free: {gpu.memoryFree} MB\")\n",
    "        print(f\"GPU Load: {gpu.load*100}%\")\n",
    "else:\n",
    "    print(\"No GPUs found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d617671-24dc-46a2-bf9c-c685c4be0343",
   "metadata": {},
   "source": [
    "### Timing Sequential Operations\n",
    "\n",
    "Get the timing for different sequential matching requests loads: 1, 10, 100, 1000, 10000, 100000, ...\n",
    "\n",
    "Break this down by the tasks:\n",
    "- Get Embedding Vector For Question\n",
    "- Get Matching Chunks\n",
    "- Construct Prompt From Matches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208e1086-2e2e-42bb-830a-ca3af509bc22",
   "metadata": {},
   "source": [
    "#### Get Embedding Vector For Question: API\n",
    "\n",
    "This test sequential request.  The Text Embeddings API has many option for asynchronous and multi-instance request that could also be used for efficiency.  See more in [Vertex AI Text Embeddings API](../Embeddings/Vertex%20AI%20Text%20Embeddings%20API.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "9ea67402-6885-4889-bd7e-7936383c8c5d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time for n = 1: 0.080102 seconds\n",
      "Execution time for n = 10: 0.669986 seconds\n",
      "Execution time for n = 100: 5.302307 seconds\n",
      "Execution time for n = 1000: 52.841818 seconds\n"
     ]
    }
   ],
   "source": [
    "embed_time = []\n",
    "for x in range(4):\n",
    "    n = 10**x\n",
    "    start_time = time.time()\n",
    "    for i in range(n):\n",
    "        # get embedding for question\n",
    "        question_embedding = embedder.get_embeddings([question])[0].values\n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "    print(f\"Execution time for n = {n}: {execution_time:.6f} seconds\")\n",
    "    embed_time.append(execution_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca0707b-62ab-49d9-ac1f-f5ce236f9da2",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Get Matching Chunks: Python + Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "eb19fde8-87b0-463d-9b17-7d152e37078f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time for n = 1: 0.004354 seconds\n",
      "Execution time for n = 10: 0.041247 seconds\n",
      "Execution time for n = 100: 0.402753 seconds\n",
      "Execution time for n = 1000: 4.015273 seconds\n"
     ]
    }
   ],
   "source": [
    "# get embedding for question\n",
    "question_embedding = embedder.get_embeddings([question])[0].values\n",
    "\n",
    "match_time = []\n",
    "for x in range(4):\n",
    "    n = 10**x\n",
    "    start_time = time.time()\n",
    "    for i in range(n):\n",
    "        # get top_n matches:\n",
    "        top_n = 10\n",
    "        matches = retrieve_numpy(question_embedding, n_matches = top_n)\n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "    print(f\"Execution time for n = {n}: {execution_time:.6f} seconds\")\n",
    "    match_time.append(execution_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff490ed7-4b64-4002-9bb0-eb9ce7b148d4",
   "metadata": {},
   "source": [
    "### Profile Sequential Operations Timing\n",
    "\n",
    "Now collect the individual timings for local operations and review the profile of timing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ad7599ab-1145-4ee0-9c23-14274ec9e961",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get embedding for question\n",
    "question_embedding = embedder.get_embeddings([question])[0].values\n",
    "\n",
    "combined_time_profile = []\n",
    "\n",
    "n = 10000\n",
    "\n",
    "for i in range(n):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # get top_n matches:\n",
    "    top_n = 10\n",
    "    matches = retrieve_numpy(question_embedding, n_matches = top_n)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "    combined_time_profile.append(execution_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b06eb689-e5c9-4afc-a20d-8208acbfc109",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time for all requests: 47.152343 seconds\n",
      "Average time per request: 0.004715 seconds\n",
      "Range of time across all requests: 0.023218 seconds\n",
      "99th percentile of request times: 0.012341 seconds\n"
     ]
    }
   ],
   "source": [
    "# Total time for all requests\n",
    "total_time = sum(combined_time_profile)\n",
    "print(f\"Total time for all requests: {total_time:.6f} seconds\")\n",
    "\n",
    "# Average time per request\n",
    "average_time = total_time / len(combined_time_profile)\n",
    "print(f\"Average time per request: {average_time:.6f} seconds\")\n",
    "\n",
    "# Range of time across all requests\n",
    "time_range = max(combined_time_profile) - min(combined_time_profile)\n",
    "print(f\"Range of time across all requests: {time_range:.6f} seconds\")\n",
    "\n",
    "# 99th percentile of request times\n",
    "percentile_99 = np.percentile(combined_time_profile, 99)\n",
    "print(f\"99th percentile of request times: {percentile_99:.6f} seconds\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08360ec-c65d-4b5b-903f-3b6c9cf1c4e8",
   "metadata": {},
   "source": [
    "---\n",
    "## Approximate Search With IVF using K-Means\n",
    "\n",
    "The solution above is fast at the current size and scale.  As the number of embeddings increase it could be helpful to search a subset of embeddings for faster responses.  A simple way to extend the brute force search to a subset is an Inverted File (IVF) index. How?\n",
    "- Cluster the embeddings into k groups, using [k-means clustering](https://en.wikipedia.org/wiki/K-means_clustering)\n",
    "- Create an inverted list that assigns embeddings to clusters\n",
    "- Search by first finding the closest cluster then only searching within those\n",
    "\n",
    "Here the clustering with k-means is trained with [scikit-learn `sklearn.cluster.KMeans`](https://scikit-learn.org/1.5/modules/generated/sklearn.cluster.KMeans.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e4b84f-bacc-40ca-9b35-6993d2afecbb",
   "metadata": {},
   "source": [
    "### Cluster With k-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "3e04f8f7-9922-45f3-9e79-b16f2c7e2ecf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "k = 100\n",
    "kmeans = sklearn.cluster.KMeans(n_clusters = k, random_state = 0)\n",
    "cluster_assignments = kmeans.fit_predict(vector_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2bc1df-f162-4df1-ad6f-34272bf671d8",
   "metadata": {},
   "source": [
    "### Create Inverted Lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "1b5523c6-9dd9-4387-ac1c-63b3b6048447",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inverted_lists = [[] for _ in range(k)]\n",
    "for i, cluster_id in enumerate(cluster_assignments):\n",
    "    inverted_lists[cluster_id].append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e7c579b4-57d8-4bb4-87d2-d2ec39c5ec5e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(inverted_lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a2739745-2687-4bc1-90af-47c388041a0e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1072,\n",
       " 1148,\n",
       " 1199,\n",
       " 1233,\n",
       " 1247,\n",
       " 1311,\n",
       " 1327,\n",
       " 1363,\n",
       " 1413,\n",
       " 1421,\n",
       " 1432,\n",
       " 1443,\n",
       " 1474,\n",
       " 1505,\n",
       " 1517,\n",
       " 1526,\n",
       " 1580,\n",
       " 1610,\n",
       " 1621,\n",
       " 1631,\n",
       " 1690,\n",
       " 1700,\n",
       " 1711,\n",
       " 1744,\n",
       " 1820,\n",
       " 1865,\n",
       " 1891,\n",
       " 2061,\n",
       " 4798,\n",
       " 4848,\n",
       " 4850,\n",
       " 4853,\n",
       " 4862,\n",
       " 4876,\n",
       " 4881,\n",
       " 4884,\n",
       " 4893,\n",
       " 4909,\n",
       " 4943,\n",
       " 4953,\n",
       " 4961,\n",
       " 4974,\n",
       " 5007,\n",
       " 5016,\n",
       " 5020,\n",
       " 5061,\n",
       " 5065,\n",
       " 5131,\n",
       " 5184,\n",
       " 5211,\n",
       " 5219,\n",
       " 5230,\n",
       " 5264,\n",
       " 5278,\n",
       " 5333,\n",
       " 5335,\n",
       " 5336,\n",
       " 5363,\n",
       " 5375,\n",
       " 5420,\n",
       " 5439,\n",
       " 5469,\n",
       " 5481,\n",
       " 5505,\n",
       " 5522,\n",
       " 5600]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inverted_lists[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23365a6-6742-42e7-85b1-61649e8ea6be",
   "metadata": {},
   "source": [
    "### Search With IVF Index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae444297-617a-4362-b899-2927bebd3dd9",
   "metadata": {},
   "source": [
    "#### Find Closest Clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae9a5fc-e194-41b1-9a6d-39add3b6b607",
   "metadata": {
    "tags": []
   },
   "source": [
    "The center of each cluster is stored:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a5e2ff1c-7e66-4c0e-ad7c-de375c118d3e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 768)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans.cluster_centers_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "19669f55-56a0-4814-b43e-b03903ccf081",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get embedding for question\n",
    "question_embedding = embedder.get_embeddings([question])[0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "3d303b0f-f5a9-4d99-83f7-a2e116a06a83",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cluster_similarity = np.dot(question_embedding, kmeans.cluster_centers_.T)\n",
    "nearest_clusters = np.argsort(cluster_similarity)[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e14f1f83-52be-44c5-a8e3-7a5517d761ea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([72, 52, 34, 76, 79, 14, 20, 24, 33, 84])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nearest_clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de18859-db89-4480-a5e5-9a92b5d6d74c",
   "metadata": {},
   "source": [
    "#### Search Within Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "6ecf9dc3-48a2-4f8c-a731-0bdc91b8d5b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "candidate_indicies = [idv for cluster_id in nearest_clusters for idv in inverted_lists[cluster_id]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2fb4e0-65b9-4575-a11c-2b2a9bc99bb0",
   "metadata": {},
   "source": [
    "#### Top Matches Within Candidate Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "386d9caa-cb43-4c74-8d80-ac2bf3ba8433",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[141, 0.7099842015202706],\n",
       " [6673, 0.6805260859043876],\n",
       " [7246, 0.6753296984114661],\n",
       " [698, 0.6723706814818046],\n",
       " [327, 0.6683496311110356],\n",
       " [7166, 0.6619843862150242],\n",
       " [190, 0.661433734537568],\n",
       " [8724, 0.6604534921319808],\n",
       " [7264, 0.6575403552108654],\n",
       " [8440, 0.6573532750493309]]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidate_index = vector_index[candidate_indicies]\n",
    "candidate_similarity = np.dot(question_embedding, candidate_index.T)\n",
    "ivf_matches = [[candidate_indicies[match], candidate_similarity[match]] for match in np.argsort(candidate_similarity)[-top_n:].tolist()]\n",
    "ivf_matches.reverse()\n",
    "ivf_matches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91abf5a1-f648-4ff9-9050-f96c1ee6a3fa",
   "metadata": {},
   "source": [
    "#### Compare To Top Matches From Brute Force"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "de8f9c53-4e54-40c4-84af-180923abb5b2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[141, 0.7099842015202706],\n",
       " [6673, 0.6805260859043876],\n",
       " [7246, 0.6753296984114661],\n",
       " [698, 0.6723706814818046],\n",
       " [327, 0.6683496311110356],\n",
       " [7166, 0.6619843862150242],\n",
       " [190, 0.661433734537568],\n",
       " [506, 0.6608578617010463],\n",
       " [8724, 0.6604534921319808],\n",
       " [7264, 0.6575403552108654]]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_n = 10\n",
    "similarity = np.dot(question_embedding, vector_index.T)\n",
    "matches = np.argsort(similarity)[-top_n:].tolist()\n",
    "matches = [[match, similarity[match]] for match in matches]\n",
    "matches.reverse()\n",
    "matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "9aa15578-5378-49e4-b4e5-0369dc496ac5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i[0] for i in matches] == [i[0] for i in ivf_matches]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0520212a-06bd-4b37-9513-1b66dbd1dd58",
   "metadata": {},
   "source": [
    "#### Put Steps Together For Efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d196e5ca-ddc6-4a15-85eb-0de0177fa37a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[141, 0.7099842015202706],\n",
       " [6673, 0.6805260859043876],\n",
       " [7246, 0.6753296984114661],\n",
       " [698, 0.6723706814818046],\n",
       " [327, 0.6683496311110356],\n",
       " [7166, 0.6619843862150242],\n",
       " [190, 0.661433734537568],\n",
       " [8724, 0.6604534921319808],\n",
       " [7264, 0.6575403552108654],\n",
       " [8440, 0.6573532750493309]]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_c = 10\n",
    "top_n = 10\n",
    "nearest_clusters = np.argsort(np.dot(question_embedding, kmeans.cluster_centers_.T))[-top_c:]\n",
    "candidate_indices = np.concatenate([inverted_lists[cluster_id] for cluster_id in nearest_clusters])\n",
    "candidate_similarity = np.dot(question_embedding, vector_index[candidate_indices].T)\n",
    "top_indices = np.argsort(candidate_similarity)[-top_n:]\n",
    "ivf_matches = [[candidate_indices[i], candidate_similarity[i]] for i in top_indices]\n",
    "ivf_matches.reverse()\n",
    "ivf_matches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e135d4b0-81e1-470f-8683-180d8878cbf6",
   "metadata": {},
   "source": [
    "### RAG: Enhanced With IVF Method\n",
    "\n",
    "The `augment` and `generate` functions can remain the same.  Here, a new `retrieve_numpy_ivf` function is created along with an updated `rag` function that points to it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49c5571-c67a-4700-9c4b-d3ef4ceebe8c",
   "metadata": {},
   "source": [
    "#### Retrieve Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "972419e0-586d-4c49-aef9-f63d13a17fc3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def retrieve_numpy_ivf(query_embedding, n_matches = 5):\n",
    "\n",
    "    n_clusters = 10\n",
    "    nearest_clusters = np.argsort(np.dot(question_embedding, kmeans.cluster_centers_.T))[-n_clusters:]\n",
    "    candidate_indices = np.concatenate([inverted_lists[cluster_id] for cluster_id in nearest_clusters])\n",
    "    candidate_similarity = np.dot(question_embedding, vector_index[candidate_indices].T)\n",
    "    top_indices = np.argsort(candidate_similarity)[-n_matches:]\n",
    "    ivf_matches = [[candidate_indices[i], candidate_similarity[i]] for i in top_indices]\n",
    "    ivf_matches.reverse()\n",
    "    \n",
    "    matches = []\n",
    "    for m, match in enumerate(ivf_matches):\n",
    "        matches.append(dict(\n",
    "            chunk_id = vector_db[match[0]][0],\n",
    "            content = chunk_lookup[vector_db[match[0]][0]]\n",
    "        ))\n",
    "    \n",
    "    return matches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3fc05f-32bd-46d2-b108-93c84593ac42",
   "metadata": {},
   "source": [
    "#### RAG Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "63a10cd6-c99c-42b4-a183-d1b3c11c01f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rag(query):\n",
    "    \n",
    "    query_embedding = embedder.get_embeddings([query])[0].values\n",
    "    matches = retrieve_numpy_ivf(query_embedding)\n",
    "    prompt = augment(matches) + query\n",
    "    result = generate(prompt)\n",
    "    \n",
    "    return result.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dba3d30-f32f-432d-94b7-df4c76a18a97",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Example In Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "f7d745b4-a846-4d49-b7c7-0426163c9318",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Does a lender have to perform servicing functions directly?'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "8b2e73fd-bce3-4df5-8740-c5b8f38f9a25",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No.  A lender may use other organizations to perform some or all of its servicing functions through subservicing arrangements (Context 1).  However, the lender (master servicer) remains contractually responsible (Context 1), and there are specific requirements for these arrangements, including ensuring the subservicer's ability to meet Fannie Mae's requirements (Context 4).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(rag(question))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772bb8ed-ed8b-4484-b5d4-580ed7d2c090",
   "metadata": {},
   "source": [
    "---\n",
    "### Profiling Performance\n",
    "\n",
    "Profile the timing of each step in the RAG function for sequential calls. The environment choosen for this workflow is a minimal testing enviornment so load testing (simoultaneous requests) would not be helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "3a2aac80-96ca-4139-a09f-bbeba86f34d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "profile = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "5d97b1b0-777d-48f1-b925-9ac7be406949",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rag(query, profile = profile):\n",
    "    \n",
    "    timings = {}\n",
    "    start_time = time.time()\n",
    "    \n",
    "    \n",
    "    # 1. Get embeddings\n",
    "    embedding_start = time.time()\n",
    "    query_embedding = embedder.get_embeddings([query])[0].values\n",
    "    timings['embedding'] = time.time() - embedding_start\n",
    "\n",
    "    # 2. Retrieve from Bigtable\n",
    "    retrieval_start = time.time()\n",
    "    matches = retrieve_numpy_ivf(query_embedding)\n",
    "    timings['retrieve_numpy_ivf'] = time.time() - retrieval_start\n",
    "\n",
    "    # 3. Augment the prompt\n",
    "    augment_start = time.time()\n",
    "    prompt = augment(matches) + query\n",
    "    timings['augment'] = time.time() - augment_start\n",
    "\n",
    "    # 4. Generate text\n",
    "    generate_start = time.time()\n",
    "    result = generate(prompt)\n",
    "    timings['generate'] = time.time() - generate_start\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    timings['total'] = total_time\n",
    "    \n",
    "    profile.append(timings)\n",
    "    \n",
    "    return result.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "11573d2e-0beb-4747-86a5-03379fe83981",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No.  A lender may use other organizations to perform some or all of its servicing functions through subservicing arrangements (Context 1).  However, the lender (master servicer) remains contractually responsible (Context 1).  There are specific requirements and limitations on the use of subservicers, including Fannie Mae approval (Context 4).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(rag(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "b0272c9d-6eb0-4e4f-912d-08195fdf3d8f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'embedding': 0.056551456451416016,\n",
       "  'retrieve_numpy_ivf': 0.008069276809692383,\n",
       "  'augment': 3.981590270996094e-05,\n",
       "  'generate': 0.6398637294769287,\n",
       "  'total': 0.7045333385467529}]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "6f4e1f50-8959-4c14-8a89-b37667968c57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    response = rag(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8fc24f-9fa1-47cc-b29f-36404f293c18",
   "metadata": {},
   "source": [
    "### Report From Profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "f3a19b8c-14db-457a-8efc-ff37365de4a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_timings = {}\n",
    "for timings in profile:\n",
    "    for key, value in timings.items():\n",
    "        if key not in all_timings:\n",
    "            all_timings[key] = []\n",
    "        all_timings[key].append(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "67b0ed4a-e46b-497e-a794-eb97a3863292",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics for 'embedding':\n",
      "  Min: 0.0471 seconds\n",
      "  Max: 0.1558 seconds\n",
      "  Mean: 0.0548 seconds\n",
      "  Median: 0.0516 seconds\n",
      "  Std Dev: 0.0132 seconds\n",
      "  P95: 0.0714 seconds\n",
      "  P99: 0.1010 seconds\n",
      "\n",
      "Statistics for 'retrieve_numpy_ivf':\n",
      "  Min: 0.0026 seconds\n",
      "  Max: 0.0089 seconds\n",
      "  Mean: 0.0038 seconds\n",
      "  Median: 0.0036 seconds\n",
      "  Std Dev: 0.0010 seconds\n",
      "  P95: 0.0048 seconds\n",
      "  P99: 0.0088 seconds\n",
      "\n",
      "Statistics for 'augment':\n",
      "  Min: 0.0000 seconds\n",
      "  Max: 0.0001 seconds\n",
      "  Mean: 0.0000 seconds\n",
      "  Median: 0.0000 seconds\n",
      "  Std Dev: 0.0000 seconds\n",
      "  P95: 0.0001 seconds\n",
      "  P99: 0.0001 seconds\n",
      "\n",
      "Statistics for 'generate':\n",
      "  Min: 0.5629 seconds\n",
      "  Max: 0.9085 seconds\n",
      "  Mean: 0.7162 seconds\n",
      "  Median: 0.7038 seconds\n",
      "  Std Dev: 0.0847 seconds\n",
      "  P95: 0.8653 seconds\n",
      "  P99: 0.8972 seconds\n",
      "\n",
      "Statistics for 'total':\n",
      "  Min: 0.6165 seconds\n",
      "  Max: 0.9648 seconds\n",
      "  Mean: 0.7749 seconds\n",
      "  Median: 0.7603 seconds\n",
      "  Std Dev: 0.0867 seconds\n",
      "  P95: 0.9280 seconds\n",
      "  P99: 0.9509 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for key, values in all_timings.items():\n",
    "    arr = np.array(values)\n",
    "    print(f\"Statistics for '{key}':\")\n",
    "    print(f\"  Min: {np.min(arr):.4f} seconds\")\n",
    "    print(f\"  Max: {np.max(arr):.4f} seconds\")\n",
    "    print(f\"  Mean: {np.mean(arr):.4f} seconds\")\n",
    "    print(f\"  Median: {np.median(arr):.4f} seconds\")\n",
    "    print(f\"  Std Dev: {np.std(arr):.4f} seconds\")\n",
    "    print(f\"  P95: {np.percentile(arr, 95):.4f} seconds\")\n",
    "    print(f\"  P99: {np.percentile(arr, 99):.4f} seconds\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4617ac2c-f9b0-4967-9b39-36d26201cf58",
   "metadata": {},
   "source": [
    "### Time Sequential Operations\n",
    "\n",
    "Similar to the brute force timing above, calculate the time for various numbers of sequential operations:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6118fd78-d12f-4a90-90a4-02fce2fe1916",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Get Matching Chunks: Python + Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "d6d28c66-3823-4f34-ac6c-589b78141043",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time for n = 1: 0.003629 seconds\n",
      "Execution time for n = 10: 0.056653 seconds\n",
      "Execution time for n = 100: 0.328526 seconds\n",
      "Execution time for n = 1000: 3.100171 seconds\n"
     ]
    }
   ],
   "source": [
    "# get embedding for question\n",
    "question_embedding = embedder.get_embeddings([question])[0].values\n",
    "\n",
    "match_time_ivf = []\n",
    "for x in range(4):\n",
    "    n = 10**x\n",
    "    start_time = time.time()\n",
    "    for i in range(n):\n",
    "        # get top_n matches:\n",
    "        top_n = 10\n",
    "        matches = retrieve_numpy_ivf(question_embedding, n_matches = top_n)\n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "    print(f\"Execution time for n = {n}: {execution_time:.6f} seconds\")\n",
    "    match_time_ivf.append(execution_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0979f7-0314-4672-a8b7-552695f608e3",
   "metadata": {},
   "source": [
    "Compare timing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "6b965a50-06da-46ce-8114-7d46bb6b0087",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For 1 iterations: IVF was 16.66% faster\n",
      "For 10 iterations IVF was 37.35% slower\n",
      "For 100 iterations: IVF was 18.43% faster\n",
      "For 1000 iterations: IVF was 22.79% faster\n"
     ]
    }
   ],
   "source": [
    "for i, (t, ivf_t) in enumerate(zip(match_time, match_time_ivf)):\n",
    "    adiff = abs(t-ivf_t)\n",
    "    if t <= ivf_t:\n",
    "        print(f\"For {10**i} iterations IVF was {100*(adiff/t):.2f}% slower\")\n",
    "    else:\n",
    "        print(f\"For {10**i} iterations: IVF was {100*(adiff/t):.2f}% faster\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38fb12d-aedc-4456-81b2-78de738e5d6d",
   "metadata": {},
   "source": [
    "### Profile Sequential Operations Timing\n",
    "\n",
    "Now collect the individual timings for local operations and review the profile of timing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "cc95add4-dded-4b9a-bef8-311d76f9ddef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get embedding for question\n",
    "question_embedding = embedder.get_embeddings([question])[0].values\n",
    "\n",
    "combined_time_profile_ivf = []\n",
    "\n",
    "n = 10000\n",
    "\n",
    "for i in range(n):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # get top_n matches:\n",
    "    top_n = 10\n",
    "    matches = retrieve_numpy_ivf(question_embedding, n_matches = top_n)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "    combined_time_profile_ivf.append(execution_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "1e8fd0b7-4b0f-4a1a-a240-c632d66e6018",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time for all requests: 32.304379 seconds\n",
      "Average time per request: 0.003230 seconds\n",
      "Range of time across all requests: 0.034760 seconds\n",
      "99th percentile of request times: 0.008415 seconds\n"
     ]
    }
   ],
   "source": [
    "# Total time for all requests\n",
    "total_time = sum(combined_time_profile_ivf)\n",
    "print(f\"Total time for all requests: {total_time:.6f} seconds\")\n",
    "\n",
    "# Average time per request\n",
    "average_time = total_time / len(combined_time_profile_ivf)\n",
    "print(f\"Average time per request: {average_time:.6f} seconds\")\n",
    "\n",
    "# Range of time across all requests\n",
    "time_range = max(combined_time_profile_ivf) - min(combined_time_profile_ivf)\n",
    "print(f\"Range of time across all requests: {time_range:.6f} seconds\")\n",
    "\n",
    "# 99th percentile of request times\n",
    "percentile_99 = np.percentile(combined_time_profile_ivf, 99)\n",
    "print(f\"99th percentile of request times: {percentile_99:.6f} seconds\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8581a45-c30f-41ed-a52e-d2fc6723cd50",
   "metadata": {},
   "source": [
    "Compare Timing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "2388cca8-fb96-4ba7-9954-4d5fe676c674",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- IVF -----\n",
      "Total time: 32.304379 seconds\n",
      "Average time: 0.003230 seconds\n",
      "Range: 0.034760 seconds\n",
      "99th percentile: 0.008415 seconds\n",
      "97th percentile: 0.006985 seconds\n",
      "95th percentile: 0.006576 seconds\n",
      "----- Brute Force -----\n",
      "Total time: 47.152343 seconds\n",
      "Average time: 0.004715 seconds\n",
      "Range: 0.023218 seconds\n",
      "99th percentile: 0.012341 seconds\n",
      "97th percentile: 0.009365 seconds\n",
      "95th percentile: 0.008123 seconds\n"
     ]
    }
   ],
   "source": [
    "def print_time_stats(label, times):\n",
    "    \"\"\"Prints timing statistics for a given list of times.\"\"\"\n",
    "    total_time = sum(times)\n",
    "    average_time = total_time / len(times)\n",
    "    time_range = max(times) - min(times)\n",
    "    percentile_99 = np.percentile(times, 99)\n",
    "    percentile_97 = np.percentile(times, 97)\n",
    "    percentile_95 = np.percentile(times, 95)\n",
    "    print(f\"----- {label} -----\")\n",
    "    print(f\"Total time: {total_time:.6f} seconds\")\n",
    "    print(f\"Average time: {average_time:.6f} seconds\")\n",
    "    print(f\"Range: {time_range:.6f} seconds\")\n",
    "    print(f\"99th percentile: {percentile_99:.6f} seconds\")\n",
    "    print(f\"97th percentile: {percentile_97:.6f} seconds\")\n",
    "    print(f\"95th percentile: {percentile_95:.6f} seconds\")\n",
    "    return (total_time, average_time, time_range, percentile_99, percentile_97, percentile_95)\n",
    "    \n",
    "# Print individual statistics\n",
    "results_ivf = print_time_stats(\"IVF\", combined_time_profile_ivf)\n",
    "results_bf = print_time_stats(\"Brute Force\", combined_time_profile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "521fa90d-430c-4ea9-afcd-602563fec8ad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the 'Total time' IVF was 31.49% faster:\n",
      "\tbrute force = 47.152343\n",
      "\tIVF = 32.304379\n",
      "For the 'Average Time' IVF was 31.49% faster:\n",
      "\tbrute force = 0.004715\n",
      "\tIVF = 0.003230\n",
      "For the 'Range' IVF was 49.72% slower:\n",
      "\tbrute force = 0.023218\n",
      "\tIVF = 0.034760\n",
      "For the '99th percentile' IVF was 31.81% faster:\n",
      "\tbrute force = 0.012341\n",
      "\tIVF = 0.008415\n",
      "For the '97th percentile' IVF was 25.41% faster:\n",
      "\tbrute force = 0.009365\n",
      "\tIVF = 0.006985\n",
      "For the '95th percentile' IVF was 19.05% faster:\n",
      "\tbrute force = 0.008123\n",
      "\tIVF = 0.006576\n"
     ]
    }
   ],
   "source": [
    "metrics = {'1' : 'Total time', '2' : 'Average Time', '3' : 'Range', '4' : '99th percentile', '5':'97th percentile', '6':'95th percentile'}\n",
    "for i, (bf, ivf) in enumerate(zip(results_bf, results_ivf)):\n",
    "    adiff = abs(bf-ivf)\n",
    "    if bf <= ivf:\n",
    "        print(f\"For the '{metrics[str(i+1)]}' IVF was {100*(adiff/bf):.2f}% slower:\\n\\tbrute force = {bf:.6f}\\n\\tIVF = {ivf:.6f}\")\n",
    "    else:\n",
    "        print(f\"For the '{metrics[str(i+1)]}' IVF was {100*(adiff/bf):.2f}% faster:\\n\\tbrute force = {bf:.6f}\\n\\tIVF = {ivf:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0a01ea-7fcf-40a9-9459-e766a83e9fad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m125",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m125"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
