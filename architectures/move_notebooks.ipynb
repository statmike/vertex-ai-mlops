{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83f4886d",
   "metadata": {},
   "source": [
    "![tracker](https://us-central1-vertex-ai-mlops-369716.cloudfunctions.net/pixel-tracking?path=statmike%2Fvertex-ai-mlops%2Farchitectures&file=move_notebooks.ipynb)\n",
    "<!--- header table --->\n",
    "<table align=\"left\">\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://colab.research.google.com/github/statmike/vertex-ai-mlops/blob/main/architectures/move_notebooks.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Google Colaboratory logo\">\n",
    "      <br>Run in<br>Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https%3A%2F%2Fraw.githubusercontent.com%2Fstatmike%2Fvertex-ai-mlops%2Fmain%2Farchitectures%2Fmove_notebooks.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\">\n",
    "      <br>Run in<br>Colab Enterprise\n",
    "    </a>\n",
    "  </td>      \n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://github.com/statmike/vertex-ai-mlops/blob/main/architectures/move_notebooks.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      <br>View on<br>GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/statmike/vertex-ai-mlops/main/architectures/move_notebooks.ipynb\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
    "      <br>Open in<br>Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f2d1c2-da30-4756-93cf-47ac14bce0db",
   "metadata": {},
   "source": [
    "# Moving Notebook Files\n",
    "\n",
    "Used to move files or folders to different folders within the repo\n",
    "- preserves files commmit history by using `git mv`\n",
    "- Will fix relative links within markdown cells of notebooks and within markdown files.\n",
    "- adds banner at top of file to indicate the files move and status\n",
    "\n",
    "Details:\n",
    "- Move the file/folder with `git mv`\n",
    "    - commit the staged move\n",
    "- Create a list of moved files that are either `.md` or `.ipynb` files\n",
    "- Find and fix and relative links inside these files\n",
    "- Detect which files had changes and stage+commit them\n",
    "\n",
    "IN PROGRESS:\n",
    "- Add/Edit a banner for the file that has notes on the files location change: date, old location, new/current location\n",
    "    - stage+commit these changes\n",
    "- Check all other files in the repository for links to the old file location and update to the new file location\n",
    "    - stage+commit these changes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f76654f-35f2-43ad-a162-132b8a1698e5",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c10b8a8-2e5b-4b6b-8ad0-af72590506fe",
   "metadata": {},
   "source": [
    "Installs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a5dd3fe-6e2e-4edd-a64d-92b9ce0f8466",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install GitPython"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f51afa-a5d7-4100-ad3b-68d9610b2447",
   "metadata": {},
   "source": [
    "Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05f17c9c-bd1d-46d8-83cf-8453c0cc1a65",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nbformat.NO_CONVERT"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, json, urllib.parse, IPython, pathlib, nbformat, re, git, datetime\n",
    "\n",
    "nbformat.NO_CONVERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f861a9f-b679-4ccf-a301-602780ab76f2",
   "metadata": {},
   "source": [
    "Parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01bc93a0-f0c0-47b7-98a8-3be92a5d64a8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jupyter/vertex-ai-mlops/architectures'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d458b2f-9af4-4fee-af5b-27709ce91720",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(PosixPath('/home/jupyter/vertex-ai-mlops'),\n",
       " PosixPath('/home/jupyter/vertex-ai-mlops/Applied GenAI/Evaluation/Vertex AI Agent Builder Check Grounding API.ipynb'),\n",
       " PosixPath('/home/jupyter/vertex-ai-mlops/Applied GenAI/Validate/Vertex AI Agent Builder Check Grounding API.ipynb'))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# `from_path` and `to_path` can be a folder or a specific file.\n",
    "\n",
    "repo_path = pathlib.Path('/home/jupyter/vertex-ai-mlops')\n",
    "from_path = repo_path.joinpath(f'Applied GenAI/Evaluation/Vertex AI Agent Builder Check Grounding API.ipynb') #f'MLOps/Vertex AI Pipelines - Pattern - Modular and Reusable.ipynb')\n",
    "to_path = repo_path.joinpath(f'Applied GenAI/Validate/Vertex AI Agent Builder Check Grounding API.ipynb') #f'MLOps/Pipelines/Vertex AI Pipelines - Pattern - Modular and Reusable.ipynb')\n",
    "repo_path, from_path, to_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d04f689-f552-4657-8e03-be5bf3cff149",
   "metadata": {},
   "source": [
    "Clients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "058dae52-4435-4612-aa80-56800f24f978",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "repo = git.Repo(repo_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfec3bf5-3fa6-44d7-9a64-7f7f3a6b926c",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## Prepare For File Changes\n",
    "\n",
    "As the code in this workflow makes changes it will also stage and commit the changes.  For this reason it is important that no files be currently staged.  Take a moment to review staged files and finish committing them or remove them from staging.  This section will print staged files.  There is also code that will unstage any staged files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bbd35e98-2873-4ece-9615-e79154e4582a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No files are staged for commit.\n"
     ]
    }
   ],
   "source": [
    "staged_files = [item.a_path for item in repo.index.diff(\"HEAD\")]\n",
    "\n",
    "if staged_files:\n",
    "    print(\"Staged files:\")\n",
    "    for file in staged_files:\n",
    "        print(file)\n",
    "else:\n",
    "    print(\"No files are staged for commit.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebea210c-0e1c-4f6b-8db1-ddf3759c3811",
   "metadata": {},
   "source": [
    "**UNSTAGE ALL FILES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0396edf2-c066-4105-a05a-016ed8fb134f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No files to unstage.\n"
     ]
    }
   ],
   "source": [
    "if staged_files:\n",
    "    print('Unstaging files listed above:')\n",
    "    repo.git.restore('--staged', '.')\n",
    "else:\n",
    "    print('No files to unstage.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908460ab-7d36-4191-bfeb-3d7bfd6305ca",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## Move Folder/File\n",
    "\n",
    "This is a git repository so it is important to move the files with the commit history preserved using `git mv old_file new_file`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66b44546-c915-425f-8074-24387e37fb7a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "repo = git.Repo(repo_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "987a02bb-1adb-44be-bd92-d39211b1f248",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from_path.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9aa40a35-9a47-4f6e-a7aa-473b1e398140",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_path.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca537a9e-e001-477f-aa51-5d6aff0128cb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files moved from: \n",
      "\t/home/jupyter/vertex-ai-mlops/Applied GenAI/Evaluation/Vertex AI Agent Builder Check Grounding API.ipynb\n",
      "to:\n",
      "\t/home/jupyter/vertex-ai-mlops/Applied GenAI/Validate/Vertex AI Agent Builder Check Grounding API.ipynb\n",
      "Moved files commited.\n"
     ]
    }
   ],
   "source": [
    "if from_path.exists():\n",
    "    repo.git.mv(from_path, to_path)\n",
    "    print(f'Files moved from: \\n\\t{from_path}\\nto:\\n\\t{to_path}')\n",
    "    repo.index.commit('Moved file')\n",
    "    print(f'Moved files commited.')\n",
    "elif to_path.exists():\n",
    "    print(f'It appears the file(s) have already moved to:\\n\\t{to_path}')\n",
    "else:\n",
    "    print('Make sure the file(s) exists.  Currently not found in the from or to location')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ddcdcd3-9106-4faf-b563-e8f885db29f0",
   "metadata": {},
   "source": [
    "---\n",
    "## Files List\n",
    "\n",
    "Create a list of files (.md and .ipynb) including their new full path. If `to_path` was a file then the files list will have just the one file in it.  If `to_path` was a folder then all files in the folder will be included in the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a0d98b49-244f-4cb2-abee-26610834ec12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def file_list(from_path, to_path):\n",
    "    # returns a list of tuples for files that contain (from_file_path, to_file_path)\n",
    "    files = []\n",
    "    if to_path.is_dir():\n",
    "        for nb in to_path.glob(\"*.ipynb\"):\n",
    "            files.append(\n",
    "                (\n",
    "                    from_path.joinpath(nb.name),\n",
    "                    nb\n",
    "                )\n",
    "            )\n",
    "        for md in to_path.glob(\"*.md\"):\n",
    "            files.append(\n",
    "                (\n",
    "                    from_path.joinpath(md.name),\n",
    "                    md\n",
    "                )\n",
    "            )\n",
    "    elif to_path.is_file() and to_path.suffix in ['.md', '.ipynb']:\n",
    "        files.append(\n",
    "            (\n",
    "                from_path.parent.joinpath(to_path.name),\n",
    "                to_path\n",
    "            )\n",
    "        )\n",
    "    else:\n",
    "        print(f'Check for existance of file/folder: {to_path}')\n",
    "\n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0cbeb89b-a80c-4351-b5d2-57d993eba875",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(PosixPath('/home/jupyter/vertex-ai-mlops/Applied GenAI/Evaluation/Vertex AI Agent Builder Check Grounding API.ipynb'),\n",
       "  PosixPath('/home/jupyter/vertex-ai-mlops/Applied GenAI/Validate/Vertex AI Agent Builder Check Grounding API.ipynb'))]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = file_list(from_path, to_path)\n",
    "files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af29f511-03f7-4903-940b-4a619d7381bc",
   "metadata": {},
   "source": [
    "---\n",
    "## Find and Fix Links\n",
    "\n",
    "Go through the contents of each `.ipynb` and `.md` file and detect any relative links used in markdown or HTML:\n",
    "- resolve the link to an absolute path using the `from_path`\n",
    "- check to see if the link exists\n",
    "- prepare a new version that is relative given the `to_path`\n",
    "- check to see if the new link exists\n",
    "- update the relative link in the file\n",
    "- stage file for commit\n",
    "- complete commit after all file changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "35c0be0c-c528-47ef-9488-3969b2a16c43",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# new version\n",
    "def find_relative_links(files):\n",
    "    # files is a list of tuples (from_file_path, to_file_path) for all files in this move (like a full directory)\n",
    "    #      from_file_path is a pathlib.Path object with full path to the original file location\n",
    "    #      to_file_path is a pathlib.Path object with full path to the new file location\n",
    "    # returns: a list of tuples for files that contain (from_file_path, to_file_path, link, new_link)\n",
    "    # actively updates the files in place with new relative links\n",
    "    \n",
    "    \n",
    "    relative_links = []\n",
    "    regex = r\"(?:\\[.*?\\]\\((.*?)\\)|<\\w+\\s+[^>]*?(?:href|src)=(['\\\"])(.*?)\\2)\" # capture markdown and qouted links in href and src\n",
    "    \n",
    "    def link_fixer(from_file_path, to_file_path, link, files):\n",
    "        link = urllib.parse.unquote(link)\n",
    "        abs_link = (from_file_path.parent / link).resolve()\n",
    "        link_also_moved = any(ffp == abs_link for ffp, ttp in files)\n",
    "        common_path = pathlib.Path(os.path.commonpath([to_file_path, abs_link]))\n",
    "        new_link = (len(to_file_path.parent.parts) - len(common_path.parts))*'../' + (len(to_file_path.parent.parts) == len(common_path.parts)) *'./' + str(abs_link.relative_to(common_path))\n",
    "        abs_new_link = (to_file_path.parent / new_link).resolve()\n",
    "        if link_also_moved:\n",
    "            new_link = link\n",
    "        elif not abs_link.exists():\n",
    "            print(f\"Error fixing link: this link was broken before the move:\\n\\t- file: {to_file_path}\\n\\t- link: {link}\\n\\t- abs_link: {abs_link}\")\n",
    "            new_link = None\n",
    "            check_link = (to_file_path.parent / link).resolve()\n",
    "            # maybe it was already fixed by a run of this code???\n",
    "            if check_link.exists():\n",
    "                print(f\"\\t- Wait! It looks like the link has possibly already been fixed!\")\n",
    "                new_link = link\n",
    "        elif not abs_new_link.exists():\n",
    "            print(f\"Failed to fix the link:\\n\\t- old link = {link}\\n\\t- new link = {abs_new_link}\")\n",
    "            new_link = None\n",
    "        \n",
    "        if new_link:\n",
    "            new_link = urllib.parse.quote(new_link)\n",
    "        \n",
    "        return new_link\n",
    "\n",
    "    \n",
    "    for from_file_path, to_file_path in files:\n",
    "        if to_file_path.suffix == '.ipynb':\n",
    "            modification = False\n",
    "            nb = nbformat.read(to_file_path, nbformat.NO_CONVERT)\n",
    "            for cell in nb.cells:\n",
    "                if cell.cell_type == 'markdown':\n",
    "                    links = re.findall(regex, cell.source)\n",
    "                    for link in links:\n",
    "                        link = link[0] or link[2]\n",
    "                        if not link.startswith(\"http\") and not link.startswith('/'):\n",
    "                            new_link = link_fixer(from_file_path, to_file_path, link, files)\n",
    "                            if new_link and new_link != link:  # Check if the link actually changed\n",
    "                                modification = True\n",
    "                                relative_links.append((from_file_path, to_file_path, link, new_link))\n",
    "                                cell.source = cell.source.replace(link, new_link)\n",
    "            if modification:\n",
    "                print(f'Modifying: {to_file_path}')\n",
    "                # save changes\n",
    "                nbformat.write(nb, to_file_path)\n",
    "                # stage file\n",
    "                repo.git.add(str(to_file_path))\n",
    "\n",
    "        elif to_file_path.suffix == '.md':\n",
    "            modification = False\n",
    "            with open(to_file_path, \"r\") as f:\n",
    "                content = f.read()\n",
    "                links = re.findall(regex, content)\n",
    "                for link in links:\n",
    "                    link = link[0] or link[2]\n",
    "                    if not link.startswith('http') and not link.startswith('/'):\n",
    "                        new_link = link_fixer(from_file_path, to_file_path, link, files)\n",
    "                        if new_link and new_link != link:\n",
    "                            modification = True\n",
    "                            relative_links.append((from_file_path, to_file_path, link, new_link))\n",
    "                            content = content.replace(link, new_link)\n",
    "            if modification:\n",
    "                print(f'Modifying: {to_file_path}')\n",
    "                # save changes\n",
    "                with open(to_file_path, \"w\") as f:\n",
    "                    f.write(content)\n",
    "                # stage file\n",
    "                repo.git.add(str(to_file_path))\n",
    "\n",
    "    # if staged changes then commit them\n",
    "    if repo.index.diff(\"HEAD\"):\n",
    "        repo.index.commit(\"Fixed relative links within moved files, include to each other\")\n",
    "        print('Commited Changed Files')\n",
    "                \n",
    "    return relative_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "01b5259e-a28d-4576-9a61-f77af31a2f89",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#del(relative_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "abba860a-019f-43c1-b4a2-0be3ffff7833",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fixing link: this link was broken before the move:\n",
      "\t- file: /home/jupyter/vertex-ai-mlops/Applied GenAI/Validate/Vertex AI Agent Builder Check Grounding API.ipynb\n",
      "\t- link: ./The Math of Similarity.ipynb\n",
      "\t- abs_link: /home/jupyter/vertex-ai-mlops/Applied GenAI/Evaluation/The Math of Similarity.ipynb\n"
     ]
    }
   ],
   "source": [
    "if 'relative_links' in locals() or 'relative_links' in globals():\n",
    "    print(f'Relative Links already reviewed and fixed: {len(relative_links)}')\n",
    "else:\n",
    "    relative_links = find_relative_links(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c37fea7c-ee52-403c-a09e-a0b00eafeed8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relative_links"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e83e2b-c56f-40db-a23a-779754ccc05b",
   "metadata": {},
   "source": [
    "---\n",
    "## Add/Edit Banner With Location Change History\n",
    "Make a section at the top of .md and .ipynb files indicating location changes with dates.  Files with changes will be staged+commit.\n",
    "\n",
    "---\n",
    "Not production code but this is a working version.  These are test that I do to make sure it is working properly:\n",
    "- run and check each file for markdown code and preview the display\n",
    "- rerun without changes to verify that it does not further update the files\n",
    "- rerun with the `change_note` changed to the version with tomorrows date, ensure that it adds change note to the files\n",
    "- rerun with the same tomorrow date for `change_note` and verify it does not further update the files\n",
    "- discard all changes\n",
    "- ensure `change_note` is set to today's date version.\n",
    "\n",
    "\n",
    "**NOTE** Running this on the same moved files on different dates will result in multiple banner entries...\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "73169355-2df6-4a0c-92d8-5bb59c6201ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "update = 0\n",
    "for from_file_path, to_file_path in files:\n",
    "    # change note construction\n",
    "    change_lead = f\"---\\n\\n**File Move Notices**\\n\\nThis file moved locations:\"\n",
    "    change_note = f\"\"\"\\n- On {datetime.date.today().strftime(\"%m/%d/%Y\")} (mm/dd/yyyy)\\n\\t- From: `{from_file_path.relative_to(repo_path)}`\\n\\t- To: `{to_file_path.relative_to(repo_path)}`\"\"\"\n",
    "    #change_note = f\"\"\"\\n- On {(datetime.date.today()+datetime.timedelta(days=1)).strftime(\"%m/%d/%Y\")} (mm/dd/yyyy)\\n\\t- From: `{from_file_path.relative_to(repo_path)}`\\n\\t- To: `{to_file_path.relative_to(repo_path)}`\"\"\"\n",
    "    change_wrap = f\"\\n---\\n<!---end of move notices--->\\n\\n\"\n",
    "    \n",
    "    # notebook files\n",
    "    if to_file_path.suffix == \".ipynb\":\n",
    "        nb = nbformat.read(to_file_path, nbformat.NO_CONVERT)\n",
    "    \n",
    "        # detect existing header in file\n",
    "        if nb['cells'][0]['cell_type'] == 'markdown':\n",
    "            content = nb['cells'][0]['source']\n",
    "            if content.startswith('<!--- header table --->') or content.startswith('![tracker](https://'):\n",
    "                start_cell = 1 # second cell\n",
    "            else:\n",
    "                start_cell = 0 # first cell\n",
    "        \n",
    "        # detect existing file change info - after header\n",
    "        if nb['cells'][start_cell]['cell_type'] == 'markdown':\n",
    "            content = nb['cells'][start_cell]['source']\n",
    "            if content.startswith(change_lead):\n",
    "                changes = content[0:(content.index(change_wrap)+len(change_wrap))]\n",
    "                content = content[(content.index(change_wrap)+len(change_wrap)):]\n",
    "            else:\n",
    "                changes = ''\n",
    "        else:\n",
    "            print('No starting markdown cell')\n",
    "            \n",
    "        # edit/update change info\n",
    "        if change_lead not in changes and len(changes) < 1:\n",
    "            changes = change_lead+change_note+change_wrap\n",
    "            content = changes+content\n",
    "            update += 1\n",
    "            nb['cells'][start_cell]['source'] = content\n",
    "            nbformat.write(nb, to_file_path)\n",
    "            # stage file here\n",
    "            repo.git.add(str(to_file_path))\n",
    "        elif change_note not in changes:\n",
    "            changes = changes[0:-1*len(change_wrap)]+change_note+change_wrap\n",
    "            content = changes+content\n",
    "            update += 1\n",
    "            nb['cells'][start_cell]['source'] = content\n",
    "            nbformat.write(nb, to_file_path)\n",
    "            # stage file here\n",
    "            repo.git.add(str(to_file_path))\n",
    "        elif change_note in changes:\n",
    "            update += 0\n",
    "            \n",
    "    # markdown files\n",
    "    elif to_file_path.suffix == \".md\":\n",
    "        with open(to_file_path, \"r\") as f:\n",
    "            content = f.read()\n",
    "            \n",
    "        # detect existing header in file\n",
    "        if content.startswith('<!--- header table --->') or content.startswith('![tracker](https://'):\n",
    "            end_str = '</table><br/><br/><br/><br/>\\n\\n'\n",
    "            end_index = content.index(end_str)+len(end_str)\n",
    "            header = content[0:end_index]\n",
    "            content = content[end_index:]\n",
    "        else:\n",
    "            print(f'Header is missing from: {to_file_path}')\n",
    "            header = ''\n",
    "            content = content\n",
    "        \n",
    "        # detect existing file change info - after header\n",
    "        if change_lead not in content:\n",
    "            changes = ''\n",
    "            content = content[content.index('#'):]\n",
    "        else:\n",
    "            changes = content[0:(content.index(change_wrap)+len(change_wrap))]\n",
    "            content = content[content.index('#'):]\n",
    "        \n",
    "        # edit/update change info\n",
    "        if change_lead not in changes and len(changes) < 1:\n",
    "            changes = change_lead+change_note+change_wrap\n",
    "            content = header+changes+content\n",
    "            update += 1\n",
    "            with open(to_file_path, 'w') as f:\n",
    "                f.write(content)\n",
    "            # stage file here\n",
    "            repo.git.add(str(to_file_path))\n",
    "        elif change_note not in changes:\n",
    "            changes = changes[0:-1*len(change_wrap)]+change_note+change_wrap\n",
    "            content = header+changes+content\n",
    "            update += 1\n",
    "            with open(to_file_path, 'w') as f:\n",
    "                f.write(content)\n",
    "            # stage file here\n",
    "            repo.git.add(str(to_file_path))\n",
    "        elif change_note in changes:\n",
    "            update += 0        \n",
    "            \n",
    "    else:\n",
    "        print(f'No changes made to: {to_file_path}')\n",
    "\n",
    "# commit here if update > 0 (staged above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "63413d25-8275-4b8f-99ac-8772e333a3ca",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(files), update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ece0ee54-618a-4a54-a8a1-0d4b10da5eef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# if staged changes then commit them\n",
    "if update > 0 and repo.index.diff(\"HEAD\"):\n",
    "    repo.index.commit(\"Added banner to files indicate moved location\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939feb0e-c3cc-4258-8773-23d45df11b48",
   "metadata": {},
   "source": [
    "---\n",
    "## Check and Update references to the moved file\n",
    "\n",
    "Now that the file is moved any references to it will need to be updated.  This code will review all files in the repository for references to the `from_file_path` and update them to the `to_file_path`.  This includes relative links.  All the changed files will then be staged+commit.\n",
    "\n",
    "- list files\n",
    "- loop through files:\n",
    "    - read contents\n",
    "    - find relative links\n",
    "    - create absolute link\n",
    "    - see if absolute link in list of from files\n",
    "    - create new link to to_files\n",
    "    - replace link in file\n",
    "    - save file\n",
    "    - stage file\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "955bf176-4a59-4e6b-8472-84e3d4dac920",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(PosixPath('/home/jupyter/vertex-ai-mlops/Applied GenAI/Evaluation/Vertex AI Agent Builder Check Grounding API.ipynb'),\n",
       "  PosixPath('/home/jupyter/vertex-ai-mlops/Applied GenAI/Validate/Vertex AI Agent Builder Check Grounding API.ipynb'))]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files # a list of (from_file_path, to_file_path) tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d5aeacdf-8eb6-4185-84a8-3ad79c614834",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get a list of .md and .ipynb files from the full repository:\n",
    "matches = list(repo_path.rglob(\"*.md\")) + list(repo_path.rglob(\"*.ipynb\"))\n",
    "# filter out any that are in .ipynb_checkpoints directory or a known to_file_path (already edited above)\n",
    "matches = [\n",
    "    match_file_path\n",
    "    for match_file_path in matches\n",
    "    if match_file_path not in [to_file_path for _, to_file_path in files]\n",
    "    and '.ipynb_checkpoints' not in match_file_path.parts\n",
    "]\n",
    "#matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b77c270f-42ca-4a8a-bf03-949a7ef5ff93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_cross_links(matches, files):\n",
    "    # matches is a list of file in the repo that did not move: .md, .ipynb\n",
    "    # files is a list (from_file_path, to_file_path) of files that did move: .md, .ipynb\n",
    "    # returns cross_links = [(match, link, new_link)]\n",
    "    regex = r\"(?:\\[.*?\\]\\((.*?)\\)|<\\w+\\s+[^>]*?(?:href|src)=(['\\\"])(.*?)\\2)\" # capture markdown and qouted links in href and src\n",
    "    cross_links = []\n",
    "    \n",
    "    def eval_cross_link(match_file_path, link, files):\n",
    "        link = urllib.parse.unquote(link)\n",
    "        abs_link = (match_file_path.parent / link).resolve()\n",
    "        for from_file_path, to_file_path in files:\n",
    "            if abs_link == from_file_path:\n",
    "                # get the common path between the new location and the match_file_path\n",
    "                common_path = pathlib.Path(os.path.commonpath([to_file_path, match_file_path]))\n",
    "\n",
    "                # calculate the number of directory levels:\n",
    "                common_parts = len(common_path.parts)\n",
    "                match_parts = len(match_file_path.parent.parts)\n",
    "\n",
    "                # construct the relative link from the match_file_path to the to_file_path \n",
    "                new_link = (match_parts - common_parts)*'../' + (match_parts == common_parts)*'./' +str(to_file_path.relative_to(common_path))\n",
    "                if to_file_path.exists():\n",
    "                    new_link = urllib.parse.quote(new_link)\n",
    "                    return new_link\n",
    "                break # can only match one from_file_path\n",
    "            \n",
    "    \n",
    "    for match in matches:\n",
    "        if match.suffix == '.ipynb':\n",
    "            modification = False\n",
    "            nb = nbformat.read(match, nbformat.NO_CONVERT)\n",
    "            for cell in nb.cells:\n",
    "                if cell.cell_type == 'markdown':\n",
    "                    links = re.findall(regex, cell.source)\n",
    "                    for link in links:\n",
    "                        link = link[0] or link[2]\n",
    "                        if not link.startswith(\"http\") and not link.startswith('/'):\n",
    "                            new_link = eval_cross_link(match, link, files)\n",
    "                            if new_link and new_link != link:\n",
    "                                modification = True\n",
    "                                cross_links.append((match, link, new_link))\n",
    "                                cell.source = cell.source.replace(link, new_link)\n",
    "            if modification:\n",
    "                print(f'Modifying: {match}')\n",
    "                # save changes\n",
    "                nbformat.write(nb, match)\n",
    "                # stage file\n",
    "                repo.git.add(str(match))\n",
    "                \n",
    "        elif match.suffix == '.md':\n",
    "            modification = False\n",
    "            with open(match, \"r\") as f:\n",
    "                content = f.read()\n",
    "                links = re.findall(regex, content)\n",
    "                for link in links:\n",
    "                    link = link[0] or link[2]\n",
    "                    if not link.startswith('http') and not link.startswith('/'):\n",
    "                        new_link = eval_cross_link(match, link, files)\n",
    "                        if new_link and new_link != link:\n",
    "                            modification = True\n",
    "                            cross_links.append((match, link, new_link))\n",
    "                            content = content.replace(link, new_link)\n",
    "            if modification:\n",
    "                print(f'Modifying: {match}')\n",
    "                # save changes\n",
    "                with open(match, \"w\") as f:\n",
    "                    f.write(content)\n",
    "                # stage file\n",
    "                repo.git.add(str(match))\n",
    "    \n",
    "    # if staged changes then commit them\n",
    "    if repo.index.diff(\"HEAD\"):\n",
    "        repo.index.commit(\"Fixed relative links to moved files in non-moved files (cross-links)\")\n",
    "        print('Commited Changed Files')\n",
    "    \n",
    "    return cross_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "88f7d363-12aa-4535-9b76-ff5cafcbac59",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/jupyter/vertex-ai-mlops/03 - BigQuery ML (BQML)/BQML Remote Model Tutorial.md')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matches[39]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e33ed486-aaa7-418b-8de3-7092e013582f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modifying: /home/jupyter/vertex-ai-mlops/Applied GenAI/Evaluation/readme.md\n",
      "Commited Changed Files\n"
     ]
    }
   ],
   "source": [
    "cross_links = find_cross_links(matches, files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "10159d9e-fe4d-4c5d-a94a-b1c1740faba6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(PosixPath('/home/jupyter/vertex-ai-mlops/Applied GenAI/Evaluation/readme.md'),\n",
       "  'Vertex%20AI%20Agent%20Builder%20Check%20Grounding%20API.ipynb',\n",
       "  '../Validate/Vertex%20AI%20Agent%20Builder%20Check%20Grounding%20API.ipynb')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8b3abc10-7ef0-466f-80c0-f50614a0670b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(PosixPath('/home/jupyter/vertex-ai-mlops/Applied GenAI/Evaluation/Vertex AI Agent Builder Check Grounding API.ipynb'),\n",
       "  PosixPath('/home/jupyter/vertex-ai-mlops/Applied GenAI/Validate/Vertex AI Agent Builder Check Grounding API.ipynb'))]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075f520b-c47b-4d7d-aa19-7db916e6ace3",
   "metadata": {},
   "source": [
    "---\n",
    "## Some Checks Using `relative_links`\n",
    "\n",
    "(from_file_path, to_file_path, link, new_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5e90ac66-d937-4842-a04e-424ad4f8ae2f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(PosixPath('/home/jupyter/vertex-ai-mlops/Applied GenAI/Vertex AI GenAI For Document Q&A - Annual Report.ipynb'),\n",
       " PosixPath('/home/jupyter/vertex-ai-mlops/Applied GenAI/legacy/Vertex AI GenAI For Document Q&A - Annual Report.ipynb'),\n",
       " '../architectures/notebooks/applied/genai/doc_qa.png',\n",
       " '../../architectures/notebooks/applied/genai/doc_qa.png')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relative_links[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "62d65d25-de0b-4f5a-84a2-c2f241e81792",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(PosixPath('/home/jupyter/vertex-ai-mlops/Applied GenAI/Vertex AI GenAI For Document Q&A - Annual Report.ipynb'),\n",
       " PosixPath('/home/jupyter/vertex-ai-mlops/Applied GenAI/legacy/Vertex AI GenAI For Document Q&A - Annual Report.ipynb'),\n",
       " './Vertex%20AI%20Matching%20Engine%20For%20Document%20Q&A.ipynb',\n",
       " './Vertex%20AI%20Matching%20Engine%20For%20Document%20Q%26A.ipynb')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relative_links[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a1cd6e59-fad9-459f-b588-7b8c2b937a4b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(relative_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "df4ecff0-5f72-4f99-a6d6-ba299de001cd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./vertex_search_setup.md False\n",
      "./vertex_search_setup.md False\n",
      "../../../architectures/notebooks/applied/genai/vertex_ai_search/vertex_search_step_0.png True\n",
      "../../../architectures/notebooks/applied/genai/vertex_ai_search/vertex_search_step_1.png True\n",
      "../../../architectures/notebooks/applied/genai/vertex_ai_search/vertex_search_step_2.png True\n",
      "../../../architectures/notebooks/applied/genai/vertex_ai_search/vertex_search_step_3.png True\n",
      "../../../architectures/notebooks/applied/genai/vertex_ai_search/vertex_search_step_4.png True\n",
      "../../../architectures/notebooks/applied/genai/vertex_ai_search/vertex_search_step_5.png True\n",
      "../../../architectures/notebooks/applied/genai/vertex_ai_search/vertex_search_step_6.png True\n",
      "../../../architectures/notebooks/applied/genai/vertex_ai_search/vertex_search_step_7.png True\n",
      "../../../architectures/notebooks/applied/genai/vertex_ai_search/vertex_search_step_8.png True\n",
      "../../../architectures/notebooks/applied/genai/vertex_ai_search/vertex_search_step_9.png True\n",
      "./vertex_search_setup.md False\n",
      "./Vertex%20AI%20Search%20Python%20Client%20Overview.ipynb False\n"
     ]
    }
   ],
   "source": [
    "for rl in relative_links:\n",
    "    print(rl[3], rl[3]!=rl[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79a4ee3-df59-46d7-a0c6-d4089de55c33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3fddbd3-9202-4c7e-9696-f40197471a89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m125",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m125"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
