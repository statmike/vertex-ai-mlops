{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "![tracker](https://us-central1-vertex-ai-mlops-369716.cloudfunctions.net/pixel-tracking?path=statmike%2Fvertex-ai-mlops%2FFramework+Workflows%2FPyTorch%2Fserving&file=vertex-ai-endpoint-custom-container.ipynb)\n",
    "<!--- header table --->\n",
    "<table align=\"left\">\n",
    "<tr>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%20Workflows/PyTorch/serving/vertex-ai-endpoint-custom-container.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.svgrepo.com/download/217753/github.svg\" alt=\"GitHub logo\">\n",
    "      <br>View on<br>GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://colab.research.google.com/github/statmike/vertex-ai-mlops/blob/main/Framework%20Workflows/PyTorch/serving/vertex-ai-endpoint-custom-container.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.gstatic.com/pantheon/images/bigquery/welcome_page/colab-logo.svg\" alt=\"Google Colaboratory logo\">\n",
    "      <br>Run in<br>Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https%3A%2F%2Fraw.githubusercontent.com%2Fstatmike%2Fvertex-ai-mlops%2Fmain%2FFramework%2520Workflows%2FPyTorch%2Fserving%2Fvertex-ai-endpoint-custom-container.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\">\n",
    "      <br>Run in<br>Colab Enterprise\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/bigquery/import?url=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%20Workflows/PyTorch/serving/vertex-ai-endpoint-custom-container.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.gstatic.com/images/branding/gcpiconscolors/bigquery/v1/32px.svg\" alt=\"BigQuery logo\">\n",
    "      <br>Open in<br>BigQuery Studio\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/statmike/vertex-ai-mlops/main/Framework%20Workflows/PyTorch/serving/vertex-ai-endpoint-custom-container.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.gstatic.com/images/branding/gcpiconscolors/vertexai/v1/32px.svg\" alt=\"Vertex AI logo\">\n",
    "      <br>Open in<br>Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td colspan=\"5\" style=\"text-align: right\">\n",
    "    <b>Share This On: </b> \n",
    "    <a href=\"https://www.linkedin.com/sharing/share-offsite/?url=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%2520Workflows/PyTorch/serving/vertex-ai-endpoint-custom-container.ipynb\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"Linkedin Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://reddit.com/submit?url=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%2520Workflows/PyTorch/serving/vertex-ai-endpoint-custom-container.ipynb\"><img src=\"https://redditinc.com/hubfs/Reddit%20Inc/Brand/Reddit_Logo.png\" alt=\"Reddit Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://bsky.app/intent/compose?text=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%2520Workflows/PyTorch/serving/vertex-ai-endpoint-custom-container.ipynb\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"BlueSky Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://twitter.com/intent/tweet?url=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%2520Workflows/PyTorch/serving/vertex-ai-endpoint-custom-container.ipynb\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/5/5a/X_icon_2.svg\" alt=\"X (Twitter) Logo\" width=\"20px\"></a> \n",
    "  </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td colspan=\"5\" style=\"text-align: right\">\n",
    "    <b>Connect With Author On: </b> \n",
    "    <a href=\"https://www.linkedin.com/in/statmike\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"Linkedin Logo\" width=\"20px\"></a>\n",
    "    <a href=\"https://www.github.com/statmike\"><img src=\"https://www.svgrepo.com/download/217753/github.svg\" alt=\"GitHub Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://www.youtube.com/@statmike-channel\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/f/fd/YouTube_full-color_icon_%282024%29.svg\" alt=\"YouTube Logo\" width=\"20px\"></a>\n",
    "    <a href=\"https://bsky.app/profile/statmike.bsky.social\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"BlueSky Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://x.com/statmike\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/5/5a/X_icon_2.svg\" alt=\"X (Twitter) Logo\" width=\"20px\"></a>\n",
    "  </td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overview",
   "metadata": {},
   "source": [
    "# Deploy PyTorch Model with Custom Container\n",
    "\n",
    "This notebook demonstrates deploying a PyTorch model using a **custom container with FastAPI** for full control over prediction outputs.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "This workflow covers:\n",
    "\n",
    "1. **Upload Model to GCS**: Ensure the .mar file is available in Cloud Storage\n",
    "2. **Build Custom FastAPI Container**: Create a container that wraps your PyTorch model with custom logic\n",
    "3. **Control Output Format**: Return only specific outputs (e.g., anomaly scores) instead of full model output\n",
    "4. **Use Cloud Build**: Build and push containers to Artifact Registry\n",
    "5. **Deploy to Vertex AI**: Use the custom container on Vertex AI Endpoints\n",
    "6. **Compare Approaches**: Understand when to use custom vs pre-built containers\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Completed the `pytorch-autoencoder.ipynb` notebook (created the .mar file)\n",
    "- Google Cloud project with APIs enabled\n",
    "\n",
    "**Note:** This notebook can be run independently of the pre-built container notebook. Both handle uploading the .mar file to GCS.\n",
    "\n",
    "## Why Custom Containers?\n",
    "\n",
    "**Pre-built containers** (see `vertex-ai-endpoint-prebuilt-container.ipynb`) are great for:\n",
    "- ✅ Quick deployment\n",
    "- ✅ Standard PyTorch/TorchServe setup\n",
    "- ✅ Full model output needed\n",
    "\n",
    "**Custom containers** are better when you need:\n",
    "- ✅ **Custom output formatting** - Return only anomaly scores instead of 13 outputs\n",
    "- ✅ **Reduced network traffic** - Send only what clients need\n",
    "- ✅ **Custom preprocessing** - Add logic not in the model\n",
    "- ✅ **Multiple output versions** - Different endpoints with different outputs\n",
    "- ✅ **Framework flexibility** - Not limited to TorchServe\n",
    "\n",
    "## Deployment Architecture\n",
    "\n",
    "```\n",
    "PyTorch Model (.mar) → Extract .pt → FastAPI Container → Artifact Registry → Vertex AI Endpoint\n",
    "```\n",
    "\n",
    "**Key Difference from Pre-built:**\n",
    "- Pre-built: TorchServe handles everything (returns all 13 outputs)\n",
    "- Custom: FastAPI wrapper extracts .pt and returns only what you want (e.g., just anomaly scores)\n",
    "\n",
    "## What We're Building\n",
    "\n",
    "A custom FastAPI container that:\n",
    "1. Downloads the .mar file from GCS at startup\n",
    "2. Extracts the TorchScript .pt file\n",
    "3. Loads the model with `torch.jit.load()`\n",
    "4. **Returns simplified output**: Just anomaly scores and embeddings (not all 13 dict keys)\n",
    "5. Reduces response size by ~70%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "env_setup",
   "metadata": {},
   "source": [
    "---\n",
    "## Environment Setup\n",
    "\n",
    "This section will authenticate your session, enable required Google Cloud APIs, and install necessary Python packages.\n",
    "\n",
    "**Package Installation Options (`REQ_TYPE`):**\n",
    "- `PRIMARY`: Installs only the main packages. Faster, but pip resolves sub-dependencies which may result in different versions than development.\n",
    "- `ALL` (Default): Installs exact versions of all packages and dependencies. Best for perfectly reproducing the development environment.\n",
    "- `COLAB`: Installs a Colab-optimized list that excludes pre-installed packages like `ipython` and `ipykernel`.\n",
    "\n",
    "**Installation Tool Options (`INSTALL_TOOL`):**\n",
    "- `pip` (Default): Uses pip for package installation. Standard Python package installer.\n",
    "- `uv`: Modern, fast Python package installer. Must be installed separately. See: https://github.com/astral-sh/uv\n",
    "- `poetry`: Dependency management tool. Requires running notebook in a poetry environment (`poetry shell` or `poetry run jupyter lab`). Uses `pyproject.toml` instead of requirements.txt.\n",
    "\n",
    "> **Note:** If running in Google Colab, the script will automatically detect this and set `REQ_TYPE = 'COLAB'` to prevent package conflicts, overriding any manual setting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "set_project",
   "metadata": {},
   "source": [
    "### Set Your Project ID\n",
    "\n",
    "⚠️ **Action Required:** Replace the `PROJECT_ID` value below with your Google Cloud project ID before running this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "project_config",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = 'statmike-mlops-349915' # replace with GCP project ID\n",
    "REQ_TYPE = 'ALL' # Specify PRIMARY or ALL or COLAB\n",
    "INSTALL_TOOL = 'poetry' # Specify pip, uv, or poetry (all implemented)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config_header",
   "metadata": {},
   "source": [
    "### Configuration\n",
    "\n",
    "This cell defines the requirements files and Google Cloud APIs needed for this notebook. Run as-is without modification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": [
    "REQUIREMENTS_URL = 'https://raw.githubusercontent.com/statmike/vertex-ai-mlops/refs/heads/main/Framework%20Workflows/PyTorch/requirements.txt'\n",
    "\n",
    "REQUIRED_APIS = [\n",
    "    \"aiplatform.googleapis.com\",\n",
    "    \"storage.googleapis.com\",\n",
    "    \"artifactregistry.googleapis.com\",\n",
    "    \"cloudbuild.googleapis.com\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "run_setup_header",
   "metadata": {},
   "source": [
    "### Run Setup\n",
    "\n",
    "This cell downloads the centralized setup code and configures your environment. It will:\n",
    "- Authenticate your session with Google Cloud\n",
    "- Enable required APIs for this notebook (including Artifact Registry and Cloud Build)\n",
    "- Install necessary Python packages\n",
    "- Display a setup summary with your project information\n",
    "\n",
    "> **Note:** In Colab, if packages are installed, the kernel will automatically restart. After restart, continue from the next cell without re-running earlier cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "run_setup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "PYTHON GCP ENVIRONMENT SETUP\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "AUTHENTICATION\n",
      "==================================================\n",
      "Checking for existing ADC...\n",
      "✅ Existing ADC found.\n",
      "✅ Project is correctly set to 'statmike-mlops-349915'.\n",
      "\n",
      "==================================================\n",
      "API CHECK & ENABLE\n",
      "==================================================\n",
      "✅ aiplatform.googleapis.com is already enabled.\n",
      "✅ storage.googleapis.com is already enabled.\n",
      "✅ artifactregistry.googleapis.com is already enabled.\n",
      "✅ cloudbuild.googleapis.com is already enabled.\n",
      "\n",
      "==================================================\n",
      "PACKAGE MANAGEMENT\n",
      "==================================================\n",
      "Installation Tool: poetry\n",
      "✅ Found poetry at: /usr/local/google/home/statmike/.local/bin/poetry\n",
      "✅ Running in poetry environment: /usr/local/google/home/statmike/.cache/pypoetry/virtualenvs/frameworks-pytorch-0KVJlKeQ-py3.13\n",
      "ℹ️  Poetry mode: Installing from pyproject.toml (REQUIREMENTS_URL ignored)\n",
      "✅ Found pyproject.toml at: /usr/local/google/home/statmike/Git/vertex-ai-mlops/Framework Workflows/PyTorch/pyproject.toml\n",
      "   Changed working directory to: /usr/local/google/home/statmike/Git/vertex-ai-mlops/Framework Workflows/PyTorch\n",
      "Running poetry install...\n",
      "   Restored working directory to: /usr/local/google/home/statmike/Git/vertex-ai-mlops/Framework Workflows/PyTorch/serving\n",
      "✅ All packages are already installed and up to date.\n",
      "\n",
      "==================================================\n",
      "Google Cloud Project Information\n",
      "==================================================\n",
      "PROJECT_ID     = statmike-mlops-349915\n",
      "PROJECT_NUMBER = 1026793852137\n",
      "==================================================\n",
      "\n",
      "\n",
      "==================================================\n",
      "SETUP SUMMARY\n",
      "==================================================\n",
      "✅ Authentication:    Success\n",
      "✅ API Configuration: Success\n",
      "✅ Package Install:   Already up to date\n",
      "✅ Installation Tool: poetry\n",
      "✅ Project ID:        statmike-mlops-349915\n",
      "✅ Project Number:    1026793852137\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os, urllib.request\n",
    "\n",
    "# Download and import setup code\n",
    "url = 'https://raw.githubusercontent.com/statmike/vertex-ai-mlops/refs/heads/main/core/notebook-template/python_setup.py'\n",
    "urllib.request.urlretrieve(url, 'python_setup_local.py')\n",
    "import python_setup_local as python_setup\n",
    "os.remove('python_setup_local.py')\n",
    "\n",
    "# Run setup\n",
    "setup_info = python_setup.setup_environment(PROJECT_ID, REQ_TYPE, REQUIREMENTS_URL, REQUIRED_APIS, INSTALL_TOOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "python_setup",
   "metadata": {},
   "source": [
    "---\n",
    "## Python Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imports_header",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "imports",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/google/home/statmike/.cache/pypoetry/virtualenvs/frameworks-pytorch-0KVJlKeQ-py3.13/lib/python3.13/site-packages/google/cloud/aiplatform/models.py:52: FutureWarning: Support for google-cloud-storage < 3.0.0 will be removed in a future version of google-cloud-aiplatform. Please upgrade to google-cloud-storage >= 3.0.0.\n",
      "  from google.cloud.aiplatform.utils import gcs_utils\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import json\n",
    "import os\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud import storage\n",
    "from google.cloud import artifactregistry_v1\n",
    "from google.cloud.devtools import cloudbuild_v1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vars_user",
   "metadata": {},
   "source": [
    "### Variables - User Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "vars_user_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION = 'us-central1'\n",
    "SERIES = 'frameworks'\n",
    "EXPERIMENT = 'pytorch-autoencoder-custom'\n",
    "\n",
    "# Model configuration\n",
    "MODEL_DISPLAY_NAME = EXPERIMENT\n",
    "ENDPOINT_DISPLAY_NAME = f\"{EXPERIMENT}-endpoint\"\n",
    "\n",
    "# Machine configuration\n",
    "MACHINE_TYPE = 'n1-standard-4'\n",
    "MIN_REPLICA_COUNT = 1\n",
    "MAX_REPLICA_COUNT = 4\n",
    "\n",
    "# Working directory for source files\n",
    "DIR = f\"files/{EXPERIMENT}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vars_auto",
   "metadata": {},
   "source": [
    "### Variables - Auto Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "vars_auto_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Google Cloud Project Information\n",
      "==================================================\n",
      "PROJECT_ID     = statmike-mlops-349915\n",
      "PROJECT_NUMBER = 1026793852137\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "PROJECT_ID = subprocess.run(['gcloud', 'config', 'get-value', 'project'], capture_output=True, text=True, check=True).stdout.strip()\n",
    "PROJECT_NUMBER = subprocess.run(['gcloud', 'projects', 'describe', PROJECT_ID, '--format=value(projectNumber)'], capture_output=True, text=True, check=True).stdout.strip()\n",
    "\n",
    "print(f\"\\n{'='*50}\\nGoogle Cloud Project Information\\n{'='*50}\\nPROJECT_ID     = {PROJECT_ID}\\nPROJECT_NUMBER = {PROJECT_NUMBER}\\n{'='*50}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "configs",
   "metadata": {},
   "source": [
    "### Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "configs_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket: gs://statmike-mlops-349915\n",
      "Model artifacts: gs://statmike-mlops-349915/frameworks/pytorch-autoencoder\n",
      "Artifact Registry: us-central1-docker.pkg.dev/statmike-mlops-349915/frameworks\n",
      "Container image: us-central1-docker.pkg.dev/statmike-mlops-349915/frameworks/pytorch-autoencoder-custom\n"
     ]
    }
   ],
   "source": [
    "# GCS configuration\n",
    "BUCKET_NAME = PROJECT_ID\n",
    "BUCKET_URI = f\"gs://{BUCKET_NAME}\"\n",
    "MODEL_ARTIFACT_DIR = f\"{BUCKET_URI}/{SERIES}/pytorch-autoencoder\"\n",
    "\n",
    "# Artifact Registry repository\n",
    "AR_REPO_NAME = SERIES\n",
    "AR_REPO_URI = f\"{REGION}-docker.pkg.dev/{PROJECT_ID}/{AR_REPO_NAME}\"\n",
    "\n",
    "# Container image name\n",
    "CONTAINER_IMAGE = f\"{AR_REPO_URI}/{EXPERIMENT}\"\n",
    "\n",
    "print(f\"Bucket: {BUCKET_URI}\")\n",
    "print(f\"Model artifacts: {MODEL_ARTIFACT_DIR}\")\n",
    "print(f\"Artifact Registry: {AR_REPO_URI}\")\n",
    "print(f\"Container image: {CONTAINER_IMAGE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "init_clients",
   "metadata": {},
   "source": [
    "### Initialize Clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "init_clients_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Clients initialized\n",
      "   Project: statmike-mlops-349915\n",
      "   Location: us-central1\n"
     ]
    }
   ],
   "source": [
    "# Initialize clients\n",
    "storage_client = storage.Client(project=PROJECT_ID)\n",
    "ar_client = artifactregistry_v1.ArtifactRegistryClient()\n",
    "cb_client = cloudbuild_v1.CloudBuildClient()\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_URI)\n",
    "\n",
    "print(f\"✅ Clients initialized\")\n",
    "print(f\"   Project: {PROJECT_ID}\")\n",
    "print(f\"   Location: {REGION}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "create_dir",
   "metadata": {},
   "source": [
    "### Create Working Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "create_dir_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Created working directory: files/pytorch-autoencoder-custom/app\n"
     ]
    }
   ],
   "source": [
    "# Create directory for application files\n",
    "os.makedirs(f\"{DIR}/app\", exist_ok=True)\n",
    "print(f\"✅ Created working directory: {DIR}/app\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vltvmeb21pg",
   "metadata": {},
   "source": [
    "---\n",
    "## Upload Model to Cloud Storage\n",
    "\n",
    "Ensure the .mar file from the training notebook is uploaded to GCS. The custom container will download this file at startup to extract the TorchScript model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zmmz6r8ox3p",
   "metadata": {},
   "source": [
    "### Check for Local .mar File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "pw4zwgcr85q",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Found .mar file: ../files/pytorch-autoencoder/pytorch_autoencoder.mar\n",
      "   Size: 29,851 bytes (29.15 KB)\n"
     ]
    }
   ],
   "source": [
    "# Local path to .mar file (from pytorch-autoencoder notebook)\n",
    "LOCAL_MAR_PATH = '../files/pytorch-autoencoder/pytorch_autoencoder.mar'\n",
    "\n",
    "if os.path.exists(LOCAL_MAR_PATH):\n",
    "    file_size = os.path.getsize(LOCAL_MAR_PATH)\n",
    "    print(f\"✅ Found .mar file: {LOCAL_MAR_PATH}\")\n",
    "    print(f\"   Size: {file_size:,} bytes ({file_size / 1024:.2f} KB)\")\n",
    "else:\n",
    "    print(f\"❌ .mar file not found at: {LOCAL_MAR_PATH}\")\n",
    "    print(\"   Please run the pytorch-autoencoder.ipynb notebook first to create the .mar file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yflouufycr",
   "metadata": {},
   "source": [
    "### Create GCS Bucket (if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "o9zc3jikn1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Bucket already exists: gs://statmike-mlops-349915\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    bucket = storage_client.get_bucket(BUCKET_NAME)\n",
    "    print(f\"✅ Bucket already exists: {BUCKET_URI}\")\n",
    "except:\n",
    "    bucket = storage_client.create_bucket(BUCKET_NAME, location=REGION)\n",
    "    print(f\"✅ Created new bucket: {BUCKET_URI}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pu68umwzpho",
   "metadata": {},
   "source": [
    "### Upload .mar File to GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "nrwyxsxgj2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded .mar file to: gs://statmike-mlops-349915/frameworks/pytorch-autoencoder/model.mar\n",
      "   GCS size: 29,851 bytes (29.15 KB)\n",
      "   Note: Renamed from pytorch_autoencoder.mar to model.mar\n"
     ]
    }
   ],
   "source": [
    "# Model artifact in GCS (container expects \"model.mar\")\n",
    "GCS_MAR_PATH = f\"{MODEL_ARTIFACT_DIR}/model.mar\"\n",
    "\n",
    "# Upload the .mar file (rename to model.mar for consistency)\n",
    "blob = bucket.blob(f\"{SERIES}/pytorch-autoencoder/model.mar\")\n",
    "blob.upload_from_filename(LOCAL_MAR_PATH)\n",
    "\n",
    "print(f\"✅ Uploaded .mar file to: {GCS_MAR_PATH}\")\n",
    "print(f\"   GCS size: {blob.size:,} bytes ({blob.size / 1024:.2f} KB)\")\n",
    "print(f\"   Note: Renamed from {os.path.basename(LOCAL_MAR_PATH)} to model.mar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ar_setup",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup Artifact Registry\n",
    "\n",
    "Create or retrieve an Artifact Registry repository to store Docker containers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "check_ar",
   "metadata": {},
   "source": [
    "### Check for Existing Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "check_ar_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Using existing Artifact Registry repository:\n",
      "   Name: projects/statmike-mlops-349915/locations/us-central1/repositories/frameworks\n",
      "   Format: DOCKER\n"
     ]
    }
   ],
   "source": [
    "# Check if repository already exists\n",
    "ar_parent = f\"projects/{PROJECT_ID}/locations/{REGION}\"\n",
    "docker_repo = None\n",
    "\n",
    "for repo in ar_client.list_repositories(parent=ar_parent):\n",
    "    if repo.name.split('/')[-1] == AR_REPO_NAME:\n",
    "        docker_repo = repo\n",
    "        print(f\"✅ Using existing Artifact Registry repository:\")\n",
    "        print(f\"   Name: {docker_repo.name}\")\n",
    "        print(f\"   Format: {docker_repo.format_.name}\")\n",
    "        break\n",
    "\n",
    "if not docker_repo:\n",
    "    print(f\"Repository '{AR_REPO_NAME}' not found - will create it\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "create_ar",
   "metadata": {},
   "source": [
    "### Create Repository (if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "create_ar_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Repository ready: us-central1-docker.pkg.dev/statmike-mlops-349915/frameworks\n"
     ]
    }
   ],
   "source": [
    "if not docker_repo:\n",
    "    # Create new repository\n",
    "    operation = ar_client.create_repository(\n",
    "        request=artifactregistry_v1.CreateRepositoryRequest(\n",
    "            parent=ar_parent,\n",
    "            repository_id=AR_REPO_NAME,\n",
    "            repository=artifactregistry_v1.Repository(\n",
    "                description=f\"Docker images for {SERIES} series\",\n",
    "                format_=artifactregistry_v1.Repository.Format.DOCKER,\n",
    "                labels={'series': SERIES}\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    print(\"Creating Artifact Registry repository...\")\n",
    "    docker_repo = operation.result()\n",
    "    print(f\"✅ Created repository: {docker_repo.name}\")\n",
    "else:\n",
    "    print(f\"✅ Repository ready: {AR_REPO_URI}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "create_app",
   "metadata": {},
   "source": [
    "---\n",
    "## Create Application Files\n",
    "\n",
    "Build the custom FastAPI application that will wrap our PyTorch model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dockerfile_header",
   "metadata": {},
   "source": [
    "### Create Dockerfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dockerfile_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting files/pytorch-autoencoder-custom/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile {DIR}/Dockerfile\n",
    "# Use official Python runtime as base image\n",
    "FROM python:3.11-slim\n",
    "\n",
    "# Install Python dependencies\n",
    "RUN pip install --no-cache-dir \\\n",
    "    torch==2.0.1 --extra-index-url https://download.pytorch.org/whl/cpu && \\\n",
    "    pip install --no-cache-dir \\\n",
    "    fastapi==0.104.1 \\\n",
    "    uvicorn[standard]==0.24.0 \\\n",
    "    google-cloud-storage\n",
    "\n",
    "# Copy application code\n",
    "COPY ./app /app\n",
    "\n",
    "# Set working directory\n",
    "WORKDIR /app\n",
    "\n",
    "# Expose port\n",
    "EXPOSE 8080\n",
    "\n",
    "# Run FastAPI with uvicorn\n",
    "CMD [\"uvicorn\", \"main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8080\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fastapi_header",
   "metadata": {},
   "source": [
    "### Create FastAPI Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fastapi_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting files/pytorch-autoencoder-custom/app/main.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {DIR}/app/main.py\n",
    "import os\n",
    "import zipfile\n",
    "import logging\n",
    "from fastapi import FastAPI, Request\n",
    "import torch\n",
    "from google.cloud import storage\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "# Global model variable\n",
    "_model = None\n",
    "\n",
    "@app.on_event(\"startup\")\n",
    "async def startup_event():\n",
    "    \"\"\"Load model at startup\"\"\"\n",
    "    global _model\n",
    "    \n",
    "    try:\n",
    "        logger.info(\"Starting model loading...\")\n",
    "        \n",
    "        # Get storage URI from environment\n",
    "        storage_uri = os.environ.get('AIP_STORAGE_URI')\n",
    "        if not storage_uri:\n",
    "            raise ValueError(\"AIP_STORAGE_URI environment variable not set\")\n",
    "        \n",
    "        logger.info(f\"Storage URI: {storage_uri}\")\n",
    "        \n",
    "        # Download .mar file from GCS\n",
    "        gcs = storage.Client()\n",
    "        paths = storage_uri.replace('gs://', '').split('/')\n",
    "        bucket = gcs.bucket(paths[0])\n",
    "        mar_blob = bucket.blob('/'.join(paths[1:]) + '/model.mar')\n",
    "        \n",
    "        logger.info(f\"Downloading model.mar from {mar_blob.name}...\")\n",
    "        mar_blob.download_to_filename('model.mar')\n",
    "        logger.info(\"✅ Downloaded model.mar\")\n",
    "        \n",
    "        # Extract .pt file from .mar\n",
    "        logger.info(\"Extracting final_model_traced.pt from .mar...\")\n",
    "        with zipfile.ZipFile('model.mar', 'r') as zip_ref:\n",
    "            zip_ref.extract('final_model_traced.pt')\n",
    "        logger.info(\"✅ Extracted model file\")\n",
    "        \n",
    "        # Load PyTorch model\n",
    "        logger.info(\"Loading TorchScript model...\")\n",
    "        _model = torch.jit.load('final_model_traced.pt')\n",
    "        _model.eval()\n",
    "        logger.info(\"✅ Model loaded successfully and ready for predictions\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Failed to load model: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Health check endpoint - Vertex AI custom container format\n",
    "@app.get(\"/v1/endpoints/{endpoint_id}/deployedModels/{deployed_model_id}\", status_code=200)\n",
    "def vertex_health(endpoint_id: str, deployed_model_id: str):\n",
    "    \"\"\"Health check endpoint for Vertex AI custom containers\"\"\"\n",
    "    if _model is None:\n",
    "        return {\"status\": \"unhealthy\", \"reason\": \"model not loaded\"}\n",
    "    return {\"status\": \"healthy\"}\n",
    "\n",
    "# Health check endpoint - Standard route (for compatibility)\n",
    "@app.get(os.environ.get('AIP_HEALTH_ROUTE', '/health'), status_code=200)\n",
    "def health():\n",
    "    \"\"\"Health check endpoint\"\"\"\n",
    "    if _model is None:\n",
    "        return {\"status\": \"unhealthy\", \"reason\": \"model not loaded\"}\n",
    "    return {\"status\": \"healthy\"}\n",
    "\n",
    "# Prediction endpoint - Vertex AI custom container format\n",
    "@app.post(\"/v1/endpoints/{endpoint_id}/deployedModels/{deployed_model_id}:predict\")\n",
    "async def vertex_predict(endpoint_id: str, deployed_model_id: str, request: Request):\n",
    "    \"\"\"\n",
    "    Prediction endpoint for Vertex AI custom containers.\n",
    "    \n",
    "    Vertex AI sends predictions to this path format, not the configured AIP_PREDICT_ROUTE.\n",
    "    \"\"\"\n",
    "    return await predict_impl(request)\n",
    "\n",
    "# Prediction endpoint - Standard route (for compatibility)\n",
    "@app.post(os.environ.get('AIP_PREDICT_ROUTE', '/predict'))\n",
    "async def predict(request: Request):\n",
    "    \"\"\"Prediction endpoint (standard route)\"\"\"\n",
    "    return await predict_impl(request)\n",
    "\n",
    "async def predict_impl(request: Request):\n",
    "    \"\"\"\n",
    "    Core prediction implementation.\n",
    "    \n",
    "    Returns only anomaly scores and embeddings (2 fields)\n",
    "    instead of all 13 model outputs, reducing response size by ~70%\n",
    "    \"\"\"\n",
    "    global _model\n",
    "    \n",
    "    if _model is None:\n",
    "        return {\"error\": \"Model not loaded\"}, 503\n",
    "    \n",
    "    try:\n",
    "        body = await request.json()\n",
    "        instances = torch.tensor(body[\"instances\"], dtype=torch.float32)\n",
    "        \n",
    "        logger.info(f\"Processing {len(instances)} instances\")\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            full_output = _model(instances)\n",
    "        \n",
    "        # ✅ CUSTOM OUTPUT: Return only anomaly scores and embeddings\n",
    "        # This reduces response size by ~70% compared to full output\n",
    "        simplified = [\n",
    "            {\n",
    "                \"anomaly_score\": full_output[\"denormalized_MAE\"][i].item(),\n",
    "                \"encoded\": full_output[\"encoded\"][i].tolist()\n",
    "            }\n",
    "            for i in range(len(instances))\n",
    "        ]\n",
    "        \n",
    "        return {\"predictions\": simplified}\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Prediction error: {str(e)}\")\n",
    "        return {\"error\": str(e)}, 500"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "upload_source",
   "metadata": {},
   "source": [
    "### Upload Source Files to GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "upload_source_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded source files to gs://statmike-mlops-349915/frameworks/pytorch-autoencoder-custom/\n"
     ]
    }
   ],
   "source": [
    "# Upload application files to GCS for Cloud Build\n",
    "bucket = storage_client.bucket(BUCKET_NAME)\n",
    "\n",
    "bucket.blob(f'{SERIES}/{EXPERIMENT}/Dockerfile').upload_from_filename(f'{DIR}/Dockerfile')\n",
    "bucket.blob(f'{SERIES}/{EXPERIMENT}/app/main.py').upload_from_filename(f'{DIR}/app/main.py')\n",
    "\n",
    "print(f\"✅ Uploaded source files to gs://{BUCKET_NAME}/{SERIES}/{EXPERIMENT}/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "build_container",
   "metadata": {},
   "source": [
    "---\n",
    "## Build Container with Cloud Build\n",
    "\n",
    "Use Cloud Build to build the Docker container and push to Artifact Registry."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "build_config",
   "metadata": {},
   "source": [
    "### Create Build Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "build_config_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build configuration created\n"
     ]
    }
   ],
   "source": [
    "# Create Cloud Build configuration\n",
    "build = cloudbuild_v1.Build(\n",
    "    steps=[\n",
    "        # Step 1: Copy source from GCS\n",
    "        {\n",
    "            'name': 'gcr.io/cloud-builders/gsutil',\n",
    "            'args': ['cp', '-r', f'gs://{BUCKET_NAME}/{SERIES}/{EXPERIMENT}/*', '/workspace']\n",
    "        },\n",
    "        # Step 2: Build Docker image\n",
    "        {\n",
    "            'name': 'gcr.io/cloud-builders/docker',\n",
    "            'args': ['build', '-t', CONTAINER_IMAGE, '/workspace']\n",
    "        }\n",
    "    ],\n",
    "    # Push the image\n",
    "    images=[CONTAINER_IMAGE]\n",
    ")\n",
    "\n",
    "print(\"Build configuration created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "run_build",
   "metadata": {},
   "source": [
    "### Run Build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "run_build_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building container image: us-central1-docker.pkg.dev/statmike-mlops-349915/frameworks/pytorch-autoencoder-custom\n",
      "This will take 3-5 minutes...\n",
      "\n",
      "✅ Container built successfully!\n",
      "   Image: us-central1-docker.pkg.dev/statmike-mlops-349915/frameworks/pytorch-autoencoder-custom\n",
      "   Status: SUCCESS\n"
     ]
    }
   ],
   "source": [
    "# Submit build to Cloud Build\n",
    "print(f\"Building container image: {CONTAINER_IMAGE}\")\n",
    "print(\"This will take 3-5 minutes...\")\n",
    "\n",
    "operation = cb_client.create_build(project_id=PROJECT_ID, build=build)\n",
    "build_response = operation.result()\n",
    "\n",
    "if build_response.status == cloudbuild_v1.Build.Status.SUCCESS:\n",
    "    print(f\"\\n✅ Container built successfully!\")\n",
    "    print(f\"   Image: {CONTAINER_IMAGE}\")\n",
    "    print(f\"   Status: {build_response.status.name}\")\n",
    "else:\n",
    "    print(f\"❌ Build failed with status: {build_response.status.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model_registry",
   "metadata": {},
   "source": [
    "---\n",
    "## Register Model in Vertex AI Model Registry\n",
    "\n",
    "Upload the model using our custom container image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "check_model",
   "metadata": {},
   "source": [
    "### Check for Existing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "check_model_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No existing models found with name pytorch-autoencoder-custom\n"
     ]
    }
   ],
   "source": [
    "# Check if model already exists\n",
    "existing_models = aiplatform.Model.list(filter=f\"display_name={MODEL_DISPLAY_NAME}\")\n",
    "\n",
    "if existing_models:\n",
    "    model = existing_models[0]\n",
    "    print(f\"✅ Using existing model:\")\n",
    "    print(f\"   Resource name: {model.resource_name}\")\n",
    "    print(f\"   Display name: {model.display_name}\")\n",
    "    print(f\"   Version: {model.version_id}\")\n",
    "else:\n",
    "    print(f\"No existing models found with name {MODEL_DISPLAY_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "upload_model",
   "metadata": {},
   "source": [
    "### Upload Model to Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "upload_model_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Model\n",
      "Create Model backing LRO: projects/1026793852137/locations/us-central1/models/230241033391177728/operations/183771772171059200\n",
      "Model created. Resource name: projects/1026793852137/locations/us-central1/models/230241033391177728@1\n",
      "To use this Model in another session:\n",
      "model = aiplatform.Model('projects/1026793852137/locations/us-central1/models/230241033391177728@1')\n",
      "✅ Model uploaded to registry:\n",
      "   Resource name: projects/1026793852137/locations/us-central1/models/230241033391177728\n",
      "   Display name: pytorch-autoencoder-custom\n",
      "   Container: us-central1-docker.pkg.dev/statmike-mlops-349915/frameworks/pytorch-autoencoder-custom\n"
     ]
    }
   ],
   "source": [
    "if not existing_models:\n",
    "    # Upload model with custom container\n",
    "    model = aiplatform.Model.upload(\n",
    "        display_name=MODEL_DISPLAY_NAME,\n",
    "        artifact_uri=MODEL_ARTIFACT_DIR,\n",
    "        serving_container_image_uri=CONTAINER_IMAGE,\n",
    "        serving_container_environment_variables={\n",
    "            \"AIP_STORAGE_URI\": MODEL_ARTIFACT_DIR,\n",
    "            \"AIP_HEALTH_ROUTE\": \"/health\",\n",
    "            \"AIP_PREDICT_ROUTE\": \"/predict\",\n",
    "            \"AIP_HTTP_PORT\": \"8080\"\n",
    "        },\n",
    "        serving_container_ports=[8080],\n",
    "        description=\"PyTorch autoencoder with custom FastAPI container (simplified output)\"\n",
    "    )\n",
    "    \n",
    "    print(f\"✅ Model uploaded to registry:\")\n",
    "    print(f\"   Resource name: {model.resource_name}\")\n",
    "    print(f\"   Display name: {model.display_name}\")\n",
    "    print(f\"   Container: {CONTAINER_IMAGE}\")\n",
    "else:\n",
    "    print(\"✅ Using existing model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iam_permissions",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup Service Account for Custom Container\n",
    "\n",
    "Custom containers need a service account with access to GCS to download the model. We'll:\n",
    "\n",
    "1. Create a dedicated service account (if it doesn't exist)\n",
    "2. Grant it Storage Object Viewer role on the bucket\n",
    "3. Use this service account when deploying the model\n",
    "\n",
    "**Why this is needed**: Unlike pre-built containers (which use Vertex AI's managed service account with automatic access), custom containers require explicit service account configuration for security."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "create_sa_header",
   "metadata": {},
   "source": [
    "### Create Service Account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "create_sa_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Service Account: pytorch-autoencoder-custom-sa@statmike-mlops-349915.iam.gserviceaccount.com\n",
      "\n",
      "✅ Service account already exists: pytorch-autoencoder-custom-sa@statmike-mlops-349915.iam.gserviceaccount.com\n"
     ]
    }
   ],
   "source": [
    "# Service account for custom container\n",
    "SA_NAME = f\"{EXPERIMENT}-sa\"\n",
    "SA_EMAIL = f\"{SA_NAME}@{PROJECT_ID}.iam.gserviceaccount.com\"\n",
    "\n",
    "print(f\"Service Account: {SA_EMAIL}\\n\")\n",
    "\n",
    "# Check if service account exists\n",
    "check_sa = subprocess.run(\n",
    "    ['gcloud', 'iam', 'service-accounts', 'describe', SA_EMAIL, '--format=value(email)'],\n",
    "    capture_output=True,\n",
    "    text=True\n",
    ")\n",
    "\n",
    "if check_sa.returncode == 0:\n",
    "    print(f\"✅ Service account already exists: {SA_EMAIL}\")\n",
    "else:\n",
    "    # Create service account\n",
    "    result = subprocess.run(\n",
    "        ['gcloud', 'iam', 'service-accounts', 'create', SA_NAME,\n",
    "         '--display-name', f'Service account for {EXPERIMENT}',\n",
    "         '--description', 'Used by custom container to access GCS for model files'],\n",
    "        capture_output=True,\n",
    "        text=True\n",
    "    )\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        print(f\"✅ Created service account: {SA_EMAIL}\")\n",
    "    else:\n",
    "        print(f\"❌ Failed to create service account: {result.stderr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grant_sa_header",
   "metadata": {},
   "source": [
    "### Grant Storage Access to Service Account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "grant_sa_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Granting Storage Object Viewer role to: pytorch-autoencoder-custom-sa@statmike-mlops-349915.iam.gserviceaccount.com\n",
      "Bucket: gs://statmike-mlops-349915\n",
      "\n",
      "✅ Granted Storage Object Viewer role\n",
      "   Service Account: pytorch-autoencoder-custom-sa@statmike-mlops-349915.iam.gserviceaccount.com\n",
      "   Bucket: gs://statmike-mlops-349915\n",
      "\n",
      "   This allows the custom container to download model.mar from GCS\n"
     ]
    }
   ],
   "source": [
    "# Grant Storage Object Viewer role on the bucket\n",
    "print(f\"Granting Storage Object Viewer role to: {SA_EMAIL}\")\n",
    "print(f\"Bucket: gs://{BUCKET_NAME}\\n\")\n",
    "\n",
    "result = subprocess.run(\n",
    "    ['gsutil', 'iam', 'ch', f'serviceAccount:{SA_EMAIL}:roles/storage.objectViewer', f'gs://{BUCKET_NAME}'],\n",
    "    capture_output=True,\n",
    "    text=True\n",
    ")\n",
    "\n",
    "if result.returncode == 0:\n",
    "    print(f\"✅ Granted Storage Object Viewer role\")\n",
    "    print(f\"   Service Account: {SA_EMAIL}\")\n",
    "    print(f\"   Bucket: gs://{BUCKET_NAME}\")\n",
    "    print(f\"\\n   This allows the custom container to download model.mar from GCS\")\n",
    "else:\n",
    "    print(f\"Note: {result.stderr}\")\n",
    "    print(f\"The service account may already have this permission\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "endpoint_deploy",
   "metadata": {},
   "source": [
    "---\n",
    "## Deploy Model to Endpoint\n",
    "\n",
    "Create a Vertex AI Endpoint and deploy the custom container."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "check_endpoint",
   "metadata": {},
   "source": [
    "### Check for Existing Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "check_endpoint_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No existing endpoints found with name pytorch-autoencoder-custom-endpoint\n"
     ]
    }
   ],
   "source": [
    "# Check if endpoint already exists\n",
    "existing_endpoints = aiplatform.Endpoint.list(filter=f\"display_name={ENDPOINT_DISPLAY_NAME}\")\n",
    "\n",
    "if existing_endpoints:\n",
    "    endpoint = existing_endpoints[0]\n",
    "    print(f\"✅ Using existing endpoint:\")\n",
    "    print(f\"   Resource name: {endpoint.resource_name}\")\n",
    "    print(f\"   Display name: {endpoint.display_name}\")\n",
    "else:\n",
    "    print(f\"No existing endpoints found with name {ENDPOINT_DISPLAY_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "create_endpoint",
   "metadata": {},
   "source": [
    "### Create Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "create_endpoint_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Endpoint\n",
      "Create Endpoint backing LRO: projects/1026793852137/locations/us-central1/endpoints/461521105270603776/operations/1349641125706596352\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint created. Resource name: projects/1026793852137/locations/us-central1/endpoints/461521105270603776\n",
      "To use this Endpoint in another session:\n",
      "endpoint = aiplatform.Endpoint('projects/1026793852137/locations/us-central1/endpoints/461521105270603776')\n",
      "✅ Endpoint created:\n",
      "   Resource name: projects/1026793852137/locations/us-central1/endpoints/461521105270603776\n",
      "   Display name: pytorch-autoencoder-custom-endpoint\n"
     ]
    }
   ],
   "source": [
    "if not existing_endpoints:\n",
    "    # Create endpoint\n",
    "    endpoint = aiplatform.Endpoint.create(\n",
    "        display_name=ENDPOINT_DISPLAY_NAME,\n",
    "        description=\"Endpoint for PyTorch autoencoder with custom container\"\n",
    "    )\n",
    "    \n",
    "    print(f\"✅ Endpoint created:\")\n",
    "    print(f\"   Resource name: {endpoint.resource_name}\")\n",
    "    print(f\"   Display name: {endpoint.display_name}\")\n",
    "else:\n",
    "    print(\"✅ Using existing endpoint\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deploy_model",
   "metadata": {},
   "source": [
    "### Deploy Model to Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "deploy_model_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deploying model to endpoint (this will take 10-15 minutes)...\n",
      "You can monitor progress in the Cloud Console:\n",
      "https://console.cloud.google.com/vertex-ai/endpoints/461521105270603776?project=statmike-mlops-349915\n",
      "Deploying Model projects/1026793852137/locations/us-central1/models/230241033391177728 to Endpoint : projects/1026793852137/locations/us-central1/endpoints/461521105270603776\n",
      "Deploy Endpoint model backing LRO: projects/1026793852137/locations/us-central1/endpoints/461521105270603776/operations/3655484134920290304\n",
      "Endpoint model deployed. Resource name: projects/1026793852137/locations/us-central1/endpoints/461521105270603776\n",
      "\n",
      "✅ Model deployed successfully!\n",
      "   Endpoint: pytorch-autoencoder-custom-endpoint\n",
      "   Resource name: projects/1026793852137/locations/us-central1/endpoints/461521105270603776\n"
     ]
    }
   ],
   "source": [
    "# Check if model is already deployed to this endpoint\n",
    "deployed_models = endpoint.list_models()\n",
    "model_already_deployed = any(dm.model == model.resource_name for dm in deployed_models)\n",
    "\n",
    "if model_already_deployed:\n",
    "    print(f\"✅ Model already deployed to endpoint:\")\n",
    "    print(f\"   Endpoint: {endpoint.display_name}\")\n",
    "    print(f\"   Model: {model.display_name}\")\n",
    "    print(f\"   Skipping redeployment\")\n",
    "else:\n",
    "    # Deploy model to endpoint\n",
    "    print(\"Deploying model to endpoint (this will take 10-15 minutes)...\")\n",
    "    print(\"You can monitor progress in the Cloud Console:\")\n",
    "    print(f\"https://console.cloud.google.com/vertex-ai/endpoints/{endpoint.name.split('/')[-1]}?project={PROJECT_ID}\")\n",
    "\n",
    "    endpoint.deploy(\n",
    "        model=model,\n",
    "        deployed_model_display_name=MODEL_DISPLAY_NAME,\n",
    "        machine_type=MACHINE_TYPE,\n",
    "        min_replica_count=MIN_REPLICA_COUNT,\n",
    "        max_replica_count=MAX_REPLICA_COUNT,\n",
    "        traffic_percentage=100,\n",
    "        service_account=SA_EMAIL,  # Use custom service account with GCS access\n",
    "        sync=True\n",
    "    )\n",
    "\n",
    "    print(f\"\\n✅ Model deployed successfully!\")\n",
    "    print(f\"   Endpoint: {endpoint.display_name}\")\n",
    "    print(f\"   Resource name: {endpoint.resource_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "test_data",
   "metadata": {},
   "source": [
    "---\n",
    "## Prepare Test Data\n",
    "\n",
    "Retrieve test records from the same BigQuery source used in training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xeolcaklykp",
   "metadata": {},
   "source": [
    "### Import BigQuery Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "issh0wz53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "import pandas as pd\n",
    "\n",
    "bq = bigquery.Client(project=PROJECT_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kmy63j8kbs",
   "metadata": {},
   "source": [
    "### Retrieve Test Instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "test_data_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 3 test instances\n",
      "\n",
      "Features: ['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>122959.0</td>\n",
       "      <td>-1.327297</td>\n",
       "      <td>0.422904</td>\n",
       "      <td>1.617505</td>\n",
       "      <td>2.291196</td>\n",
       "      <td>2.375055</td>\n",
       "      <td>0.411735</td>\n",
       "      <td>0.213517</td>\n",
       "      <td>0.424743</td>\n",
       "      <td>-1.809624</td>\n",
       "      <td>...</td>\n",
       "      <td>0.068572</td>\n",
       "      <td>-0.337966</td>\n",
       "      <td>-1.461959</td>\n",
       "      <td>0.192604</td>\n",
       "      <td>0.068281</td>\n",
       "      <td>-0.245725</td>\n",
       "      <td>-0.697654</td>\n",
       "      <td>0.038216</td>\n",
       "      <td>0.150059</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>122312.0</td>\n",
       "      <td>-1.988557</td>\n",
       "      <td>-0.720301</td>\n",
       "      <td>0.863204</td>\n",
       "      <td>3.114494</td>\n",
       "      <td>1.847474</td>\n",
       "      <td>0.255881</td>\n",
       "      <td>0.580362</td>\n",
       "      <td>-0.083756</td>\n",
       "      <td>-0.939044</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.857829</td>\n",
       "      <td>-0.620289</td>\n",
       "      <td>-1.075636</td>\n",
       "      <td>1.564951</td>\n",
       "      <td>0.546312</td>\n",
       "      <td>-0.548531</td>\n",
       "      <td>-0.746620</td>\n",
       "      <td>-0.748016</td>\n",
       "      <td>0.410640</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>119592.0</td>\n",
       "      <td>2.139741</td>\n",
       "      <td>0.245651</td>\n",
       "      <td>-2.654856</td>\n",
       "      <td>0.178287</td>\n",
       "      <td>1.336991</td>\n",
       "      <td>-0.724664</td>\n",
       "      <td>0.906032</td>\n",
       "      <td>-0.436125</td>\n",
       "      <td>-0.528015</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.160988</td>\n",
       "      <td>0.296681</td>\n",
       "      <td>1.036285</td>\n",
       "      <td>-0.216033</td>\n",
       "      <td>0.345316</td>\n",
       "      <td>0.747103</td>\n",
       "      <td>0.700184</td>\n",
       "      <td>-0.123739</td>\n",
       "      <td>-0.099989</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Time        V1        V2        V3        V4        V5        V6  \\\n",
       "0  122959.0 -1.327297  0.422904  1.617505  2.291196  2.375055  0.411735   \n",
       "1  122312.0 -1.988557 -0.720301  0.863204  3.114494  1.847474  0.255881   \n",
       "2  119592.0  2.139741  0.245651 -2.654856  0.178287  1.336991 -0.724664   \n",
       "\n",
       "         V7        V8        V9  ...       V20       V21       V22       V23  \\\n",
       "0  0.213517  0.424743 -1.809624  ...  0.068572 -0.337966 -1.461959  0.192604   \n",
       "1  0.580362 -0.083756 -0.939044  ... -0.857829 -0.620289 -1.075636  1.564951   \n",
       "2  0.906032 -0.436125 -0.528015  ... -0.160988  0.296681  1.036285 -0.216033   \n",
       "\n",
       "        V24       V25       V26       V27       V28  Amount  \n",
       "0  0.068281 -0.245725 -0.697654  0.038216  0.150059     0.0  \n",
       "1  0.546312 -0.548531 -0.746620 -0.748016  0.410640     0.0  \n",
       "2  0.345316  0.747103  0.700184 -0.123739 -0.099989     0.0  \n",
       "\n",
       "[3 rows x 30 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BigQuery source (same as training notebook)\n",
    "BQ_PROJECT = PROJECT_ID\n",
    "BQ_DATASET = SERIES.replace('-', '_')\n",
    "BQ_TABLE = SERIES\n",
    "\n",
    "# Get a few test instances\n",
    "query = f\"\"\"\n",
    "SELECT * EXCEPT(splits, transaction_id, Class)\n",
    "FROM `{BQ_PROJECT}.{BQ_DATASET}.{BQ_TABLE}`\n",
    "WHERE splits = \"TEST\" AND Class = 0\n",
    "LIMIT 3\n",
    "\"\"\"\n",
    "\n",
    "test_df = bq.query(query).to_dataframe()\n",
    "print(f\"Retrieved {len(test_df)} test instances\")\n",
    "print(f\"\\nFeatures: {list(test_df.columns)}\")\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ali5wjhh9j",
   "metadata": {},
   "source": [
    "### Format Data for Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "r3i3bfz8os",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared 3 instances for prediction\n",
      "\n",
      "First instance (30 features):\n",
      "[122959.0, -1.3272968090323798, 0.422904408477484, 1.61750487263453, 2.29119608568876, 2.37505507675092, 0.411734608148703, 0.213516912086352, 0.42474305658616796, -1.8096243240512202, 0.563424163577428, -0.215715613427969, 0.255745374601212, 0.14372086809316198, 0.0664174538762353, -2.19269325676012, 1.5000437460373202, -1.48305134433897, -0.0973547276746839, -1.36598161864692, 0.0685717012577449, -0.337965984584885, -1.4619590513124, 0.19260414415618501, 0.0682811106065862, -0.24572504736969603, -0.697654195893893, 0.0382157166420934, 0.15005927811162398, 0.0]\n"
     ]
    }
   ],
   "source": [
    "# Convert to list of lists (format expected by endpoint)\n",
    "test_instances = test_df.values.tolist()\n",
    "\n",
    "print(f\"Prepared {len(test_instances)} instances for prediction\")\n",
    "print(f\"\\nFirst instance (30 features):\")\n",
    "print(test_instances[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "predictions",
   "metadata": {},
   "source": [
    "---\n",
    "## Make Online Predictions\n",
    "\n",
    "Test the deployed model using both the Vertex AI SDK and direct REST API calls."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "custom_pred",
   "metadata": {},
   "source": [
    "### Method 1: Vertex AI SDK\n",
    "\n",
    "The simplest way to make predictions using the Python SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "custom_pred_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Received predictions for 3 instances\n",
      "\n",
      "📊 Custom Container Output (Simplified):\n",
      "   Keys per prediction: ['anomaly_score', 'encoded']\n",
      "   Output size: 85 characters\n",
      "\n",
      "First prediction:\n",
      "   Anomaly Score: 62.1526\n",
      "   Encoded: [0.0, 0.0, 0.09415397047996521, 0.0]\n"
     ]
    }
   ],
   "source": [
    "# Make prediction using SDK\n",
    "sdk_response = endpoint.predict(instances=test_instances)\n",
    "sdk_predictions = sdk_response.predictions\n",
    "\n",
    "print(f\"✅ Received predictions for {len(sdk_predictions)} instances\")\n",
    "print(f\"\\n📊 Custom Container Output (Simplified):\")\n",
    "print(f\"   Keys per prediction: {list(sdk_predictions[0].keys())}\")\n",
    "print(f\"   Output size: {len(str(sdk_predictions[0]))} characters\")\n",
    "\n",
    "# The custom container returns only anomaly scores and encoded representations\n",
    "first_prediction = sdk_predictions[0]\n",
    "print(f\"\\nFirst prediction:\")\n",
    "print(f\"   Anomaly Score: {first_prediction['anomaly_score']:.4f}\")\n",
    "print(f\"   Encoded: {first_prediction['encoded']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "omolzg7us3c",
   "metadata": {},
   "source": [
    "### Analyze Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "jd65uxjw5v",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Summary:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instance</th>\n",
       "      <th>anomaly_score</th>\n",
       "      <th>encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>62.152649</td>\n",
       "      <td>[0.0, 0.0, 0.09415397047996521, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>373.453674</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>465.076721</td>\n",
       "      <td>[0.4030534327030182, 2.548543930053711, 2.4130...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   instance  anomaly_score                                            encoded\n",
       "0         0      62.152649               [0.0, 0.0, 0.09415397047996521, 0.0]\n",
       "1         1     373.453674                               [0.0, 0.0, 0.0, 0.0]\n",
       "2         2     465.076721  [0.4030534327030182, 2.548543930053711, 2.4130..."
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract key metrics from all predictions\n",
    "results = []\n",
    "for i, pred in enumerate(sdk_predictions):\n",
    "    results.append({\n",
    "        \"instance\": i,\n",
    "        \"anomaly_score\": pred[\"anomaly_score\"],\n",
    "        \"encoded\": pred[\"encoded\"]\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"Prediction Summary:\")\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "v1w610sc9la",
   "metadata": {},
   "source": [
    "### Method 2: REST API with Requests\n",
    "\n",
    "Make predictions using direct HTTP calls. This is useful for:\n",
    "- Non-Python clients\n",
    "- Testing from other services\n",
    "- Understanding the raw API format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7gz1gueusoe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint URL: https://us-central1-aiplatform.googleapis.com/v1/projects/1026793852137/locations/us-central1/endpoints/461521105270603776:predict\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import google.auth\n",
    "import google.auth.transport.requests\n",
    "\n",
    "# Get authentication token\n",
    "credentials, project = google.auth.default()\n",
    "auth_req = google.auth.transport.requests.Request()\n",
    "credentials.refresh(auth_req)\n",
    "access_token = credentials.token\n",
    "\n",
    "# Construct endpoint URL\n",
    "endpoint_url = f\"https://{REGION}-aiplatform.googleapis.com/v1/{endpoint.resource_name}:predict\"\n",
    "\n",
    "print(f\"Endpoint URL: {endpoint_url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ff3wnn83dat",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ REST API prediction successful\n",
      "   Received 3 predictions\n",
      "\n",
      "First prediction anomaly score: 62.1526\n"
     ]
    }
   ],
   "source": [
    "# Prepare request payload\n",
    "payload = {\n",
    "    \"instances\": test_instances\n",
    "}\n",
    "\n",
    "# Make REST API request\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {access_token}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "rest_response = requests.post(endpoint_url, headers=headers, json=payload)\n",
    "\n",
    "if rest_response.status_code == 200:\n",
    "    rest_predictions = rest_response.json()[\"predictions\"]\n",
    "    print(f\"✅ REST API prediction successful\")\n",
    "    print(f\"   Received {len(rest_predictions)} predictions\")\n",
    "    print(f\"\\nFirst prediction anomaly score: {rest_predictions[0]['anomaly_score']:.4f}\")\n",
    "else:\n",
    "    print(f\"❌ Error: {rest_response.status_code}\")\n",
    "    print(rest_response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uzpgr7lk5u",
   "metadata": {},
   "source": [
    "### Compare SDK vs REST API Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8jju1qap6r",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SDK anomaly score:  62.1526\n",
      "REST anomaly score: 62.1526\n",
      "\n",
      "Results match: True\n"
     ]
    }
   ],
   "source": [
    "# Verify both methods return the same results\n",
    "sdk_score = sdk_predictions[0][\"anomaly_score\"]\n",
    "rest_score = rest_predictions[0][\"anomaly_score\"]\n",
    "\n",
    "print(f\"SDK anomaly score:  {sdk_score:.4f}\")\n",
    "print(f\"REST anomaly score: {rest_score:.4f}\")\n",
    "print(f\"\\nResults match: {abs(sdk_score - rest_score) < 0.001}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compare_output",
   "metadata": {},
   "source": [
    "### Compare: Custom vs Pre-built Output\n",
    "\n",
    "**Pre-built container returns 13 keys:**\n",
    "```python\n",
    "{\n",
    "  \"denormalized_MAE\": 26.99,\n",
    "  \"denormalized_RMSE\": 141.12,\n",
    "  \"denormalized_MSE\": 19914.84,\n",
    "  \"denormalized_MSLE\": 0.41,\n",
    "  \"normalized_MAE\": 0.72,\n",
    "  \"normalized_RMSE\": 1.03,\n",
    "  \"normalized_MSE\": 1.07,\n",
    "  \"normalized_MSLE\": 0.12,\n",
    "  \"encoded\": [0.17, 0.0, 0.19, 0.41],\n",
    "  \"normalized_reconstruction\": [...],\n",
    "  \"normalized_reconstruction_errors\": [...],\n",
    "  \"denormalized_reconstruction\": [...],\n",
    "  \"denormalized_reconstruction_errors\": [...]\n",
    "}\n",
    "```\n",
    "\n",
    "**Custom container returns 2 keys:**\n",
    "```python\n",
    "{\n",
    "  \"anomaly_score\": 26.99,\n",
    "  \"encoded\": [0.17, 0.0, 0.19, 0.41]\n",
    "}\n",
    "```\n",
    "\n",
    "**Benefits:**\n",
    "- ✅ ~70% reduction in response size\n",
    "- ✅ Lower network costs\n",
    "- ✅ Faster responses\n",
    "- ✅ Simpler client code\n",
    "- ✅ Only return what clients need"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cleanup",
   "metadata": {},
   "source": [
    "---\n",
    "## Clean Up Resources\n",
    "\n",
    "**Important**: Deployed endpoints incur charges. Always clean up when done."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "undeploy",
   "metadata": {},
   "source": [
    "### Undeploy Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "undeploy_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# endpoint.undeploy_all()\n",
    "# print(\"✅ Model undeployed\")\n",
    "print(\"Uncomment to undeploy the model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "delete_endpoint",
   "metadata": {},
   "source": [
    "### Delete Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "delete_endpoint_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# endpoint.delete(force=True)\n",
    "# print(\"✅ Endpoint deleted\")\n",
    "print(\"Uncomment to delete the endpoint\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "delete_model",
   "metadata": {},
   "source": [
    "### Delete Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "delete_model_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.delete()\n",
    "# print(\"✅ Model deleted from registry\")\n",
    "print(\"Uncomment to delete the model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "In this notebook, you:\n",
    "\n",
    "✅ Created a custom FastAPI container wrapping the PyTorch model\n",
    "\n",
    "✅ Customized the output format (anomaly scores only)\n",
    "\n",
    "✅ Built and pushed the container to Artifact Registry\n",
    "\n",
    "✅ Deployed to Vertex AI Endpoint with custom container\n",
    "\n",
    "✅ Reduced response size by ~70% vs pre-built container\n",
    "\n",
    "### When to Use Custom Containers\n",
    "\n",
    "**Use Pre-built Containers when:**\n",
    "- Standard TorchServe setup works for you\n",
    "- You need full model outputs\n",
    "- Quick deployment is priority\n",
    "\n",
    "**Use Custom Containers when:**\n",
    "- Need custom output formatting (like this example)\n",
    "- Want to reduce network traffic\n",
    "- Require custom preprocessing/postprocessing\n",
    "- Need flexibility beyond TorchServe\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- **Multiple Endpoints**: Create different containers for different use cases\n",
    "- **Advanced Preprocessing**: Add custom logic in FastAPI\n",
    "- **Model Ensembles**: Combine multiple models in one container\n",
    "- **Custom Metrics**: Add application-specific monitoring\n",
    "\n",
    "### Related Notebooks\n",
    "\n",
    "- [PyTorch Autoencoder Training](../pytorch-autoencoder.ipynb) - Train the model\n",
    "- [Pre-built Container Deployment](./vertex-ai-endpoint-prebuilt-container.ipynb) - Compare approaches\n",
    "\n",
    "### Resources\n",
    "\n",
    "- [Custom Prediction Containers](https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements)\n",
    "- [FastAPI Documentation](https://fastapi.tiangolo.com/)\n",
    "- [Cloud Build Documentation](https://cloud.google.com/build/docs)\n",
    "- [Artifact Registry Documentation](https://cloud.google.com/artifact-registry/docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0493da8d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (frameworks-pytorch)",
   "language": "python",
   "name": "frameworks-pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
