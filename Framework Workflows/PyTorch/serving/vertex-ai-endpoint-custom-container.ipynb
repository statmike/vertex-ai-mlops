{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "![tracker](https://us-central1-vertex-ai-mlops-369716.cloudfunctions.net/pixel-tracking?path=statmike%2Fvertex-ai-mlops%2FFramework+Workflows%2FPyTorch%2Fserving&file=vertex-ai-endpoint-custom-container.ipynb)\n",
    "<!--- header table --->\n",
    "<table align=\"left\">\n",
    "<tr>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%20Workflows/PyTorch/serving/vertex-ai-endpoint-custom-container.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.svgrepo.com/download/217753/github.svg\" alt=\"GitHub logo\">\n",
    "      <br>View on<br>GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://colab.research.google.com/github/statmike/vertex-ai-mlops/blob/main/Framework%20Workflows/PyTorch/serving/vertex-ai-endpoint-custom-container.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.gstatic.com/pantheon/images/bigquery/welcome_page/colab-logo.svg\" alt=\"Google Colaboratory logo\">\n",
    "      <br>Run in<br>Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https%3A%2F%2Fraw.githubusercontent.com%2Fstatmike%2Fvertex-ai-mlops%2Fmain%2FFramework%2520Workflows%2FPyTorch%2Fserving%2Fvertex-ai-endpoint-custom-container.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\">\n",
    "      <br>Run in<br>Colab Enterprise\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/bigquery/import?url=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%20Workflows/PyTorch/serving/vertex-ai-endpoint-custom-container.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.gstatic.com/images/branding/gcpiconscolors/bigquery/v1/32px.svg\" alt=\"BigQuery logo\">\n",
    "      <br>Open in<br>BigQuery Studio\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/statmike/vertex-ai-mlops/main/Framework%20Workflows/PyTorch/serving/vertex-ai-endpoint-custom-container.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.gstatic.com/images/branding/gcpiconscolors/vertexai/v1/32px.svg\" alt=\"Vertex AI logo\">\n",
    "      <br>Open in<br>Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td colspan=\"5\" style=\"text-align: right\">\n",
    "    <b>Share This On: </b> \n",
    "    <a href=\"https://www.linkedin.com/sharing/share-offsite/?url=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%2520Workflows/PyTorch/serving/vertex-ai-endpoint-custom-container.ipynb\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"Linkedin Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://reddit.com/submit?url=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%2520Workflows/PyTorch/serving/vertex-ai-endpoint-custom-container.ipynb\"><img src=\"https://redditinc.com/hubfs/Reddit%20Inc/Brand/Reddit_Logo.png\" alt=\"Reddit Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://bsky.app/intent/compose?text=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%2520Workflows/PyTorch/serving/vertex-ai-endpoint-custom-container.ipynb\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"BlueSky Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://twitter.com/intent/tweet?url=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%2520Workflows/PyTorch/serving/vertex-ai-endpoint-custom-container.ipynb\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/5/5a/X_icon_2.svg\" alt=\"X (Twitter) Logo\" width=\"20px\"></a> \n",
    "  </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td colspan=\"5\" style=\"text-align: right\">\n",
    "    <b>Connect With Author On: </b> \n",
    "    <a href=\"https://www.linkedin.com/in/statmike\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"Linkedin Logo\" width=\"20px\"></a>\n",
    "    <a href=\"https://www.github.com/statmike\"><img src=\"https://www.svgrepo.com/download/217753/github.svg\" alt=\"GitHub Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://www.youtube.com/@statmike-channel\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/f/fd/YouTube_full-color_icon_%282024%29.svg\" alt=\"YouTube Logo\" width=\"20px\"></a>\n",
    "    <a href=\"https://bsky.app/profile/statmike.bsky.social\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"BlueSky Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://x.com/statmike\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/5/5a/X_icon_2.svg\" alt=\"X (Twitter) Logo\" width=\"20px\"></a>\n",
    "  </td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overview",
   "metadata": {},
   "source": [
    "# Deploy PyTorch Model with Custom Container\n",
    "\n",
    "This notebook demonstrates deploying a PyTorch model using a **custom container with FastAPI** for full control over prediction outputs.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "This workflow covers:\n",
    "\n",
    "1. **Build Custom FastAPI Container**: Create a container that wraps your PyTorch model with custom logic\n",
    "2. **Control Output Format**: Return only specific outputs (e.g., anomaly scores) instead of full model output\n",
    "3. **Use Cloud Build**: Build and push containers to Artifact Registry\n",
    "4. **Deploy to Vertex AI**: Use the custom container on Vertex AI Endpoints\n",
    "5. **Compare Approaches**: Understand when to use custom vs pre-built containers\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Completed the `pytorch-autoencoder.ipynb` notebook (created the .mar file)\n",
    "- .mar file in GCS at: `gs://{PROJECT_ID}/frameworks/pytorch-autoencoder/model.mar`\n",
    "- Google Cloud project with APIs enabled\n",
    "\n",
    "## Why Custom Containers?\n",
    "\n",
    "**Pre-built containers** (previous notebook) are great for:\n",
    "- ‚úÖ Quick deployment\n",
    "- ‚úÖ Standard PyTorch/TorchServe setup\n",
    "- ‚úÖ Full model output needed\n",
    "\n",
    "**Custom containers** are better when you need:\n",
    "- ‚úÖ **Custom output formatting** - Return only anomaly scores instead of 13 outputs\n",
    "- ‚úÖ **Reduced network traffic** - Send only what clients need\n",
    "- ‚úÖ **Custom preprocessing** - Add logic not in the model\n",
    "- ‚úÖ **Multiple output versions** - Different endpoints with different outputs\n",
    "- ‚úÖ **Framework flexibility** - Not limited to TorchServe\n",
    "\n",
    "## Deployment Architecture\n",
    "\n",
    "```\n",
    "PyTorch Model (.mar) ‚Üí Extract .pt ‚Üí FastAPI Container ‚Üí Artifact Registry ‚Üí Vertex AI Endpoint\n",
    "```\n",
    "\n",
    "**Key Difference from Pre-built:**\n",
    "- Pre-built: TorchServe handles everything (returns all 13 outputs)\n",
    "- Custom: FastAPI wrapper extracts .pt and returns only what you want (e.g., just anomaly scores)\n",
    "\n",
    "## What We're Building\n",
    "\n",
    "A custom FastAPI container that:\n",
    "1. Downloads the .mar file from GCS at startup\n",
    "2. Extracts the TorchScript .pt file\n",
    "3. Loads the model with `torch.jit.load()`\n",
    "4. **Returns simplified output**: Just anomaly scores and embeddings (not all 13 dict keys)\n",
    "5. Reduces response size by ~70%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "env_setup",
   "metadata": {},
   "source": [
    "---\n",
    "## Environment Setup\n",
    "\n",
    "This section will authenticate your session, enable required Google Cloud APIs, and install necessary Python packages.\n",
    "\n",
    "**Package Installation Options (`REQ_TYPE`):**\n",
    "- `PRIMARY`: Installs only the main packages. Faster, but pip resolves sub-dependencies which may result in different versions than development.\n",
    "- `ALL` (Default): Installs exact versions of all packages and dependencies. Best for perfectly reproducing the development environment.\n",
    "- `COLAB`: Installs a Colab-optimized list that excludes pre-installed packages like `ipython` and `ipykernel`.\n",
    "\n",
    "**Installation Tool Options (`INSTALL_TOOL`):**\n",
    "- `pip` (Default): Uses pip for package installation. Standard Python package installer.\n",
    "- `uv`: Modern, fast Python package installer. Must be installed separately. See: https://github.com/astral-sh/uv\n",
    "- `poetry`: Dependency management tool. Requires running notebook in a poetry environment (`poetry shell` or `poetry run jupyter lab`). Uses `pyproject.toml` instead of requirements.txt.\n",
    "\n",
    "> **Note:** If running in Google Colab, the script will automatically detect this and set `REQ_TYPE = 'COLAB'` to prevent package conflicts, overriding any manual setting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "set_project",
   "metadata": {},
   "source": [
    "### Set Your Project ID\n",
    "\n",
    "‚ö†Ô∏è **Action Required:** Replace the `PROJECT_ID` value below with your Google Cloud project ID before running this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "project_config",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = 'statmike-mlops-349915' # replace with GCP project ID\n",
    "REQ_TYPE = 'ALL' # Specify PRIMARY or ALL or COLAB\n",
    "INSTALL_TOOL = 'poetry' # Specify pip, uv, or poetry (all implemented)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config_header",
   "metadata": {},
   "source": [
    "### Configuration\n",
    "\n",
    "This cell defines the requirements files and Google Cloud APIs needed for this notebook. Run as-is without modification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": [
    "REQUIREMENTS_URL = 'https://raw.githubusercontent.com/statmike/vertex-ai-mlops/refs/heads/main/Framework%20Workflows/PyTorch/requirements.txt'\n",
    "\n",
    "REQUIRED_APIS = [\n",
    "    \"aiplatform.googleapis.com\",\n",
    "    \"storage.googleapis.com\",\n",
    "    \"artifactregistry.googleapis.com\",\n",
    "    \"cloudbuild.googleapis.com\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "run_setup_header",
   "metadata": {},
   "source": [
    "### Run Setup\n",
    "\n",
    "This cell downloads the centralized setup code and configures your environment. It will:\n",
    "- Authenticate your session with Google Cloud\n",
    "- Enable required APIs for this notebook (including Artifact Registry and Cloud Build)\n",
    "- Install necessary Python packages\n",
    "- Display a setup summary with your project information\n",
    "\n",
    "> **Note:** In Colab, if packages are installed, the kernel will automatically restart. After restart, continue from the next cell without re-running earlier cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "run_setup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "PYTHON GCP ENVIRONMENT SETUP\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "AUTHENTICATION\n",
      "==================================================\n",
      "Checking for existing ADC...\n",
      "‚úÖ Existing ADC found.\n",
      "‚úÖ Project is correctly set to 'statmike-mlops-349915'.\n",
      "\n",
      "==================================================\n",
      "API CHECK & ENABLE\n",
      "==================================================\n",
      "‚úÖ aiplatform.googleapis.com is already enabled.\n",
      "‚úÖ storage.googleapis.com is already enabled.\n",
      "‚úÖ artifactregistry.googleapis.com is already enabled.\n",
      "‚úÖ cloudbuild.googleapis.com is already enabled.\n",
      "\n",
      "==================================================\n",
      "PACKAGE MANAGEMENT\n",
      "==================================================\n",
      "Installation Tool: poetry\n",
      "‚úÖ Found poetry at: /usr/local/google/home/statmike/.local/bin/poetry\n",
      "‚úÖ Running in poetry environment: /usr/local/google/home/statmike/.cache/pypoetry/virtualenvs/frameworks-pytorch-0KVJlKeQ-py3.13\n",
      "‚ÑπÔ∏è  Poetry mode: Installing from pyproject.toml (REQUIREMENTS_URL ignored)\n",
      "‚ö†Ô∏è  No local pyproject.toml found\n",
      "   Attempting to download from: https://raw.githubusercontent.com/statmike/vertex-ai-mlops/refs/heads/main/Framework%20Workflows/PyTorch/pyproject.toml\n",
      "‚úÖ Downloaded pyproject.toml\n",
      "Running poetry install...\n",
      "‚úÖ All packages are already installed and up to date.\n",
      "\n",
      "==================================================\n",
      "Google Cloud Project Information\n",
      "==================================================\n",
      "PROJECT_ID     = statmike-mlops-349915\n",
      "PROJECT_NUMBER = 1026793852137\n",
      "==================================================\n",
      "\n",
      "\n",
      "==================================================\n",
      "SETUP SUMMARY\n",
      "==================================================\n",
      "‚úÖ Authentication:    Success\n",
      "‚úÖ API Configuration: Success\n",
      "‚úÖ Package Install:   Already up to date\n",
      "‚úÖ Installation Tool: poetry\n",
      "‚úÖ Project ID:        statmike-mlops-349915\n",
      "‚úÖ Project Number:    1026793852137\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os, urllib.request\n",
    "\n",
    "# Download and import setup code\n",
    "url = 'https://raw.githubusercontent.com/statmike/vertex-ai-mlops/refs/heads/main/core/notebook-template/python_setup.py'\n",
    "urllib.request.urlretrieve(url, 'python_setup_local.py')\n",
    "import python_setup_local as python_setup\n",
    "os.remove('python_setup_local.py')\n",
    "\n",
    "# Run setup\n",
    "setup_info = python_setup.setup_environment(PROJECT_ID, REQ_TYPE, REQUIREMENTS_URL, REQUIRED_APIS, INSTALL_TOOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "python_setup",
   "metadata": {},
   "source": [
    "---\n",
    "## Python Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imports_header",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import json\n",
    "import os\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud import storage\n",
    "from google.cloud import artifactregistry_v1\n",
    "from google.cloud.devtools import cloudbuild_v1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vars_user",
   "metadata": {},
   "source": [
    "### Variables - User Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "vars_user_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION = 'us-central1'\n",
    "SERIES = 'frameworks'\n",
    "EXPERIMENT = 'pytorch-autoencoder-custom'\n",
    "\n",
    "# Model configuration\n",
    "MODEL_DISPLAY_NAME = EXPERIMENT\n",
    "ENDPOINT_DISPLAY_NAME = f\"{EXPERIMENT}-endpoint\"\n",
    "\n",
    "# Machine configuration\n",
    "MACHINE_TYPE = 'n1-standard-4'\n",
    "MIN_REPLICA_COUNT = 1\n",
    "MAX_REPLICA_COUNT = 4\n",
    "\n",
    "# Working directory for source files\n",
    "DIR = f\"files/{EXPERIMENT}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vars_auto",
   "metadata": {},
   "source": [
    "### Variables - Auto Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "vars_auto_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Google Cloud Project Information\n",
      "==================================================\n",
      "PROJECT_ID     = statmike-mlops-349915\n",
      "PROJECT_NUMBER = 1026793852137\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "PROJECT_ID = subprocess.run(['gcloud', 'config', 'get-value', 'project'], capture_output=True, text=True, check=True).stdout.strip()\n",
    "PROJECT_NUMBER = subprocess.run(['gcloud', 'projects', 'describe', PROJECT_ID, '--format=value(projectNumber)'], capture_output=True, text=True, check=True).stdout.strip()\n",
    "\n",
    "print(f\"\\n{'='*50}\\nGoogle Cloud Project Information\\n{'='*50}\\nPROJECT_ID     = {PROJECT_ID}\\nPROJECT_NUMBER = {PROJECT_NUMBER}\\n{'='*50}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "configs",
   "metadata": {},
   "source": [
    "### Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "configs_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket: gs://statmike-mlops-349915\n",
      "Model artifacts: gs://statmike-mlops-349915/frameworks/pytorch-autoencoder\n",
      "Artifact Registry: us-central1-docker.pkg.dev/statmike-mlops-349915/frameworks\n",
      "Container image: us-central1-docker.pkg.dev/statmike-mlops-349915/frameworks/pytorch-autoencoder-custom\n"
     ]
    }
   ],
   "source": [
    "# GCS configuration\n",
    "BUCKET_NAME = PROJECT_ID\n",
    "BUCKET_URI = f\"gs://{BUCKET_NAME}\"\n",
    "MODEL_ARTIFACT_DIR = f\"{BUCKET_URI}/{SERIES}/pytorch-autoencoder\"\n",
    "\n",
    "# Artifact Registry repository\n",
    "AR_REPO_NAME = SERIES\n",
    "AR_REPO_URI = f\"{REGION}-docker.pkg.dev/{PROJECT_ID}/{AR_REPO_NAME}\"\n",
    "\n",
    "# Container image name\n",
    "CONTAINER_IMAGE = f\"{AR_REPO_URI}/{EXPERIMENT}\"\n",
    "\n",
    "print(f\"Bucket: {BUCKET_URI}\")\n",
    "print(f\"Model artifacts: {MODEL_ARTIFACT_DIR}\")\n",
    "print(f\"Artifact Registry: {AR_REPO_URI}\")\n",
    "print(f\"Container image: {CONTAINER_IMAGE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "init_clients",
   "metadata": {},
   "source": [
    "### Initialize Clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "init_clients_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Clients initialized\n",
      "   Project: statmike-mlops-349915\n",
      "   Location: us-central1\n"
     ]
    }
   ],
   "source": [
    "# Initialize clients\n",
    "storage_client = storage.Client(project=PROJECT_ID)\n",
    "ar_client = artifactregistry_v1.ArtifactRegistryClient()\n",
    "cb_client = cloudbuild_v1.CloudBuildClient()\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_URI)\n",
    "\n",
    "print(f\"‚úÖ Clients initialized\")\n",
    "print(f\"   Project: {PROJECT_ID}\")\n",
    "print(f\"   Location: {REGION}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "create_dir",
   "metadata": {},
   "source": [
    "### Create Working Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "create_dir_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Created working directory: files/pytorch-autoencoder-custom/app\n"
     ]
    }
   ],
   "source": [
    "# Create directory for application files\n",
    "os.makedirs(f\"{DIR}/app\", exist_ok=True)\n",
    "print(f\"‚úÖ Created working directory: {DIR}/app\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ar_setup",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup Artifact Registry\n",
    "\n",
    "Create or retrieve an Artifact Registry repository to store Docker containers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "check_ar",
   "metadata": {},
   "source": [
    "### Check for Existing Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "check_ar_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Using existing Artifact Registry repository:\n",
      "   Name: projects/statmike-mlops-349915/locations/us-central1/repositories/frameworks\n",
      "   Format: DOCKER\n"
     ]
    }
   ],
   "source": [
    "# Check if repository already exists\n",
    "ar_parent = f\"projects/{PROJECT_ID}/locations/{REGION}\"\n",
    "docker_repo = None\n",
    "\n",
    "for repo in ar_client.list_repositories(parent=ar_parent):\n",
    "    if repo.name.split('/')[-1] == AR_REPO_NAME:\n",
    "        docker_repo = repo\n",
    "        print(f\"‚úÖ Using existing Artifact Registry repository:\")\n",
    "        print(f\"   Name: {docker_repo.name}\")\n",
    "        print(f\"   Format: {docker_repo.format_.name}\")\n",
    "        break\n",
    "\n",
    "if not docker_repo:\n",
    "    print(f\"Repository '{AR_REPO_NAME}' not found - will create it\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "create_ar",
   "metadata": {},
   "source": [
    "### Create Repository (if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "create_ar_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Repository ready: us-central1-docker.pkg.dev/statmike-mlops-349915/frameworks\n"
     ]
    }
   ],
   "source": [
    "if not docker_repo:\n",
    "    # Create new repository\n",
    "    operation = ar_client.create_repository(\n",
    "        request=artifactregistry_v1.CreateRepositoryRequest(\n",
    "            parent=ar_parent,\n",
    "            repository_id=AR_REPO_NAME,\n",
    "            repository=artifactregistry_v1.Repository(\n",
    "                description=f\"Docker images for {SERIES} series\",\n",
    "                format_=artifactregistry_v1.Repository.Format.DOCKER,\n",
    "                labels={'series': SERIES}\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    print(\"Creating Artifact Registry repository...\")\n",
    "    docker_repo = operation.result()\n",
    "    print(f\"‚úÖ Created repository: {docker_repo.name}\")\n",
    "else:\n",
    "    print(f\"‚úÖ Repository ready: {AR_REPO_URI}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "create_app",
   "metadata": {},
   "source": [
    "---\n",
    "## Create Application Files\n",
    "\n",
    "Build the custom FastAPI application that will wrap our PyTorch model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dockerfile_header",
   "metadata": {},
   "source": [
    "### Create Dockerfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dockerfile_code",
   "metadata": {},
   "outputs": [],
   "source": "%%writefile {DIR}/Dockerfile\n# Start from Google's PyTorch pre-built container for base dependencies\nFROM us-docker.pkg.dev/vertex-ai/prediction/pytorch-cpu.1-13:latest\n\n# Install FastAPI and dependencies\nRUN pip install --no-cache-dir \\\n    fastapi==0.104.1 \\\n    uvicorn[standard]==0.24.0 \\\n    google-cloud-storage\n\n# Copy application code\nCOPY ./app /app\n\n# Set working directory\nWORKDIR /app\n\n# Expose port\nEXPOSE 8080\n\n# Run FastAPI with uvicorn\nCMD [\"uvicorn\", \"main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8080\"]"
  },
  {
   "cell_type": "markdown",
   "id": "fastapi_header",
   "metadata": {},
   "source": [
    "### Create FastAPI Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fastapi_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing files/pytorch-autoencoder-custom/app/main.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {DIR}/app/main.py\n",
    "import os\n",
    "import zipfile\n",
    "from fastapi import FastAPI, Request\n",
    "import torch\n",
    "from google.cloud import storage\n",
    "\n",
    "app = FastAPI()\n",
    "gcs = storage.Client()\n",
    "\n",
    "# Download .mar file and extract .pt at startup\n",
    "print(\"Loading model...\")\n",
    "storage_uri = os.environ['AIP_STORAGE_URI']\n",
    "paths = storage_uri.replace('gs://', '').split('/')\n",
    "bucket = gcs.bucket(paths[0])\n",
    "mar_blob = bucket.blob('/'.join(paths[1:]) + '/model.mar')\n",
    "mar_blob.download_to_filename('model.mar')\n",
    "\n",
    "# Extract .pt file from .mar\n",
    "with zipfile.ZipFile('model.mar', 'r') as zip_ref:\n",
    "    zip_ref.extract('final_model_traced.pt')\n",
    "\n",
    "# Load PyTorch model\n",
    "_model = torch.jit.load('final_model_traced.pt')\n",
    "_model.eval()\n",
    "print(\"‚úÖ Model loaded successfully\")\n",
    "\n",
    "# Health check endpoint\n",
    "@app.get(os.environ.get('AIP_HEALTH_ROUTE', '/health'), status_code=200)\n",
    "def health():\n",
    "    return {\"status\": \"healthy\"}\n",
    "\n",
    "# Prediction endpoint with custom output\n",
    "@app.post(os.environ.get('AIP_PREDICT_ROUTE', '/predict'))\n",
    "async def predict(request: Request):\n",
    "    body = await request.json()\n",
    "    instances = torch.tensor(body[\"instances\"], dtype=torch.float32)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        full_output = _model(instances)\n",
    "    \n",
    "    # ‚úÖ CUSTOM OUTPUT: Return only anomaly scores and embeddings\n",
    "    # This reduces response size by ~70% compared to full output\n",
    "    simplified = [\n",
    "        {\n",
    "            \"anomaly_score\": full_output[\"denormalized_MAE\"][i].item(),\n",
    "            \"encoded\": full_output[\"encoded\"][i].tolist()\n",
    "        }\n",
    "        for i in range(len(instances))\n",
    "    ]\n",
    "    \n",
    "    return {\"predictions\": simplified}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "upload_source",
   "metadata": {},
   "source": [
    "### Upload Source Files to GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "upload_source_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Uploaded source files to gs://statmike-mlops-349915/frameworks/pytorch-autoencoder-custom/\n"
     ]
    }
   ],
   "source": [
    "# Upload application files to GCS for Cloud Build\n",
    "bucket = storage_client.bucket(BUCKET_NAME)\n",
    "\n",
    "bucket.blob(f'{SERIES}/{EXPERIMENT}/Dockerfile').upload_from_filename(f'{DIR}/Dockerfile')\n",
    "bucket.blob(f'{SERIES}/{EXPERIMENT}/app/main.py').upload_from_filename(f'{DIR}/app/main.py')\n",
    "\n",
    "print(f\"‚úÖ Uploaded source files to gs://{BUCKET_NAME}/{SERIES}/{EXPERIMENT}/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "build_container",
   "metadata": {},
   "source": [
    "---\n",
    "## Build Container with Cloud Build\n",
    "\n",
    "Use Cloud Build to build the Docker container and push to Artifact Registry."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "build_config",
   "metadata": {},
   "source": [
    "### Create Build Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "build_config_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build configuration created\n"
     ]
    }
   ],
   "source": [
    "# Create Cloud Build configuration\n",
    "build = cloudbuild_v1.Build(\n",
    "    steps=[\n",
    "        # Step 1: Copy source from GCS\n",
    "        {\n",
    "            'name': 'gcr.io/cloud-builders/gsutil',\n",
    "            'args': ['cp', '-r', f'gs://{BUCKET_NAME}/{SERIES}/{EXPERIMENT}/*', '/workspace']\n",
    "        },\n",
    "        # Step 2: Build Docker image\n",
    "        {\n",
    "            'name': 'gcr.io/cloud-builders/docker',\n",
    "            'args': ['build', '-t', CONTAINER_IMAGE, '/workspace']\n",
    "        }\n",
    "    ],\n",
    "    # Push the image\n",
    "    images=[CONTAINER_IMAGE]\n",
    ")\n",
    "\n",
    "print(\"Build configuration created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "run_build",
   "metadata": {},
   "source": [
    "### Run Build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "run_build_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building container image: us-central1-docker.pkg.dev/statmike-mlops-349915/frameworks/pytorch-autoencoder-custom\n",
      "This will take 3-5 minutes...\n"
     ]
    },
    {
     "ename": "Unknown",
     "evalue": "None Build failed; check build logs for details 2: Build failed; check build logs for details",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUnknown\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mThis will take 3-5 minutes...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m operation = cb_client.create_build(project_id=PROJECT_ID, build=build)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m build_response = \u001b[43moperation\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m build_response.status == cloudbuild_v1.Build.Status.SUCCESS:\n\u001b[32m      9\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m‚úÖ Container built successfully!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/frameworks-pytorch-0KVJlKeQ-py3.13/lib/python3.13/site-packages/google/api_core/future/polling.py:261\u001b[39m, in \u001b[36mPollingFuture.result\u001b[39m\u001b[34m(self, timeout, retry, polling)\u001b[39m\n\u001b[32m    256\u001b[39m \u001b[38;5;28mself\u001b[39m._blocking_poll(timeout=timeout, retry=retry, polling=polling)\n\u001b[32m    258\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    259\u001b[39m     \u001b[38;5;66;03m# pylint: disable=raising-bad-type\u001b[39;00m\n\u001b[32m    260\u001b[39m     \u001b[38;5;66;03m# Pylint doesn't recognize that this is valid in this case.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m261\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    263\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._result\n",
      "\u001b[31mUnknown\u001b[39m: None Build failed; check build logs for details 2: Build failed; check build logs for details"
     ]
    }
   ],
   "source": [
    "# Submit build to Cloud Build\n",
    "print(f\"Building container image: {CONTAINER_IMAGE}\")\n",
    "print(\"This will take 3-5 minutes...\")\n",
    "\n",
    "operation = cb_client.create_build(project_id=PROJECT_ID, build=build)\n",
    "build_response = operation.result()\n",
    "\n",
    "if build_response.status == cloudbuild_v1.Build.Status.SUCCESS:\n",
    "    print(f\"\\n‚úÖ Container built successfully!\")\n",
    "    print(f\"   Image: {CONTAINER_IMAGE}\")\n",
    "    print(f\"   Status: {build_response.status.name}\")\n",
    "else:\n",
    "    print(f\"‚ùå Build failed with status: {build_response.status.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model_registry",
   "metadata": {},
   "source": [
    "---\n",
    "## Register Model in Vertex AI Model Registry\n",
    "\n",
    "Upload the model using our custom container image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "check_model",
   "metadata": {},
   "source": [
    "### Check for Existing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "check_model_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if model already exists\n",
    "existing_models = aiplatform.Model.list(filter=f\"display_name={MODEL_DISPLAY_NAME}\")\n",
    "\n",
    "if existing_models:\n",
    "    model = existing_models[0]\n",
    "    print(f\"‚úÖ Using existing model:\")\n",
    "    print(f\"   Resource name: {model.resource_name}\")\n",
    "    print(f\"   Display name: {model.display_name}\")\n",
    "    print(f\"   Version: {model.version_id}\")\n",
    "else:\n",
    "    print(f\"No existing models found with name {MODEL_DISPLAY_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "upload_model",
   "metadata": {},
   "source": [
    "### Upload Model to Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "upload_model_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not existing_models:\n",
    "    # Upload model with custom container\n",
    "    model = aiplatform.Model.upload(\n",
    "        display_name=MODEL_DISPLAY_NAME,\n",
    "        artifact_uri=MODEL_ARTIFACT_DIR,\n",
    "        serving_container_image_uri=CONTAINER_IMAGE,\n",
    "        serving_container_environment_variables={\n",
    "            \"AIP_STORAGE_URI\": MODEL_ARTIFACT_DIR,\n",
    "            \"AIP_HEALTH_ROUTE\": \"/health\",\n",
    "            \"AIP_PREDICT_ROUTE\": \"/predict\",\n",
    "            \"AIP_HTTP_PORT\": \"8080\"\n",
    "        },\n",
    "        serving_container_ports=[8080],\n",
    "        description=\"PyTorch autoencoder with custom FastAPI container (simplified output)\"\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Model uploaded to registry:\")\n",
    "    print(f\"   Resource name: {model.resource_name}\")\n",
    "    print(f\"   Display name: {model.display_name}\")\n",
    "    print(f\"   Container: {CONTAINER_IMAGE}\")\n",
    "else:\n",
    "    print(\"‚úÖ Using existing model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "endpoint_deploy",
   "metadata": {},
   "source": [
    "---\n",
    "## Deploy Model to Endpoint\n",
    "\n",
    "Create a Vertex AI Endpoint and deploy the custom container."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "check_endpoint",
   "metadata": {},
   "source": [
    "### Check for Existing Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "check_endpoint_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if endpoint already exists\n",
    "existing_endpoints = aiplatform.Endpoint.list(filter=f\"display_name={ENDPOINT_DISPLAY_NAME}\")\n",
    "\n",
    "if existing_endpoints:\n",
    "    endpoint = existing_endpoints[0]\n",
    "    print(f\"‚úÖ Using existing endpoint:\")\n",
    "    print(f\"   Resource name: {endpoint.resource_name}\")\n",
    "    print(f\"   Display name: {endpoint.display_name}\")\n",
    "else:\n",
    "    print(f\"No existing endpoints found with name {ENDPOINT_DISPLAY_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "create_endpoint",
   "metadata": {},
   "source": [
    "### Create Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create_endpoint_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not existing_endpoints:\n",
    "    # Create endpoint\n",
    "    endpoint = aiplatform.Endpoint.create(\n",
    "        display_name=ENDPOINT_DISPLAY_NAME,\n",
    "        description=\"Endpoint for PyTorch autoencoder with custom container\"\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Endpoint created:\")\n",
    "    print(f\"   Resource name: {endpoint.resource_name}\")\n",
    "    print(f\"   Display name: {endpoint.display_name}\")\n",
    "else:\n",
    "    print(\"‚úÖ Using existing endpoint\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deploy_model",
   "metadata": {},
   "source": [
    "### Deploy Model to Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deploy_model_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy model to endpoint\n",
    "print(\"Deploying model to endpoint (this will take 10-15 minutes)...\")\n",
    "print(\"You can monitor progress in the Cloud Console:\")\n",
    "print(f\"https://console.cloud.google.com/vertex-ai/endpoints/{endpoint.name.split('/')[-1]}?project={PROJECT_ID}\")\n",
    "\n",
    "endpoint.deploy(\n",
    "    model=model,\n",
    "    deployed_model_display_name=MODEL_DISPLAY_NAME,\n",
    "    machine_type=MACHINE_TYPE,\n",
    "    min_replica_count=MIN_REPLICA_COUNT,\n",
    "    max_replica_count=MAX_REPLICA_COUNT,\n",
    "    traffic_percentage=100,\n",
    "    sync=True\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Model deployed successfully!\")\n",
    "print(f\"   Endpoint: {endpoint.display_name}\")\n",
    "print(f\"   Resource name: {endpoint.resource_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "test_data",
   "metadata": {},
   "source": [
    "---\n",
    "## Prepare Test Data\n",
    "\n",
    "Retrieve test records from BigQuery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test_data_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "import pandas as pd\n",
    "\n",
    "bq = bigquery.Client(project=PROJECT_ID)\n",
    "\n",
    "# BigQuery source\n",
    "BQ_PROJECT = PROJECT_ID\n",
    "BQ_DATASET = SERIES.replace('-', '_')\n",
    "BQ_TABLE = SERIES\n",
    "\n",
    "# Get test instances\n",
    "query = f\"\"\"\n",
    "SELECT * EXCEPT(splits, transaction_id, Class)\n",
    "FROM `{BQ_PROJECT}.{BQ_DATASET}.{BQ_TABLE}`\n",
    "WHERE splits = \"TEST\" AND Class = 0\n",
    "LIMIT 3\n",
    "\"\"\"\n",
    "\n",
    "test_df = bq.query(query).to_dataframe()\n",
    "test_instances = test_df.values.tolist()\n",
    "\n",
    "print(f\"Retrieved {len(test_instances)} test instances\")\n",
    "print(f\"Features: {list(test_df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "predictions",
   "metadata": {},
   "source": [
    "---\n",
    "## Make Online Predictions\n",
    "\n",
    "Test the custom container and compare output to the pre-built container."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "custom_pred",
   "metadata": {},
   "source": [
    "### Custom Container Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "custom_pred_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make prediction using custom container\n",
    "response = endpoint.predict(instances=test_instances)\n",
    "\n",
    "print(f\"‚úÖ Received predictions for {len(response.predictions)} instances\")\n",
    "print(f\"\\nüìä Custom Container Output (Simplified):\")\n",
    "print(f\"   Keys per prediction: {list(response.predictions[0].keys())}\")\n",
    "print(f\"   Output size: {len(str(response.predictions[0]))} characters\")\n",
    "\n",
    "for i, pred in enumerate(response.predictions):\n",
    "    print(f\"\\nInstance {i}:\")\n",
    "    print(f\"   Anomaly Score: {pred['anomaly_score']:.4f}\")\n",
    "    print(f\"   Encoded: {pred['encoded']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compare_output",
   "metadata": {},
   "source": [
    "### Compare: Custom vs Pre-built Output\n",
    "\n",
    "**Pre-built container returns 13 keys:**\n",
    "```python\n",
    "{\n",
    "  \"denormalized_MAE\": 26.99,\n",
    "  \"denormalized_RMSE\": 141.12,\n",
    "  \"denormalized_MSE\": 19914.84,\n",
    "  \"denormalized_MSLE\": 0.41,\n",
    "  \"normalized_MAE\": 0.72,\n",
    "  \"normalized_RMSE\": 1.03,\n",
    "  \"normalized_MSE\": 1.07,\n",
    "  \"normalized_MSLE\": 0.12,\n",
    "  \"encoded\": [0.17, 0.0, 0.19, 0.41],\n",
    "  \"normalized_reconstruction\": [...],\n",
    "  \"normalized_reconstruction_errors\": [...],\n",
    "  \"denormalized_reconstruction\": [...],\n",
    "  \"denormalized_reconstruction_errors\": [...]\n",
    "}\n",
    "```\n",
    "\n",
    "**Custom container returns 2 keys:**\n",
    "```python\n",
    "{\n",
    "  \"anomaly_score\": 26.99,\n",
    "  \"encoded\": [0.17, 0.0, 0.19, 0.41]\n",
    "}\n",
    "```\n",
    "\n",
    "**Benefits:**\n",
    "- ‚úÖ ~70% reduction in response size\n",
    "- ‚úÖ Lower network costs\n",
    "- ‚úÖ Faster responses\n",
    "- ‚úÖ Simpler client code\n",
    "- ‚úÖ Only return what clients need"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cleanup",
   "metadata": {},
   "source": [
    "---\n",
    "## Clean Up Resources\n",
    "\n",
    "**Important**: Deployed endpoints incur charges. Always clean up when done."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "undeploy",
   "metadata": {},
   "source": [
    "### Undeploy Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "undeploy_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# endpoint.undeploy_all()\n",
    "# print(\"‚úÖ Model undeployed\")\n",
    "print(\"Uncomment to undeploy the model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "delete_endpoint",
   "metadata": {},
   "source": [
    "### Delete Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "delete_endpoint_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# endpoint.delete(force=True)\n",
    "# print(\"‚úÖ Endpoint deleted\")\n",
    "print(\"Uncomment to delete the endpoint\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "delete_model",
   "metadata": {},
   "source": [
    "### Delete Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "delete_model_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.delete()\n",
    "# print(\"‚úÖ Model deleted from registry\")\n",
    "print(\"Uncomment to delete the model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "In this notebook, you:\n",
    "\n",
    "‚úÖ Created a custom FastAPI container wrapping the PyTorch model\n",
    "\n",
    "‚úÖ Customized the output format (anomaly scores only)\n",
    "\n",
    "‚úÖ Built and pushed the container to Artifact Registry\n",
    "\n",
    "‚úÖ Deployed to Vertex AI Endpoint with custom container\n",
    "\n",
    "‚úÖ Reduced response size by ~70% vs pre-built container\n",
    "\n",
    "### When to Use Custom Containers\n",
    "\n",
    "**Use Pre-built Containers when:**\n",
    "- Standard TorchServe setup works for you\n",
    "- You need full model outputs\n",
    "- Quick deployment is priority\n",
    "\n",
    "**Use Custom Containers when:**\n",
    "- Need custom output formatting (like this example)\n",
    "- Want to reduce network traffic\n",
    "- Require custom preprocessing/postprocessing\n",
    "- Need flexibility beyond TorchServe\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- **Multiple Endpoints**: Create different containers for different use cases\n",
    "- **Advanced Preprocessing**: Add custom logic in FastAPI\n",
    "- **Model Ensembles**: Combine multiple models in one container\n",
    "- **Custom Metrics**: Add application-specific monitoring\n",
    "\n",
    "### Related Notebooks\n",
    "\n",
    "- [PyTorch Autoencoder Training](../pytorch-autoencoder.ipynb) - Train the model\n",
    "- [Pre-built Container Deployment](./vertex-ai-endpoint-prebuilt-container.ipynb) - Compare approaches\n",
    "- [Dataflow RunInference](./dataflow-runinference.ipynb) - Batch/streaming inference (coming soon)\n",
    "\n",
    "### Resources\n",
    "\n",
    "- [Custom Prediction Containers](https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements)\n",
    "- [FastAPI Documentation](https://fastapi.tiangolo.com/)\n",
    "- [Cloud Build Documentation](https://cloud.google.com/build/docs)\n",
    "- [Artifact Registry Documentation](https://cloud.google.com/artifact-registry/docs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (frameworks-pytorch)",
   "language": "python",
   "name": "frameworks-pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}