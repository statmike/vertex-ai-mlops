{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "![tracker](https://us-central1-vertex-ai-mlops-369716.cloudfunctions.net/pixel-tracking?path=statmike%2Fvertex-ai-mlops%2FFramework+Workflows%2FPyTorch%2Fserving&file=vertex-ai-endpoint-custom-container.ipynb)\n",
    "<!--- header table --->\n",
    "<table align=\"left\">\n",
    "<tr>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%20Workflows/PyTorch/serving/vertex-ai-endpoint-custom-container.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.svgrepo.com/download/217753/github.svg\" alt=\"GitHub logo\">\n",
    "      <br>View on<br>GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://colab.research.google.com/github/statmike/vertex-ai-mlops/blob/main/Framework%20Workflows/PyTorch/serving/vertex-ai-endpoint-custom-container.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.gstatic.com/pantheon/images/bigquery/welcome_page/colab-logo.svg\" alt=\"Google Colaboratory logo\">\n",
    "      <br>Run in<br>Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https%3A%2F%2Fraw.githubusercontent.com%2Fstatmike%2Fvertex-ai-mlops%2Fmain%2FFramework%2520Workflows%2FPyTorch%2Fserving%2Fvertex-ai-endpoint-custom-container.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\">\n",
    "      <br>Run in<br>Colab Enterprise\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/bigquery/import?url=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%20Workflows/PyTorch/serving/vertex-ai-endpoint-custom-container.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.gstatic.com/images/branding/gcpiconscolors/bigquery/v1/32px.svg\" alt=\"BigQuery logo\">\n",
    "      <br>Open in<br>BigQuery Studio\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/statmike/vertex-ai-mlops/main/Framework%20Workflows/PyTorch/serving/vertex-ai-endpoint-custom-container.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.gstatic.com/images/branding/gcpiconscolors/vertexai/v1/32px.svg\" alt=\"Vertex AI logo\">\n",
    "      <br>Open in<br>Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td colspan=\"5\" style=\"text-align: right\">\n",
    "    <b>Share This On: </b> \n",
    "    <a href=\"https://www.linkedin.com/sharing/share-offsite/?url=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%2520Workflows/PyTorch/serving/vertex-ai-endpoint-custom-container.ipynb\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"Linkedin Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://reddit.com/submit?url=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%2520Workflows/PyTorch/serving/vertex-ai-endpoint-custom-container.ipynb\"><img src=\"https://redditinc.com/hubfs/Reddit%20Inc/Brand/Reddit_Logo.png\" alt=\"Reddit Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://bsky.app/intent/compose?text=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%2520Workflows/PyTorch/serving/vertex-ai-endpoint-custom-container.ipynb\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"BlueSky Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://twitter.com/intent/tweet?url=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%2520Workflows/PyTorch/serving/vertex-ai-endpoint-custom-container.ipynb\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/5/5a/X_icon_2.svg\" alt=\"X (Twitter) Logo\" width=\"20px\"></a> \n",
    "  </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td colspan=\"5\" style=\"text-align: right\">\n",
    "    <b>Connect With Author On: </b> \n",
    "    <a href=\"https://www.linkedin.com/in/statmike\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"Linkedin Logo\" width=\"20px\"></a>\n",
    "    <a href=\"https://www.github.com/statmike\"><img src=\"https://www.svgrepo.com/download/217753/github.svg\" alt=\"GitHub Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://www.youtube.com/@statmike-channel\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/f/fd/YouTube_full-color_icon_%282024%29.svg\" alt=\"YouTube Logo\" width=\"20px\"></a>\n",
    "    <a href=\"https://bsky.app/profile/statmike.bsky.social\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"BlueSky Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://x.com/statmike\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/5/5a/X_icon_2.svg\" alt=\"X (Twitter) Logo\" width=\"20px\"></a>\n",
    "  </td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overview",
   "metadata": {},
   "source": [
    "# Deploy PyTorch Model with Custom Container\n",
    "\n",
    "This notebook demonstrates deploying a PyTorch model using a **custom container with FastAPI** for full control over prediction outputs.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "This workflow covers:\n",
    "\n",
    "1. **Upload Model to GCS**: Ensure the .mar file is available in Cloud Storage\n",
    "2. **Build Custom FastAPI Container**: Create a container that wraps your PyTorch model with custom logic\n",
    "3. **Control Output Format**: Return only specific outputs (e.g., anomaly scores) instead of full model output\n",
    "4. **Use Cloud Build**: Build and push containers to Artifact Registry\n",
    "5. **Deploy to Vertex AI**: Use the custom container on Vertex AI Endpoints\n",
    "6. **Compare Approaches**: Understand when to use custom vs pre-built containers\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Completed the `pytorch-autoencoder.ipynb` notebook (created the .mar file)\n",
    "- Google Cloud project with APIs enabled\n",
    "\n",
    "**Note:** This notebook can be run independently of the pre-built container notebook. Both handle uploading the .mar file to GCS.\n",
    "\n",
    "## Why Custom Containers?\n",
    "\n",
    "**Pre-built containers** (see `vertex-ai-endpoint-prebuilt-container.ipynb`) are great for:\n",
    "- \u2705 Quick deployment\n",
    "- \u2705 Standard PyTorch/TorchServe setup\n",
    "- \u2705 Full model output needed\n",
    "\n",
    "**Custom containers** are better when you need:\n",
    "- \u2705 **Custom output formatting** - Return only anomaly scores instead of 13 outputs\n",
    "- \u2705 **Reduced network traffic** - Send only what clients need\n",
    "- \u2705 **Custom preprocessing** - Add logic not in the model\n",
    "- \u2705 **Multiple output versions** - Different endpoints with different outputs\n",
    "- \u2705 **Framework flexibility** - Not limited to TorchServe\n",
    "\n",
    "## Deployment Architecture\n",
    "\n",
    "```\n",
    "PyTorch Model (.mar) \u2192 Extract .pt \u2192 FastAPI Container \u2192 Artifact Registry \u2192 Vertex AI Endpoint\n",
    "```\n",
    "\n",
    "**Key Difference from Pre-built:**\n",
    "- Pre-built: TorchServe handles everything (returns all 13 outputs)\n",
    "- Custom: FastAPI wrapper extracts .pt and returns only what you want (e.g., just anomaly scores)\n",
    "\n",
    "## What We're Building\n",
    "\n",
    "A custom FastAPI container that:\n",
    "1. Downloads the .mar file from GCS at startup\n",
    "2. Extracts the TorchScript .pt file\n",
    "3. Loads the model with `torch.jit.load()`\n",
    "4. **Returns simplified output**: Just anomaly scores and embeddings (not all 13 dict keys)\n",
    "5. Reduces response size by ~70%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "env_setup",
   "metadata": {},
   "source": [
    "---\n",
    "## Environment Setup\n",
    "\n",
    "This section will authenticate your session, enable required Google Cloud APIs, and install necessary Python packages.\n",
    "\n",
    "**Package Installation Options (`REQ_TYPE`):**\n",
    "- `PRIMARY`: Installs only the main packages. Faster, but pip resolves sub-dependencies which may result in different versions than development.\n",
    "- `ALL` (Default): Installs exact versions of all packages and dependencies. Best for perfectly reproducing the development environment.\n",
    "- `COLAB`: Installs a Colab-optimized list that excludes pre-installed packages like `ipython` and `ipykernel`.\n",
    "\n",
    "**Installation Tool Options (`INSTALL_TOOL`):**\n",
    "- `pip` (Default): Uses pip for package installation. Standard Python package installer.\n",
    "- `uv`: Modern, fast Python package installer. Must be installed separately. See: https://github.com/astral-sh/uv\n",
    "- `poetry`: Dependency management tool. Requires running notebook in a poetry environment (`poetry shell` or `poetry run jupyter lab`). Uses `pyproject.toml` instead of requirements.txt.\n",
    "\n",
    "> **Note:** If running in Google Colab, the script will automatically detect this and set `REQ_TYPE = 'COLAB'` to prevent package conflicts, overriding any manual setting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "set_project",
   "metadata": {},
   "source": [
    "### Set Your Project ID\n",
    "\n",
    "\u26a0\ufe0f **Action Required:** Replace the `PROJECT_ID` value below with your Google Cloud project ID before running this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "project_config",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = 'statmike-mlops-349915' # replace with GCP project ID\n",
    "REQ_TYPE = 'ALL' # Specify PRIMARY or ALL or COLAB\n",
    "INSTALL_TOOL = 'poetry' # Specify pip, uv, or poetry (all implemented)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config_header",
   "metadata": {},
   "source": [
    "### Configuration\n",
    "\n",
    "This cell defines the requirements files and Google Cloud APIs needed for this notebook. Run as-is without modification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": [
    "REQUIREMENTS_URL = 'https://raw.githubusercontent.com/statmike/vertex-ai-mlops/refs/heads/main/Framework%20Workflows/PyTorch/requirements.txt'\n",
    "\n",
    "REQUIRED_APIS = [\n",
    "    \"aiplatform.googleapis.com\",\n",
    "    \"storage.googleapis.com\",\n",
    "    \"artifactregistry.googleapis.com\",\n",
    "    \"cloudbuild.googleapis.com\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "run_setup_header",
   "metadata": {},
   "source": [
    "### Run Setup\n",
    "\n",
    "This cell downloads the centralized setup code and configures your environment. It will:\n",
    "- Authenticate your session with Google Cloud\n",
    "- Enable required APIs for this notebook (including Artifact Registry and Cloud Build)\n",
    "- Install necessary Python packages\n",
    "- Display a setup summary with your project information\n",
    "\n",
    "> **Note:** In Colab, if packages are installed, the kernel will automatically restart. After restart, continue from the next cell without re-running earlier cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "run_setup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "PYTHON GCP ENVIRONMENT SETUP\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "AUTHENTICATION\n",
      "==================================================\n",
      "Checking for existing ADC...\n",
      "\u2705 Existing ADC found.\n",
      "\u2705 Project is correctly set to 'statmike-mlops-349915'.\n",
      "\n",
      "==================================================\n",
      "API CHECK & ENABLE\n",
      "==================================================\n",
      "\u2705 aiplatform.googleapis.com is already enabled.\n",
      "\u2705 storage.googleapis.com is already enabled.\n",
      "\u2705 artifactregistry.googleapis.com is already enabled.\n",
      "\u2705 cloudbuild.googleapis.com is already enabled.\n",
      "\n",
      "==================================================\n",
      "PACKAGE MANAGEMENT\n",
      "==================================================\n",
      "Installation Tool: poetry\n",
      "\u2705 Found poetry at: /usr/local/google/home/statmike/.local/bin/poetry\n",
      "\u2705 Running in poetry environment: /usr/local/google/home/statmike/.cache/pypoetry/virtualenvs/frameworks-pytorch-0KVJlKeQ-py3.13\n",
      "\u2139\ufe0f  Poetry mode: Installing from pyproject.toml (REQUIREMENTS_URL ignored)\n",
      "\u2705 Found pyproject.toml at: /usr/local/google/home/statmike/Git/vertex-ai-mlops/Framework Workflows/PyTorch/pyproject.toml\n",
      "   Changed working directory to: /usr/local/google/home/statmike/Git/vertex-ai-mlops/Framework Workflows/PyTorch\n",
      "Running poetry install...\n",
      "   Restored working directory to: /usr/local/google/home/statmike/Git/vertex-ai-mlops/Framework Workflows/PyTorch/serving\n",
      "\u2705 All packages are already installed and up to date.\n",
      "\n",
      "==================================================\n",
      "Google Cloud Project Information\n",
      "==================================================\n",
      "PROJECT_ID     = statmike-mlops-349915\n",
      "PROJECT_NUMBER = 1026793852137\n",
      "==================================================\n",
      "\n",
      "\n",
      "==================================================\n",
      "SETUP SUMMARY\n",
      "==================================================\n",
      "\u2705 Authentication:    Success\n",
      "\u2705 API Configuration: Success\n",
      "\u2705 Package Install:   Already up to date\n",
      "\u2705 Installation Tool: poetry\n",
      "\u2705 Project ID:        statmike-mlops-349915\n",
      "\u2705 Project Number:    1026793852137\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os, urllib.request\n",
    "\n",
    "# Download and import setup code\n",
    "url = 'https://raw.githubusercontent.com/statmike/vertex-ai-mlops/refs/heads/main/core/notebook-template/python_setup.py'\n",
    "urllib.request.urlretrieve(url, 'python_setup_local.py')\n",
    "import python_setup_local as python_setup\n",
    "os.remove('python_setup_local.py')\n",
    "\n",
    "# Run setup\n",
    "setup_info = python_setup.setup_environment(PROJECT_ID, REQ_TYPE, REQUIREMENTS_URL, REQUIRED_APIS, INSTALL_TOOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "python_setup",
   "metadata": {},
   "source": [
    "---\n",
    "## Python Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imports_header",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "imports",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/google/home/statmike/.cache/pypoetry/virtualenvs/frameworks-pytorch-0KVJlKeQ-py3.13/lib/python3.13/site-packages/google/cloud/aiplatform/models.py:52: FutureWarning: Support for google-cloud-storage < 3.0.0 will be removed in a future version of google-cloud-aiplatform. Please upgrade to google-cloud-storage >= 3.0.0.\n",
      "  from google.cloud.aiplatform.utils import gcs_utils\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import json\n",
    "import os\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud import storage\n",
    "from google.cloud import artifactregistry_v1\n",
    "from google.cloud.devtools import cloudbuild_v1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vars_user",
   "metadata": {},
   "source": [
    "### Variables - User Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "vars_user_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION = 'us-central1'\n",
    "SERIES = 'frameworks'\n",
    "EXPERIMENT = 'pytorch-autoencoder-custom'\n",
    "\n",
    "# Model configuration\n",
    "MODEL_DISPLAY_NAME = EXPERIMENT\n",
    "ENDPOINT_DISPLAY_NAME = f\"{EXPERIMENT}-endpoint\"\n",
    "\n",
    "# Machine configuration\n",
    "MACHINE_TYPE = 'n1-standard-4'\n",
    "MIN_REPLICA_COUNT = 1\n",
    "MAX_REPLICA_COUNT = 4\n",
    "\n",
    "# Working directory for source files\n",
    "DIR = f\"files/{EXPERIMENT}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vars_auto",
   "metadata": {},
   "source": [
    "### Variables - Auto Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "vars_auto_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Google Cloud Project Information\n",
      "==================================================\n",
      "PROJECT_ID     = statmike-mlops-349915\n",
      "PROJECT_NUMBER = 1026793852137\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "PROJECT_ID = subprocess.run(['gcloud', 'config', 'get-value', 'project'], capture_output=True, text=True, check=True).stdout.strip()\n",
    "PROJECT_NUMBER = subprocess.run(['gcloud', 'projects', 'describe', PROJECT_ID, '--format=value(projectNumber)'], capture_output=True, text=True, check=True).stdout.strip()\n",
    "\n",
    "print(f\"\\n{'='*50}\\nGoogle Cloud Project Information\\n{'='*50}\\nPROJECT_ID     = {PROJECT_ID}\\nPROJECT_NUMBER = {PROJECT_NUMBER}\\n{'='*50}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "configs",
   "metadata": {},
   "source": [
    "### Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "configs_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket: gs://statmike-mlops-349915\n",
      "Model artifacts: gs://statmike-mlops-349915/frameworks/pytorch-autoencoder\n",
      "Artifact Registry: us-central1-docker.pkg.dev/statmike-mlops-349915/frameworks\n",
      "Container image: us-central1-docker.pkg.dev/statmike-mlops-349915/frameworks/pytorch-autoencoder-custom\n"
     ]
    }
   ],
   "source": [
    "# GCS configuration\n",
    "BUCKET_NAME = PROJECT_ID\n",
    "BUCKET_URI = f\"gs://{BUCKET_NAME}\"\n",
    "MODEL_ARTIFACT_DIR = f\"{BUCKET_URI}/{SERIES}/pytorch-autoencoder\"\n",
    "\n",
    "# Artifact Registry repository\n",
    "AR_REPO_NAME = SERIES\n",
    "AR_REPO_URI = f\"{REGION}-docker.pkg.dev/{PROJECT_ID}/{AR_REPO_NAME}\"\n",
    "\n",
    "# Container image name\n",
    "CONTAINER_IMAGE = f\"{AR_REPO_URI}/{EXPERIMENT}\"\n",
    "\n",
    "print(f\"Bucket: {BUCKET_URI}\")\n",
    "print(f\"Model artifacts: {MODEL_ARTIFACT_DIR}\")\n",
    "print(f\"Artifact Registry: {AR_REPO_URI}\")\n",
    "print(f\"Container image: {CONTAINER_IMAGE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "init_clients",
   "metadata": {},
   "source": [
    "### Initialize Clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "init_clients_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2705 Clients initialized\n",
      "   Project: statmike-mlops-349915\n",
      "   Location: us-central1\n"
     ]
    }
   ],
   "source": [
    "# Initialize clients\n",
    "storage_client = storage.Client(project=PROJECT_ID)\n",
    "ar_client = artifactregistry_v1.ArtifactRegistryClient()\n",
    "cb_client = cloudbuild_v1.CloudBuildClient()\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_URI)\n",
    "\n",
    "print(f\"\u2705 Clients initialized\")\n",
    "print(f\"   Project: {PROJECT_ID}\")\n",
    "print(f\"   Location: {REGION}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "create_dir",
   "metadata": {},
   "source": [
    "### Create Working Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "create_dir_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2705 Created working directory: files/pytorch-autoencoder-custom/app\n"
     ]
    }
   ],
   "source": [
    "# Create directory for application files\n",
    "os.makedirs(f\"{DIR}/app\", exist_ok=True)\n",
    "print(f\"\u2705 Created working directory: {DIR}/app\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vltvmeb21pg",
   "metadata": {},
   "source": [
    "---\n",
    "## Upload Model to Cloud Storage\n",
    "\n",
    "Ensure the .mar file from the training notebook is uploaded to GCS. The custom container will download this file at startup to extract the TorchScript model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zmmz6r8ox3p",
   "metadata": {},
   "source": [
    "### Check for Local .mar File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "pw4zwgcr85q",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2705 Found .mar file: ../files/pytorch-autoencoder/pytorch_autoencoder.mar\n",
      "   Size: 29,851 bytes (29.15 KB)\n"
     ]
    }
   ],
   "source": [
    "# Local path to .mar file (from pytorch-autoencoder notebook)\n",
    "LOCAL_MAR_PATH = '../files/pytorch-autoencoder/pytorch_autoencoder.mar'\n",
    "\n",
    "if os.path.exists(LOCAL_MAR_PATH):\n",
    "    file_size = os.path.getsize(LOCAL_MAR_PATH)\n",
    "    print(f\"\u2705 Found .mar file: {LOCAL_MAR_PATH}\")\n",
    "    print(f\"   Size: {file_size:,} bytes ({file_size / 1024:.2f} KB)\")\n",
    "else:\n",
    "    print(f\"\u274c .mar file not found at: {LOCAL_MAR_PATH}\")\n",
    "    print(\"   Please run the pytorch-autoencoder.ipynb notebook first to create the .mar file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yflouufycr",
   "metadata": {},
   "source": [
    "### Create GCS Bucket (if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "o9zc3jikn1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2705 Bucket already exists: gs://statmike-mlops-349915\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    bucket = storage_client.get_bucket(BUCKET_NAME)\n",
    "    print(f\"\u2705 Bucket already exists: {BUCKET_URI}\")\n",
    "except:\n",
    "    bucket = storage_client.create_bucket(BUCKET_NAME, location=REGION)\n",
    "    print(f\"\u2705 Created new bucket: {BUCKET_URI}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pu68umwzpho",
   "metadata": {},
   "source": [
    "### Upload .mar File to GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "nrwyxsxgj2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2705 Uploaded .mar file to: gs://statmike-mlops-349915/frameworks/pytorch-autoencoder/model.mar\n",
      "   GCS size: 29,851 bytes (29.15 KB)\n",
      "   Note: Renamed from pytorch_autoencoder.mar to model.mar\n"
     ]
    }
   ],
   "source": [
    "# Model artifact in GCS (container expects \"model.mar\")\n",
    "GCS_MAR_PATH = f\"{MODEL_ARTIFACT_DIR}/model.mar\"\n",
    "\n",
    "# Upload the .mar file (rename to model.mar for consistency)\n",
    "blob = bucket.blob(f\"{SERIES}/pytorch-autoencoder/model.mar\")\n",
    "blob.upload_from_filename(LOCAL_MAR_PATH)\n",
    "\n",
    "print(f\"\u2705 Uploaded .mar file to: {GCS_MAR_PATH}\")\n",
    "print(f\"   GCS size: {blob.size:,} bytes ({blob.size / 1024:.2f} KB)\")\n",
    "print(f\"   Note: Renamed from {os.path.basename(LOCAL_MAR_PATH)} to model.mar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ar_setup",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup Artifact Registry\n",
    "\n",
    "Create or retrieve an Artifact Registry repository to store Docker containers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "check_ar",
   "metadata": {},
   "source": [
    "### Check for Existing Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "check_ar_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2705 Using existing Artifact Registry repository:\n",
      "   Name: projects/statmike-mlops-349915/locations/us-central1/repositories/frameworks\n",
      "   Format: DOCKER\n"
     ]
    }
   ],
   "source": [
    "# Check if repository already exists\n",
    "ar_parent = f\"projects/{PROJECT_ID}/locations/{REGION}\"\n",
    "docker_repo = None\n",
    "\n",
    "for repo in ar_client.list_repositories(parent=ar_parent):\n",
    "    if repo.name.split('/')[-1] == AR_REPO_NAME:\n",
    "        docker_repo = repo\n",
    "        print(f\"\u2705 Using existing Artifact Registry repository:\")\n",
    "        print(f\"   Name: {docker_repo.name}\")\n",
    "        print(f\"   Format: {docker_repo.format_.name}\")\n",
    "        break\n",
    "\n",
    "if not docker_repo:\n",
    "    print(f\"Repository '{AR_REPO_NAME}' not found - will create it\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "create_ar",
   "metadata": {},
   "source": [
    "### Create Repository (if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "create_ar_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2705 Repository ready: us-central1-docker.pkg.dev/statmike-mlops-349915/frameworks\n"
     ]
    }
   ],
   "source": [
    "if not docker_repo:\n",
    "    # Create new repository\n",
    "    operation = ar_client.create_repository(\n",
    "        request=artifactregistry_v1.CreateRepositoryRequest(\n",
    "            parent=ar_parent,\n",
    "            repository_id=AR_REPO_NAME,\n",
    "            repository=artifactregistry_v1.Repository(\n",
    "                description=f\"Docker images for {SERIES} series\",\n",
    "                format_=artifactregistry_v1.Repository.Format.DOCKER,\n",
    "                labels={'series': SERIES}\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    print(\"Creating Artifact Registry repository...\")\n",
    "    docker_repo = operation.result()\n",
    "    print(f\"\u2705 Created repository: {docker_repo.name}\")\n",
    "else:\n",
    "    print(f\"\u2705 Repository ready: {AR_REPO_URI}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "create_app",
   "metadata": {},
   "source": [
    "---\n",
    "## Create Application Files\n",
    "\n",
    "Build the custom FastAPI application that will wrap our PyTorch model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dockerfile_header",
   "metadata": {},
   "source": [
    "### Create Dockerfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dockerfile_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting files/pytorch-autoencoder-custom/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile {DIR}/Dockerfile\n",
    "# Use official Python runtime as base image\n",
    "FROM python:3.11-slim\n",
    "\n",
    "# Install Python dependencies\n",
    "RUN pip install --no-cache-dir \\\n",
    "    torch==2.0.1 --extra-index-url https://download.pytorch.org/whl/cpu && \\\n",
    "    pip install --no-cache-dir \\\n",
    "    fastapi==0.104.1 \\\n",
    "    uvicorn[standard]==0.24.0 \\\n",
    "    google-cloud-storage\n",
    "\n",
    "# Copy application code\n",
    "COPY ./app /app\n",
    "\n",
    "# Set working directory\n",
    "WORKDIR /app\n",
    "\n",
    "# Expose port\n",
    "EXPOSE 8080\n",
    "\n",
    "# Run FastAPI with uvicorn\n",
    "CMD [\"uvicorn\", \"main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8080\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fastapi_header",
   "metadata": {},
   "source": [
    "### Create FastAPI Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fastapi_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting files/pytorch-autoencoder-custom/app/main.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {DIR}/app/main.py\n",
    "import os\n",
    "import zipfile\n",
    "import logging\n",
    "from fastapi import FastAPI, Request\n",
    "import torch\n",
    "from google.cloud import storage\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "# Global model variable\n",
    "_model = None\n",
    "\n",
    "@app.on_event(\"startup\")\n",
    "async def startup_event():\n",
    "    \"\"\"Load model at startup\"\"\"\n",
    "    global _model\n",
    "    \n",
    "    try:\n",
    "        logger.info(\"Starting model loading...\")\n",
    "        \n",
    "        # Get storage URI from environment\n",
    "        storage_uri = os.environ.get('AIP_STORAGE_URI')\n",
    "        if not storage_uri:\n",
    "            raise ValueError(\"AIP_STORAGE_URI environment variable not set\")\n",
    "        \n",
    "        logger.info(f\"Storage URI: {storage_uri}\")\n",
    "        \n",
    "        # Download .mar file from GCS\n",
    "        gcs = storage.Client()\n",
    "        paths = storage_uri.replace('gs://', '').split('/')\n",
    "        bucket = gcs.bucket(paths[0])\n",
    "        mar_blob = bucket.blob('/'.join(paths[1:]) + '/model.mar')\n",
    "        \n",
    "        logger.info(f\"Downloading model.mar from {mar_blob.name}...\")\n",
    "        mar_blob.download_to_filename('model.mar')\n",
    "        logger.info(\"\u2705 Downloaded model.mar\")\n",
    "        \n",
    "        # Extract .pt file from .mar\n",
    "        logger.info(\"Extracting final_model_traced.pt from .mar...\")\n",
    "        with zipfile.ZipFile('model.mar', 'r') as zip_ref:\n",
    "            zip_ref.extract('final_model_traced.pt')\n",
    "        logger.info(\"\u2705 Extracted model file\")\n",
    "        \n",
    "        # Load PyTorch model\n",
    "        logger.info(\"Loading TorchScript model...\")\n",
    "        _model = torch.jit.load('final_model_traced.pt')\n",
    "        _model.eval()\n",
    "        logger.info(\"\u2705 Model loaded successfully and ready for predictions\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"\u274c Failed to load model: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Health check endpoint\n",
    "@app.get(os.environ.get('AIP_HEALTH_ROUTE', '/health'), status_code=200)\n",
    "def health():\n",
    "    \"\"\"Health check endpoint\"\"\"\n",
    "    if _model is None:\n",
    "        return {\"status\": \"unhealthy\", \"reason\": \"model not loaded\"}\n",
    "    return {\"status\": \"healthy\"}\n",
    "\n",
    "# Prediction endpoint with custom output\n",
    "@app.post(os.environ.get('AIP_PREDICT_ROUTE', '/predict'))\n",
    "async def predict(request: Request):\n",
    "    \"\"\"\n",
    "    Prediction endpoint that returns simplified output.\n",
    "    \n",
    "    Returns only anomaly scores and embeddings (2 fields)\n",
    "    instead of all 13 model outputs, reducing response size by ~70%\n",
    "    \"\"\"\n",
    "    global _model\n",
    "    \n",
    "    if _model is None:\n",
    "        return {\"error\": \"Model not loaded\"}, 503\n",
    "    \n",
    "    try:\n",
    "        body = await request.json()\n",
    "        instances = torch.tensor(body[\"instances\"], dtype=torch.float32)\n",
    "        \n",
    "        logger.info(f\"Processing {len(instances)} instances\")\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            full_output = _model(instances)\n",
    "        \n",
    "        # \u2705 CUSTOM OUTPUT: Return only anomaly scores and embeddings\n",
    "        # This reduces response size by ~70% compared to full output\n",
    "        simplified = [\n",
    "            {\n",
    "                \"anomaly_score\": full_output[\"denormalized_MAE\"][i].item(),\n",
    "                \"encoded\": full_output[\"encoded\"][i].tolist()\n",
    "            }\n",
    "            for i in range(len(instances))\n",
    "        ]\n",
    "        \n",
    "        return {\"predictions\": simplified}\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Prediction error: {str(e)}\")\n",
    "        return {\"error\": str(e)}, 500"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "upload_source",
   "metadata": {},
   "source": [
    "### Upload Source Files to GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "upload_source_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2705 Uploaded source files to gs://statmike-mlops-349915/frameworks/pytorch-autoencoder-custom/\n"
     ]
    }
   ],
   "source": [
    "# Upload application files to GCS for Cloud Build\n",
    "bucket = storage_client.bucket(BUCKET_NAME)\n",
    "\n",
    "bucket.blob(f'{SERIES}/{EXPERIMENT}/Dockerfile').upload_from_filename(f'{DIR}/Dockerfile')\n",
    "bucket.blob(f'{SERIES}/{EXPERIMENT}/app/main.py').upload_from_filename(f'{DIR}/app/main.py')\n",
    "\n",
    "print(f\"\u2705 Uploaded source files to gs://{BUCKET_NAME}/{SERIES}/{EXPERIMENT}/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "build_container",
   "metadata": {},
   "source": [
    "---\n",
    "## Build Container with Cloud Build\n",
    "\n",
    "Use Cloud Build to build the Docker container and push to Artifact Registry."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "build_config",
   "metadata": {},
   "source": [
    "### Create Build Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "build_config_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build configuration created\n"
     ]
    }
   ],
   "source": [
    "# Create Cloud Build configuration\n",
    "build = cloudbuild_v1.Build(\n",
    "    steps=[\n",
    "        # Step 1: Copy source from GCS\n",
    "        {\n",
    "            'name': 'gcr.io/cloud-builders/gsutil',\n",
    "            'args': ['cp', '-r', f'gs://{BUCKET_NAME}/{SERIES}/{EXPERIMENT}/*', '/workspace']\n",
    "        },\n",
    "        # Step 2: Build Docker image\n",
    "        {\n",
    "            'name': 'gcr.io/cloud-builders/docker',\n",
    "            'args': ['build', '-t', CONTAINER_IMAGE, '/workspace']\n",
    "        }\n",
    "    ],\n",
    "    # Push the image\n",
    "    images=[CONTAINER_IMAGE]\n",
    ")\n",
    "\n",
    "print(\"Build configuration created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "run_build",
   "metadata": {},
   "source": [
    "### Run Build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "run_build_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building container image: us-central1-docker.pkg.dev/statmike-mlops-349915/frameworks/pytorch-autoencoder-custom\n",
      "This will take 3-5 minutes...\n",
      "\n",
      "\u2705 Container built successfully!\n",
      "   Image: us-central1-docker.pkg.dev/statmike-mlops-349915/frameworks/pytorch-autoencoder-custom\n",
      "   Status: SUCCESS\n"
     ]
    }
   ],
   "source": [
    "# Submit build to Cloud Build\n",
    "print(f\"Building container image: {CONTAINER_IMAGE}\")\n",
    "print(\"This will take 3-5 minutes...\")\n",
    "\n",
    "operation = cb_client.create_build(project_id=PROJECT_ID, build=build)\n",
    "build_response = operation.result()\n",
    "\n",
    "if build_response.status == cloudbuild_v1.Build.Status.SUCCESS:\n",
    "    print(f\"\\n\u2705 Container built successfully!\")\n",
    "    print(f\"   Image: {CONTAINER_IMAGE}\")\n",
    "    print(f\"   Status: {build_response.status.name}\")\n",
    "else:\n",
    "    print(f\"\u274c Build failed with status: {build_response.status.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model_registry",
   "metadata": {},
   "source": [
    "---\n",
    "## Register Model in Vertex AI Model Registry\n",
    "\n",
    "Upload the model using our custom container image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "check_model",
   "metadata": {},
   "source": [
    "### Check for Existing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "check_model_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No existing models found with name pytorch-autoencoder-custom\n"
     ]
    }
   ],
   "source": [
    "# Check if model already exists\n",
    "existing_models = aiplatform.Model.list(filter=f\"display_name={MODEL_DISPLAY_NAME}\")\n",
    "\n",
    "if existing_models:\n",
    "    model = existing_models[0]\n",
    "    print(f\"\u2705 Using existing model:\")\n",
    "    print(f\"   Resource name: {model.resource_name}\")\n",
    "    print(f\"   Display name: {model.display_name}\")\n",
    "    print(f\"   Version: {model.version_id}\")\n",
    "else:\n",
    "    print(f\"No existing models found with name {MODEL_DISPLAY_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "upload_model",
   "metadata": {},
   "source": [
    "### Upload Model to Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "upload_model_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Model\n",
      "Create Model backing LRO: projects/1026793852137/locations/us-central1/models/41089849041616896/operations/391605858099789824\n",
      "Model created. Resource name: projects/1026793852137/locations/us-central1/models/41089849041616896@1\n",
      "To use this Model in another session:\n",
      "model = aiplatform.Model('projects/1026793852137/locations/us-central1/models/41089849041616896@1')\n",
      "\u2705 Model uploaded to registry:\n",
      "   Resource name: projects/1026793852137/locations/us-central1/models/41089849041616896\n",
      "   Display name: pytorch-autoencoder-custom\n",
      "   Container: us-central1-docker.pkg.dev/statmike-mlops-349915/frameworks/pytorch-autoencoder-custom\n"
     ]
    }
   ],
   "source": [
    "if not existing_models:\n",
    "    # Upload model with custom container\n",
    "    model = aiplatform.Model.upload(\n",
    "        display_name=MODEL_DISPLAY_NAME,\n",
    "        artifact_uri=MODEL_ARTIFACT_DIR,\n",
    "        serving_container_image_uri=CONTAINER_IMAGE,\n",
    "        serving_container_environment_variables={\n",
    "            \"AIP_STORAGE_URI\": MODEL_ARTIFACT_DIR,\n",
    "            \"AIP_HEALTH_ROUTE\": \"/health\",\n",
    "            \"AIP_PREDICT_ROUTE\": \"/predict\",\n",
    "            \"AIP_HTTP_PORT\": \"8080\"\n",
    "        },\n",
    "        serving_container_ports=[8080],\n",
    "        description=\"PyTorch autoencoder with custom FastAPI container (simplified output)\"\n",
    "    )\n",
    "    \n",
    "    print(f\"\u2705 Model uploaded to registry:\")\n",
    "    print(f\"   Resource name: {model.resource_name}\")\n",
    "    print(f\"   Display name: {model.display_name}\")\n",
    "    print(f\"   Container: {CONTAINER_IMAGE}\")\n",
    "else:\n",
    "    print(\"\u2705 Using existing model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iam_permissions",
   "metadata": {},
   "source": [
    "---\n",
    "## Grant GCS Access to Vertex AI Service Account\n",
    "\n",
    "When using custom containers, Vertex AI creates a service account to run your container. This service account needs permission to download the model from GCS.\n",
    "\n",
    "**Service Account Pattern**: `service-{PROJECT_NUMBER}@gcp-sa-aiplatform-cc.iam.gserviceaccount.com`\n",
    "\n",
    "We'll grant this account the `Storage Object Viewer` role on our bucket."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grant_iam_header",
   "metadata": {},
   "source": [
    "### Grant Storage Access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grant_iam_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vertex AI custom container service account\n",
    "vertex_sa = f\"service-{PROJECT_NUMBER}@gcp-sa-aiplatform-cc.iam.gserviceaccount.com\"\n",
    "\n",
    "print(f\"Granting Storage Object Viewer role to: {vertex_sa}\")\n",
    "print(f\"Bucket: {BUCKET_NAME}\\n\")\n",
    "\n",
    "# Grant IAM permission using gsutil\n",
    "result = subprocess.run(\n",
    "    ['gsutil', 'iam', 'ch', f'serviceAccount:{vertex_sa}:roles/storage.objectViewer', f'gs://{BUCKET_NAME}'],\n",
    "    capture_output=True,\n",
    "    text=True\n",
    ")\n",
    "\n",
    "if result.returncode == 0:\n",
    "    print(f\"\u2705 Granted Storage Object Viewer role to Vertex AI service account\")\n",
    "    print(f\"   Service Account: {vertex_sa}\")\n",
    "    print(f\"   Bucket: gs://{BUCKET_NAME}\")\n",
    "    print(f\"\\n   This allows the custom container to download model.mar from GCS\")\n",
    "else:\n",
    "    print(f\"\u26a0\ufe0f  Note: {result.stderr}\")\n",
    "    print(f\"   The service account may already have access\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "endpoint_deploy",
   "metadata": {},
   "source": [
    "---\n",
    "## Deploy Model to Endpoint\n",
    "\n",
    "Create a Vertex AI Endpoint and deploy the custom container."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "check_endpoint",
   "metadata": {},
   "source": [
    "### Check for Existing Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "check_endpoint_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No existing endpoints found with name pytorch-autoencoder-custom-endpoint\n"
     ]
    }
   ],
   "source": [
    "# Check if endpoint already exists\n",
    "existing_endpoints = aiplatform.Endpoint.list(filter=f\"display_name={ENDPOINT_DISPLAY_NAME}\")\n",
    "\n",
    "if existing_endpoints:\n",
    "    endpoint = existing_endpoints[0]\n",
    "    print(f\"\u2705 Using existing endpoint:\")\n",
    "    print(f\"   Resource name: {endpoint.resource_name}\")\n",
    "    print(f\"   Display name: {endpoint.display_name}\")\n",
    "else:\n",
    "    print(f\"No existing endpoints found with name {ENDPOINT_DISPLAY_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "create_endpoint",
   "metadata": {},
   "source": [
    "### Create Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "create_endpoint_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Endpoint\n",
      "Create Endpoint backing LRO: projects/1026793852137/locations/us-central1/endpoints/4469724773630345216/operations/5336980461417660416\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint created. Resource name: projects/1026793852137/locations/us-central1/endpoints/4469724773630345216\n",
      "To use this Endpoint in another session:\n",
      "endpoint = aiplatform.Endpoint('projects/1026793852137/locations/us-central1/endpoints/4469724773630345216')\n",
      "\u2705 Endpoint created:\n",
      "   Resource name: projects/1026793852137/locations/us-central1/endpoints/4469724773630345216\n",
      "   Display name: pytorch-autoencoder-custom-endpoint\n"
     ]
    }
   ],
   "source": [
    "if not existing_endpoints:\n",
    "    # Create endpoint\n",
    "    endpoint = aiplatform.Endpoint.create(\n",
    "        display_name=ENDPOINT_DISPLAY_NAME,\n",
    "        description=\"Endpoint for PyTorch autoencoder with custom container\"\n",
    "    )\n",
    "    \n",
    "    print(f\"\u2705 Endpoint created:\")\n",
    "    print(f\"   Resource name: {endpoint.resource_name}\")\n",
    "    print(f\"   Display name: {endpoint.display_name}\")\n",
    "else:\n",
    "    print(\"\u2705 Using existing endpoint\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deploy_model",
   "metadata": {},
   "source": [
    "### Deploy Model to Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "deploy_model_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deploying model to endpoint (this will take 10-15 minutes)...\n",
      "You can monitor progress in the Cloud Console:\n",
      "https://console.cloud.google.com/vertex-ai/endpoints/4469724773630345216?project=statmike-mlops-349915\n",
      "Deploying Model projects/1026793852137/locations/us-central1/models/41089849041616896 to Endpoint : projects/1026793852137/locations/us-central1/endpoints/4469724773630345216\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deploy Endpoint model backing LRO: projects/1026793852137/locations/us-central1/endpoints/4469724773630345216/operations/340236674850095104\n"
     ]
    },
    {
     "ename": "FailedPrecondition",
     "evalue": "400 Model server exited unexpectedly. Model server logs can be found at https://console.cloud.google.com/logs/viewer?project=1026793852137&resource=aiplatform.googleapis.com%2FEndpoint&advancedFilter=resource.type%3D%22aiplatform.googleapis.com%2FEndpoint%22%0Aresource.labels.endpoint_id%3D%224469724773630345216%22%0Aresource.labels.location%3D%22us-central1%22. 9: Model server exited unexpectedly. Model server logs can be found at https://console.cloud.google.com/logs/viewer?project=1026793852137&resource=aiplatform.googleapis.com%2FEndpoint&advancedFilter=resource.type%3D%22aiplatform.googleapis.com%2FEndpoint%22%0Aresource.labels.endpoint_id%3D%224469724773630345216%22%0Aresource.labels.location%3D%22us-central1%22.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFailedPrecondition\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mYou can monitor progress in the Cloud Console:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mhttps://console.cloud.google.com/vertex-ai/endpoints/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mendpoint.name.split(\u001b[33m'\u001b[39m\u001b[33m/\u001b[39m\u001b[33m'\u001b[39m)[-\u001b[32m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m?project=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mPROJECT_ID\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[43mendpoint\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdeploy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdeployed_model_display_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mMODEL_DISPLAY_NAME\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmachine_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mMACHINE_TYPE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmin_replica_count\u001b[49m\u001b[43m=\u001b[49m\u001b[43mMIN_REPLICA_COUNT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_replica_count\u001b[49m\u001b[43m=\u001b[49m\u001b[43mMAX_REPLICA_COUNT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtraffic_percentage\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43msync\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m     24\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\u2705 Model deployed successfully!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     27\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   Endpoint: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mendpoint.display_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/frameworks-pytorch-0KVJlKeQ-py3.13/lib/python3.13/site-packages/google/cloud/aiplatform/models.py:1535\u001b[39m, in \u001b[36mEndpoint.deploy\u001b[39m\u001b[34m(self, model, deployed_model_display_name, traffic_percentage, traffic_split, machine_type, min_replica_count, max_replica_count, accelerator_type, accelerator_count, gpu_partition_size, tpu_topology, service_account, explanation_metadata, explanation_parameters, metadata, sync, deploy_request_timeout, autoscaling_target_cpu_utilization, autoscaling_target_accelerator_duty_cycle, autoscaling_target_request_count_per_minute, autoscaling_target_pubsub_num_undelivered_messages, autoscaling_pubsub_subscription_labels, enable_access_logging, disable_container_logging, deployment_resource_pool, reservation_affinity_type, reservation_affinity_key, reservation_affinity_values, spot, fast_tryout_enabled, system_labels, required_replica_count)\u001b[39m\n\u001b[32m   1519\u001b[39m \u001b[38;5;28mself\u001b[39m._validate_deploy_args(\n\u001b[32m   1520\u001b[39m     min_replica_count=min_replica_count,\n\u001b[32m   1521\u001b[39m     max_replica_count=max_replica_count,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1527\u001b[39m     required_replica_count=required_replica_count,\n\u001b[32m   1528\u001b[39m )\n\u001b[32m   1530\u001b[39m explanation_spec = _explanation_utils.create_and_validate_explanation_spec(\n\u001b[32m   1531\u001b[39m     explanation_metadata=explanation_metadata,\n\u001b[32m   1532\u001b[39m     explanation_parameters=explanation_parameters,\n\u001b[32m   1533\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1535\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_deploy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1536\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1537\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdeployed_model_display_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdeployed_model_display_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1538\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtraffic_percentage\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtraffic_percentage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1539\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtraffic_split\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtraffic_split\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1540\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmachine_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmachine_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1541\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmin_replica_count\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmin_replica_count\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1542\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_replica_count\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_replica_count\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1543\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccelerator_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43maccelerator_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1544\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccelerator_count\u001b[49m\u001b[43m=\u001b[49m\u001b[43maccelerator_count\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1545\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgpu_partition_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgpu_partition_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1546\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtpu_topology\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtpu_topology\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1547\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreservation_affinity_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreservation_affinity_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1548\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreservation_affinity_key\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreservation_affinity_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1549\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreservation_affinity_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreservation_affinity_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1550\u001b[39m \u001b[43m    \u001b[49m\u001b[43mservice_account\u001b[49m\u001b[43m=\u001b[49m\u001b[43mservice_account\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1551\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexplanation_spec\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexplanation_spec\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1552\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1553\u001b[39m \u001b[43m    \u001b[49m\u001b[43msync\u001b[49m\u001b[43m=\u001b[49m\u001b[43msync\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1554\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdeploy_request_timeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdeploy_request_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1555\u001b[39m \u001b[43m    \u001b[49m\u001b[43mautoscaling_target_cpu_utilization\u001b[49m\u001b[43m=\u001b[49m\u001b[43mautoscaling_target_cpu_utilization\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1556\u001b[39m \u001b[43m    \u001b[49m\u001b[43mautoscaling_target_accelerator_duty_cycle\u001b[49m\u001b[43m=\u001b[49m\u001b[43mautoscaling_target_accelerator_duty_cycle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1557\u001b[39m \u001b[43m    \u001b[49m\u001b[43mautoscaling_target_request_count_per_minute\u001b[49m\u001b[43m=\u001b[49m\u001b[43mautoscaling_target_request_count_per_minute\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1558\u001b[39m \u001b[43m    \u001b[49m\u001b[43mautoscaling_target_pubsub_num_undelivered_messages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mautoscaling_target_pubsub_num_undelivered_messages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1559\u001b[39m \u001b[43m    \u001b[49m\u001b[43mautoscaling_pubsub_subscription_labels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mautoscaling_pubsub_subscription_labels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1560\u001b[39m \u001b[43m    \u001b[49m\u001b[43mspot\u001b[49m\u001b[43m=\u001b[49m\u001b[43mspot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1561\u001b[39m \u001b[43m    \u001b[49m\u001b[43menable_access_logging\u001b[49m\u001b[43m=\u001b[49m\u001b[43menable_access_logging\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1562\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdisable_container_logging\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisable_container_logging\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1563\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdeployment_resource_pool\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdeployment_resource_pool\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1564\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfast_tryout_enabled\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfast_tryout_enabled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1565\u001b[39m \u001b[43m    \u001b[49m\u001b[43msystem_labels\u001b[49m\u001b[43m=\u001b[49m\u001b[43msystem_labels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1566\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequired_replica_count\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequired_replica_count\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1567\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/frameworks-pytorch-0KVJlKeQ-py3.13/lib/python3.13/site-packages/google/cloud/aiplatform/base.py:862\u001b[39m, in \u001b[36moptional_sync.<locals>.optional_run_in_thread.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    860\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m    861\u001b[39m         VertexAiResourceNounWithFutureManager.wait(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m862\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    864\u001b[39m \u001b[38;5;66;03m# callbacks to call within the Future (in same Thread)\u001b[39;00m\n\u001b[32m    865\u001b[39m internal_callbacks = []\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/frameworks-pytorch-0KVJlKeQ-py3.13/lib/python3.13/site-packages/google/cloud/aiplatform/models.py:1736\u001b[39m, in \u001b[36mEndpoint._deploy\u001b[39m\u001b[34m(self, model, deployed_model_display_name, traffic_percentage, traffic_split, machine_type, min_replica_count, max_replica_count, accelerator_type, accelerator_count, gpu_partition_size, tpu_topology, reservation_affinity_type, reservation_affinity_key, reservation_affinity_values, service_account, explanation_spec, metadata, sync, deploy_request_timeout, autoscaling_target_cpu_utilization, autoscaling_target_accelerator_duty_cycle, autoscaling_target_request_count_per_minute, autoscaling_target_pubsub_num_undelivered_messages, autoscaling_pubsub_subscription_labels, spot, enable_access_logging, disable_container_logging, deployment_resource_pool, fast_tryout_enabled, system_labels, required_replica_count)\u001b[39m\n\u001b[32m   1604\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Deploys a Model to the Endpoint.\u001b[39;00m\n\u001b[32m   1605\u001b[39m \n\u001b[32m   1606\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1730\u001b[39m \u001b[33;03m        rest of the replicas will be retried.\u001b[39;00m\n\u001b[32m   1731\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1732\u001b[39m _LOGGER.log_action_start_against_resource(\n\u001b[32m   1733\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDeploying Model \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel.resource_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m to\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\n\u001b[32m   1734\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_deploy_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1737\u001b[39m \u001b[43m    \u001b[49m\u001b[43mapi_client\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapi_client\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1738\u001b[39m \u001b[43m    \u001b[49m\u001b[43mendpoint_resource_name\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mresource_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1739\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1740\u001b[39m \u001b[43m    \u001b[49m\u001b[43mendpoint_resource_traffic_split\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_gca_resource\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtraffic_split\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1741\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnetwork\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnetwork\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1742\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdeployed_model_display_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdeployed_model_display_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1743\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtraffic_percentage\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtraffic_percentage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1744\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtraffic_split\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtraffic_split\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1745\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmachine_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmachine_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1746\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmin_replica_count\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmin_replica_count\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1747\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_replica_count\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_replica_count\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1748\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccelerator_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43maccelerator_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1749\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccelerator_count\u001b[49m\u001b[43m=\u001b[49m\u001b[43maccelerator_count\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1750\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgpu_partition_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgpu_partition_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1751\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtpu_topology\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtpu_topology\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1752\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreservation_affinity_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreservation_affinity_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1753\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreservation_affinity_key\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreservation_affinity_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1754\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreservation_affinity_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreservation_affinity_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1755\u001b[39m \u001b[43m    \u001b[49m\u001b[43mservice_account\u001b[49m\u001b[43m=\u001b[49m\u001b[43mservice_account\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1756\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexplanation_spec\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexplanation_spec\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1757\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1758\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdeploy_request_timeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdeploy_request_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1759\u001b[39m \u001b[43m    \u001b[49m\u001b[43mautoscaling_target_cpu_utilization\u001b[49m\u001b[43m=\u001b[49m\u001b[43mautoscaling_target_cpu_utilization\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1760\u001b[39m \u001b[43m    \u001b[49m\u001b[43mautoscaling_target_accelerator_duty_cycle\u001b[49m\u001b[43m=\u001b[49m\u001b[43mautoscaling_target_accelerator_duty_cycle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1761\u001b[39m \u001b[43m    \u001b[49m\u001b[43mautoscaling_target_request_count_per_minute\u001b[49m\u001b[43m=\u001b[49m\u001b[43mautoscaling_target_request_count_per_minute\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1762\u001b[39m \u001b[43m    \u001b[49m\u001b[43mautoscaling_target_pubsub_num_undelivered_messages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mautoscaling_target_pubsub_num_undelivered_messages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1763\u001b[39m \u001b[43m    \u001b[49m\u001b[43mautoscaling_pubsub_subscription_labels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mautoscaling_pubsub_subscription_labels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1764\u001b[39m \u001b[43m    \u001b[49m\u001b[43mspot\u001b[49m\u001b[43m=\u001b[49m\u001b[43mspot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1765\u001b[39m \u001b[43m    \u001b[49m\u001b[43menable_access_logging\u001b[49m\u001b[43m=\u001b[49m\u001b[43menable_access_logging\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1766\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdisable_container_logging\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisable_container_logging\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1767\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdeployment_resource_pool\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdeployment_resource_pool\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1768\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfast_tryout_enabled\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfast_tryout_enabled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1769\u001b[39m \u001b[43m    \u001b[49m\u001b[43msystem_labels\u001b[49m\u001b[43m=\u001b[49m\u001b[43msystem_labels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1770\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequired_replica_count\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequired_replica_count\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1771\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1773\u001b[39m _LOGGER.log_action_completed_against_resource(\u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mdeployed\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m)\n\u001b[32m   1775\u001b[39m \u001b[38;5;28mself\u001b[39m._sync_gca_resource()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/frameworks-pytorch-0KVJlKeQ-py3.13/lib/python3.13/site-packages/google/cloud/aiplatform/models.py:2237\u001b[39m, in \u001b[36mEndpoint._deploy_call\u001b[39m\u001b[34m(cls, api_client, endpoint_resource_name, model, endpoint_resource_traffic_split, network, deployed_model_display_name, traffic_percentage, traffic_split, machine_type, min_replica_count, max_replica_count, accelerator_type, accelerator_count, gpu_partition_size, tpu_topology, reservation_affinity_type, reservation_affinity_key, reservation_affinity_values, service_account, explanation_spec, metadata, deploy_request_timeout, autoscaling_target_cpu_utilization, autoscaling_target_accelerator_duty_cycle, autoscaling_target_request_count_per_minute, autoscaling_target_pubsub_num_undelivered_messages, autoscaling_pubsub_subscription_labels, spot, enable_access_logging, disable_container_logging, deployment_resource_pool, fast_tryout_enabled, system_labels, required_replica_count)\u001b[39m\n\u001b[32m   2225\u001b[39m operation_future = api_client.deploy_model(\n\u001b[32m   2226\u001b[39m     endpoint=endpoint_resource_name,\n\u001b[32m   2227\u001b[39m     deployed_model=deployed_model,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2230\u001b[39m     timeout=deploy_request_timeout,\n\u001b[32m   2231\u001b[39m )\n\u001b[32m   2233\u001b[39m _LOGGER.log_action_started_against_resource_with_lro(\n\u001b[32m   2234\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mDeploy\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mcls\u001b[39m, operation_future\n\u001b[32m   2235\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m2237\u001b[39m \u001b[43moperation_future\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/frameworks-pytorch-0KVJlKeQ-py3.13/lib/python3.13/site-packages/google/api_core/future/polling.py:261\u001b[39m, in \u001b[36mPollingFuture.result\u001b[39m\u001b[34m(self, timeout, retry, polling)\u001b[39m\n\u001b[32m    256\u001b[39m \u001b[38;5;28mself\u001b[39m._blocking_poll(timeout=timeout, retry=retry, polling=polling)\n\u001b[32m    258\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    259\u001b[39m     \u001b[38;5;66;03m# pylint: disable=raising-bad-type\u001b[39;00m\n\u001b[32m    260\u001b[39m     \u001b[38;5;66;03m# Pylint doesn't recognize that this is valid in this case.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m261\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    263\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._result\n",
      "\u001b[31mFailedPrecondition\u001b[39m: 400 Model server exited unexpectedly. Model server logs can be found at https://console.cloud.google.com/logs/viewer?project=1026793852137&resource=aiplatform.googleapis.com%2FEndpoint&advancedFilter=resource.type%3D%22aiplatform.googleapis.com%2FEndpoint%22%0Aresource.labels.endpoint_id%3D%224469724773630345216%22%0Aresource.labels.location%3D%22us-central1%22. 9: Model server exited unexpectedly. Model server logs can be found at https://console.cloud.google.com/logs/viewer?project=1026793852137&resource=aiplatform.googleapis.com%2FEndpoint&advancedFilter=resource.type%3D%22aiplatform.googleapis.com%2FEndpoint%22%0Aresource.labels.endpoint_id%3D%224469724773630345216%22%0Aresource.labels.location%3D%22us-central1%22."
     ]
    }
   ],
   "source": [
    "# Check if model is already deployed to this endpoint\n",
    "deployed_models = endpoint.list_models()\n",
    "model_already_deployed = any(dm.model == model.resource_name for dm in deployed_models)\n",
    "\n",
    "if model_already_deployed:\n",
    "    print(f\"\u2705 Model already deployed to endpoint:\")\n",
    "    print(f\"   Endpoint: {endpoint.display_name}\")\n",
    "    print(f\"   Model: {model.display_name}\")\n",
    "    print(f\"   Skipping redeployment\")\n",
    "else:\n",
    "    # Deploy model to endpoint\n",
    "    print(\"Deploying model to endpoint (this will take 10-15 minutes)...\")\n",
    "    print(\"You can monitor progress in the Cloud Console:\")\n",
    "    print(f\"https://console.cloud.google.com/vertex-ai/endpoints/{endpoint.name.split('/')[-1]}?project={PROJECT_ID}\")\n",
    "\n",
    "    endpoint.deploy(\n",
    "        model=model,\n",
    "        deployed_model_display_name=MODEL_DISPLAY_NAME,\n",
    "        machine_type=MACHINE_TYPE,\n",
    "        min_replica_count=MIN_REPLICA_COUNT,\n",
    "        max_replica_count=MAX_REPLICA_COUNT,\n",
    "        traffic_percentage=100,\n",
    "        sync=True\n",
    "    )\n",
    "\n",
    "    print(f\"\\n\u2705 Model deployed successfully!\")\n",
    "    print(f\"   Endpoint: {endpoint.display_name}\")\n",
    "    print(f\"   Resource name: {endpoint.resource_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "test_data",
   "metadata": {},
   "source": [
    "---\n",
    "## Prepare Test Data\n",
    "\n",
    "Retrieve test records from the same BigQuery source used in training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xeolcaklykp",
   "metadata": {},
   "source": [
    "### Import BigQuery Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "issh0wz53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "import pandas as pd\n",
    "\n",
    "bq = bigquery.Client(project=PROJECT_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kmy63j8kbs",
   "metadata": {},
   "source": [
    "### Retrieve Test Instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test_data_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BigQuery source (same as training notebook)\n",
    "BQ_PROJECT = PROJECT_ID\n",
    "BQ_DATASET = SERIES.replace('-', '_')\n",
    "BQ_TABLE = SERIES\n",
    "\n",
    "# Get a few test instances\n",
    "query = f\"\"\"\n",
    "SELECT * EXCEPT(splits, transaction_id, Class)\n",
    "FROM `{BQ_PROJECT}.{BQ_DATASET}.{BQ_TABLE}`\n",
    "WHERE splits = \"TEST\" AND Class = 0\n",
    "LIMIT 3\n",
    "\"\"\"\n",
    "\n",
    "test_df = bq.query(query).to_dataframe()\n",
    "print(f\"Retrieved {len(test_df)} test instances\")\n",
    "print(f\"\\nFeatures: {list(test_df.columns)}\")\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ali5wjhh9j",
   "metadata": {},
   "source": [
    "### Format Data for Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "r3i3bfz8os",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to list of lists (format expected by endpoint)\n",
    "test_instances = test_df.values.tolist()\n",
    "\n",
    "print(f\"Prepared {len(test_instances)} instances for prediction\")\n",
    "print(f\"\\nFirst instance (30 features):\")\n",
    "print(test_instances[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "predictions",
   "metadata": {},
   "source": [
    "---\n",
    "## Make Online Predictions\n",
    "\n",
    "Test the deployed model using both the Vertex AI SDK and direct REST API calls."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "custom_pred",
   "metadata": {},
   "source": [
    "### Method 1: Vertex AI SDK\n",
    "\n",
    "The simplest way to make predictions using the Python SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "custom_pred_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make prediction using SDK\n",
    "sdk_response = endpoint.predict(instances=test_instances)\n",
    "sdk_predictions = sdk_response.predictions\n",
    "\n",
    "print(f\"\u2705 Received predictions for {len(sdk_predictions)} instances\")\n",
    "print(f\"\\n\ud83d\udcca Custom Container Output (Simplified):\")\n",
    "print(f\"   Keys per prediction: {list(sdk_predictions[0].keys())}\")\n",
    "print(f\"   Output size: {len(str(sdk_predictions[0]))} characters\")\n",
    "\n",
    "# The custom container returns only anomaly scores and encoded representations\n",
    "first_prediction = sdk_predictions[0]\n",
    "print(f\"\\nFirst prediction:\")\n",
    "print(f\"   Anomaly Score: {first_prediction['anomaly_score']:.4f}\")\n",
    "print(f\"   Encoded: {first_prediction['encoded']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "omolzg7us3c",
   "metadata": {},
   "source": [
    "### Analyze Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jd65uxjw5v",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract key metrics from all predictions\n",
    "results = []\n",
    "for i, pred in enumerate(sdk_predictions):\n",
    "    results.append({\n",
    "        \"instance\": i,\n",
    "        \"anomaly_score\": pred[\"anomaly_score\"],\n",
    "        \"encoded\": pred[\"encoded\"]\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"Prediction Summary:\")\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "v1w610sc9la",
   "metadata": {},
   "source": [
    "### Method 2: REST API with Requests\n",
    "\n",
    "Make predictions using direct HTTP calls. This is useful for:\n",
    "- Non-Python clients\n",
    "- Testing from other services\n",
    "- Understanding the raw API format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7gz1gueusoe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import google.auth\n",
    "import google.auth.transport.requests\n",
    "\n",
    "# Get authentication token\n",
    "credentials, project = google.auth.default()\n",
    "auth_req = google.auth.transport.requests.Request()\n",
    "credentials.refresh(auth_req)\n",
    "access_token = credentials.token\n",
    "\n",
    "# Construct endpoint URL\n",
    "endpoint_url = f\"https://{REGION}-aiplatform.googleapis.com/v1/{endpoint.resource_name}:predict\"\n",
    "\n",
    "print(f\"Endpoint URL: {endpoint_url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3wnn83dat",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare request payload\n",
    "payload = {\n",
    "    \"instances\": test_instances\n",
    "}\n",
    "\n",
    "# Make REST API request\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {access_token}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "rest_response = requests.post(endpoint_url, headers=headers, json=payload)\n",
    "\n",
    "if rest_response.status_code == 200:\n",
    "    rest_predictions = rest_response.json()[\"predictions\"]\n",
    "    print(f\"\u2705 REST API prediction successful\")\n",
    "    print(f\"   Received {len(rest_predictions)} predictions\")\n",
    "    print(f\"\\nFirst prediction anomaly score: {rest_predictions[0]['anomaly_score']:.4f}\")\n",
    "else:\n",
    "    print(f\"\u274c Error: {rest_response.status_code}\")\n",
    "    print(rest_response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uzpgr7lk5u",
   "metadata": {},
   "source": [
    "### Compare SDK vs REST API Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8jju1qap6r",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify both methods return the same results\n",
    "sdk_score = sdk_predictions[0][\"anomaly_score\"]\n",
    "rest_score = rest_predictions[0][\"anomaly_score\"]\n",
    "\n",
    "print(f\"SDK anomaly score:  {sdk_score:.4f}\")\n",
    "print(f\"REST anomaly score: {rest_score:.4f}\")\n",
    "print(f\"\\nResults match: {abs(sdk_score - rest_score) < 0.001}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compare_output",
   "metadata": {},
   "source": [
    "### Compare: Custom vs Pre-built Output\n",
    "\n",
    "**Pre-built container returns 13 keys:**\n",
    "```python\n",
    "{\n",
    "  \"denormalized_MAE\": 26.99,\n",
    "  \"denormalized_RMSE\": 141.12,\n",
    "  \"denormalized_MSE\": 19914.84,\n",
    "  \"denormalized_MSLE\": 0.41,\n",
    "  \"normalized_MAE\": 0.72,\n",
    "  \"normalized_RMSE\": 1.03,\n",
    "  \"normalized_MSE\": 1.07,\n",
    "  \"normalized_MSLE\": 0.12,\n",
    "  \"encoded\": [0.17, 0.0, 0.19, 0.41],\n",
    "  \"normalized_reconstruction\": [...],\n",
    "  \"normalized_reconstruction_errors\": [...],\n",
    "  \"denormalized_reconstruction\": [...],\n",
    "  \"denormalized_reconstruction_errors\": [...]\n",
    "}\n",
    "```\n",
    "\n",
    "**Custom container returns 2 keys:**\n",
    "```python\n",
    "{\n",
    "  \"anomaly_score\": 26.99,\n",
    "  \"encoded\": [0.17, 0.0, 0.19, 0.41]\n",
    "}\n",
    "```\n",
    "\n",
    "**Benefits:**\n",
    "- \u2705 ~70% reduction in response size\n",
    "- \u2705 Lower network costs\n",
    "- \u2705 Faster responses\n",
    "- \u2705 Simpler client code\n",
    "- \u2705 Only return what clients need"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cleanup",
   "metadata": {},
   "source": [
    "---\n",
    "## Clean Up Resources\n",
    "\n",
    "**Important**: Deployed endpoints incur charges. Always clean up when done."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "undeploy",
   "metadata": {},
   "source": [
    "### Undeploy Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "undeploy_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# endpoint.undeploy_all()\n",
    "# print(\"\u2705 Model undeployed\")\n",
    "print(\"Uncomment to undeploy the model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "delete_endpoint",
   "metadata": {},
   "source": [
    "### Delete Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "delete_endpoint_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# endpoint.delete(force=True)\n",
    "# print(\"\u2705 Endpoint deleted\")\n",
    "print(\"Uncomment to delete the endpoint\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "delete_model",
   "metadata": {},
   "source": [
    "### Delete Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "delete_model_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.delete()\n",
    "# print(\"\u2705 Model deleted from registry\")\n",
    "print(\"Uncomment to delete the model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "In this notebook, you:\n",
    "\n",
    "\u2705 Created a custom FastAPI container wrapping the PyTorch model\n",
    "\n",
    "\u2705 Customized the output format (anomaly scores only)\n",
    "\n",
    "\u2705 Built and pushed the container to Artifact Registry\n",
    "\n",
    "\u2705 Deployed to Vertex AI Endpoint with custom container\n",
    "\n",
    "\u2705 Reduced response size by ~70% vs pre-built container\n",
    "\n",
    "### When to Use Custom Containers\n",
    "\n",
    "**Use Pre-built Containers when:**\n",
    "- Standard TorchServe setup works for you\n",
    "- You need full model outputs\n",
    "- Quick deployment is priority\n",
    "\n",
    "**Use Custom Containers when:**\n",
    "- Need custom output formatting (like this example)\n",
    "- Want to reduce network traffic\n",
    "- Require custom preprocessing/postprocessing\n",
    "- Need flexibility beyond TorchServe\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- **Multiple Endpoints**: Create different containers for different use cases\n",
    "- **Advanced Preprocessing**: Add custom logic in FastAPI\n",
    "- **Model Ensembles**: Combine multiple models in one container\n",
    "- **Custom Metrics**: Add application-specific monitoring\n",
    "\n",
    "### Related Notebooks\n",
    "\n",
    "- [PyTorch Autoencoder Training](../pytorch-autoencoder.ipynb) - Train the model\n",
    "- [Pre-built Container Deployment](./vertex-ai-endpoint-prebuilt-container.ipynb) - Compare approaches\n",
    "\n",
    "### Resources\n",
    "\n",
    "- [Custom Prediction Containers](https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements)\n",
    "- [FastAPI Documentation](https://fastapi.tiangolo.com/)\n",
    "- [Cloud Build Documentation](https://cloud.google.com/build/docs)\n",
    "- [Artifact Registry Documentation](https://cloud.google.com/artifact-registry/docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0493da8d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (frameworks-pytorch)",
   "language": "python",
   "name": "frameworks-pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}