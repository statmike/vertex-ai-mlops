{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![tracker](https://us-central1-vertex-ai-mlops-369716.cloudfunctions.net/pixel-tracking?path=statmike%2Fvertex-ai-mlops%2FFramework+Workflows%2FPyTorch%2Fserving&file=vertex-ai-endpoint-custom-container.ipynb)\n",
        "<!--- header table --->\n",
        "<table align=\"left\">\n",
        "<tr>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%20Workflows/PyTorch/serving/vertex-ai-endpoint-custom-container.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://www.svgrepo.com/download/217753/github.svg\" alt=\"GitHub logo\">\n",
        "      <br>View on<br>GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://colab.research.google.com/github/statmike/vertex-ai-mlops/blob/main/Framework%20Workflows/PyTorch/serving/vertex-ai-endpoint-custom-container.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://www.gstatic.com/pantheon/images/bigquery/welcome_page/colab-logo.svg\" alt=\"Google Colaboratory logo\">\n",
        "      <br>Run in<br>Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https%3A%2F%2Fraw.githubusercontent.com%2Fstatmike%2Fvertex-ai-mlops%2Fmain%2FFramework%2520Workflows%2FPyTorch%2Fserving%2Fvertex-ai-endpoint-custom-container.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\">\n",
        "      <br>Run in<br>Colab Enterprise\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/bigquery/import?url=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%20Workflows/PyTorch/serving/vertex-ai-endpoint-custom-container.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://www.gstatic.com/images/branding/gcpiconscolors/bigquery/v1/32px.svg\" alt=\"BigQuery logo\">\n",
        "      <br>Open in<br>BigQuery Studio\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/statmike/vertex-ai-mlops/main/Framework%20Workflows/PyTorch/serving/vertex-ai-endpoint-custom-container.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://www.gstatic.com/images/branding/gcpiconscolors/vertexai/v1/32px.svg\" alt=\"Vertex AI logo\">\n",
        "      <br>Open in<br>Vertex AI Workbench\n",
        "    </a>\n",
        "  </td>\n",
        "</tr>\n",
        "<tr>\n",
        "  <td colspan=\"5\" style=\"text-align: right\">\n",
        "    <b>Share This On: </b> \n",
        "    <a href=\"https://www.linkedin.com/sharing/share-offsite/?url=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%2520Workflows/PyTorch/serving/vertex-ai-endpoint-custom-container.ipynb\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"Linkedin Logo\" width=\"20px\"></a> \n",
        "    <a href=\"https://reddit.com/submit?url=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%2520Workflows/PyTorch/serving/vertex-ai-endpoint-custom-container.ipynb\"><img src=\"https://redditinc.com/hubfs/Reddit%20Inc/Brand/Reddit_Logo.png\" alt=\"Reddit Logo\" width=\"20px\"></a> \n",
        "    <a href=\"https://bsky.app/intent/compose?text=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%2520Workflows/PyTorch/serving/vertex-ai-endpoint-custom-container.ipynb\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"BlueSky Logo\" width=\"20px\"></a> \n",
        "    <a href=\"https://twitter.com/intent/tweet?url=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%2520Workflows/PyTorch/serving/vertex-ai-endpoint-custom-container.ipynb\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/5/5a/X_icon_2.svg\" alt=\"X (Twitter) Logo\" width=\"20px\"></a> \n",
        "  </td>\n",
        "</tr>\n",
        "<tr>\n",
        "  <td colspan=\"5\" style=\"text-align: right\">\n",
        "    <b>Connect With Author On: </b> \n",
        "    <a href=\"https://www.linkedin.com/in/statmike\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"Linkedin Logo\" width=\"20px\"></a>\n",
        "    <a href=\"https://www.github.com/statmike\"><img src=\"https://www.svgrepo.com/download/217753/github.svg\" alt=\"GitHub Logo\" width=\"20px\"></a> \n",
        "    <a href=\"https://www.youtube.com/@statmike-channel\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/f/fd/YouTube_full-color_icon_%282024%29.svg\" alt=\"YouTube Logo\" width=\"20px\"></a>\n",
        "    <a href=\"https://bsky.app/profile/statmike.bsky.social\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"BlueSky Logo\" width=\"20px\"></a> \n",
        "    <a href=\"https://x.com/statmike\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/5/5a/X_icon_2.svg\" alt=\"X (Twitter) Logo\" width=\"20px\"></a>\n",
        "  </td>\n",
        "</tr>\n",
        "</table>"
      ],
      "id": "header"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Deploy PyTorch Model with Custom Container\n",
        "\n",
        "This notebook demonstrates deploying a PyTorch model using a **custom container with FastAPI** for full control over prediction outputs.\n",
        "\n",
        "## What You'll Learn\n",
        "\n",
        "This workflow covers:\n",
        "\n",
        "1. **Build Custom FastAPI Container**: Create a container that wraps your PyTorch model with custom logic\n",
        "2. **Control Output Format**: Return only specific outputs (e.g., anomaly scores) instead of full model output\n",
        "3. **Use Cloud Build**: Build and push containers to Artifact Registry\n",
        "4. **Deploy to Vertex AI**: Use the custom container on Vertex AI Endpoints\n",
        "5. **Compare Approaches**: Understand when to use custom vs pre-built containers\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "- Completed the `pytorch-autoencoder.ipynb` notebook (created the .mar file)\n",
        "- .mar file in GCS at: `gs://{PROJECT_ID}/frameworks/pytorch-autoencoder/pytorch_autoencoder.mar`\n",
        "- Google Cloud project with APIs enabled\n",
        "\n",
        "## Why Custom Containers?\n",
        "\n",
        "**Pre-built containers** (previous notebook) are great for:\n",
        "- \u2705 Quick deployment\n",
        "- \u2705 Standard PyTorch/TorchServe setup\n",
        "- \u2705 Full model output needed\n",
        "\n",
        "**Custom containers** are better when you need:\n",
        "- \u2705 **Custom output formatting** - Return only anomaly scores instead of 13 outputs\n",
        "- \u2705 **Reduced network traffic** - Send only what clients need\n",
        "- \u2705 **Custom preprocessing** - Add logic not in the model\n",
        "- \u2705 **Multiple output versions** - Different endpoints with different outputs\n",
        "- \u2705 **Framework flexibility** - Not limited to TorchServe\n",
        "\n",
        "## Deployment Architecture\n",
        "\n",
        "```\n",
        "PyTorch Model (.mar) \u2192 Extract .pt \u2192 FastAPI Container \u2192 Artifact Registry \u2192 Vertex AI Endpoint\n",
        "```\n",
        "\n",
        "**Key Difference from Pre-built:**\n",
        "- Pre-built: TorchServe handles everything (returns all 13 outputs)\n",
        "- Custom: FastAPI wrapper extracts .pt and returns only what you want (e.g., just anomaly scores)\n",
        "\n",
        "## What We're Building\n",
        "\n",
        "A custom FastAPI container that:\n",
        "1. Downloads the .mar file from GCS at startup\n",
        "2. Extracts the TorchScript .pt file\n",
        "3. Loads the model with `torch.jit.load()`\n",
        "4. **Returns simplified output**: Just anomaly scores and embeddings (not all 13 dict keys)\n",
        "5. Reduces response size by ~70%"
      ],
      "id": "overview"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Environment Setup\n",
        "\n",
        "This section will authenticate your session, enable required Google Cloud APIs, and install necessary Python packages.\n",
        "\n",
        "**Package Installation Options (`REQ_TYPE`):**\n",
        "- `PRIMARY`: Installs only the main packages. Faster, but pip resolves sub-dependencies which may result in different versions than development.\n",
        "- `ALL` (Default): Installs exact versions of all packages and dependencies. Best for perfectly reproducing the development environment.\n",
        "- `COLAB`: Installs a Colab-optimized list that excludes pre-installed packages like `ipython` and `ipykernel`.\n",
        "\n",
        "**Installation Tool Options (`INSTALL_TOOL`):**\n",
        "- `pip` (Default): Uses pip for package installation. Standard Python package installer.\n",
        "- `uv`: Modern, fast Python package installer. Must be installed separately. See: https://github.com/astral-sh/uv\n",
        "- `poetry`: Dependency management tool. Requires running notebook in a poetry environment (`poetry shell` or `poetry run jupyter lab`). Uses `pyproject.toml` instead of requirements.txt.\n",
        "\n",
        "> **Note:** If running in Google Colab, the script will automatically detect this and set `REQ_TYPE = 'COLAB'` to prevent package conflicts, overriding any manual setting."
      ],
      "id": "env_setup"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Set Your Project ID\n\n\u26a0\ufe0f **Action Required:** Replace the `PROJECT_ID` value below with your Google Cloud project ID before running this cell."
      ],
      "id": "set_project"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "PROJECT_ID = 'statmike-mlops-349915' # replace with GCP project ID\n",
        "REQ_TYPE = 'ALL' # Specify PRIMARY or ALL or COLAB\n",
        "INSTALL_TOOL = 'poetry' # Specify pip, uv, or poetry (all implemented)"
      ],
      "id": "project_config"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Configuration\n\nThis cell defines the requirements files and Google Cloud APIs needed for this notebook. Run as-is without modification."
      ],
      "id": "config_header"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "REQUIREMENTS_URL = 'https://raw.githubusercontent.com/statmike/vertex-ai-mlops/refs/heads/main/Framework%20Workflows/PyTorch/requirements.txt'\n",
        "\n",
        "REQUIRED_APIS = [\n",
        "    \"aiplatform.googleapis.com\",\n",
        "    \"storage.googleapis.com\",\n",
        "    \"artifactregistry.googleapis.com\",\n",
        "    \"cloudbuild.googleapis.com\"\n",
        "]"
      ],
      "id": "config"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Run Setup\n",
        "\n",
        "This cell downloads the centralized setup code and configures your environment. It will:\n",
        "- Authenticate your session with Google Cloud\n",
        "- Enable required APIs for this notebook (including Artifact Registry and Cloud Build)\n",
        "- Install necessary Python packages\n",
        "- Display a setup summary with your project information\n",
        "\n",
        "> **Note:** In Colab, if packages are installed, the kernel will automatically restart. After restart, continue from the next cell without re-running earlier cells."
      ],
      "id": "run_setup_header"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, urllib.request\n",
        "\n",
        "# Download and import setup code\n",
        "url = 'https://raw.githubusercontent.com/statmike/vertex-ai-mlops/refs/heads/main/core/notebook-template/python_setup.py'\n",
        "urllib.request.urlretrieve(url, 'python_setup_local.py')\n",
        "import python_setup_local as python_setup\n",
        "os.remove('python_setup_local.py')\n",
        "\n",
        "# Run setup\n",
        "setup_info = python_setup.setup_environment(PROJECT_ID, REQ_TYPE, REQUIREMENTS_URL, REQUIRED_APIS, INSTALL_TOOL)"
      ],
      "id": "run_setup"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n## Python Setup"
      ],
      "id": "python_setup"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Imports"
      ],
      "id": "imports_header"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import subprocess\n",
        "import json\n",
        "import os\n",
        "from google.cloud import aiplatform\n",
        "from google.cloud import storage\n",
        "from google.cloud import artifactregistry_v1\n",
        "from google.cloud.devtools import cloudbuild_v1"
      ],
      "id": "imports"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Variables - User Set"
      ],
      "id": "vars_user"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "REGION = 'us-central1'\n",
        "SERIES = 'frameworks'\n",
        "EXPERIMENT = 'pytorch-autoencoder-custom'\n",
        "\n",
        "# Model configuration\n",
        "MODEL_DISPLAY_NAME = EXPERIMENT\n",
        "ENDPOINT_DISPLAY_NAME = f\"{EXPERIMENT}-endpoint\"\n",
        "\n",
        "# Machine configuration\n",
        "MACHINE_TYPE = 'n1-standard-4'\n",
        "MIN_REPLICA_COUNT = 1\n",
        "MAX_REPLICA_COUNT = 4\n",
        "\n",
        "# Working directory for source files\n",
        "DIR = f\"files/{EXPERIMENT}\""
      ],
      "id": "vars_user_code"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Variables - Auto Set"
      ],
      "id": "vars_auto"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "PROJECT_ID = subprocess.run(['gcloud', 'config', 'get-value', 'project'], capture_output=True, text=True, check=True).stdout.strip()\n",
        "PROJECT_NUMBER = subprocess.run(['gcloud', 'projects', 'describe', PROJECT_ID, '--format=value(projectNumber)'], capture_output=True, text=True, check=True).stdout.strip()\n",
        "\n",
        "print(f\"\\n{'='*50}\\nGoogle Cloud Project Information\\n{'='*50}\\nPROJECT_ID     = {PROJECT_ID}\\nPROJECT_NUMBER = {PROJECT_NUMBER}\\n{'='*50}\\n\")"
      ],
      "id": "vars_auto_code"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Configurations"
      ],
      "id": "configs"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# GCS configuration\n",
        "BUCKET_NAME = PROJECT_ID\n",
        "BUCKET_URI = f\"gs://{BUCKET_NAME}\"\n",
        "MODEL_ARTIFACT_DIR = f\"{BUCKET_URI}/{SERIES}/pytorch-autoencoder\"\n",
        "\n",
        "# Artifact Registry repository\n",
        "AR_REPO_NAME = SERIES\n",
        "AR_REPO_URI = f\"{REGION}-docker.pkg.dev/{PROJECT_ID}/{AR_REPO_NAME}\"\n",
        "\n",
        "# Container image name\n",
        "CONTAINER_IMAGE = f\"{AR_REPO_URI}/{EXPERIMENT}\"\n",
        "\n",
        "print(f\"Bucket: {BUCKET_URI}\")\n",
        "print(f\"Model artifacts: {MODEL_ARTIFACT_DIR}\")\n",
        "print(f\"Artifact Registry: {AR_REPO_URI}\")\n",
        "print(f\"Container image: {CONTAINER_IMAGE}\")"
      ],
      "id": "configs_code"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Initialize Clients"
      ],
      "id": "init_clients"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize clients\n",
        "storage_client = storage.Client(project=PROJECT_ID)\n",
        "ar_client = artifactregistry_v1.ArtifactRegistryClient()\n",
        "cb_client = cloudbuild_v1.CloudBuildClient()\n",
        "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_URI)\n",
        "\n",
        "print(f\"\u2705 Clients initialized\")\n",
        "print(f\"   Project: {PROJECT_ID}\")\n",
        "print(f\"   Location: {REGION}\")"
      ],
      "id": "init_clients_code"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create Working Directory"
      ],
      "id": "create_dir"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create directory for application files\n",
        "os.makedirs(f\"{DIR}/app\", exist_ok=True)\n",
        "print(f\"\u2705 Created working directory: {DIR}/app\")"
      ],
      "id": "create_dir_code"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Setup Artifact Registry\n",
        "\n",
        "Create or retrieve an Artifact Registry repository to store Docker containers."
      ],
      "id": "ar_setup"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Check for Existing Repository"
      ],
      "id": "check_ar"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check if repository already exists\n",
        "ar_parent = f\"projects/{PROJECT_ID}/locations/{REGION}\"\n",
        "docker_repo = None\n",
        "\n",
        "for repo in ar_client.list_repositories(parent=ar_parent):\n",
        "    if repo.name.split('/')[-1] == AR_REPO_NAME:\n",
        "        docker_repo = repo\n",
        "        print(f\"\u2705 Using existing Artifact Registry repository:\")\n",
        "        print(f\"   Name: {docker_repo.name}\")\n",
        "        print(f\"   Format: {docker_repo.format_.name}\")\n",
        "        break\n",
        "\n",
        "if not docker_repo:\n",
        "    print(f\"Repository '{AR_REPO_NAME}' not found - will create it\")"
      ],
      "id": "check_ar_code"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create Repository (if needed)"
      ],
      "id": "create_ar"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if not docker_repo:\n",
        "    # Create new repository\n",
        "    operation = ar_client.create_repository(\n",
        "        request=artifactregistry_v1.CreateRepositoryRequest(\n",
        "            parent=ar_parent,\n",
        "            repository_id=AR_REPO_NAME,\n",
        "            repository=artifactregistry_v1.Repository(\n",
        "                description=f\"Docker images for {SERIES} series\",\n",
        "                format_=artifactregistry_v1.Repository.Format.DOCKER,\n",
        "                labels={'series': SERIES}\n",
        "            )\n",
        "        )\n",
        "    )\n",
        "    print(\"Creating Artifact Registry repository...\")\n",
        "    docker_repo = operation.result()\n",
        "    print(f\"\u2705 Created repository: {docker_repo.name}\")\n",
        "else:\n",
        "    print(f\"\u2705 Repository ready: {AR_REPO_URI}\")"
      ],
      "id": "create_ar_code"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Create Application Files\n",
        "\n",
        "Build the custom FastAPI application that will wrap our PyTorch model."
      ],
      "id": "create_app"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create Dockerfile"
      ],
      "id": "dockerfile_header"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile {DIR}/Dockerfile\n",
        "# Start from Google's PyTorch pre-built container for base dependencies\n",
        "FROM us-docker.pkg.dev/vertex-ai/prediction/pytorch-cpu.1-13:latest\n",
        "\n",
        "# Install FastAPI and dependencies\n",
        "RUN pip install --no-cache-dir \\\\\n",
        "    fastapi==0.104.1 \\\\\n",
        "    uvicorn[standard]==0.24.0 \\\\\n",
        "    google-cloud-storage\n",
        "\n",
        "# Copy application code\n",
        "COPY ./app /app\n",
        "\n",
        "# Set working directory\n",
        "WORKDIR /app\n",
        "\n",
        "# Expose port\n",
        "EXPOSE 8080\n",
        "\n",
        "# Run FastAPI with uvicorn\n",
        "CMD [\"uvicorn\", \"main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8080\"]"
      ],
      "id": "dockerfile_code"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create FastAPI Application"
      ],
      "id": "fastapi_header"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile {DIR}/app/main.py\n",
        "import os\n",
        "import zipfile\n",
        "from fastapi import FastAPI, Request\n",
        "import torch\n",
        "from google.cloud import storage\n",
        "\n",
        "app = FastAPI()\n",
        "gcs = storage.Client()\n",
        "\n",
        "# Download .mar file and extract .pt at startup\n",
        "print(\"Loading model...\")\n",
        "storage_uri = os.environ['AIP_STORAGE_URI']\n",
        "paths = storage_uri.replace('gs://', '').split('/')\n",
        "bucket = gcs.bucket(paths[0])\n",
        "mar_blob = bucket.blob('/'.join(paths[1:]) + '/pytorch_autoencoder.mar')\n",
        "mar_blob.download_to_filename('model.mar')\n",
        "\n",
        "# Extract .pt file from .mar\n",
        "with zipfile.ZipFile('model.mar', 'r') as zip_ref:\n",
        "    zip_ref.extract('final_model_traced.pt')\n",
        "\n",
        "# Load PyTorch model\n",
        "_model = torch.jit.load('final_model_traced.pt')\n",
        "_model.eval()\n",
        "print(\"\u2705 Model loaded successfully\")\n",
        "\n",
        "# Health check endpoint\n",
        "@app.get(os.environ.get('AIP_HEALTH_ROUTE', '/health'), status_code=200)\n",
        "def health():\n",
        "    return {\"status\": \"healthy\"}\n",
        "\n",
        "# Prediction endpoint with custom output\n",
        "@app.post(os.environ.get('AIP_PREDICT_ROUTE', '/predict'))\n",
        "async def predict(request: Request):\n",
        "    body = await request.json()\n",
        "    instances = torch.tensor(body[\"instances\"], dtype=torch.float32)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        full_output = _model(instances)\n",
        "    \n",
        "    # \u2705 CUSTOM OUTPUT: Return only anomaly scores and embeddings\n",
        "    # This reduces response size by ~70% compared to full output\n",
        "    simplified = [\n",
        "        {\n",
        "            \"anomaly_score\": full_output[\"denormalized_MAE\"][i].item(),\n",
        "            \"encoded\": full_output[\"encoded\"][i].tolist()\n",
        "        }\n",
        "        for i in range(len(instances))\n",
        "    ]\n",
        "    \n",
        "    return {\"predictions\": simplified}"
      ],
      "id": "fastapi_code"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Upload Source Files to GCS"
      ],
      "id": "upload_source"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Upload application files to GCS for Cloud Build\n",
        "bucket = storage_client.bucket(BUCKET_NAME)\n",
        "\n",
        "bucket.blob(f'{SERIES}/{EXPERIMENT}/Dockerfile').upload_from_filename(f'{DIR}/Dockerfile')\n",
        "bucket.blob(f'{SERIES}/{EXPERIMENT}/app/main.py').upload_from_filename(f'{DIR}/app/main.py')\n",
        "\n",
        "print(f\"\u2705 Uploaded source files to gs://{BUCKET_NAME}/{SERIES}/{EXPERIMENT}/\")"
      ],
      "id": "upload_source_code"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Build Container with Cloud Build\n",
        "\n",
        "Use Cloud Build to build the Docker container and push to Artifact Registry."
      ],
      "id": "build_container"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create Build Configuration"
      ],
      "id": "build_config"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create Cloud Build configuration\n",
        "build = cloudbuild_v1.Build(\n",
        "    steps=[\n",
        "        # Step 1: Copy source from GCS\n",
        "        {\n",
        "            'name': 'gcr.io/cloud-builders/gsutil',\n",
        "            'args': ['cp', '-r', f'gs://{BUCKET_NAME}/{SERIES}/{EXPERIMENT}/*', '/workspace']\n",
        "        },\n",
        "        # Step 2: Build Docker image\n",
        "        {\n",
        "            'name': 'gcr.io/cloud-builders/docker',\n",
        "            'args': ['build', '-t', CONTAINER_IMAGE, '/workspace']\n",
        "        }\n",
        "    ],\n",
        "    # Push the image\n",
        "    images=[CONTAINER_IMAGE]\n",
        ")\n",
        "\n",
        "print(\"Build configuration created\")"
      ],
      "id": "build_config_code"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Run Build"
      ],
      "id": "run_build"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Submit build to Cloud Build\n",
        "print(f\"Building container image: {CONTAINER_IMAGE}\")\n",
        "print(\"This will take 3-5 minutes...\")\n",
        "\n",
        "operation = cb_client.create_build(project_id=PROJECT_ID, build=build)\n",
        "build_response = operation.result()\n",
        "\n",
        "if build_response.status == cloudbuild_v1.Build.Status.SUCCESS:\n",
        "    print(f\"\\n\u2705 Container built successfully!\")\n",
        "    print(f\"   Image: {CONTAINER_IMAGE}\")\n",
        "    print(f\"   Status: {build_response.status.name}\")\n",
        "else:\n",
        "    print(f\"\u274c Build failed with status: {build_response.status.name}\")"
      ],
      "id": "run_build_code"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Register Model in Vertex AI Model Registry\n",
        "\n",
        "Upload the model using our custom container image."
      ],
      "id": "model_registry"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Check for Existing Model"
      ],
      "id": "check_model"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check if model already exists\n",
        "existing_models = aiplatform.Model.list(filter=f\"display_name={MODEL_DISPLAY_NAME}\")\n",
        "\n",
        "if existing_models:\n",
        "    model = existing_models[0]\n",
        "    print(f\"\u2705 Using existing model:\")\n",
        "    print(f\"   Resource name: {model.resource_name}\")\n",
        "    print(f\"   Display name: {model.display_name}\")\n",
        "    print(f\"   Version: {model.version_id}\")\n",
        "else:\n",
        "    print(f\"No existing models found with name {MODEL_DISPLAY_NAME}\")"
      ],
      "id": "check_model_code"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Upload Model to Registry"
      ],
      "id": "upload_model"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if not existing_models:\n",
        "    # Upload model with custom container\n",
        "    model = aiplatform.Model.upload(\n",
        "        display_name=MODEL_DISPLAY_NAME,\n",
        "        artifact_uri=MODEL_ARTIFACT_DIR,\n",
        "        serving_container_image_uri=CONTAINER_IMAGE,\n",
        "        serving_container_environment_variables={\n",
        "            \"AIP_STORAGE_URI\": MODEL_ARTIFACT_DIR,\n",
        "            \"AIP_HEALTH_ROUTE\": \"/health\",\n",
        "            \"AIP_PREDICT_ROUTE\": \"/predict\",\n",
        "            \"AIP_HTTP_PORT\": \"8080\"\n",
        "        },\n",
        "        serving_container_ports=[8080],\n",
        "        description=\"PyTorch autoencoder with custom FastAPI container (simplified output)\"\n",
        "    )\n",
        "    \n",
        "    print(f\"\u2705 Model uploaded to registry:\")\n",
        "    print(f\"   Resource name: {model.resource_name}\")\n",
        "    print(f\"   Display name: {model.display_name}\")\n",
        "    print(f\"   Container: {CONTAINER_IMAGE}\")\n",
        "else:\n",
        "    print(\"\u2705 Using existing model\")"
      ],
      "id": "upload_model_code"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Deploy Model to Endpoint\n",
        "\n",
        "Create a Vertex AI Endpoint and deploy the custom container."
      ],
      "id": "endpoint_deploy"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Check for Existing Endpoint"
      ],
      "id": "check_endpoint"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check if endpoint already exists\n",
        "existing_endpoints = aiplatform.Endpoint.list(filter=f\"display_name={ENDPOINT_DISPLAY_NAME}\")\n",
        "\n",
        "if existing_endpoints:\n",
        "    endpoint = existing_endpoints[0]\n",
        "    print(f\"\u2705 Using existing endpoint:\")\n",
        "    print(f\"   Resource name: {endpoint.resource_name}\")\n",
        "    print(f\"   Display name: {endpoint.display_name}\")\n",
        "else:\n",
        "    print(f\"No existing endpoints found with name {ENDPOINT_DISPLAY_NAME}\")"
      ],
      "id": "check_endpoint_code"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create Endpoint"
      ],
      "id": "create_endpoint"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if not existing_endpoints:\n",
        "    # Create endpoint\n",
        "    endpoint = aiplatform.Endpoint.create(\n",
        "        display_name=ENDPOINT_DISPLAY_NAME,\n",
        "        description=\"Endpoint for PyTorch autoencoder with custom container\"\n",
        "    )\n",
        "    \n",
        "    print(f\"\u2705 Endpoint created:\")\n",
        "    print(f\"   Resource name: {endpoint.resource_name}\")\n",
        "    print(f\"   Display name: {endpoint.display_name}\")\n",
        "else:\n",
        "    print(\"\u2705 Using existing endpoint\")"
      ],
      "id": "create_endpoint_code"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Deploy Model to Endpoint"
      ],
      "id": "deploy_model"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Deploy model to endpoint\n",
        "print(\"Deploying model to endpoint (this will take 10-15 minutes)...\")\n",
        "print(\"You can monitor progress in the Cloud Console:\")\n",
        "print(f\"https://console.cloud.google.com/vertex-ai/endpoints/{endpoint.name.split('/')[-1]}?project={PROJECT_ID}\")\n",
        "\n",
        "endpoint.deploy(\n",
        "    model=model,\n",
        "    deployed_model_display_name=MODEL_DISPLAY_NAME,\n",
        "    machine_type=MACHINE_TYPE,\n",
        "    min_replica_count=MIN_REPLICA_COUNT,\n",
        "    max_replica_count=MAX_REPLICA_COUNT,\n",
        "    traffic_percentage=100,\n",
        "    sync=True\n",
        ")\n",
        "\n",
        "print(f\"\\n\u2705 Model deployed successfully!\")\n",
        "print(f\"   Endpoint: {endpoint.display_name}\")\n",
        "print(f\"   Resource name: {endpoint.resource_name}\")"
      ],
      "id": "deploy_model_code"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Prepare Test Data\n",
        "\n",
        "Retrieve test records from BigQuery."
      ],
      "id": "test_data"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.cloud import bigquery\n",
        "import pandas as pd\n",
        "\n",
        "bq = bigquery.Client(project=PROJECT_ID)\n",
        "\n",
        "# BigQuery source\n",
        "BQ_PROJECT = PROJECT_ID\n",
        "BQ_DATASET = SERIES.replace('-', '_')\n",
        "BQ_TABLE = SERIES\n",
        "\n",
        "# Get test instances\n",
        "query = f\"\"\"\n",
        "SELECT * EXCEPT(splits, transaction_id, Class)\n",
        "FROM `{BQ_PROJECT}.{BQ_DATASET}.{BQ_TABLE}`\n",
        "WHERE splits = \"TEST\" AND Class = 0\n",
        "LIMIT 3\n",
        "\"\"\"\n",
        "\n",
        "test_df = bq.query(query).to_dataframe()\n",
        "test_instances = test_df.values.tolist()\n",
        "\n",
        "print(f\"Retrieved {len(test_instances)} test instances\")\n",
        "print(f\"Features: {list(test_df.columns)}\")"
      ],
      "id": "test_data_code"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Make Online Predictions\n",
        "\n",
        "Test the custom container and compare output to the pre-built container."
      ],
      "id": "predictions"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Custom Container Predictions"
      ],
      "id": "custom_pred"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Make prediction using custom container\n",
        "response = endpoint.predict(instances=test_instances)\n",
        "\n",
        "print(f\"\u2705 Received predictions for {len(response.predictions)} instances\")\n",
        "print(f\"\\n\ud83d\udcca Custom Container Output (Simplified):\")\n",
        "print(f\"   Keys per prediction: {list(response.predictions[0].keys())}\")\n",
        "print(f\"   Output size: {len(str(response.predictions[0]))} characters\")\n",
        "\n",
        "for i, pred in enumerate(response.predictions):\n",
        "    print(f\"\\nInstance {i}:\")\n",
        "    print(f\"   Anomaly Score: {pred['anomaly_score']:.4f}\")\n",
        "    print(f\"   Encoded: {pred['encoded']}\")"
      ],
      "id": "custom_pred_code"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Compare: Custom vs Pre-built Output\n",
        "\n",
        "**Pre-built container returns 13 keys:**\n",
        "```python\n",
        "{\n",
        "  \"denormalized_MAE\": 26.99,\n",
        "  \"denormalized_RMSE\": 141.12,\n",
        "  \"denormalized_MSE\": 19914.84,\n",
        "  \"denormalized_MSLE\": 0.41,\n",
        "  \"normalized_MAE\": 0.72,\n",
        "  \"normalized_RMSE\": 1.03,\n",
        "  \"normalized_MSE\": 1.07,\n",
        "  \"normalized_MSLE\": 0.12,\n",
        "  \"encoded\": [0.17, 0.0, 0.19, 0.41],\n",
        "  \"normalized_reconstruction\": [...],\n",
        "  \"normalized_reconstruction_errors\": [...],\n",
        "  \"denormalized_reconstruction\": [...],\n",
        "  \"denormalized_reconstruction_errors\": [...]\n",
        "}\n",
        "```\n",
        "\n",
        "**Custom container returns 2 keys:**\n",
        "```python\n",
        "{\n",
        "  \"anomaly_score\": 26.99,\n",
        "  \"encoded\": [0.17, 0.0, 0.19, 0.41]\n",
        "}\n",
        "```\n",
        "\n",
        "**Benefits:**\n",
        "- \u2705 ~70% reduction in response size\n",
        "- \u2705 Lower network costs\n",
        "- \u2705 Faster responses\n",
        "- \u2705 Simpler client code\n",
        "- \u2705 Only return what clients need"
      ],
      "id": "compare_output"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Clean Up Resources\n",
        "\n",
        "**Important**: Deployed endpoints incur charges. Always clean up when done."
      ],
      "id": "cleanup"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Undeploy Model"
      ],
      "id": "undeploy"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# endpoint.undeploy_all()\n",
        "# print(\"\u2705 Model undeployed\")\n",
        "print(\"Uncomment to undeploy the model\")"
      ],
      "id": "undeploy_code"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Delete Endpoint"
      ],
      "id": "delete_endpoint"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# endpoint.delete(force=True)\n",
        "# print(\"\u2705 Endpoint deleted\")\n",
        "print(\"Uncomment to delete the endpoint\")"
      ],
      "id": "delete_endpoint_code"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Delete Model"
      ],
      "id": "delete_model"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# model.delete()\n",
        "# print(\"\u2705 Model deleted from registry\")\n",
        "print(\"Uncomment to delete the model\")"
      ],
      "id": "delete_model_code"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Summary\n",
        "\n",
        "In this notebook, you:\n",
        "\n",
        "\u2705 Created a custom FastAPI container wrapping the PyTorch model\n",
        "\n",
        "\u2705 Customized the output format (anomaly scores only)\n",
        "\n",
        "\u2705 Built and pushed the container to Artifact Registry\n",
        "\n",
        "\u2705 Deployed to Vertex AI Endpoint with custom container\n",
        "\n",
        "\u2705 Reduced response size by ~70% vs pre-built container\n",
        "\n",
        "### When to Use Custom Containers\n",
        "\n",
        "**Use Pre-built Containers when:**\n",
        "- Standard TorchServe setup works for you\n",
        "- You need full model outputs\n",
        "- Quick deployment is priority\n",
        "\n",
        "**Use Custom Containers when:**\n",
        "- Need custom output formatting (like this example)\n",
        "- Want to reduce network traffic\n",
        "- Require custom preprocessing/postprocessing\n",
        "- Need flexibility beyond TorchServe\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "- **Multiple Endpoints**: Create different containers for different use cases\n",
        "- **Advanced Preprocessing**: Add custom logic in FastAPI\n",
        "- **Model Ensembles**: Combine multiple models in one container\n",
        "- **Custom Metrics**: Add application-specific monitoring\n",
        "\n",
        "### Related Notebooks\n",
        "\n",
        "- [PyTorch Autoencoder Training](../pytorch-autoencoder.ipynb) - Train the model\n",
        "- [Pre-built Container Deployment](./vertex-ai-endpoint-prebuilt-container.ipynb) - Compare approaches\n",
        "- [Dataflow RunInference](./dataflow-runinference.ipynb) - Batch/streaming inference (coming soon)\n",
        "\n",
        "### Resources\n",
        "\n",
        "- [Custom Prediction Containers](https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements)\n",
        "- [FastAPI Documentation](https://fastapi.tiangolo.com/)\n",
        "- [Cloud Build Documentation](https://cloud.google.com/build/docs)\n",
        "- [Artifact Registry Documentation](https://cloud.google.com/artifact-registry/docs)"
      ],
      "id": "summary"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.13.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}