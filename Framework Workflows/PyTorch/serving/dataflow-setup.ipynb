{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07331bfc",
   "metadata": {},
   "source": [
    "![tracker](https://us-central1-vertex-ai-mlops-369716.cloudfunctions.net/pixel-tracking?path=statmike%2Fvertex-ai-mlops%2FFramework+Workflows%2FPyTorch%2Fserving&file=dataflow-setup.ipynb)\n",
    "<!--- header table --->\n",
    "<table align=\"left\">\n",
    "<tr>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%20Workflows/PyTorch/serving/dataflow-setup.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.svgrepo.com/download/217753/github.svg\" alt=\"GitHub logo\">\n",
    "      <br>View on<br>GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://colab.research.google.com/github/statmike/vertex-ai-mlops/blob/main/Framework%20Workflows/PyTorch/serving/dataflow-setup.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.gstatic.com/pantheon/images/bigquery/welcome_page/colab-logo.svg\" alt=\"Google Colaboratory logo\">\n",
    "      <br>Run in<br>Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https%3A%2F%2Fraw.githubusercontent.com%2Fstatmike%2Fvertex-ai-mlops%2Fmain%2FFramework%2520Workflows%2FPyTorch%2Fserving%2Fdataflow-setup.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\">\n",
    "      <br>Run in<br>Colab Enterprise\n",
    "    </a>\n",
    "  </td>      \n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/bigquery/import?url=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%20Workflows/PyTorch/serving/dataflow-setup.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.gstatic.com/images/branding/gcpiconscolors/bigquery/v1/32px.svg\" alt=\"BigQuery logo\">\n",
    "      <br>Open in<br>BigQuery Studio\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/statmike/vertex-ai-mlops/main/Framework%20Workflows/PyTorch/serving/dataflow-setup.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.gstatic.com/images/branding/gcpiconscolors/vertexai/v1/32px.svg\" alt=\"Vertex AI logo\">\n",
    "      <br>Open in<br>Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td colspan=\"5\" style=\"text-align: right\">\n",
    "    <b>Share This On: </b> \n",
    "    <a href=\"https://www.linkedin.com/sharing/share-offsite/?url=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%2520Workflows/PyTorch/serving/dataflow-setup.ipynb\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"Linkedin Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://reddit.com/submit?url=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%2520Workflows/PyTorch/serving/dataflow-setup.ipynb\"><img src=\"https://redditinc.com/hubfs/Reddit%20Inc/Brand/Reddit_Logo.png\" alt=\"Reddit Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://bsky.app/intent/compose?text=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%2520Workflows/PyTorch/serving/dataflow-setup.ipynb\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"BlueSky Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://twitter.com/intent/tweet?url=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%2520Workflows/PyTorch/serving/dataflow-setup.ipynb\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/5/5a/X_icon_2.svg\" alt=\"X (Twitter) Logo\" width=\"20px\"></a> \n",
    "  </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td colspan=\"5\" style=\"text-align: right\">\n",
    "    <b>Connect With Author On: </b> \n",
    "    <a href=\"https://www.linkedin.com/in/statmike\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"Linkedin Logo\" width=\"20px\"></a>\n",
    "    <a href=\"https://www.github.com/statmike\"><img src=\"https://www.svgrepo.com/download/217753/github.svg\" alt=\"GitHub Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://www.youtube.com/@statmike-channel\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/f/fd/YouTube_full-color_icon_%282024%29.svg\" alt=\"YouTube Logo\" width=\"20px\"></a>\n",
    "    <a href=\"https://bsky.app/profile/statmike.bsky.social\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"BlueSky Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://x.com/statmike\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/5/5a/X_icon_2.svg\" alt=\"X (Twitter) Logo\" width=\"20px\"></a>\n",
    "  </td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overview",
   "metadata": {},
   "source": [
    "# Dataflow Infrastructure Setup\n",
    "\n",
    "This notebook performs one-time setup for all Dataflow inference workflows.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "This workflow covers:\n",
    "\n",
    "1. **Extract Model from .mar File**: Extract the traced .pt model for RunInference\n",
    "2. **Upload to Cloud Storage**: Store the .pt file for Dataflow access\n",
    "3. **Create BigQuery Tables**: Set up tables for batch and streaming results\n",
    "4. **Create Pub/Sub Topics**: Set up topics and subscriptions for streaming\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Completed the `pytorch-autoencoder.ipynb` notebook (created the .mar file)\n",
    "- Completed `vertex-ai-endpoint-prebuilt-container.ipynb` OR `vertex-ai-endpoint-custom-container.ipynb` (uploaded model to GCS)\n",
    "- .mar file in GCS: `gs://{PROJECT_ID}/frameworks/pytorch-autoencoder/model.mar`\n",
    "- Google Cloud project with required APIs enabled\n",
    "\n",
    "## Why This Setup?\n",
    "\n",
    "**RunInference vs TorchServe:**\n",
    "- **TorchServe** (Vertex Endpoints) uses `.mar` files with custom handlers\n",
    "- **RunInference** (Dataflow) loads PyTorch models directly from `.pt` files\n",
    "- We extract the `.pt` from the `.mar` to use with RunInference\n",
    "\n",
    "**One-Time Setup:**\n",
    "- BigQuery tables persist across multiple pipeline runs\n",
    "- Pub/Sub topics can be reused for streaming\n",
    "- Model file stays in GCS for all Dataflow jobs\n",
    "\n",
    "## Architecture\n",
    "\n",
    "```\n",
    ".mar file in GCS → Extract .pt → Upload .pt to GCS\n",
    "                                      ↓\n",
    "                         Used by all Dataflow pipelines\n",
    "                                      ↓\n",
    "                    ┌─────────────────┴──────────────────┐\n",
    "                    ↓                                    ↓\n",
    "            Batch RunInference              Streaming RunInference\n",
    "                    ↓                                    ↓\n",
    "              BigQuery Results                  Pub/Sub + BigQuery\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "env_setup",
   "metadata": {},
   "source": [
    "---\n",
    "## Environment Setup\n",
    "\n",
    "This section will authenticate your session, enable required Google Cloud APIs, and install necessary Python packages.\n",
    "\n",
    "**Package Installation Options (`REQ_TYPE`):**\n",
    "- `PRIMARY`: Installs only the main packages\n",
    "- `ALL` (Default): Installs exact versions of all packages and dependencies\n",
    "- `COLAB`: Installs a Colab-optimized list\n",
    "\n",
    "**Installation Tool Options (`INSTALL_TOOL`):**\n",
    "- `pip` (Default): Standard Python package installer\n",
    "- `uv`: Modern, fast Python package installer\n",
    "- `poetry`: Dependency management tool\n",
    "\n",
    "> **Note:** If running in Google Colab, the script will automatically set `REQ_TYPE = 'COLAB' to prevent package conflicts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "set_project",
   "metadata": {},
   "source": [
    "### Set Your Project ID\n",
    "\n",
    "⚠️ **Action Required:** Replace the `PROJECT_ID` value below with your Google Cloud project ID before running this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "project_config",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = 'statmike-mlops-349915' # replace with GCP project ID\n",
    "REQ_TYPE = 'ALL' # Specify PRIMARY or ALL or COLAB\n",
    "INSTALL_TOOL = 'poetry' # Specify pip, uv, or poetry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config_header",
   "metadata": {},
   "source": [
    "### Configuration\n",
    "\n",
    "This cell defines the requirements files and Google Cloud APIs needed for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": [
    "REQUIREMENTS_URL = 'https://raw.githubusercontent.com/statmike/vertex-ai-mlops/refs/heads/main/Framework%20Workflows/PyTorch/requirements.txt'\n",
    "\n",
    "REQUIRED_APIS = [\n",
    "    \"dataflow.googleapis.com\",\n",
    "    \"bigquery.googleapis.com\",\n",
    "    \"pubsub.googleapis.com\",\n",
    "    \"storage.googleapis.com\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "run_setup_header",
   "metadata": {},
   "source": [
    "### Run Setup\n",
    "\n",
    "This cell downloads the centralized setup code and configures your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run_setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, urllib.request\n",
    "\n",
    "# Download and import setup code\n",
    "url = 'https://raw.githubusercontent.com/statmike/vertex-ai-mlops/refs/heads/main/core/notebook-template/python_setup.py'\n",
    "urllib.request.urlretrieve(url, 'python_setup_local.py')\n",
    "import python_setup_local as python_setup\n",
    "os.remove('python_setup_local.py')\n",
    "\n",
    "# Run setup\n",
    "setup_info = python_setup.setup_environment(PROJECT_ID, REQ_TYPE, REQUIREMENTS_URL, REQUIRED_APIS, INSTALL_TOOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "python_setup",
   "metadata": {},
   "source": [
    "---\n",
    "## Python Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imports_header",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import json\n",
    "import os\n",
    "import zipfile\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import storage\n",
    "from google.cloud import pubsub_v1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vars_user",
   "metadata": {},
   "source": [
    "### Variables - User Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vars_user_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION = 'us-central1'\n",
    "SERIES = 'frameworks'\n",
    "EXPERIMENT = 'pytorch-autoencoder'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vars_auto",
   "metadata": {},
   "source": [
    "### Variables - Auto Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vars_auto_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = subprocess.run(['gcloud', 'config', 'get-value', 'project'], capture_output=True, text=True, check=True).stdout.strip()\n",
    "PROJECT_NUMBER = subprocess.run(['gcloud', 'projects', 'describe', PROJECT_ID, '--format=value(projectNumber)'], capture_output=True, text=True, check=True).stdout.strip()\n",
    "\n",
    "print(f\"\\n{'='*50}\\nGoogle Cloud Project Information\\n{'='*50}\\nPROJECT_ID     = {PROJECT_ID}\\nPROJECT_NUMBER = {PROJECT_NUMBER}\\n{'='*50}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "configs",
   "metadata": {},
   "source": [
    "### Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "configs_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GCS configuration\n",
    "BUCKET_NAME = PROJECT_ID\n",
    "BUCKET_URI = f\"gs://{BUCKET_NAME}\"\n",
    "\n",
    "# Model paths\n",
    "MODEL_DIR = f\"{BUCKET_URI}/{SERIES}/{EXPERIMENT}\"\n",
    "MAR_PATH = f\"{MODEL_DIR}/model.mar\"\n",
    "PT_PATH = f\"{MODEL_DIR}/final_model_traced.pt\"\n",
    "\n",
    "# BigQuery configuration\n",
    "BQ_DATASET = SERIES.replace('-', '_')\n",
    "BQ_TABLE_BATCH = f\"{EXPERIMENT.replace('-', '_')}_batch_results\"\n",
    "BQ_TABLE_STREAMING = f\"{EXPERIMENT.replace('-', '_')}_streaming_results\"\n",
    "\n",
    "# Pub/Sub configuration\n",
    "PUBSUB_TOPIC_INPUT = f\"{EXPERIMENT}-input\"\n",
    "PUBSUB_TOPIC_OUTPUT = f\"{EXPERIMENT}-output\"\n",
    "PUBSUB_SUB_INPUT = f\"{EXPERIMENT}-input-sub\"\n",
    "PUBSUB_SUB_OUTPUT = f\"{EXPERIMENT}-output-sub\"\n",
    "\n",
    "# Dataflow configuration\n",
    "DATAFLOW_STAGING = f\"{BUCKET_URI}/dataflow/staging\"\n",
    "DATAFLOW_TEMP = f\"{BUCKET_URI}/dataflow/temp\"\n",
    "\n",
    "print(f\"Bucket: {BUCKET_URI}\")\n",
    "print(f\"Model directory: {MODEL_DIR}\")\n",
    "print(f\"MAR file: {MAR_PATH}\")\n",
    "print(f\"PT model will be at: {PT_PATH}\")\n",
    "print(f\"\\nBigQuery dataset: {BQ_DATASET}\")\n",
    "print(f\"Batch results table: {BQ_TABLE_BATCH}\")\n",
    "print(f\"Streaming results table: {BQ_TABLE_STREAMING}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "init_clients",
   "metadata": {},
   "source": [
    "### Initialize Clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "init_clients_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize clients\n",
    "storage_client = storage.Client(project=PROJECT_ID)\n",
    "bq_client = bigquery.Client(project=PROJECT_ID)\n",
    "pubsub_publisher = pubsub_v1.PublisherClient()\n",
    "pubsub_subscriber = pubsub_v1.SubscriberClient()\n",
    "\n",
    "# Get GCS bucket\n",
    "bucket = storage_client.bucket(BUCKET_NAME)\n",
    "\n",
    "print(f\"✅ Clients initialized\")\n",
    "print(f\"   Project: {PROJECT_ID}\")\n",
    "print(f\"   Region: {REGION}\")\n",
    "print(f\"   Bucket: {BUCKET_URI}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model_prep",
   "metadata": {},
   "source": [
    "---\n",
    "## Prepare Model for RunInference\n",
    "\n",
    "Extract the TorchScript `.pt` file from the `.mar` archive for use with Dataflow RunInference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "check_mar",
   "metadata": {},
   "source": [
    "### Check for .mar File in GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "check_mar_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if .mar file exists in GCS\n",
    "mar_blob = bucket.blob(f\"{SERIES}/{EXPERIMENT}/model.mar\")\n",
    "\n",
    "if mar_blob.exists():\n",
    "    print(f\"✅ Found .mar file in GCS: {MAR_PATH}\")\n",
    "    print(f\"   Size: {mar_blob.size:,} bytes ({mar_blob.size / 1024:.2f} KB)\")\n",
    "else:\n",
    "    print(f\"❌ .mar file not found in GCS: {MAR_PATH}\")\n",
    "    print(f\"\\\\nPlease run one of these notebooks first to upload the model:\")\n",
    "    print(f\"  - vertex-ai-endpoint-prebuilt-container.ipynb\")\n",
    "    print(f\"  - vertex-ai-endpoint-custom-container.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extract_pt",
   "metadata": {},
   "source": [
    "### Extract .pt File from .mar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kd12t7hzlsa",
   "metadata": {},
   "source": [
    "### Upload .pt File to GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fo7jxbpc1n",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload .pt file to GCS for RunInference\n",
    "pt_blob = bucket.blob(f\"{SERIES}/{EXPERIMENT}/final_model_traced.pt\")\n",
    "pt_blob.upload_from_filename(\"final_model_traced.pt\")\n",
    "\n",
    "print(f\"✅ Uploaded .pt file to GCS: {PT_PATH}\")\n",
    "print(f\"   Size: {pt_blob.size:,} bytes ({pt_blob.size / 1024:.2f} KB)\")\n",
    "\n",
    "# Clean up local files\n",
    "os.remove(\"model.mar\")\n",
    "os.remove(\"final_model_traced.pt\")\n",
    "print(f\"\\n✅ Cleaned up local files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extract_pt_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download .mar file from GCS\n",
    "print(f\"Downloading {MAR_PATH}...\")\n",
    "mar_blob.download_to_filename(\"model.mar\")\n",
    "print(f\"✅ Downloaded model.mar\")\n",
    "\n",
    "# Extract the .pt file from the .mar archive\n",
    "with zipfile.ZipFile(\"model.mar\", 'r') as zip_ref:\n",
    "    # List contents\n",
    "    print(\"\\nContents of .mar file:\")\n",
    "    for name in zip_ref.namelist():\n",
    "        print(f\"  - {name}\")\n",
    "    \n",
    "    # Extract the traced model\n",
    "    zip_ref.extract('final_model_traced.pt', '.')\n",
    "\n",
    "print(f\"\\n✅ Extracted final_model_traced.pt from model.mar\")\n",
    "\n",
    "# Check extracted file size\n",
    "pt_size = os.path.getsize(\"final_model_traced.pt\")\n",
    "print(f\"   Extracted model size: {pt_size:,} bytes ({pt_size / 1024:.2f} KB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bigquery_setup",
   "metadata": {},
   "source": [
    "---\n",
    "## Create BigQuery Result Tables\n",
    "\n",
    "Create tables to store results from batch and streaming Dataflow jobs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "check_dataset",
   "metadata": {},
   "source": [
    "### Check for Existing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "check_dataset_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if dataset exists\n",
    "dataset_id = f\"{PROJECT_ID}.{BQ_DATASET}\"\n",
    "\n",
    "try:\n",
    "    dataset = bq_client.get_dataset(dataset_id)\n",
    "    print(f\"✅ Dataset already exists: {dataset_id}\")\n",
    "except:\n",
    "    # Create dataset\n",
    "    dataset = bigquery.Dataset(dataset_id)\n",
    "    dataset.location = REGION\n",
    "    dataset = bq_client.create_dataset(dataset)\n",
    "    print(f\"✅ Created dataset: {dataset_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "create_batch_table",
   "metadata": {},
   "source": [
    "### Create Batch Results Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create_batch_table_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define schema for batch results\n",
    "batch_schema = [\n",
    "    bigquery.SchemaField(\"instance_id\", \"STRING\"),\n",
    "    bigquery.SchemaField(\"anomaly_score\", \"FLOAT64\"),\n",
    "    bigquery.SchemaField(\"encoded\", \"FLOAT64\", mode=\"REPEATED\"),\n",
    "    bigquery.SchemaField(\"timestamp\", \"TIMESTAMP\"),\n",
    "]\n",
    "\n",
    "# Create or get table\n",
    "batch_table_id = f\"{dataset_id}.{BQ_TABLE_BATCH}\"\n",
    "\n",
    "try:\n",
    "    table = bq_client.get_table(batch_table_id)\n",
    "    print(f\"✅ Batch results table already exists: {batch_table_id}\")\n",
    "except:\n",
    "    table = bigquery.Table(batch_table_id, schema=batch_schema)\n",
    "    table = bq_client.create_table(table)\n",
    "    print(f\"✅ Created batch results table: {batch_table_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "create_streaming_table",
   "metadata": {},
   "source": [
    "### Create Streaming Results Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create_streaming_table_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define schema for streaming results (same as batch)\n",
    "streaming_schema = [\n",
    "    bigquery.SchemaField(\"instance_id\", \"STRING\"),\n",
    "    bigquery.SchemaField(\"anomaly_score\", \"FLOAT64\"),\n",
    "    bigquery.SchemaField(\"encoded\", \"FLOAT64\", mode=\"REPEATED\"),\n",
    "    bigquery.SchemaField(\"timestamp\", \"TIMESTAMP\"),\n",
    "    bigquery.SchemaField(\"window_start\", \"TIMESTAMP\"),\n",
    "    bigquery.SchemaField(\"window_end\", \"TIMESTAMP\"),\n",
    "]\n",
    "\n",
    "# Create or get table\n",
    "streaming_table_id = f\"{dataset_id}.{BQ_TABLE_STREAMING}\"\n",
    "\n",
    "try:\n",
    "    table = bq_client.get_table(streaming_table_id)\n",
    "    print(f\"✅ Streaming results table already exists: {streaming_table_id}\")\n",
    "except:\n",
    "    table = bigquery.Table(streaming_table_id, schema=streaming_schema)\n",
    "    table = bq_client.create_table(table)\n",
    "    print(f\"✅ Created streaming results table: {streaming_table_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pubsub_setup",
   "metadata": {},
   "source": [
    "---\n",
    "## Create Pub/Sub Topics and Subscriptions\n",
    "\n",
    "Create Pub/Sub infrastructure for streaming inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "create_input_topic",
   "metadata": {},
   "source": [
    "### Create Input Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create_input_topic_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create input topic\n",
    "input_topic_path = pubsub_publisher.topic_path(PROJECT_ID, PUBSUB_TOPIC_INPUT)\n",
    "\n",
    "try:\n",
    "    topic = pubsub_publisher.get_topic(request={\"topic\": input_topic_path})\n",
    "    print(f\"✅ Input topic already exists: {input_topic_path}\")\n",
    "except:\n",
    "    topic = pubsub_publisher.create_topic(request={\"name\": input_topic_path})\n",
    "    print(f\"✅ Created input topic: {input_topic_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "create_input_sub",
   "metadata": {},
   "source": [
    "### Create Input Subscription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create_input_sub_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subscription for input topic\n",
    "input_sub_path = pubsub_subscriber.subscription_path(PROJECT_ID, PUBSUB_SUB_INPUT)\n",
    "\n",
    "try:\n",
    "    subscription = pubsub_subscriber.get_subscription(request={\"subscription\": input_sub_path})\n",
    "    print(f\"✅ Input subscription already exists: {input_sub_path}\")\n",
    "except:\n",
    "    subscription = pubsub_subscriber.create_subscription(\n",
    "        request={\n",
    "            \"name\": input_sub_path,\n",
    "            \"topic\": input_topic_path,\n",
    "            \"ack_deadline_seconds\": 60\n",
    "        }\n",
    "    )\n",
    "    print(f\"✅ Created input subscription: {input_sub_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "create_output_topic",
   "metadata": {},
   "source": [
    "### Create Output Topic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nsvbmh4w0yj",
   "metadata": {},
   "source": [
    "### Create Output Subscription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ut8ta9p2kjr",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subscription for output topic\n",
    "output_sub_path = pubsub_subscriber.subscription_path(PROJECT_ID, PUBSUB_SUB_OUTPUT)\n",
    "\n",
    "try:\n",
    "    subscription = pubsub_subscriber.get_subscription(request={\"subscription\": output_sub_path})\n",
    "    print(f\"✅ Output subscription already exists: {output_sub_path}\")\n",
    "except:\n",
    "    subscription = pubsub_subscriber.create_subscription(\n",
    "        request={\n",
    "            \"name\": output_sub_path,\n",
    "            \"topic\": output_topic_path,\n",
    "            \"ack_deadline_seconds\": 60\n",
    "        }\n",
    "    )\n",
    "    print(f\"✅ Created output subscription: {output_sub_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create_output_topic_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output topic\n",
    "output_topic_path = pubsub_publisher.topic_path(PROJECT_ID, PUBSUB_TOPIC_OUTPUT)\n",
    "\n",
    "try:\n",
    "    topic = pubsub_publisher.get_topic(request={\"topic\": output_topic_path})\n",
    "    print(f\"✅ Output topic already exists: {output_topic_path}\")\n",
    "except:\n",
    "    topic = pubsub_publisher.create_topic(request={\"name\": output_topic_path})\n",
    "    print(f\"✅ Created output topic: {output_topic_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verify",
   "metadata": {},
   "source": [
    "---\n",
    "## Verify Setup\n",
    "\n",
    "Check that all resources are ready for Dataflow jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verify_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"DATAFLOW SETUP VERIFICATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n✅ Model Files:\")\n",
    "print(f\"   .mar file: {MAR_PATH}\")\n",
    "print(f\"   .pt file:  {PT_PATH}\")\n",
    "\n",
    "print(\"\\n✅ BigQuery Tables:\")\n",
    "print(f\"   Dataset: {dataset_id}\")\n",
    "print(f\"   Batch results: {batch_table_id}\")\n",
    "print(f\"   Streaming results: {streaming_table_id}\")\n",
    "\n",
    "print(\"\\n✅ Pub/Sub Resources:\")\n",
    "print(f\"   Input topic: {input_topic_path}\")\n",
    "print(f\"   Input subscription: {input_sub_path}\")\n",
    "print(f\"   Output topic: {output_topic_path}\")\n",
    "print(f\"   Output subscription: {output_sub_path}\")\n",
    "\n",
    "print(\"\\n✅ Dataflow Locations:\")\n",
    "print(f\"   Staging: {DATAFLOW_STAGING}\")\n",
    "print(f\"   Temp: {DATAFLOW_TEMP}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"READY FOR DATAFLOW JOBS!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "In this notebook, you:\n",
    "\n",
    "✅ Extracted `.pt` file from `.mar` for RunInference\n",
    "\n",
    "✅ Created BigQuery tables for batch and streaming results\n",
    "\n",
    "✅ Created Pub/Sub topics and subscriptions\n",
    "\n",
    "✅ Verified all resources are ready\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "You are now ready to run Dataflow inference jobs:\n",
    "\n",
    "### 1. Batch Inference\n",
    "Run [dataflow-batch-runinference.ipynb](./dataflow-batch-runinference.ipynb) to:\n",
    "- Read transactions from BigQuery\n",
    "- Apply RunInference with the PyTorch model\n",
    "- Write anomaly scores back to BigQuery\n",
    "- Analyze results\n",
    "\n",
    "### 2. Streaming Inference\n",
    "Run [dataflow-streaming-runinference.ipynb](./dataflow-streaming-runinference.ipynb) to:\n",
    "- Read from Pub/Sub input topic\n",
    "- Apply RunInference in real-time\n",
    "- Write to Pub/Sub output topic and BigQuery\n",
    "- Monitor streaming job\n",
    "\n",
    "### 3. Vertex AI Endpoint Integration\n",
    "Run [dataflow-vertex-endpoint.ipynb](./dataflow-vertex-endpoint.ipynb) to:\n",
    "- Call Vertex AI Endpoint from Dataflow\n",
    "- Compare performance vs RunInference\n",
    "- Understand trade-offs\n",
    "\n",
    "## Resources\n",
    "\n",
    "- [Dataflow Documentation](https://cloud.google.com/dataflow/docs)\n",
    "- [Apache Beam RunInference](https://beam.apache.org/documentation/ml/about-ml/)\n",
    "- [PyTorch RunInference](https://beam.apache.org/documentation/ml/pytorch-inference/)\n",
    "- [Pub/Sub Documentation](https://cloud.google.com/pubsub/docs)\n",
    "- [BigQuery Documentation](https://cloud.google.com/bigquery/docs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (frameworks-pytorch)",
   "language": "python",
   "name": "frameworks-pytorch"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
