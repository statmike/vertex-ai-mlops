{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07331bfc",
   "metadata": {},
   "source": [
    "![tracker](https://us-central1-vertex-ai-mlops-369716.cloudfunctions.net/pixel-tracking?path=statmike%2Fvertex-ai-mlops%2FFramework+Workflows%2FPyTorch%2Fserving&file=dataflow-setup.ipynb)\n",
    "<!--- header table --->\n",
    "<table align=\"left\">\n",
    "<tr>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%20Workflows/PyTorch/serving/dataflow-setup.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.svgrepo.com/download/217753/github.svg\" alt=\"GitHub logo\">\n",
    "      <br>View on<br>GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://colab.research.google.com/github/statmike/vertex-ai-mlops/blob/main/Framework%20Workflows/PyTorch/serving/dataflow-setup.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.gstatic.com/pantheon/images/bigquery/welcome_page/colab-logo.svg\" alt=\"Google Colaboratory logo\">\n",
    "      <br>Run in<br>Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https%3A%2F%2Fraw.githubusercontent.com%2Fstatmike%2Fvertex-ai-mlops%2Fmain%2FFramework%2520Workflows%2FPyTorch%2Fserving%2Fdataflow-setup.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\">\n",
    "      <br>Run in<br>Colab Enterprise\n",
    "    </a>\n",
    "  </td>      \n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/bigquery/import?url=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%20Workflows/PyTorch/serving/dataflow-setup.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.gstatic.com/images/branding/gcpiconscolors/bigquery/v1/32px.svg\" alt=\"BigQuery logo\">\n",
    "      <br>Open in<br>BigQuery Studio\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/statmike/vertex-ai-mlops/main/Framework%20Workflows/PyTorch/serving/dataflow-setup.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.gstatic.com/images/branding/gcpiconscolors/vertexai/v1/32px.svg\" alt=\"Vertex AI logo\">\n",
    "      <br>Open in<br>Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td colspan=\"5\" style=\"text-align: right\">\n",
    "    <b>Share This On: </b> \n",
    "    <a href=\"https://www.linkedin.com/sharing/share-offsite/?url=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%2520Workflows/PyTorch/serving/dataflow-setup.ipynb\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"Linkedin Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://reddit.com/submit?url=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%2520Workflows/PyTorch/serving/dataflow-setup.ipynb\"><img src=\"https://redditinc.com/hubfs/Reddit%20Inc/Brand/Reddit_Logo.png\" alt=\"Reddit Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://bsky.app/intent/compose?text=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%2520Workflows/PyTorch/serving/dataflow-setup.ipynb\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"BlueSky Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://twitter.com/intent/tweet?url=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%2520Workflows/PyTorch/serving/dataflow-setup.ipynb\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/5/5a/X_icon_2.svg\" alt=\"X (Twitter) Logo\" width=\"20px\"></a> \n",
    "  </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td colspan=\"5\" style=\"text-align: right\">\n",
    "    <b>Connect With Author On: </b> \n",
    "    <a href=\"https://www.linkedin.com/in/statmike\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"Linkedin Logo\" width=\"20px\"></a>\n",
    "    <a href=\"https://www.github.com/statmike\"><img src=\"https://www.svgrepo.com/download/217753/github.svg\" alt=\"GitHub Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://www.youtube.com/@statmike-channel\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/f/fd/YouTube_full-color_icon_%282024%29.svg\" alt=\"YouTube Logo\" width=\"20px\"></a>\n",
    "    <a href=\"https://bsky.app/profile/statmike.bsky.social\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"BlueSky Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://x.com/statmike\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/5/5a/X_icon_2.svg\" alt=\"X (Twitter) Logo\" width=\"20px\"></a>\n",
    "  </td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overview",
   "metadata": {},
   "source": [
    "# Dataflow Infrastructure Setup\n",
    "\n",
    "This notebook performs one-time setup for all Dataflow inference workflows.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "This workflow covers:\n",
    "\n",
    "1. **Upload Model Archive**: Upload `.mar` file from training to GCS (if not already there)\n",
    "2. **Extract .pt File**: Extract the traced `.pt` model from `.mar` for RunInference\n",
    "3. **Create BigQuery Tables**: Set up tables for batch and streaming results\n",
    "4. **Create Pub/Sub Topics**: Set up topics and subscriptions for streaming\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- **Completed** `../pytorch-autoencoder.ipynb` (created the `.mar` file)\n",
    "  - Local `.mar` file: `../files/pytorch-autoencoder/pytorch_autoencoder.mar`\n",
    "- Google Cloud project with required APIs enabled\n",
    "\n",
    "## File Organization\n",
    "\n",
    "This notebook uses isolated workspace to avoid conflicts with other serving workflows:\n",
    "\n",
    "**Local Workspace:**\n",
    "```\n",
    "./files/dataflow/\n",
    "├── pytorch_autoencoder.mar  # Downloaded temporarily for extraction\n",
    "└── final_model_traced.pt    # Extracted, then uploaded to GCS\n",
    "```\n",
    "\n",
    "**GCS Structure:**\n",
    "```\n",
    "gs://{PROJECT_ID}/frameworks/pytorch-autoencoder/dataflow/\n",
    "├── pytorch_autoencoder.mar   # Model archive\n",
    "└── final_model_traced.pt     # Traced model for RunInference\n",
    "```\n",
    "\n",
    "## Why This Setup?\n",
    "\n",
    "**RunInference vs TorchServe:**\n",
    "- **TorchServe** (Vertex Endpoints, Cloud Run, GCE, GKE) uses `.mar` files with custom handlers\n",
    "- **RunInference** (Dataflow) loads PyTorch models directly from `.pt` files\n",
    "- We extract the `.pt` from the `.mar` to use with RunInference\n",
    "\n",
    "**One-Time Setup:**\n",
    "- BigQuery tables persist across multiple pipeline runs\n",
    "- Pub/Sub topics can be reused for streaming\n",
    "- Model files stay in GCS for all Dataflow jobs\n",
    "\n",
    "## Architecture\n",
    "\n",
    "```\n",
    "Local .mar file → Upload to GCS → Extract .pt → Upload .pt to GCS\n",
    "                                                       ↓\n",
    "                                      Used by all Dataflow pipelines\n",
    "                                                       ↓\n",
    "                                     ┌─────────────────┴──────────────────┐\n",
    "                                     ↓                                    ↓\n",
    "                             Batch RunInference              Streaming RunInference\n",
    "                                     ↓                                    ↓\n",
    "                               BigQuery Results                  Pub/Sub + BigQuery\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "env_setup",
   "metadata": {},
   "source": [
    "---\n",
    "## Environment Setup\n",
    "\n",
    "This section will authenticate your session, enable required Google Cloud APIs, and install necessary Python packages.\n",
    "\n",
    "**Package Installation Options (`REQ_TYPE`):**\n",
    "- `PRIMARY`: Installs only the main packages\n",
    "- `ALL` (Default): Installs exact versions of all packages and dependencies\n",
    "- `COLAB`: Installs a Colab-optimized list\n",
    "\n",
    "**Installation Tool Options (`INSTALL_TOOL`):**\n",
    "- `pip` (Default): Standard Python package installer\n",
    "- `uv`: Modern, fast Python package installer\n",
    "- `poetry`: Dependency management tool\n",
    "\n",
    "> **Note:** If running in Google Colab, the script will automatically set `REQ_TYPE = 'COLAB' to prevent package conflicts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "set_project",
   "metadata": {},
   "source": [
    "### Set Your Project ID\n",
    "\n",
    "⚠️ **Action Required:** Replace the `PROJECT_ID` value below with your Google Cloud project ID before running this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "project_config",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = 'statmike-mlops-349915' # replace with GCP project ID\n",
    "REQ_TYPE = 'ALL' # Specify PRIMARY or ALL or COLAB\n",
    "INSTALL_TOOL = 'poetry' # Specify pip, uv, or poetry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config_header",
   "metadata": {},
   "source": [
    "### Configuration\n",
    "\n",
    "This cell defines the requirements files and Google Cloud APIs needed for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": [
    "REQUIREMENTS_URL = 'https://raw.githubusercontent.com/statmike/vertex-ai-mlops/refs/heads/main/Framework%20Workflows/PyTorch/requirements.txt'\n",
    "\n",
    "REQUIRED_APIS = [\n",
    "    \"dataflow.googleapis.com\",\n",
    "    \"bigquery.googleapis.com\",\n",
    "    \"pubsub.googleapis.com\",\n",
    "    \"storage.googleapis.com\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "run_setup_header",
   "metadata": {},
   "source": [
    "### Run Setup\n",
    "\n",
    "This cell downloads the centralized setup code and configures your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "run_setup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "PYTHON GCP ENVIRONMENT SETUP\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "AUTHENTICATION\n",
      "==================================================\n",
      "Checking for existing ADC...\n",
      "✅ Existing ADC found.\n",
      "✅ Project is correctly set to 'statmike-mlops-349915'.\n",
      "\n",
      "==================================================\n",
      "API CHECK & ENABLE\n",
      "==================================================\n",
      "✅ dataflow.googleapis.com is already enabled.\n",
      "✅ bigquery.googleapis.com is already enabled.\n",
      "✅ pubsub.googleapis.com is already enabled.\n",
      "✅ storage.googleapis.com is already enabled.\n",
      "\n",
      "==================================================\n",
      "PACKAGE MANAGEMENT\n",
      "==================================================\n",
      "Installation Tool: poetry\n",
      "✅ Found poetry at: /usr/local/google/home/statmike/.local/bin/poetry\n",
      "✅ Running in poetry environment: /usr/local/google/home/statmike/.cache/pypoetry/virtualenvs/frameworks-pytorch-0KVJlKeQ-py3.13\n",
      "ℹ️  Poetry mode: Installing from pyproject.toml (REQUIREMENTS_URL ignored)\n",
      "✅ Found pyproject.toml at: /usr/local/google/home/statmike/Git/vertex-ai-mlops/Framework Workflows/PyTorch/pyproject.toml\n",
      "   Changed working directory to: /usr/local/google/home/statmike/Git/vertex-ai-mlops/Framework Workflows/PyTorch\n",
      "Running poetry install...\n",
      "   Restored working directory to: /usr/local/google/home/statmike/Git/vertex-ai-mlops/Framework Workflows/PyTorch/serving\n",
      "✅ All packages are already installed and up to date.\n",
      "\n",
      "==================================================\n",
      "Google Cloud Project Information\n",
      "==================================================\n",
      "PROJECT_ID     = statmike-mlops-349915\n",
      "PROJECT_NUMBER = 1026793852137\n",
      "==================================================\n",
      "\n",
      "\n",
      "==================================================\n",
      "SETUP SUMMARY\n",
      "==================================================\n",
      "✅ Authentication:    Success\n",
      "✅ API Configuration: Success\n",
      "✅ Package Install:   Already up to date\n",
      "✅ Installation Tool: poetry\n",
      "✅ Project ID:        statmike-mlops-349915\n",
      "✅ Project Number:    1026793852137\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os, urllib.request\n",
    "\n",
    "# Download and import setup code\n",
    "url = 'https://raw.githubusercontent.com/statmike/vertex-ai-mlops/refs/heads/main/core/notebook-template/python_setup.py'\n",
    "urllib.request.urlretrieve(url, 'python_setup_local.py')\n",
    "import python_setup_local as python_setup\n",
    "os.remove('python_setup_local.py')\n",
    "\n",
    "# Run setup\n",
    "setup_info = python_setup.setup_environment(PROJECT_ID, REQ_TYPE, REQUIREMENTS_URL, REQUIRED_APIS, INSTALL_TOOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "python_setup",
   "metadata": {},
   "source": [
    "---\n",
    "## Python Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imports_header",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import json\n",
    "import os\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import storage\n",
    "from google.cloud import pubsub_v1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vars_user",
   "metadata": {},
   "source": [
    "### Variables - User Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "vars_user_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION = 'us-central1'\n",
    "SERIES = 'frameworks'\n",
    "EXPERIMENT = 'pytorch-autoencoder'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vars_auto",
   "metadata": {},
   "source": [
    "### Variables - Auto Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "vars_auto_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Google Cloud Project Information\n",
      "==================================================\n",
      "PROJECT_ID     = statmike-mlops-349915\n",
      "PROJECT_NUMBER = 1026793852137\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "PROJECT_ID = subprocess.run(['gcloud', 'config', 'get-value', 'project'], capture_output=True, text=True, check=True).stdout.strip()\n",
    "PROJECT_NUMBER = subprocess.run(['gcloud', 'projects', 'describe', PROJECT_ID, '--format=value(projectNumber)'], capture_output=True, text=True, check=True).stdout.strip()\n",
    "\n",
    "print(f\"\\n{'='*50}\\nGoogle Cloud Project Information\\n{'='*50}\\nPROJECT_ID     = {PROJECT_ID}\\nPROJECT_NUMBER = {PROJECT_NUMBER}\\n{'='*50}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "configs",
   "metadata": {},
   "source": [
    "### Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "configs_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket: gs://statmike-mlops-349915\n",
      "\n",
      "Local workspace:\n",
      "  Working directory: ./files/dataflow\n",
      "  Source .mar file: ../files/pytorch-autoencoder/pytorch_autoencoder.mar\n",
      "\n",
      "GCS dataflow directory: gs://statmike-mlops-349915/frameworks/pytorch-autoencoder/dataflow\n",
      "  .mar file will be at: gs://statmike-mlops-349915/frameworks/pytorch-autoencoder/dataflow/pytorch_autoencoder.mar\n",
      "  .pt file will be at: gs://statmike-mlops-349915/frameworks/pytorch-autoencoder/dataflow/final_model_traced.pt\n",
      "\n",
      "BigQuery dataset: frameworks\n",
      "  Batch results table: pytorch_autoencoder_batch_results\n",
      "  Streaming results table: pytorch_autoencoder_streaming_results\n"
     ]
    }
   ],
   "source": [
    "# GCS configuration\n",
    "BUCKET_NAME = PROJECT_ID\n",
    "BUCKET_URI = f\"gs://{BUCKET_NAME}\"\n",
    "\n",
    "# Local paths\n",
    "LOCAL_MAR_PATH = \"../files/pytorch-autoencoder/pytorch_autoencoder.mar\"\n",
    "WORK_DIR = \"./files/dataflow\"\n",
    "\n",
    "# GCS paths (isolated for dataflow)\n",
    "GCS_DATAFLOW_DIR = f\"{BUCKET_URI}/{SERIES}/{EXPERIMENT}/dataflow\"\n",
    "GCS_MAR_PATH = f\"{GCS_DATAFLOW_DIR}/pytorch_autoencoder.mar\"\n",
    "GCS_PT_PATH = f\"{GCS_DATAFLOW_DIR}/final_model_traced.pt\"\n",
    "\n",
    "# BigQuery configuration\n",
    "BQ_DATASET = SERIES.replace('-', '_')\n",
    "BQ_TABLE_BATCH = f\"{EXPERIMENT.replace('-', '_')}_batch_results\"\n",
    "BQ_TABLE_STREAMING = f\"{EXPERIMENT.replace('-', '_')}_streaming_results\"\n",
    "\n",
    "# Pub/Sub configuration\n",
    "PUBSUB_TOPIC_INPUT = f\"{EXPERIMENT}-input\"\n",
    "PUBSUB_TOPIC_OUTPUT = f\"{EXPERIMENT}-output\"\n",
    "PUBSUB_SUB_INPUT = f\"{EXPERIMENT}-input-sub\"\n",
    "PUBSUB_SUB_OUTPUT = f\"{EXPERIMENT}-output-sub\"\n",
    "\n",
    "# Dataflow configuration\n",
    "DATAFLOW_STAGING = f\"{BUCKET_URI}/dataflow/staging\"\n",
    "DATAFLOW_TEMP = f\"{BUCKET_URI}/dataflow/temp\"\n",
    "\n",
    "print(f\"Bucket: {BUCKET_URI}\")\n",
    "print(f\"\\nLocal workspace:\")\n",
    "print(f\"  Working directory: {WORK_DIR}\")\n",
    "print(f\"  Source .mar file: {LOCAL_MAR_PATH}\")\n",
    "print(f\"\\nGCS dataflow directory: {GCS_DATAFLOW_DIR}\")\n",
    "print(f\"  .mar file will be at: {GCS_MAR_PATH}\")\n",
    "print(f\"  .pt file will be at: {GCS_PT_PATH}\")\n",
    "print(f\"\\nBigQuery dataset: {BQ_DATASET}\")\n",
    "print(f\"  Batch results table: {BQ_TABLE_BATCH}\")\n",
    "print(f\"  Streaming results table: {BQ_TABLE_STREAMING}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "init_clients",
   "metadata": {},
   "source": [
    "### Initialize Clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "init_clients_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Local workspace: ./files/dataflow\n",
      "✅ Clients initialized\n",
      "   Project: statmike-mlops-349915\n",
      "   Region: us-central1\n",
      "   Bucket: gs://statmike-mlops-349915\n"
     ]
    }
   ],
   "source": [
    "# Create local working directory\n",
    "os.makedirs(WORK_DIR, exist_ok=True)\n",
    "print(f\"✅ Local workspace: {WORK_DIR}\")\n",
    "\n",
    "# Initialize clients\n",
    "storage_client = storage.Client(project=PROJECT_ID)\n",
    "bq_client = bigquery.Client(project=PROJECT_ID)\n",
    "pubsub_publisher = pubsub_v1.PublisherClient()\n",
    "pubsub_subscriber = pubsub_v1.SubscriberClient()\n",
    "\n",
    "# Get GCS bucket\n",
    "bucket = storage_client.bucket(BUCKET_NAME)\n",
    "\n",
    "print(f\"✅ Clients initialized\")\n",
    "print(f\"   Project: {PROJECT_ID}\")\n",
    "print(f\"   Region: {REGION}\")\n",
    "print(f\"   Bucket: {BUCKET_URI}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model_prep",
   "metadata": {},
   "source": [
    "---\n",
    "## Upload and Prepare Model for RunInference\n",
    "\n",
    "Upload the model archive to GCS and extract the TorchScript `.pt` file for use with Dataflow RunInference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "check_mar",
   "metadata": {},
   "source": [
    "### Upload .mar File to GCS\n",
    "\n",
    "Upload the model archive from the training notebook to GCS for Dataflow workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "check_mar_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Found local .mar file: ../files/pytorch-autoencoder/pytorch_autoencoder.mar\n",
      "   Size: 29,851 bytes (29.15 KB)\n",
      "\n",
      "Uploading to GCS: gs://statmike-mlops-349915/frameworks/pytorch-autoencoder/dataflow/pytorch_autoencoder.mar\n",
      "✅ Upload complete!\n",
      "   Size: 29,851 bytes (29.15 KB)\n"
     ]
    }
   ],
   "source": [
    "# Check if local .mar file exists\n",
    "from pathlib import Path\n",
    "\n",
    "local_mar = Path(LOCAL_MAR_PATH)\n",
    "\n",
    "if not local_mar.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Model archive not found at {LOCAL_MAR_PATH}\\n\"\n",
    "        f\"Please run ../pytorch-autoencoder.ipynb first to create the model archive\"\n",
    "    )\n",
    "\n",
    "print(f\"✅ Found local .mar file: {LOCAL_MAR_PATH}\")\n",
    "print(f\"   Size: {local_mar.stat().st_size:,} bytes ({local_mar.stat().st_size / 1024:.2f} KB)\")\n",
    "\n",
    "# Check if already uploaded to GCS\n",
    "mar_blob = bucket.blob(f\"{SERIES}/{EXPERIMENT}/dataflow/pytorch_autoencoder.mar\")\n",
    "\n",
    "if mar_blob.exists():\n",
    "    print(f\"\\n✅ .mar file already exists in GCS: {GCS_MAR_PATH}\")\n",
    "    print(f\"   Size: {mar_blob.size:,} bytes ({mar_blob.size / 1024:.2f} KB)\")\n",
    "    print(f\"   Skipping upload...\")\n",
    "else:\n",
    "    # Upload to GCS\n",
    "    print(f\"\\nUploading to GCS: {GCS_MAR_PATH}\")\n",
    "    mar_blob.upload_from_filename(str(local_mar))\n",
    "    print(f\"✅ Upload complete!\")\n",
    "    print(f\"   Size: {mar_blob.size:,} bytes ({mar_blob.size / 1024:.2f} KB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extract_pt",
   "metadata": {},
   "source": [
    "### Extract .pt File from .mar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "extract_pt_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading gs://statmike-mlops-349915/frameworks/pytorch-autoencoder/dataflow/pytorch_autoencoder.mar...\n",
      "✅ Downloaded to ./files/dataflow/pytorch_autoencoder.mar\n",
      "\n",
      "Contents of .mar file:\n",
      "  - handler.py\n",
      "  - final_model_traced.pt\n",
      "  - MAR-INF/MANIFEST.json\n",
      "\n",
      "✅ Extracted final_model_traced.pt to ./files/dataflow\n",
      "   Extracted model size: 47,408 bytes (46.30 KB)\n"
     ]
    }
   ],
   "source": [
    "# Download .mar file from GCS to working directory\n",
    "local_mar_download = f\"{WORK_DIR}/pytorch_autoencoder.mar\"\n",
    "\n",
    "print(f\"Downloading {GCS_MAR_PATH}...\")\n",
    "mar_blob.download_to_filename(local_mar_download)\n",
    "print(f\"✅ Downloaded to {local_mar_download}\")\n",
    "\n",
    "# Extract the .pt file from the .mar archive\n",
    "with zipfile.ZipFile(local_mar_download, 'r') as zip_ref:\n",
    "    # List contents\n",
    "    print(\"\\nContents of .mar file:\")\n",
    "    for name in zip_ref.namelist():\n",
    "        print(f\"  - {name}\")\n",
    "    \n",
    "    # Extract the traced model to working directory\n",
    "    zip_ref.extract('final_model_traced.pt', WORK_DIR)\n",
    "\n",
    "local_pt_path = f\"{WORK_DIR}/final_model_traced.pt\"\n",
    "print(f\"\\n✅ Extracted final_model_traced.pt to {WORK_DIR}\")\n",
    "\n",
    "# Check extracted file size\n",
    "pt_size = os.path.getsize(local_pt_path)\n",
    "print(f\"   Extracted model size: {pt_size:,} bytes ({pt_size / 1024:.2f} KB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kd12t7hzlsa",
   "metadata": {},
   "source": [
    "### Upload .pt File to GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fo7jxbpc1n",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded .pt file to GCS: gs://statmike-mlops-349915/frameworks/pytorch-autoencoder/dataflow/final_model_traced.pt\n",
      "   Size: 47,408 bytes (46.30 KB)\n",
      "\n",
      "✅ Cleaned up local files in ./files/dataflow\n"
     ]
    }
   ],
   "source": [
    "# Upload .pt file to GCS for RunInference\n",
    "pt_blob = bucket.blob(f\"{SERIES}/{EXPERIMENT}/dataflow/final_model_traced.pt\")\n",
    "pt_blob.upload_from_filename(local_pt_path)\n",
    "\n",
    "print(f\"✅ Uploaded .pt file to GCS: {GCS_PT_PATH}\")\n",
    "print(f\"   Size: {pt_blob.size:,} bytes ({pt_blob.size / 1024:.2f} KB)\")\n",
    "\n",
    "# Clean up local files in working directory\n",
    "os.remove(local_mar_download)\n",
    "os.remove(local_pt_path)\n",
    "print(f\"\\n✅ Cleaned up local files in {WORK_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bigquery_setup",
   "metadata": {},
   "source": [
    "---\n",
    "## Create BigQuery Result Tables\n",
    "\n",
    "Create tables to store results from batch and streaming Dataflow jobs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "check_dataset",
   "metadata": {},
   "source": [
    "### Check for Existing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "check_dataset_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Dataset already exists: statmike-mlops-349915.frameworks\n"
     ]
    }
   ],
   "source": [
    "# Check if dataset exists\n",
    "dataset_id = f\"{PROJECT_ID}.{BQ_DATASET}\"\n",
    "\n",
    "try:\n",
    "    dataset = bq_client.get_dataset(dataset_id)\n",
    "    print(f\"✅ Dataset already exists: {dataset_id}\")\n",
    "except:\n",
    "    # Create dataset\n",
    "    dataset = bigquery.Dataset(dataset_id)\n",
    "    dataset.location = REGION\n",
    "    dataset = bq_client.create_dataset(dataset)\n",
    "    print(f\"✅ Created dataset: {dataset_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "create_batch_table",
   "metadata": {},
   "source": [
    "### Create Batch Results Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "create_batch_table_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Created batch results table: statmike-mlops-349915.frameworks.pytorch_autoencoder_batch_results\n"
     ]
    }
   ],
   "source": [
    "# Define schema for batch results\n",
    "batch_schema = [\n",
    "    bigquery.SchemaField(\"instance_id\", \"STRING\"),\n",
    "    bigquery.SchemaField(\"anomaly_score\", \"FLOAT64\"),\n",
    "    bigquery.SchemaField(\"encoded\", \"FLOAT64\", mode=\"REPEATED\"),\n",
    "    bigquery.SchemaField(\"timestamp\", \"TIMESTAMP\"),\n",
    "]\n",
    "\n",
    "# Create or get table\n",
    "batch_table_id = f\"{dataset_id}.{BQ_TABLE_BATCH}\"\n",
    "\n",
    "try:\n",
    "    table = bq_client.get_table(batch_table_id)\n",
    "    print(f\"✅ Batch results table already exists: {batch_table_id}\")\n",
    "except:\n",
    "    table = bigquery.Table(batch_table_id, schema=batch_schema)\n",
    "    table = bq_client.create_table(table)\n",
    "    print(f\"✅ Created batch results table: {batch_table_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "create_streaming_table",
   "metadata": {},
   "source": [
    "### Create Streaming Results Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "create_streaming_table_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Created streaming results table: statmike-mlops-349915.frameworks.pytorch_autoencoder_streaming_results\n"
     ]
    }
   ],
   "source": [
    "# Define schema for streaming results (same as batch)\n",
    "streaming_schema = [\n",
    "    bigquery.SchemaField(\"instance_id\", \"STRING\"),\n",
    "    bigquery.SchemaField(\"anomaly_score\", \"FLOAT64\"),\n",
    "    bigquery.SchemaField(\"encoded\", \"FLOAT64\", mode=\"REPEATED\"),\n",
    "    bigquery.SchemaField(\"timestamp\", \"TIMESTAMP\"),\n",
    "    bigquery.SchemaField(\"window_start\", \"TIMESTAMP\"),\n",
    "    bigquery.SchemaField(\"window_end\", \"TIMESTAMP\"),\n",
    "]\n",
    "\n",
    "# Create or get table\n",
    "streaming_table_id = f\"{dataset_id}.{BQ_TABLE_STREAMING}\"\n",
    "\n",
    "try:\n",
    "    table = bq_client.get_table(streaming_table_id)\n",
    "    print(f\"✅ Streaming results table already exists: {streaming_table_id}\")\n",
    "except:\n",
    "    table = bigquery.Table(streaming_table_id, schema=streaming_schema)\n",
    "    table = bq_client.create_table(table)\n",
    "    print(f\"✅ Created streaming results table: {streaming_table_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pubsub_setup",
   "metadata": {},
   "source": [
    "---\n",
    "## Create Pub/Sub Topics and Subscriptions\n",
    "\n",
    "Create Pub/Sub infrastructure for streaming inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "create_input_topic",
   "metadata": {},
   "source": [
    "### Create Input Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "create_input_topic_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Created input topic: projects/statmike-mlops-349915/topics/pytorch-autoencoder-input\n"
     ]
    }
   ],
   "source": [
    "# Create input topic\n",
    "input_topic_path = pubsub_publisher.topic_path(PROJECT_ID, PUBSUB_TOPIC_INPUT)\n",
    "\n",
    "try:\n",
    "    topic = pubsub_publisher.get_topic(request={\"topic\": input_topic_path})\n",
    "    print(f\"✅ Input topic already exists: {input_topic_path}\")\n",
    "except:\n",
    "    topic = pubsub_publisher.create_topic(request={\"name\": input_topic_path})\n",
    "    print(f\"✅ Created input topic: {input_topic_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "create_input_sub",
   "metadata": {},
   "source": [
    "### Create Input Subscription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "create_input_sub_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Created input subscription: projects/statmike-mlops-349915/subscriptions/pytorch-autoencoder-input-sub\n"
     ]
    }
   ],
   "source": [
    "# Create subscription for input topic\n",
    "input_sub_path = pubsub_subscriber.subscription_path(PROJECT_ID, PUBSUB_SUB_INPUT)\n",
    "\n",
    "try:\n",
    "    subscription = pubsub_subscriber.get_subscription(request={\"subscription\": input_sub_path})\n",
    "    print(f\"✅ Input subscription already exists: {input_sub_path}\")\n",
    "except:\n",
    "    subscription = pubsub_subscriber.create_subscription(\n",
    "        request={\n",
    "            \"name\": input_sub_path,\n",
    "            \"topic\": input_topic_path,\n",
    "            \"ack_deadline_seconds\": 60\n",
    "        }\n",
    "    )\n",
    "    print(f\"✅ Created input subscription: {input_sub_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "create_output_topic",
   "metadata": {},
   "source": [
    "### Create Output Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "create_output_topic_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Created output topic: projects/statmike-mlops-349915/topics/pytorch-autoencoder-output\n"
     ]
    }
   ],
   "source": [
    "# Create output topic\n",
    "output_topic_path = pubsub_publisher.topic_path(PROJECT_ID, PUBSUB_TOPIC_OUTPUT)\n",
    "\n",
    "try:\n",
    "    topic = pubsub_publisher.get_topic(request={\"topic\": output_topic_path})\n",
    "    print(f\"✅ Output topic already exists: {output_topic_path}\")\n",
    "except:\n",
    "    topic = pubsub_publisher.create_topic(request={\"name\": output_topic_path})\n",
    "    print(f\"✅ Created output topic: {output_topic_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nsvbmh4w0yj",
   "metadata": {},
   "source": [
    "### Create Output Subscription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ut8ta9p2kjr",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Created output subscription: projects/statmike-mlops-349915/subscriptions/pytorch-autoencoder-output-sub\n"
     ]
    }
   ],
   "source": [
    "# Create subscription for output topic\n",
    "output_sub_path = pubsub_subscriber.subscription_path(PROJECT_ID, PUBSUB_SUB_OUTPUT)\n",
    "\n",
    "try:\n",
    "    subscription = pubsub_subscriber.get_subscription(request={\"subscription\": output_sub_path})\n",
    "    print(f\"✅ Output subscription already exists: {output_sub_path}\")\n",
    "except:\n",
    "    subscription = pubsub_subscriber.create_subscription(\n",
    "        request={\n",
    "            \"name\": output_sub_path,\n",
    "            \"topic\": output_topic_path,\n",
    "            \"ack_deadline_seconds\": 60\n",
    "        }\n",
    "    )\n",
    "    print(f\"✅ Created output subscription: {output_sub_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verify",
   "metadata": {},
   "source": [
    "---\n",
    "## Verify Setup\n",
    "\n",
    "Check that all resources are ready for Dataflow jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "verify_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DATAFLOW SETUP VERIFICATION\n",
      "============================================================\n",
      "\n",
      "✅ Model Files:\n",
      "   .mar file: gs://statmike-mlops-349915/frameworks/pytorch-autoencoder/dataflow/pytorch_autoencoder.mar\n",
      "   .pt file:  gs://statmike-mlops-349915/frameworks/pytorch-autoencoder/dataflow/final_model_traced.pt\n",
      "\n",
      "✅ BigQuery Tables:\n",
      "   Dataset: statmike-mlops-349915.frameworks\n",
      "   Batch results: statmike-mlops-349915.frameworks.pytorch_autoencoder_batch_results\n",
      "   Streaming results: statmike-mlops-349915.frameworks.pytorch_autoencoder_streaming_results\n",
      "\n",
      "✅ Pub/Sub Resources:\n",
      "   Input topic: projects/statmike-mlops-349915/topics/pytorch-autoencoder-input\n",
      "   Input subscription: projects/statmike-mlops-349915/subscriptions/pytorch-autoencoder-input-sub\n",
      "   Output topic: projects/statmike-mlops-349915/topics/pytorch-autoencoder-output\n",
      "   Output subscription: projects/statmike-mlops-349915/subscriptions/pytorch-autoencoder-output-sub\n",
      "\n",
      "✅ Dataflow Locations:\n",
      "   Staging: gs://statmike-mlops-349915/dataflow/staging\n",
      "   Temp: gs://statmike-mlops-349915/dataflow/temp\n",
      "\n",
      "============================================================\n",
      "READY FOR DATAFLOW JOBS!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"DATAFLOW SETUP VERIFICATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n✅ Model Files:\")\n",
    "print(f\"   .mar file: {GCS_MAR_PATH}\")\n",
    "print(f\"   .pt file:  {GCS_PT_PATH}\")\n",
    "\n",
    "print(\"\\n✅ BigQuery Tables:\")\n",
    "print(f\"   Dataset: {dataset_id}\")\n",
    "print(f\"   Batch results: {batch_table_id}\")\n",
    "print(f\"   Streaming results: {streaming_table_id}\")\n",
    "\n",
    "print(\"\\n✅ Pub/Sub Resources:\")\n",
    "print(f\"   Input topic: {input_topic_path}\")\n",
    "print(f\"   Input subscription: {input_sub_path}\")\n",
    "print(f\"   Output topic: {output_topic_path}\")\n",
    "print(f\"   Output subscription: {output_sub_path}\")\n",
    "\n",
    "print(\"\\n✅ Dataflow Locations:\")\n",
    "print(f\"   Staging: {DATAFLOW_STAGING}\")\n",
    "print(f\"   Temp: {DATAFLOW_TEMP}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"READY FOR DATAFLOW JOBS!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "In this notebook, you:\n",
    "\n",
    "✅ Extracted `.pt` file from `.mar` for RunInference\n",
    "\n",
    "✅ Created BigQuery tables for batch and streaming results\n",
    "\n",
    "✅ Created Pub/Sub topics and subscriptions\n",
    "\n",
    "✅ Verified all resources are ready\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "You are now ready to run Dataflow inference jobs:\n",
    "\n",
    "### 1. Batch Inference\n",
    "Run [dataflow-batch-runinference.ipynb](./dataflow-batch-runinference.ipynb) to:\n",
    "- Read transactions from BigQuery\n",
    "- Apply RunInference with the PyTorch model\n",
    "- Write anomaly scores back to BigQuery\n",
    "- Analyze results\n",
    "\n",
    "### 2. Streaming Inference\n",
    "Run [dataflow-streaming-runinference.ipynb](./dataflow-streaming-runinference.ipynb) to:\n",
    "- Read from Pub/Sub input topic\n",
    "- Apply RunInference in real-time\n",
    "- Write to Pub/Sub output topic and BigQuery\n",
    "- Monitor streaming job\n",
    "\n",
    "### 3. Vertex AI Endpoint Integration\n",
    "Run [dataflow-vertex-endpoint.ipynb](./dataflow-vertex-endpoint.ipynb) to:\n",
    "- Call Vertex AI Endpoint from Dataflow\n",
    "- Compare performance vs RunInference\n",
    "- Understand trade-offs\n",
    "\n",
    "## Resources\n",
    "\n",
    "- [Dataflow Documentation](https://cloud.google.com/dataflow/docs)\n",
    "- [Apache Beam RunInference](https://beam.apache.org/documentation/ml/about-ml/)\n",
    "- [PyTorch RunInference](https://beam.apache.org/documentation/ml/pytorch-inference/)\n",
    "- [Pub/Sub Documentation](https://cloud.google.com/pubsub/docs)\n",
    "- [BigQuery Documentation](https://cloud.google.com/bigquery/docs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (frameworks-pytorch)",
   "language": "python",
   "name": "frameworks-pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
