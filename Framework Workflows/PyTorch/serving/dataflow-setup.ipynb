{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![tracker](https://us-central1-vertex-ai-mlops-369716.cloudfunctions.net/pixel-tracking?path=statmike%2Fvertex-ai-mlops%2FFramework+Workflows%2FPyTorch%2Fserving&file=dataflow-setup.ipynb)\n",
        "<!--- header table --->\n",
        "<table align=\"left\">\n",
        "<tr>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%20Workflows/PyTorch/serving/dataflow-setup.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://www.svgrepo.com/download/217753/github.svg\" alt=\"GitHub logo\">\n",
        "      <br>View on<br>GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://colab.research.google.com/github/statmike/vertex-ai-mlops/blob/main/Framework%20Workflows/PyTorch/serving/dataflow-setup.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://www.gstatic.com/pantheon/images/bigquery/welcome_page/colab-logo.svg\" alt=\"Google Colaboratory logo\">\n",
        "      <br>Run in<br>Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https%3A%2F%2Fraw.githubusercontent.com%2Fstatmike%2Fvertex-ai-mlops%2Fmain%2FFramework%2520Workflows%2FPyTorch%2Fserving%2Fdataflow-setup.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\">\n",
        "      <br>Run in<br>Colab Enterprise\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/bigquery/import?url=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%20Workflows/PyTorch/serving/dataflow-setup.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://www.gstatic.com/images/branding/gcpiconscolors/bigquery/v1/32px.svg\" alt=\"BigQuery logo\">\n",
        "      <br>Open in<br>BigQuery Studio\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/statmike/vertex-ai-mlops/main/Framework%20Workflows/PyTorch/serving/dataflow-setup.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://www.gstatic.com/images/branding/gcpiconscolors/vertexai/v1/32px.svg\" alt=\"Vertex AI logo\">\n",
        "      <br>Open in<br>Vertex AI Workbench\n",
        "    </a>\n",
        "  </td>\n",
        "</tr>\n",
        "<tr>\n",
        "  <td colspan=\"5\" style=\"text-align: right\">\n",
        "    <b>Share This On: </b> \n",
        "    <a href=\"https://www.linkedin.com/sharing/share-offsite/?url=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%2520Workflows/PyTorch/serving/dataflow-setup.ipynb\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"Linkedin Logo\" width=\"20px\"></a> \n",
        "    <a href=\"https://reddit.com/submit?url=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%2520Workflows/PyTorch/serving/dataflow-setup.ipynb\"><img src=\"https://redditinc.com/hubfs/Reddit%20Inc/Brand/Reddit_Logo.png\" alt=\"Reddit Logo\" width=\"20px\"></a> \n",
        "    <a href=\"https://bsky.app/intent/compose?text=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%2520Workflows/PyTorch/serving/dataflow-setup.ipynb\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"BlueSky Logo\" width=\"20px\"></a> \n",
        "    <a href=\"https://twitter.com/intent/tweet?url=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%2520Workflows/PyTorch/serving/dataflow-setup.ipynb\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/5/5a/X_icon_2.svg\" alt=\"X (Twitter) Logo\" width=\"20px\"></a> \n",
        "  </td>\n",
        "</tr>\n",
        "<tr>\n",
        "  <td colspan=\"5\" style=\"text-align: right\">\n",
        "    <b>Connect With Author On: </b> \n",
        "    <a href=\"https://www.linkedin.com/in/statmike\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"Linkedin Logo\" width=\"20px\"></a>\n",
        "    <a href=\"https://www.github.com/statmike\"><img src=\"https://www.svgrepo.com/download/217753/github.svg\" alt=\"GitHub Logo\" width=\"20px\"></a> \n",
        "    <a href=\"https://www.youtube.com/@statmike-channel\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/f/fd/YouTube_full-color_icon_%282024%29.svg\" alt=\"YouTube Logo\" width=\"20px\"></a>\n",
        "    <a href=\"https://bsky.app/profile/statmike.bsky.social\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"BlueSky Logo\" width=\"20px\"></a> \n",
        "    <a href=\"https://x.com/statmike\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/5/5a/X_icon_2.svg\" alt=\"X (Twitter) Logo\" width=\"20px\"></a>\n",
        "  </td>\n",
        "</tr>\n",
        "</table>"
      ],
      "id": "header"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Dataflow Setup for PyTorch Model Inference\n",
        "\n",
        "This notebook sets up the infrastructure needed for running PyTorch model inference with Google Cloud Dataflow.\n",
        "\n",
        "## What You'll Learn\n",
        "\n",
        "This setup workflow covers:\n",
        "\n",
        "1. **Prepare Model for RunInference**: Extract .pt file from .mar for Dataflow\n",
        "2. **Create BigQuery Tables**: Destination tables for batch and streaming results\n",
        "3. **Create Pub/Sub Topics**: Input and output topics for streaming workflows\n",
        "4. **Verify Setup**: Test all components are ready\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "- Completed `pytorch-autoencoder.ipynb` (trained model)\n",
        "- .mar file in GCS: `gs://{PROJECT_ID}/frameworks/pytorch-autoencoder/pytorch_autoencoder.mar`\n",
        "\n",
        "## What is Dataflow?\n",
        "\n",
        "**Google Cloud Dataflow** is a fully managed service for executing Apache Beam pipelines. It provides:\n",
        "\n",
        "- **Unified Model**: Same code for batch and streaming\n",
        "- **Auto-scaling**: Automatically scales workers based on load\n",
        "- **Serverless**: No infrastructure management\n",
        "- **Cost-effective**: Pay only when pipelines run\n",
        "\n",
        "## RunInference vs Vertex AI Endpoints\n",
        "\n",
        "**RunInference (In-process inference)**:\n",
        "- \u2705 Model loaded directly in Dataflow workers\n",
        "- \u2705 No network latency\n",
        "- \u2705 Cost-effective for batch/streaming\n",
        "- \u2705 Scales with pipeline\n",
        "- \u274c Model bundled with pipeline code\n",
        "\n",
        "**Vertex AI Endpoints (External service)**:\n",
        "- \u2705 Centralized model management\n",
        "- \u2705 Model updates without pipeline changes\n",
        "- \u2705 A/B testing support\n",
        "- \u274c Network latency overhead\n",
        "- \u274c Higher cost (endpoint + dataflow)\n",
        "\n",
        "## Architecture\n",
        "\n",
        "### Batch Processing\n",
        "```\n",
        "BigQuery Table \u2192 Dataflow (RunInference) \u2192 BigQuery Results\n",
        "```\n",
        "\n",
        "### Streaming Processing\n",
        "```\n",
        "Pub/Sub Input \u2192 Dataflow (RunInference) \u2192 Pub/Sub Output + BigQuery\n",
        "```\n",
        "\n",
        "### Using Vertex Endpoints\n",
        "```\n",
        "BigQuery/Pub/Sub \u2192 Dataflow \u2192 Vertex Endpoint \u2192 Results\n",
        "```\n",
        "\n",
        "## What This Notebook Does\n",
        "\n",
        "**One-time setup** - Run once, then use for all Dataflow examples:\n",
        "\n",
        "1. Extract `.pt` file from `.mar` (RunInference needs PyTorch format, not TorchServe format)\n",
        "2. Create BigQuery result tables with proper schemas\n",
        "3. Create Pub/Sub topics for streaming\n",
        "4. Set up GCS staging/temp locations\n",
        "5. Verify permissions\n",
        "\n",
        "After running this notebook, you'll be ready to run:\n",
        "- Batch inference (BigQuery \u2192 BigQuery)\n",
        "- Streaming inference (Pub/Sub \u2192 Pub/Sub)\n",
        "- Endpoint-based inference (Dataflow \u2192 Vertex AI)"
      ],
      "id": "overview"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Environment Setup\n",
        "\n",
        "This section will authenticate your session, enable required Google Cloud APIs, and install necessary Python packages.\n",
        "\n",
        "**Package Installation Options (`REQ_TYPE`):**\n",
        "- `PRIMARY`: Installs only the main packages\n",
        "- `ALL` (Default): Installs exact versions of all packages and dependencies\n",
        "- `COLAB`: Installs a Colab-optimized list\n",
        "\n",
        "**Installation Tool Options (`INSTALL_TOOL`):**\n",
        "- `pip` (Default): Standard Python package installer\n",
        "- `uv`: Modern, fast Python package installer\n",
        "- `poetry`: Dependency management tool\n",
        "\n",
        "> **Note:** If running in Google Colab, the script will automatically set `REQ_TYPE = 'COLAB' to prevent package conflicts."
      ],
      "id": "env_setup"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Set Your Project ID\n\n\u26a0\ufe0f **Action Required:** Replace the `PROJECT_ID` value below with your Google Cloud project ID before running this cell."
      ],
      "id": "set_project"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "PROJECT_ID = 'statmike-mlops-349915' # replace with GCP project ID\nREQ_TYPE = 'ALL' # Specify PRIMARY or ALL or COLAB\nINSTALL_TOOL = 'poetry' # Specify pip, uv, or poetry"
      ],
      "id": "project_config"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Configuration\n\nThis cell defines the requirements files and Google Cloud APIs needed for this notebook."
      ],
      "id": "config_header"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "REQUIREMENTS_URL = 'https://raw.githubusercontent.com/statmike/vertex-ai-mlops/refs/heads/main/Framework%20Workflows/PyTorch/requirements.txt'\n",
        "\n",
        "REQUIRED_APIS = [\n",
        "    \"dataflow.googleapis.com\",\n",
        "    \"bigquery.googleapis.com\",\n",
        "    \"pubsub.googleapis.com\",\n",
        "    \"storage.googleapis.com\"\n",
        "]"
      ],
      "id": "config"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Run Setup\n\nThis cell downloads the centralized setup code and configures your environment."
      ],
      "id": "run_setup_header"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, urllib.request\n",
        "\n",
        "# Download and import setup code\n",
        "url = 'https://raw.githubusercontent.com/statmike/vertex-ai-mlops/refs/heads/main/core/notebook-template/python_setup.py'\n",
        "urllib.request.urlretrieve(url, 'python_setup_local.py')\n",
        "import python_setup_local as python_setup\n",
        "os.remove('python_setup_local.py')\n",
        "\n",
        "# Run setup\n",
        "setup_info = python_setup.setup_environment(PROJECT_ID, REQ_TYPE, REQUIREMENTS_URL, REQUIRED_APIS, INSTALL_TOOL)"
      ],
      "id": "run_setup"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n## Python Setup"
      ],
      "id": "python_setup"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Imports"
      ],
      "id": "imports_header"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import subprocess\n",
        "import json\n",
        "import os\n",
        "import zipfile\n",
        "from google.cloud import bigquery\n",
        "from google.cloud import storage\n",
        "from google.cloud import pubsub_v1"
      ],
      "id": "imports"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Variables - User Set"
      ],
      "id": "vars_user"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "REGION = 'us-central1'\n",
        "SERIES = 'frameworks'\n",
        "EXPERIMENT = 'pytorch-autoencoder'\n",
        "\n",
        "# BigQuery configuration\n",
        "BQ_DATASET = SERIES.replace('-', '_')\n",
        "BQ_TABLE_BATCH = f\"{EXPERIMENT}_batch_results\"\n",
        "BQ_TABLE_STREAMING = f\"{EXPERIMENT}_streaming_results\"\n",
        "\n",
        "# Pub/Sub configuration\n",
        "PUBSUB_TOPIC_INPUT = f\"{EXPERIMENT}-input\"\n",
        "PUBSUB_TOPIC_OUTPUT = f\"{EXPERIMENT}-output\"\n",
        "PUBSUB_SUB_INPUT = f\"{EXPERIMENT}-input-sub\""
      ],
      "id": "vars_user_code"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Variables - Auto Set"
      ],
      "id": "vars_auto"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "PROJECT_ID = subprocess.run(['gcloud', 'config', 'get-value', 'project'], capture_output=True, text=True, check=True).stdout.strip()\n",
        "PROJECT_NUMBER = subprocess.run(['gcloud', 'projects', 'describe', PROJECT_ID, '--format=value(projectNumber)'], capture_output=True, text=True, check=True).stdout.strip()\n",
        "\n",
        "print(f\"\\n{'='*50}\\nGoogle Cloud Project Information\\n{'='*50}\\nPROJECT_ID     = {PROJECT_ID}\\nPROJECT_NUMBER = {PROJECT_NUMBER}\\n{'='*50}\\n\")"
      ],
      "id": "vars_auto_code"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Configurations"
      ],
      "id": "configs"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# GCS configuration\n",
        "BUCKET_NAME = PROJECT_ID\n",
        "BUCKET_URI = f\"gs://{BUCKET_NAME}\"\n",
        "MODEL_DIR = f\"{BUCKET_URI}/{SERIES}/{EXPERIMENT}\"\n",
        "DATAFLOW_STAGING = f\"{BUCKET_URI}/dataflow/staging\"\n",
        "DATAFLOW_TEMP = f\"{BUCKET_URI}/dataflow/temp\"\n",
        "\n",
        "# Model paths\n",
        "MAR_PATH = f\"{MODEL_DIR}/pytorch_autoencoder.mar\"\n",
        "PT_PATH = f\"{MODEL_DIR}/dataflow/final_model_traced.pt\"\n",
        "\n",
        "print(f\"Model .mar file: {MAR_PATH}\")\n",
        "print(f\"Extracted .pt file: {PT_PATH}\")\n",
        "print(f\"Dataflow staging: {DATAFLOW_STAGING}\")\n",
        "print(f\"Dataflow temp: {DATAFLOW_TEMP}\")"
      ],
      "id": "configs_code"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Initialize Clients"
      ],
      "id": "init_clients"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize clients\n",
        "storage_client = storage.Client(project=PROJECT_ID)\n",
        "bq_client = bigquery.Client(project=PROJECT_ID)\n",
        "pubsub_publisher = pubsub_v1.PublisherClient()\n",
        "pubsub_subscriber = pubsub_v1.SubscriberClient()\n",
        "\n",
        "print(f\"\u2705 Clients initialized\")\n",
        "print(f\"   Project: {PROJECT_ID}\")\n",
        "print(f\"   Region: {REGION}\")"
      ],
      "id": "init_clients_code"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Prepare Model for RunInference\n",
        "\n",
        "Extract the TorchScript `.pt` file from the `.mar` archive for use with Dataflow RunInference."
      ],
      "id": "model_prep"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Check for .mar File in GCS"
      ],
      "id": "check_mar"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "bucket = storage_client.bucket(BUCKET_NAME)\n",
        "mar_blob = bucket.blob(f\"{SERIES}/{EXPERIMENT}/pytorch_autoencoder.mar\")\n",
        "\n",
        "if mar_blob.exists():\n",
        "    print(f\"\u2705 Found .mar file: {MAR_PATH}\")\n",
        "    print(f\"   Size: {mar_blob.size:,} bytes ({mar_blob.size / 1024:.2f} KB)\")\n",
        "else:\n",
        "    print(f\"\u274c .mar file not found at: {MAR_PATH}\")\n",
        "    print(\"   Please run the pytorch-autoencoder.ipynb notebook first\")"
      ],
      "id": "check_mar_code"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Extract .pt File from .mar"
      ],
      "id": "extract_pt"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download .mar file\n",
        "mar_blob.download_to_filename(\"pytorch_autoencoder.mar\")\n",
        "\n",
        "# Extract .pt file from .mar\n",
        "with zipfile.ZipFile(\"pytorch_autoencoder.mar\", 'r') as zip_ref:\n",
        "    zip_ref.extract('final_model_traced.pt')\n",
        "\n",
        "print(f\"\u2705 Extracted final_model_traced.pt from .mar file\")\n",
        "\n",
        "# Upload .pt file to GCS for Dataflow\n",
        "pt_blob = bucket.blob(f\"{SERIES}/{EXPERIMENT}/dataflow/final_model_traced.pt\")\n",
        "pt_blob.upload_from_filename(\"final_model_traced.pt\")\n",
        "\n",
        "print(f\"\u2705 Uploaded .pt file to: {PT_PATH}\")\n",
        "print(f\"   Size: {pt_blob.size:,} bytes ({pt_blob.size / 1024:.2f} KB)\")\n",
        "\n",
        "# Clean up local files\n",
        "os.remove(\"pytorch_autoencoder.mar\")\n",
        "os.remove(\"final_model_traced.pt\")"
      ],
      "id": "extract_pt_code"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Create BigQuery Result Tables\n",
        "\n",
        "Create tables to store results from batch and streaming Dataflow jobs."
      ],
      "id": "bigquery_setup"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Check for Existing Dataset"
      ],
      "id": "check_dataset"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check if dataset exists\n",
        "dataset_id = f\"{PROJECT_ID}.{BQ_DATASET}\"\n",
        "\n",
        "try:\n",
        "    dataset = bq_client.get_dataset(dataset_id)\n",
        "    print(f\"\u2705 Dataset already exists: {dataset_id}\")\n",
        "except:\n",
        "    # Create dataset\n",
        "    dataset = bigquery.Dataset(dataset_id)\n",
        "    dataset.location = REGION\n",
        "    dataset = bq_client.create_dataset(dataset)\n",
        "    print(f\"\u2705 Created dataset: {dataset_id}\")"
      ],
      "id": "check_dataset_code"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create Batch Results Table"
      ],
      "id": "create_batch_table"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define schema for batch results\n",
        "batch_schema = [\n",
        "    bigquery.SchemaField(\"instance_id\", \"STRING\"),\n",
        "    bigquery.SchemaField(\"anomaly_score\", \"FLOAT64\"),\n",
        "    bigquery.SchemaField(\"encoded\", \"FLOAT64\", mode=\"REPEATED\"),\n",
        "    bigquery.SchemaField(\"timestamp\", \"TIMESTAMP\"),\n",
        "]\n",
        "\n",
        "# Create or get table\n",
        "batch_table_id = f\"{dataset_id}.{BQ_TABLE_BATCH}\"\n",
        "\n",
        "try:\n",
        "    table = bq_client.get_table(batch_table_id)\n",
        "    print(f\"\u2705 Batch results table already exists: {batch_table_id}\")\n",
        "except:\n",
        "    table = bigquery.Table(batch_table_id, schema=batch_schema)\n",
        "    table = bq_client.create_table(table)\n",
        "    print(f\"\u2705 Created batch results table: {batch_table_id}\")"
      ],
      "id": "create_batch_table_code"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create Streaming Results Table"
      ],
      "id": "create_streaming_table"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define schema for streaming results (same as batch)\n",
        "streaming_schema = [\n",
        "    bigquery.SchemaField(\"instance_id\", \"STRING\"),\n",
        "    bigquery.SchemaField(\"anomaly_score\", \"FLOAT64\"),\n",
        "    bigquery.SchemaField(\"encoded\", \"FLOAT64\", mode=\"REPEATED\"),\n",
        "    bigquery.SchemaField(\"timestamp\", \"TIMESTAMP\"),\n",
        "    bigquery.SchemaField(\"window_start\", \"TIMESTAMP\"),\n",
        "    bigquery.SchemaField(\"window_end\", \"TIMESTAMP\"),\n",
        "]\n",
        "\n",
        "# Create or get table\n",
        "streaming_table_id = f\"{dataset_id}.{BQ_TABLE_STREAMING}\"\n",
        "\n",
        "try:\n",
        "    table = bq_client.get_table(streaming_table_id)\n",
        "    print(f\"\u2705 Streaming results table already exists: {streaming_table_id}\")\n",
        "except:\n",
        "    table = bigquery.Table(streaming_table_id, schema=streaming_schema)\n",
        "    table = bq_client.create_table(table)\n",
        "    print(f\"\u2705 Created streaming results table: {streaming_table_id}\")"
      ],
      "id": "create_streaming_table_code"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Create Pub/Sub Topics and Subscriptions\n",
        "\n",
        "Create Pub/Sub infrastructure for streaming inference."
      ],
      "id": "pubsub_setup"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create Input Topic"
      ],
      "id": "create_input_topic"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create input topic\n",
        "input_topic_path = pubsub_publisher.topic_path(PROJECT_ID, PUBSUB_TOPIC_INPUT)\n",
        "\n",
        "try:\n",
        "    topic = pubsub_publisher.get_topic(request={\"topic\": input_topic_path})\n",
        "    print(f\"\u2705 Input topic already exists: {input_topic_path}\")\n",
        "except:\n",
        "    topic = pubsub_publisher.create_topic(request={\"name\": input_topic_path})\n",
        "    print(f\"\u2705 Created input topic: {input_topic_path}\")"
      ],
      "id": "create_input_topic_code"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create Input Subscription"
      ],
      "id": "create_input_sub"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create subscription for input topic\n",
        "input_sub_path = pubsub_subscriber.subscription_path(PROJECT_ID, PUBSUB_SUB_INPUT)\n",
        "\n",
        "try:\n",
        "    subscription = pubsub_subscriber.get_subscription(request={\"subscription\": input_sub_path})\n",
        "    print(f\"\u2705 Input subscription already exists: {input_sub_path}\")\n",
        "except:\n",
        "    subscription = pubsub_subscriber.create_subscription(\n",
        "        request={\n",
        "            \"name\": input_sub_path,\n",
        "            \"topic\": input_topic_path,\n",
        "            \"ack_deadline_seconds\": 60\n",
        "        }\n",
        "    )\n",
        "    print(f\"\u2705 Created input subscription: {input_sub_path}\")"
      ],
      "id": "create_input_sub_code"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create Output Topic"
      ],
      "id": "create_output_topic"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create output topic\n",
        "output_topic_path = pubsub_publisher.topic_path(PROJECT_ID, PUBSUB_TOPIC_OUTPUT)\n",
        "\n",
        "try:\n",
        "    topic = pubsub_publisher.get_topic(request={\"topic\": output_topic_path})\n",
        "    print(f\"\u2705 Output topic already exists: {output_topic_path}\")\n",
        "except:\n",
        "    topic = pubsub_publisher.create_topic(request={\"name\": output_topic_path})\n",
        "    print(f\"\u2705 Created output topic: {output_topic_path}\")"
      ],
      "id": "create_output_topic_code"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Verify Setup\n",
        "\n",
        "Check that all resources are ready for Dataflow jobs."
      ],
      "id": "verify"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*60)\n",
        "print(\"DATAFLOW SETUP VERIFICATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(\"\\n\u2705 Model Files:\")\n",
        "print(f\"   .mar file: {MAR_PATH}\")\n",
        "print(f\"   .pt file:  {PT_PATH}\")\n",
        "\n",
        "print(\"\\n\u2705 BigQuery Tables:\")\n",
        "print(f\"   Dataset: {dataset_id}\")\n",
        "print(f\"   Batch results: {batch_table_id}\")\n",
        "print(f\"   Streaming results: {streaming_table_id}\")\n",
        "\n",
        "print(\"\\n\u2705 Pub/Sub Resources:\")\n",
        "print(f\"   Input topic: {input_topic_path}\")\n",
        "print(f\"   Input subscription: {input_sub_path}\")\n",
        "print(f\"   Output topic: {output_topic_path}\")\n",
        "\n",
        "print(\"\\n\u2705 Dataflow Locations:\")\n",
        "print(f\"   Staging: {DATAFLOW_STAGING}\")\n",
        "print(f\"   Temp: {DATAFLOW_TEMP}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"READY FOR DATAFLOW JOBS!\")\n",
        "print(\"=\"*60)"
      ],
      "id": "verify_code"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Summary\n",
        "\n",
        "In this notebook, you:\n",
        "\n",
        "\u2705 Extracted `.pt` file from `.mar` for RunInference\n",
        "\n",
        "\u2705 Created BigQuery tables for batch and streaming results\n",
        "\n",
        "\u2705 Created Pub/Sub topics and subscriptions\n",
        "\n",
        "\u2705 Verified all resources are ready\n",
        "\n",
        "## Next Steps\n",
        "\n",
        "You are now ready to run Dataflow inference jobs:\n",
        "\n",
        "### 1. Batch Inference\n",
        "Run [dataflow-batch-runinference.ipynb](./dataflow-batch-runinference.ipynb) to:\n",
        "- Read transactions from BigQuery\n",
        "- Apply RunInference with the PyTorch model\n",
        "- Write anomaly scores back to BigQuery\n",
        "- Analyze results\n",
        "\n",
        "### 2. Streaming Inference\n",
        "Run [dataflow-streaming-runinference.ipynb](./dataflow-streaming-runinference.ipynb) to:\n",
        "- Read from Pub/Sub input topic\n",
        "- Apply RunInference in real-time\n",
        "- Write to Pub/Sub output topic and BigQuery\n",
        "- Monitor streaming job\n",
        "\n",
        "### 3. Vertex AI Endpoint Integration\n",
        "Run [dataflow-vertex-endpoint.ipynb](./dataflow-vertex-endpoint.ipynb) to:\n",
        "- Call Vertex AI Endpoint from Dataflow\n",
        "- Compare performance vs RunInference\n",
        "- Understand trade-offs\n",
        "\n",
        "## Resources\n",
        "\n",
        "- [Dataflow Documentation](https://cloud.google.com/dataflow/docs)\n",
        "- [Apache Beam RunInference](https://beam.apache.org/documentation/ml/about-ml/)\n",
        "- [PyTorch RunInference](https://beam.apache.org/documentation/ml/pytorch-inference/)\n",
        "- [Pub/Sub Documentation](https://cloud.google.com/pubsub/docs)\n",
        "- [BigQuery Documentation](https://cloud.google.com/bigquery/docs)"
      ],
      "id": "summary"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.13.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}