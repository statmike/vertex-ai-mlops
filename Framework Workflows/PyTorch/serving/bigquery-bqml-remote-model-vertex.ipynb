{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "![tracker](https://us-central1-vertex-ai-mlops-369716.cloudfunctions.net/pixel-tracking?path=statmike%2Fvertex-ai-mlops%2FFramework+Workflows%2FPyTorch%2Fserving&file=bigquery-bqml-remote-model-vertex.ipynb)\n",
    "<!--- header table --->\n",
    "<table align=\"left\">\n",
    "<tr>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%20Workflows/PyTorch/serving/bigquery-bqml-remote-model-vertex.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.svgrepo.com/download/217753/github.svg\" alt=\"GitHub logo\">\n",
    "      <br>View on<br>GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://colab.research.google.com/github/statmike/vertex-ai-mlops/blob/main/Framework%20Workflows/PyTorch/serving/bigquery-bqml-remote-model-vertex.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.gstatic.com/pantheon/images/bigquery/welcome_page/colab-logo.svg\" alt=\"Google Colaboratory logo\">\n",
    "      <br>Run in<br>Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https%3A%2F%2Fraw.githubusercontent.com%2Fstatmike%2Fvertex-ai-mlops%2Fmain%2FFramework%2520Workflows%2FPyTorch%2Fserving%2Fbigquery-bqml-remote-model-vertex.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\">\n",
    "      <br>Run in<br>Colab Enterprise\n",
    "    </a>\n",
    "  </td>      \n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/bigquery/import?url=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%20Workflows/PyTorch/serving/bigquery-bqml-remote-model-vertex.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.gstatic.com/images/branding/gcpiconscolors/bigquery/v1/32px.svg\" alt=\"BigQuery logo\">\n",
    "      <br>Open in<br>BigQuery Studio\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/statmike/vertex-ai-mlops/main/Framework%20Workflows/PyTorch/serving/bigquery-bqml-remote-model-vertex.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.gstatic.com/images/branding/gcpiconscolors/vertexai/v1/32px.svg\" alt=\"Vertex AI logo\">\n",
    "      <br>Open in<br>Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td colspan=\"5\" style=\"text-align: right\">\n",
    "    <b>Share This On: </b> \n",
    "    <a href=\"https://www.linkedin.com/sharing/share-offsite/?url=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%2520Workflows/PyTorch/serving/bigquery-bqml-remote-model-vertex.ipynb\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"Linkedin Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://reddit.com/submit?url=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%2520Workflows/PyTorch/serving/bigquery-bqml-remote-model-vertex.ipynb\"><img src=\"https://redditinc.com/hubfs/Reddit%20Inc/Brand/Reddit_Logo.png\" alt=\"Reddit Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://bsky.app/intent/compose?text=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%2520Workflows/PyTorch/serving/bigquery-bqml-remote-model-vertex.ipynb\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"BlueSky Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://twitter.com/intent/tweet?url=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%2520Workflows/PyTorch/serving/bigquery-bqml-remote-model-vertex.ipynb\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/5/5a/X_icon_2.svg\" alt=\"X (Twitter) Logo\" width=\"20px\"></a> \n",
    "  </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td colspan=\"5\" style=\"text-align: right\">\n",
    "    <b>Connect With Author On: </b> \n",
    "    <a href=\"https://www.linkedin.com/in/statmike\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"Linkedin Logo\" width=\"20px\"></a>\n",
    "    <a href=\"https://www.github.com/statmike\"><img src=\"https://www.svgrepo.com/download/217753/github.svg\" alt=\"GitHub Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://www.youtube.com/@statmike-channel\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/f/fd/YouTube_full-color_icon_%282024%29.svg\" alt=\"YouTube Logo\" width=\"20px\"></a>\n",
    "    <a href=\"https://bsky.app/profile/statmike.bsky.social\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"BlueSky Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://x.com/statmike\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/5/5a/X_icon_2.svg\" alt=\"X (Twitter) Logo\" width=\"20px\"></a>\n",
    "  </td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overview",
   "metadata": {},
   "source": [
    "# BigQuery ML Remote Model Inference with Vertex AI Endpoints\n",
    "\n",
    "This notebook demonstrates how to use **BigQuery ML Remote Models** to call Vertex AI Endpoints directly from SQL using `ML.PREDICT()`. This powerful integration enables SQL-based batch scoring without needing Python or model deployment infrastructure.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "This workflow covers:\n",
    "\n",
    "1. **Prerequisites Check**: Verify deployed Vertex AI endpoint\n",
    "2. **Test Endpoint Locally**: Validate endpoint with Python SDK\n",
    "3. **Create BigQuery Connection**: Setup Cloud Resource Connection for remote inference\n",
    "4. **Grant Permissions**: Configure IAM for connection service account\n",
    "5. **Create Remote Model**: Register endpoint as BQML model\n",
    "6. **Simple Predictions**: Use ML.PREDICT() with test data\n",
    "7. **Extract Prediction Fields**: Work with STRUCT output\n",
    "8. **Apply Business Logic**: Use predictions for decision-making\n",
    "9. **Batch Scoring**: Create enriched tables with predictions\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "This notebook requires an existing Vertex AI Endpoint with a deployed PyTorch model.\n",
    "\n",
    "**Create an endpoint using either:**\n",
    "- [vertex-ai-endpoint-prebuilt-container.ipynb](./vertex-ai-endpoint-prebuilt-container.ipynb) - Pre-built TorchServe container (recommended)\n",
    "- [vertex-ai-endpoint-custom-container.ipynb](./vertex-ai-endpoint-custom-container.ipynb) - Custom FastAPI container\n",
    "\n",
    "Both endpoints work with this notebook, but have different response signatures (see below).\n",
    "\n",
    "## Core Concepts\n",
    "\n",
    "### BigQuery ML Remote Models\n",
    "\n",
    "Remote models allow BigQuery to call external prediction endpoints (like Vertex AI) during query execution:\n",
    "\n",
    "**Benefits:**\n",
    "- ‚úÖ **SQL-based inference**: No Python code needed for batch scoring\n",
    "- ‚úÖ **Data warehouse integration**: Predictions join seamlessly with BigQuery tables\n",
    "- ‚úÖ **Scheduled scoring**: Use BigQuery scheduled queries for automated predictions\n",
    "- ‚úÖ **Leverage existing endpoints**: Reuse models already deployed for online serving\n",
    "- ‚úÖ **Large-scale batch processing**: Process millions of rows efficiently\n",
    "\n",
    "**When to use:**\n",
    "- Batch scoring large datasets already in BigQuery\n",
    "- SQL analysts need predictions without Python knowledge\n",
    "- Scheduled/recurring inference jobs\n",
    "- Models too large to import into BigQuery ML\n",
    "\n",
    "### Architecture\n",
    "\n",
    "```\n",
    "BigQuery Table ‚Üí ML.PREDICT() ‚Üí Cloud Resource Connection ‚Üí Vertex AI Endpoint ‚Üí Predictions ‚Üí BigQuery Results\n",
    "```\n",
    "\n",
    "**How it works:**\n",
    "1. SQL query with `ML.PREDICT()` sends rows to remote model\n",
    "2. BigQuery Connection forwards requests to Vertex AI endpoint\n",
    "3. Endpoint returns predictions (as STRUCT)\n",
    "4. Predictions merge back into query results\n",
    "\n",
    "### Endpoint Response Signatures\n",
    "\n",
    "The two endpoint types return different prediction structures:\n",
    "\n",
    "**Pre-built Container** (`vertex-ai-endpoint-prebuilt-container.ipynb`):\n",
    "- Returns **13 fields** (full model output)\n",
    "- Includes all error metrics, latent encodings, and reconstructions\n",
    "- Example fields: `denormalized_MAE`, `denormalized_RMSE`, `encoded`, `normalized_reconstruction`, etc.\n",
    "- **Use when**: You need full model diagnostics and multiple metrics\n",
    "\n",
    "**Custom Container** (`vertex-ai-endpoint-custom-container.ipynb`):\n",
    "- Returns **2 fields** (simplified output)\n",
    "- Only `anomaly_score` and `encoded` (latent representation)\n",
    "- Reduced response size (~70% smaller)\n",
    "- **Use when**: You only need anomaly scores for decision-making\n",
    "\n",
    "**Impact on BQML Remote Model:**\n",
    "- Pre-built: More fields to extract and analyze in SQL\n",
    "- Custom: Simpler SQL queries, faster response times\n",
    "- This notebook demonstrates **both** (you choose which endpoint to use)\n",
    "\n",
    "### BigQuery Cloud Resource Connection\n",
    "\n",
    "Connections enable BigQuery to securely call external services:\n",
    "\n",
    "- Creates a dedicated service account\n",
    "- Service account needs `Vertex AI User` role\n",
    "- Connection tied to specific region (must match endpoint region)\n",
    "- Reusable across multiple remote models\n",
    "\n",
    "## What's Different from Direct SDK Calls?\n",
    "\n",
    "| Aspect | Python SDK (Direct) | BQML Remote Model |\n",
    "|--------|-------------------|------------------|\n",
    "| **Language** | Python | SQL |\n",
    "| **Data Source** | Manual loading | BigQuery tables |\n",
    "| **Scale** | Limited by memory | Petabyte-scale |\n",
    "| **Scheduling** | Custom code | Scheduled queries |\n",
    "| **Result Storage** | Manual export | Directly to tables |\n",
    "| **Who can use** | Data scientists | SQL analysts |\n",
    "\n",
    "**Example comparison:**\n",
    "\n",
    "```python\n",
    "# Python SDK - Manual batching required\n",
    "predictions = []\n",
    "for batch in chunks(data, batch_size=1000):\n",
    "    pred = endpoint.predict(instances=batch)\n",
    "    predictions.extend(pred.predictions)\n",
    "```\n",
    "\n",
    "```sql\n",
    "-- BQML - Automatic batching, millions of rows\n",
    "SELECT * FROM ML.PREDICT(\n",
    "  MODEL `dataset.remote_model`,\n",
    "  (SELECT * FROM `dataset.transactions` WHERE date = CURRENT_DATE())\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "env_setup",
   "metadata": {},
   "source": [
    "---\n",
    "## Environment Setup\n",
    "\n",
    "This section will authenticate your session, enable required Google Cloud APIs, and install necessary Python packages.\n",
    "\n",
    "**Package Installation Options (`REQ_TYPE`):**\n",
    "- `PRIMARY`: Installs only the main packages. Faster, but pip resolves sub-dependencies which may result in different versions than development.\n",
    "- `ALL` (Default): Installs exact versions of all packages and dependencies. Best for perfectly reproducing the development environment.\n",
    "- `COLAB`: Installs a Colab-optimized list that excludes pre-installed packages like `ipython` and `ipykernel`.\n",
    "\n",
    "**Installation Tool Options (`INSTALL_TOOL`):**\n",
    "- `pip` (Default): Uses pip for package installation. Standard Python package installer.\n",
    "- `uv`: Modern, fast Python package installer. Must be installed separately. See: https://github.com/astral-sh/uv\n",
    "- `poetry`: Dependency management tool. Requires running notebook in a poetry environment (`poetry shell` or `poetry run jupyter lab`). Uses `pyproject.toml` instead of requirements.txt.\n",
    "\n",
    "> **Note:** If running in Google Colab, the script will automatically detect this and set `REQ_TYPE = 'COLAB'` to prevent package conflicts, overriding any manual setting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "set_project",
   "metadata": {},
   "source": [
    "### Set Your Project ID\n",
    "\n",
    "‚ö†Ô∏è **Action Required:** Replace the `PROJECT_ID` value below with your Google Cloud project ID before running this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "project_config",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = 'statmike-mlops-349915' # replace with GCP project ID\n",
    "REQ_TYPE = 'ALL' # Specify PRIMARY or ALL or COLAB\n",
    "INSTALL_TOOL = 'poetry' # Specify pip, uv, or poetry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config_header",
   "metadata": {},
   "source": [
    "### Configuration\n",
    "\n",
    "This cell defines the requirements files and Google Cloud APIs needed for this notebook. Run as-is without modification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": [
    "REQUIREMENTS_URL = 'https://raw.githubusercontent.com/statmike/vertex-ai-mlops/refs/heads/main/Framework%20Workflows/PyTorch/requirements.txt'\n",
    "\n",
    "REQUIRED_APIS = [\n",
    "    \"aiplatform.googleapis.com\",\n",
    "    \"bigquery.googleapis.com\",\n",
    "    \"bigqueryconnection.googleapis.com\",  # Required for remote models\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "run_setup_header",
   "metadata": {},
   "source": [
    "### Run Setup\n",
    "\n",
    "This cell downloads the centralized setup code and configures your environment. It will:\n",
    "- Authenticate your session with Google Cloud\n",
    "- Enable required APIs for this notebook (including BigQuery Connection API)\n",
    "- Install necessary Python packages\n",
    "- Display a setup summary with your project information\n",
    "\n",
    "> **Note:** In Colab, if packages are installed, the kernel will automatically restart. After restart, continue from the next cell without re-running earlier cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "run_setup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "PYTHON GCP ENVIRONMENT SETUP\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "AUTHENTICATION\n",
      "==================================================\n",
      "Checking for existing ADC...\n",
      "‚úÖ Existing ADC found.\n",
      "‚úÖ Project is correctly set to 'statmike-mlops-349915'.\n",
      "\n",
      "==================================================\n",
      "API CHECK & ENABLE\n",
      "==================================================\n",
      "‚úÖ aiplatform.googleapis.com is already enabled.\n",
      "‚úÖ bigquery.googleapis.com is already enabled.\n",
      "‚úÖ bigqueryconnection.googleapis.com is already enabled.\n",
      "\n",
      "==================================================\n",
      "PACKAGE MANAGEMENT\n",
      "==================================================\n",
      "Installation Tool: poetry\n",
      "‚úÖ Found poetry at: /usr/local/google/home/statmike/.local/bin/poetry\n",
      "‚úÖ Running in poetry environment: /usr/local/google/home/statmike/.cache/pypoetry/virtualenvs/frameworks-pytorch-0KVJlKeQ-py3.13\n",
      "‚ÑπÔ∏è  Poetry mode: Installing from pyproject.toml (REQUIREMENTS_URL ignored)\n",
      "‚úÖ Found pyproject.toml at: /usr/local/google/home/statmike/Git/vertex-ai-mlops/Framework Workflows/PyTorch/pyproject.toml\n",
      "   Changed working directory to: /usr/local/google/home/statmike/Git/vertex-ai-mlops/Framework Workflows/PyTorch\n",
      "Running poetry install...\n",
      "   Restored working directory to: /usr/local/google/home/statmike/Git/vertex-ai-mlops/Framework Workflows/PyTorch/serving\n",
      "‚úÖ All packages are already installed and up to date.\n",
      "\n",
      "==================================================\n",
      "Google Cloud Project Information\n",
      "==================================================\n",
      "PROJECT_ID     = statmike-mlops-349915\n",
      "PROJECT_NUMBER = 1026793852137\n",
      "==================================================\n",
      "\n",
      "\n",
      "==================================================\n",
      "SETUP SUMMARY\n",
      "==================================================\n",
      "‚úÖ Authentication:    Success\n",
      "‚úÖ API Configuration: Success\n",
      "‚úÖ Package Install:   Already up to date\n",
      "‚úÖ Installation Tool: poetry\n",
      "‚úÖ Project ID:        statmike-mlops-349915\n",
      "‚úÖ Project Number:    1026793852137\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os, urllib.request\n",
    "\n",
    "# Download and import setup code\n",
    "url = 'https://raw.githubusercontent.com/statmike/vertex-ai-mlops/refs/heads/main/core/notebook-template/python_setup.py'\n",
    "urllib.request.urlretrieve(url, 'python_setup_local.py')\n",
    "import python_setup_local as python_setup\n",
    "os.remove('python_setup_local.py')\n",
    "\n",
    "# Run setup\n",
    "setup_info = python_setup.setup_environment(PROJECT_ID, REQ_TYPE, REQUIREMENTS_URL, REQUIRED_APIS, INSTALL_TOOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "python_setup",
   "metadata": {},
   "source": [
    "---\n",
    "## Python Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imports_header",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "imports",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/google/home/statmike/.cache/pypoetry/virtualenvs/frameworks-pytorch-0KVJlKeQ-py3.13/lib/python3.13/site-packages/google/cloud/aiplatform/models.py:52: FutureWarning: Support for google-cloud-storage < 3.0.0 will be removed in a future version of google-cloud-aiplatform. Please upgrade to google-cloud-storage >= 3.0.0.\n",
      "  from google.cloud.aiplatform.utils import gcs_utils\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import json\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud import bigquery\n",
    "from google.cloud.bigquery_connection_v1 import ConnectionServiceClient\n",
    "from google.cloud.bigquery_connection_v1.types import Connection, CloudResourceProperties"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vars_user",
   "metadata": {},
   "source": [
    "### Variables - User Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "vars_user_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION = 'us-central1'  # Vertex AI endpoint region\n",
    "SERIES = 'frameworks'\n",
    "EXPERIMENT = 'pytorch-bqml-remote-model'\n",
    "\n",
    "# Endpoint to use (choose one)\n",
    "# Options: 'pytorch-autoencoder' (prebuilt) or 'pytorch-autoencoder-custom' (custom)\n",
    "ENDPOINT_DISPLAY_NAME = 'pytorch-autoencoder-endpoint'  # Change to 'pytorch-autoencoder-custom' if using custom container\n",
    "\n",
    "# BigQuery configuration\n",
    "BQ_REGION = 'US'  # BigQuery connection region (must match dataset location)\n",
    "BQ_DATASET = SERIES.replace('-', '_')  # 'frameworks'\n",
    "BQ_SOURCE_TABLE = SERIES  # Source data table\n",
    "BQ_PREDICTIONS_TABLE = 'bqml_remote_model_predictions'  # Will be created\n",
    "\n",
    "# Connection name for BigQuery remote model\n",
    "BQ_CONNECTION_NAME = f\"{SERIES.replace('-', '_')}_{EXPERIMENT.replace('-', '_')}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vars_auto",
   "metadata": {},
   "source": [
    "### Variables - Auto Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "vars_auto_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Google Cloud Project Information\n",
      "==================================================\n",
      "PROJECT_ID     = statmike-mlops-349915\n",
      "PROJECT_NUMBER = 1026793852137\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "PROJECT_ID = subprocess.run(['gcloud', 'config', 'get-value', 'project'], capture_output=True, text=True, check=True).stdout.strip()\n",
    "PROJECT_NUMBER = subprocess.run(['gcloud', 'projects', 'describe', PROJECT_ID, '--format=value(projectNumber)'], capture_output=True, text=True, check=True).stdout.strip()\n",
    "\n",
    "print(f\"\\n{'='*50}\\nGoogle Cloud Project Information\\n{'='*50}\\nPROJECT_ID     = {PROJECT_ID}\\nPROJECT_NUMBER = {PROJECT_NUMBER}\\n{'='*50}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "init_clients",
   "metadata": {},
   "source": [
    "### Initialize Clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "init_clients_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Clients initialized:\n",
      "   Vertex AI: statmike-mlops-349915 in us-central1\n",
      "   BigQuery: statmike-mlops-349915\n",
      "   BigQuery Connection: statmike-mlops-349915\n"
     ]
    }
   ],
   "source": [
    "# Initialize Vertex AI SDK\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION)\n",
    "\n",
    "# Initialize BigQuery client\n",
    "bq = bigquery.Client(project=PROJECT_ID)\n",
    "\n",
    "# Initialize BigQuery Connection client\n",
    "connection_client = ConnectionServiceClient()\n",
    "\n",
    "print(f\"‚úÖ Clients initialized:\")\n",
    "print(f\"   Vertex AI: {PROJECT_ID} in {REGION}\")\n",
    "print(f\"   BigQuery: {PROJECT_ID}\")\n",
    "print(f\"   BigQuery Connection: {PROJECT_ID}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prereq_check",
   "metadata": {},
   "source": [
    "---\n",
    "## Prerequisites: Verify Deployed Endpoint\n",
    "\n",
    "This notebook requires a deployed Vertex AI Endpoint with the PyTorch autoencoder model.\n",
    "\n",
    "**If you haven't deployed an endpoint yet:**\n",
    "- [vertex-ai-endpoint-prebuilt-container.ipynb](./vertex-ai-endpoint-prebuilt-container.ipynb) - Pre-built TorchServe container (recommended)\n",
    "- [vertex-ai-endpoint-custom-container.ipynb](./vertex-ai-endpoint-custom-container.ipynb) - Custom FastAPI container\n",
    "\n",
    "**This section will:**\n",
    "1. Find the endpoint by display name\n",
    "2. Verify it has a deployed model\n",
    "3. Test predictions using the Python SDK\n",
    "4. Understand the response signature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "find_endpoint",
   "metadata": {},
   "source": [
    "### Find Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "find_endpoint_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Found endpoint: pytorch-autoencoder-endpoint\n",
      "\n",
      "Endpoint Details:\n",
      "  Resource name: projects/1026793852137/locations/us-central1/endpoints/2741468416626917376\n",
      "  Display name: pytorch-autoencoder-endpoint\n",
      "  Created: 2025-11-06 15:05:29.974909+00:00\n"
     ]
    }
   ],
   "source": [
    "# Find endpoint by display name\n",
    "endpoints = aiplatform.Endpoint.list(filter=f\"display_name={ENDPOINT_DISPLAY_NAME}\")\n",
    "\n",
    "if not endpoints:\n",
    "    print(f\"‚ùå No endpoint found with display name: {ENDPOINT_DISPLAY_NAME}\")\n",
    "    print(f\"\\nAvailable endpoints:\")\n",
    "    all_endpoints = aiplatform.Endpoint.list()\n",
    "    if all_endpoints:\n",
    "        for ep in all_endpoints:\n",
    "            print(f\"  - {ep.display_name} ({ep.resource_name})\")\n",
    "    else:\n",
    "        print(f\"  No endpoints found in project {PROJECT_ID}\")\n",
    "    \n",
    "    print(f\"\\n‚ö†Ô∏è  Please deploy an endpoint first using one of these notebooks:\")\n",
    "    print(f\"   ‚Ä¢ vertex-ai-endpoint-prebuilt-container.ipynb (recommended)\")\n",
    "    print(f\"   ‚Ä¢ vertex-ai-endpoint-custom-container.ipynb\")\n",
    "    raise ValueError(f\"Endpoint not found: {ENDPOINT_DISPLAY_NAME}\")\n",
    "\n",
    "endpoint = endpoints[0]\n",
    "print(f\"‚úÖ Found endpoint: {endpoint.display_name}\")\n",
    "\n",
    "if len(endpoints) > 1:\n",
    "    print(f\"   ‚ö†Ô∏è  Multiple endpoints found with this name. Using first one.\")\n",
    "\n",
    "print(f\"\\nEndpoint Details:\")\n",
    "print(f\"  Resource name: {endpoint.resource_name}\")\n",
    "print(f\"  Display name: {endpoint.display_name}\")\n",
    "print(f\"  Created: {endpoint.create_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inspect_models",
   "metadata": {},
   "source": [
    "### Inspect Deployed Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "inspect_models_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deployed Models: 1\n",
      "\n",
      "  Model: pytorch-autoencoder\n",
      "  ID: 7796732610071232512\n",
      "  Machine type: n1-standard-4\n",
      "  Replicas: 1 - 4\n",
      "\n",
      "‚úÖ Endpoint is ready for remote model creation\n"
     ]
    }
   ],
   "source": [
    "# Get deployed models on this endpoint\n",
    "deployed_models = endpoint.list_models()\n",
    "\n",
    "if not deployed_models:\n",
    "    print(f\"‚ùå No models deployed to endpoint: {endpoint.display_name}\")\n",
    "    print(f\"\\n‚ö†Ô∏è  Please deploy a model to this endpoint first.\")\n",
    "    raise ValueError(f\"No models deployed to endpoint\")\n",
    "\n",
    "print(f\"Deployed Models: {len(deployed_models)}\")\n",
    "for dm in deployed_models:\n",
    "    print(f\"\\n  Model: {dm.display_name}\")\n",
    "    print(f\"  ID: {dm.id}\")\n",
    "    \n",
    "    # Access machine type through dedicated_resources\n",
    "    if hasattr(dm, 'dedicated_resources') and dm.dedicated_resources:\n",
    "        machine_type = dm.dedicated_resources.machine_spec.machine_type\n",
    "        min_replicas = dm.dedicated_resources.min_replica_count\n",
    "        max_replicas = dm.dedicated_resources.max_replica_count\n",
    "        \n",
    "        print(f\"  Machine type: {machine_type}\")\n",
    "        print(f\"  Replicas: {min_replicas} - {max_replicas}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Endpoint is ready for remote model creation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "test_endpoint",
   "metadata": {},
   "source": [
    "### Test Endpoint with SDK\n",
    "\n",
    "Before creating the BigQuery remote model, validate that the endpoint works and understand its response signature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "get_test_data",
   "metadata": {},
   "source": [
    "#### Get Test Data from BigQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "get_test_data_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 3 test instances\n",
      "\n",
      "Features: ['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount']\n",
      "\n",
      "First instance (30 features):\n",
      "[122959.0, -1.3272968090323798, 0.422904408477484, 1.61750487263453, 2.29119608568876] ...\n"
     ]
    }
   ],
   "source": [
    "# Retrieve a few test records\n",
    "query = f\"\"\"\n",
    "SELECT * EXCEPT(splits, transaction_id, Class)\n",
    "FROM `{PROJECT_ID}.{BQ_DATASET}.{BQ_SOURCE_TABLE}`\n",
    "WHERE splits='TEST'\n",
    "LIMIT 3\n",
    "\"\"\"\n",
    "\n",
    "test_df = bq.query(query).to_dataframe()\n",
    "print(f\"Retrieved {len(test_df)} test instances\")\n",
    "print(f\"\\nFeatures: {list(test_df.columns)}\")\n",
    "\n",
    "# Convert to format expected by endpoint (list of lists)\n",
    "test_instances = test_df.values.tolist()\n",
    "print(f\"\\nFirst instance (30 features):\")\n",
    "print(test_instances[0][:5], \"...\")  # Show first 5 features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "test_prediction",
   "metadata": {},
   "source": [
    "#### Make Test Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "test_prediction_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing endpoint with SDK...\n",
      "\n",
      "‚úÖ Prediction successful!\n",
      "\n",
      "Response structure:\n",
      "  Type: <class 'dict'>\n",
      "  Keys: ['normalized_MSE', 'normalized_MSLE', 'encoded', 'normalized_MAE', 'normalized_reconstruction', 'denormalized_RMSE', 'denormalized_reconstruction', 'denormalized_MSE', 'denormalized_reconstruction_errors', 'denormalized_MAE', 'denormalized_MSLE', 'normalized_RMSE', 'normalized_reconstruction_errors']\n",
      "  Number of fields: 13\n",
      "\n",
      "üìä Prediction output:\n",
      "\n",
      "  üîµ Pre-built Container Response (13 fields)\n",
      "     Anomaly Score (MAE): 62.15\n",
      "     RMSE: 333.5430908203125\n",
      "     MSE: 111250.984375\n",
      "     Encoded (latent): [0.0, 0.0, 0.09415392577648163, 0.0]\n",
      "\n",
      "üí° This endpoint type: prebuilt\n",
      "   Response will have different fields in BigQuery remote model\n"
     ]
    }
   ],
   "source": [
    "# Test with a single instance\n",
    "print(f\"Testing endpoint with SDK...\")\n",
    "response = endpoint.predict(instances=[test_instances[0]])\n",
    "prediction = response.predictions[0]\n",
    "\n",
    "print(f\"\\n‚úÖ Prediction successful!\")\n",
    "print(f\"\\nResponse structure:\")\n",
    "print(f\"  Type: {type(prediction)}\")\n",
    "\n",
    "if isinstance(prediction, dict):\n",
    "    print(f\"  Keys: {list(prediction.keys())}\")\n",
    "    print(f\"  Number of fields: {len(prediction)}\")\n",
    "    \n",
    "    # Show prediction details based on endpoint type\n",
    "    print(f\"\\nüìä Prediction output:\")\n",
    "    \n",
    "    # Check if this is prebuilt (13 fields) or custom (2 fields)\n",
    "    if 'denormalized_MAE' in prediction:\n",
    "        print(f\"\\n  üîµ Pre-built Container Response (13 fields)\")\n",
    "        print(f\"     Anomaly Score (MAE): {prediction['denormalized_MAE']:.2f}\")\n",
    "        print(f\"     RMSE: {prediction.get('denormalized_RMSE', 'N/A')}\")\n",
    "        print(f\"     MSE: {prediction.get('denormalized_MSE', 'N/A')}\")\n",
    "        if 'encoded' in prediction:\n",
    "            print(f\"     Encoded (latent): {prediction['encoded']}\")\n",
    "        ENDPOINT_TYPE = 'prebuilt'\n",
    "    elif 'anomaly_score' in prediction:\n",
    "        print(f\"\\n  üü¢ Custom Container Response (2 fields)\")\n",
    "        print(f\"     Anomaly Score: {prediction['anomaly_score']:.2f}\")\n",
    "        print(f\"     Encoded: {prediction['encoded']}\")\n",
    "        ENDPOINT_TYPE = 'custom'\n",
    "    else:\n",
    "        print(f\"  Unknown response format: {prediction}\")\n",
    "        ENDPOINT_TYPE = 'unknown'\n",
    "    \n",
    "    print(f\"\\nüí° This endpoint type: {ENDPOINT_TYPE}\")\n",
    "    print(f\"   Response will have different fields in BigQuery remote model\")\n",
    "else:\n",
    "    print(f\"  Value: {prediction}\")\n",
    "    ENDPOINT_TYPE = 'unknown'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bq_connection",
   "metadata": {},
   "source": [
    "---\n",
    "## Create BigQuery Cloud Resource Connection\n",
    "\n",
    "BigQuery needs a **Cloud Resource Connection** to securely call Vertex AI endpoints.\n",
    "\n",
    "**How it works:**\n",
    "1. Create connection in BigQuery (tied to specific region)\n",
    "2. Connection gets a dedicated service account\n",
    "3. Grant service account `Vertex AI User` role\n",
    "4. Use connection when creating remote model\n",
    "\n",
    "**Regional Compatibility:**\n",
    "- **Vertex AI Endpoint**: `us-central1` (regional location)\n",
    "- **BigQuery Dataset**: `US` (multi-region location)\n",
    "- **BigQuery Connection**: `US` (multi-region - compatible with both!)\n",
    "\n",
    "The `US` multi-region is compatible with `us-central1` endpoints, so we create the connection in `US` to match the dataset location.\n",
    "\n",
    "**Security:**\n",
    "- Service account has minimal permissions (only what's granted)\n",
    "- Connection can be reused across multiple remote models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "check_connection",
   "metadata": {},
   "source": [
    "### Check for Existing Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "check_connection_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for existing connections in US...\n",
      "\n",
      "Found 8 connection(s) in US:\n",
      "  - applied-genai_bq-advisor\n",
      "    Service Account: bqcx-1026793852137-dyw1@gcp-sa-bigquery-condel.iam.gserviceaccount.com\n",
      "  - applied-genai_bq-metadata\n",
      "    Service Account: bqcx-1026793852137-tqpc@gcp-sa-bigquery-condel.iam.gserviceaccount.com\n",
      "  - applied-genai_embed-feature-classifier\n",
      "    Service Account: bqcx-1026793852137-pdxa@gcp-sa-bigquery-condel.iam.gserviceaccount.com\n",
      "  - bqml_2024\n",
      "    Service Account: bqcx-1026793852137-d2h9@gcp-sa-bigquery-condel.iam.gserviceaccount.com\n",
      "  - bqml_genai-sql\n",
      "    Service Account: bqcx-1026793852137-bmph@gcp-sa-bigquery-condel.iam.gserviceaccount.com\n",
      "  - document-processing\n",
      "    Service Account: bqcx-1026793852137-vk6u@gcp-sa-bigquery-condel.iam.gserviceaccount.com\n",
      "  - frameworks_pytorch_bqml_remote_model\n",
      "    Service Account: bqcx-1026793852137-546t@gcp-sa-bigquery-condel.iam.gserviceaccount.com\n",
      "  - working-with-docai_from-bigquery\n",
      "    Service Account: bqcx-1026793852137-te86@gcp-sa-bigquery-condel.iam.gserviceaccount.com\n",
      "\n",
      "‚úÖ Connection already exists: frameworks_pytorch_bqml_remote_model\n"
     ]
    }
   ],
   "source": [
    "# List existing connections in BigQuery region\n",
    "print(f\"Checking for existing connections in {BQ_REGION}...\\n\")\n",
    "\n",
    "# Construct parent path for listing connections\n",
    "parent = f\"projects/{PROJECT_ID}/locations/{BQ_REGION}\"\n",
    "\n",
    "# List all connections in this location\n",
    "try:\n",
    "    connections = connection_client.list_connections(parent=parent)\n",
    "    \n",
    "    connection_list = list(connections)\n",
    "    \n",
    "    if connection_list:\n",
    "        print(f\"Found {len(connection_list)} connection(s) in {BQ_REGION}:\")\n",
    "        for conn in connection_list:\n",
    "            # Extract connection ID from full resource name\n",
    "            conn_id = conn.name.split('/')[-1]\n",
    "            print(f\"  - {conn_id}\")\n",
    "            print(f\"    Service Account: {conn.cloud_resource.service_account_id}\")\n",
    "    else:\n",
    "        print(f\"No connections found in {BQ_REGION}\")\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    # Check if our connection already exists\n",
    "    connection_exists = any(conn.name.endswith(BQ_CONNECTION_NAME) for conn in connection_list)\n",
    "    \n",
    "    if connection_exists:\n",
    "        print(f\"‚úÖ Connection already exists: {BQ_CONNECTION_NAME}\")\n",
    "    else:\n",
    "        print(f\"Connection '{BQ_CONNECTION_NAME}' not found - will create it\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error listing connections: {e}\")\n",
    "    connection_exists = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "create_connection",
   "metadata": {},
   "source": [
    "### Create Connection (if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "create_connection_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Using existing connection\n"
     ]
    }
   ],
   "source": [
    "if not connection_exists:\n",
    "    print(f\"Creating BigQuery Cloud Resource Connection...\")\n",
    "    print(f\"  Name: {BQ_CONNECTION_NAME}\")\n",
    "    print(f\"  Location: {BQ_REGION}\")\n",
    "    print(f\"  Type: CLOUD_RESOURCE\\n\")\n",
    "    \n",
    "    # Construct parent path\n",
    "    parent = f\"projects/{PROJECT_ID}/locations/{BQ_REGION}\"\n",
    "    \n",
    "    # Create connection object\n",
    "    connection = Connection()\n",
    "    connection.cloud_resource = CloudResourceProperties()\n",
    "    \n",
    "    # Create the connection\n",
    "    try:\n",
    "        created_connection = connection_client.create_connection(\n",
    "            parent=parent,\n",
    "            connection=connection,\n",
    "            connection_id=BQ_CONNECTION_NAME\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úÖ Connection created successfully!\")\n",
    "        print(f\"   Full name: {created_connection.name}\")\n",
    "        print(f\"   Service Account: {created_connection.cloud_resource.service_account_id}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to create connection: {e}\")\n",
    "        raise Exception(f\"Connection creation failed: {e}\")\n",
    "else:\n",
    "    print(f\"‚úÖ Using existing connection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "get_service_account",
   "metadata": {},
   "source": [
    "### Get Connection Service Account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "get_service_account_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving connection service account...\n",
      "\n",
      "‚úÖ Connection details:\n",
      "   Connection: statmike-mlops-349915.US.frameworks_pytorch_bqml_remote_model\n",
      "   Full resource name: projects/1026793852137/locations/us/connections/frameworks_pytorch_bqml_remote_model\n",
      "   Service Account: bqcx-1026793852137-546t@gcp-sa-bigquery-condel.iam.gserviceaccount.com\n",
      "   Creation time: 1762727929877\n",
      "\n",
      "üí° This service account needs Vertex AI User role to call endpoints\n"
     ]
    }
   ],
   "source": [
    "# Get connection details to find service account\n",
    "print(f\"Retrieving connection service account...\\n\")\n",
    "\n",
    "# Construct connection name\n",
    "connection_name = f\"projects/{PROJECT_ID}/locations/{BQ_REGION}/connections/{BQ_CONNECTION_NAME}\"\n",
    "\n",
    "try:\n",
    "    # Get the connection\n",
    "    connection = connection_client.get_connection(name=connection_name)\n",
    "    \n",
    "    # Extract service account\n",
    "    service_account = connection.cloud_resource.service_account_id\n",
    "    \n",
    "    print(f\"‚úÖ Connection details:\")\n",
    "    print(f\"   Connection: {PROJECT_ID}.{BQ_REGION}.{BQ_CONNECTION_NAME}\")\n",
    "    print(f\"   Full resource name: {connection.name}\")\n",
    "    print(f\"   Service Account: {service_account}\")\n",
    "    print(f\"   Creation time: {connection.creation_time}\")\n",
    "    print(f\"\\nüí° This service account needs Vertex AI User role to call endpoints\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to retrieve connection: {e}\")\n",
    "    raise Exception(f\"Could not get service account: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grant_permissions",
   "metadata": {},
   "source": [
    "### Grant Vertex AI User Role\n",
    "\n",
    "The connection's service account needs permission to invoke Vertex AI endpoints.\n",
    "\n",
    "**Required role:** `roles/aiplatform.user`\n",
    "\n",
    "**What this allows:**\n",
    "- Call Vertex AI endpoints for predictions\n",
    "- Access deployed models\n",
    "\n",
    "**What this does NOT allow:**\n",
    "- Create/delete endpoints or models\n",
    "- Modify deployment configurations\n",
    "- Access other Vertex AI resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "grant_permissions_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Granting Vertex AI User role to service account...\n",
      "  Service Account: bqcx-1026793852137-546t@gcp-sa-bigquery-condel.iam.gserviceaccount.com\n",
      "  Role: roles/aiplatform.user\n",
      "  Project: statmike-mlops-349915\n",
      "\n",
      "‚úÖ Granted Vertex AI User role\n",
      "\n",
      "üí° Service account can now:\n",
      "   ‚Ä¢ Call Vertex AI endpoints for predictions\n",
      "   ‚Ä¢ Access deployed models\n",
      "\n",
      "‚ö†Ô∏è  Note: This permission is project-wide, not endpoint-specific\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "print(f\"Granting Vertex AI User role to service account...\")\n",
    "print(f\"  Service Account: {service_account}\")\n",
    "print(f\"  Role: roles/aiplatform.user\")\n",
    "print(f\"  Project: {PROJECT_ID}\\n\")\n",
    "\n",
    "# Service accounts may take a few seconds to propagate\n",
    "# Retry up to 3 times with delays\n",
    "max_retries = 3\n",
    "retry_delay = 10  # seconds\n",
    "\n",
    "for attempt in range(max_retries):\n",
    "    if attempt > 0:\n",
    "        print(f\"‚è≥ Waiting {retry_delay} seconds for service account to propagate...\")\n",
    "        time.sleep(retry_delay)\n",
    "        print(f\"   Retry attempt {attempt + 1}/{max_retries}...\\n\")\n",
    "    \n",
    "    result = subprocess.run(\n",
    "        ['gcloud', 'projects', 'add-iam-policy-binding', PROJECT_ID,\n",
    "         f'--member=serviceAccount:{service_account}',\n",
    "         '--role=roles/aiplatform.user',\n",
    "         '--condition=None'],  # No conditions on this binding\n",
    "        capture_output=True,\n",
    "        text=True\n",
    "    )\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        print(f\"‚úÖ Granted Vertex AI User role\")\n",
    "        print(f\"\\nüí° Service account can now:\")\n",
    "        print(f\"   ‚Ä¢ Call Vertex AI endpoints for predictions\")\n",
    "        print(f\"   ‚Ä¢ Access deployed models\")\n",
    "        print(f\"\\n‚ö†Ô∏è  Note: This permission is project-wide, not endpoint-specific\")\n",
    "        break\n",
    "    elif \"already exists\" in result.stderr or \"Policy already contains role\" in result.stderr:\n",
    "        print(f\"‚úÖ Service account already has Vertex AI User role\")\n",
    "        break\n",
    "    elif \"does not exist\" in result.stderr and attempt < max_retries - 1:\n",
    "        # Service account not yet propagated, continue to retry\n",
    "        continue\n",
    "    else:\n",
    "        # Final attempt failed\n",
    "        print(f\"‚ùå Failed to grant role: {result.stderr}\")\n",
    "        print(f\"\\nüí° You may need to grant this manually:\")\n",
    "        print(f\"   1. Go to Cloud Console > IAM\")\n",
    "        print(f\"   2. Click '+ Grant Access'\")\n",
    "        print(f\"   3. Principal: {service_account}\")\n",
    "        print(f\"   4. Role: Vertex AI User\")\n",
    "        print(f\"   5. Click Save\")\n",
    "        \n",
    "        # Don't raise exception, allow user to grant manually and continue\n",
    "        print(f\"\\n‚ö†Ô∏è  Continuing anyway - you may need to grant permission manually before creating remote model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "create_remote_model",
   "metadata": {},
   "source": [
    "---\n",
    "## Create BigQuery ML Remote Model\n",
    "\n",
    "Now that the connection is configured, create the BigQuery ML remote model that points to the Vertex AI endpoint.\n",
    "\n",
    "**Endpoint Type Handling:**\n",
    "\n",
    "The two endpoint types have different response signatures, so the remote model needs different input/output specifications:\n",
    "\n",
    "**Pre-built Container** (13 output fields):\n",
    "```sql\n",
    "OUTPUT (\n",
    "  denormalized_MAE FLOAT64,\n",
    "  denormalized_RMSE FLOAT64,\n",
    "  denormalized_MSE FLOAT64,\n",
    "  denormalized_MSLE FLOAT64,\n",
    "  normalized_MAE FLOAT64,\n",
    "  ...\n",
    ")\n",
    "```\n",
    "\n",
    "**Custom Container** (2 output fields):\n",
    "```sql\n",
    "OUTPUT (\n",
    "  anomaly_score FLOAT64,\n",
    "  encoded ARRAY<FLOAT64>\n",
    ")\n",
    "```\n",
    "\n",
    "This section will:\n",
    "1. Detect which endpoint type you're using\n",
    "2. Build the appropriate CREATE MODEL statement\n",
    "3. Create the remote model with correct input/output schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "define_model_schema",
   "metadata": {},
   "source": [
    "### Define Model Input/Output Schema\n",
    "\n",
    "Based on the endpoint type detected earlier, define the appropriate schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "define_model_schema_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input schema (30 features):\n",
      "  Time FLOAT64, V1 FLOAT64, V2 FLOAT64, V3 FLOAT64, V4 FLOAT64, V5 FLOAT64, V6 FLOAT64, V7 FLOAT64, V8...\n",
      "\n",
      "üîµ Detected Pre-built Container Endpoint\n",
      "   Output: 13 fields (full model diagnostics)\n",
      "\n",
      "Output schema:\n",
      "\n",
      "        denormalized_MAE FLOAT64,\n",
      "        denormalized_RMSE FLOAT64,\n",
      "        denormalized_MSE FLOAT64,\n",
      "        denormalized_MSLE FLOAT64,\n",
      "        normalized_MAE FLOAT64,\n",
      "        normalized_RMSE FLOAT64,\n",
      "        normalized_MSE FLOAT64,\n",
      "        normalized_MSLE FLOAT64,\n",
      "        encoded ARRAY<FLOAT64>,\n",
      "        normalized_reconstruction ARRAY<FLOAT64>,\n",
      "        normalized_reconstruction_errors ARRAY<FLOAT64>,\n",
      "        denormalized_reconstruction ARRAY<FLOAT64>,\n",
      "        denormalized_reconstruction_errors ARRAY<FLOAT64>\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "# Input schema (same for both endpoint types - 30 features)\n",
    "input_features = list(test_df.columns)\n",
    "input_schema = \", \".join([f\"{col} FLOAT64\" for col in input_features])\n",
    "\n",
    "print(f\"Input schema ({len(input_features)} features):\")\n",
    "print(f\"  {input_schema[:100]}...\\n\")\n",
    "\n",
    "# Output schema depends on endpoint type\n",
    "if ENDPOINT_TYPE == 'prebuilt':\n",
    "    print(f\"üîµ Detected Pre-built Container Endpoint\")\n",
    "    print(f\"   Output: 13 fields (full model diagnostics)\\n\")\n",
    "    \n",
    "    output_schema = \"\"\"\n",
    "        denormalized_MAE FLOAT64,\n",
    "        denormalized_RMSE FLOAT64,\n",
    "        denormalized_MSE FLOAT64,\n",
    "        denormalized_MSLE FLOAT64,\n",
    "        normalized_MAE FLOAT64,\n",
    "        normalized_RMSE FLOAT64,\n",
    "        normalized_MSE FLOAT64,\n",
    "        normalized_MSLE FLOAT64,\n",
    "        encoded ARRAY<FLOAT64>,\n",
    "        normalized_reconstruction ARRAY<FLOAT64>,\n",
    "        normalized_reconstruction_errors ARRAY<FLOAT64>,\n",
    "        denormalized_reconstruction ARRAY<FLOAT64>,\n",
    "        denormalized_reconstruction_errors ARRAY<FLOAT64>\n",
    "    \"\"\"\n",
    "    \n",
    "elif ENDPOINT_TYPE == 'custom':\n",
    "    print(f\"üü¢ Detected Custom Container Endpoint\")\n",
    "    print(f\"   Output: 2 fields (simplified)\\n\")\n",
    "    \n",
    "    output_schema = \"\"\"\n",
    "        anomaly_score FLOAT64,\n",
    "        encoded ARRAY<FLOAT64>\n",
    "    \"\"\"\n",
    "    \n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è  Unknown endpoint type: {ENDPOINT_TYPE}\")\n",
    "    print(f\"   Defaulting to prebuilt schema (13 fields)\")\n",
    "    print(f\"\\nüí° If predictions fail, check the endpoint response structure\")\n",
    "    \n",
    "    # Default to prebuilt schema\n",
    "    output_schema = \"\"\"\n",
    "        denormalized_MAE FLOAT64,\n",
    "        denormalized_RMSE FLOAT64,\n",
    "        denormalized_MSE FLOAT64,\n",
    "        denormalized_MSLE FLOAT64,\n",
    "        normalized_MAE FLOAT64,\n",
    "        normalized_RMSE FLOAT64,\n",
    "        normalized_MSE FLOAT64,\n",
    "        normalized_MSLE FLOAT64,\n",
    "        encoded ARRAY<FLOAT64>,\n",
    "        normalized_reconstruction ARRAY<FLOAT64>,\n",
    "        normalized_reconstruction_errors ARRAY<FLOAT64>,\n",
    "        denormalized_reconstruction ARRAY<FLOAT64>,\n",
    "        denormalized_reconstruction_errors ARRAY<FLOAT64>\n",
    "    \"\"\"\n",
    "\n",
    "print(f\"Output schema:\")\n",
    "print(output_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "create_model_sql",
   "metadata": {},
   "source": [
    "### Create Remote Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "create_model_sql_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating BigQuery ML remote model...\n",
      "  Model: frameworks.frameworks_pytorch_bqml_remote_model\n",
      "  Connection: statmike-mlops-349915.US.frameworks_pytorch_bqml_remote_model\n",
      "  Endpoint: projects/1026793852137/locations/us-central1/endpoints/2741468416626917376\n",
      "  Endpoint region: us-central1\n",
      "  Connection region: US\n",
      "\n",
      "üí° Model uses ARRAY input column 'input_features' (BigQuery will wrap in instances key)\n",
      "\n",
      "‚úÖ Remote model created successfully!\n",
      "\n",
      "üìä Model details:\n",
      "   Full name: frameworks.frameworks_pytorch_bqml_remote_model\n",
      "   Endpoint type: prebuilt\n",
      "   Input: ARRAY<FLOAT64> (30 features)\n",
      "   Output fields: 13\n",
      "\n",
      "üí° View in BigQuery Console:\n",
      "   https://console.cloud.google.com/bigquery?project=statmike-mlops-349915&ws=!1m5!1m4!4m3!1sstatmike-mlops-349915!2sframeworks!3sframeworks_pytorch_bqml_remote_model\n"
     ]
    }
   ],
   "source": [
    "# Remote model name\n",
    "REMOTE_MODEL_NAME = f\"{BQ_DATASET}.{SERIES.replace('-', '_')}_{EXPERIMENT.replace('-', '_')}\"\n",
    "\n",
    "print(f\"Creating BigQuery ML remote model...\")\n",
    "print(f\"  Model: {REMOTE_MODEL_NAME}\")\n",
    "print(f\"  Connection: {PROJECT_ID}.{BQ_REGION}.{BQ_CONNECTION_NAME}\")\n",
    "print(f\"  Endpoint: {endpoint.resource_name}\")\n",
    "print(f\"  Endpoint region: {REGION}\")\n",
    "print(f\"  Connection region: {BQ_REGION}\\n\")\n",
    "\n",
    "# BigQuery ML will wrap the input in {\"instances\": [<your_data>]}\n",
    "# Our handler expects instances to be a list of lists: [[val1, val2, ...]]\n",
    "# So we need to pass each row as a list directly\n",
    "create_model_query = f\"\"\"\n",
    "CREATE OR REPLACE MODEL `{REMOTE_MODEL_NAME}`\n",
    "    INPUT (input_features ARRAY<FLOAT64>)\n",
    "    OUTPUT ({output_schema})\n",
    "    REMOTE WITH CONNECTION `{PROJECT_ID}.{BQ_REGION}.{BQ_CONNECTION_NAME}`\n",
    "    OPTIONS(\n",
    "        endpoint = 'https://{REGION}-aiplatform.googleapis.com/v1/{endpoint.resource_name}'\n",
    "    )\n",
    "\"\"\"\n",
    "\n",
    "print(f\"üí° Model uses ARRAY input column 'input_features' (BigQuery will wrap in instances key)\\n\")\n",
    "\n",
    "# Execute query\n",
    "job = bq.query(query=create_model_query)\n",
    "job.result()\n",
    "\n",
    "if job.state == 'DONE':\n",
    "    print(f\"‚úÖ Remote model created successfully!\")\n",
    "    print(f\"\\nüìä Model details:\")\n",
    "    print(f\"   Full name: {REMOTE_MODEL_NAME}\")\n",
    "    print(f\"   Endpoint type: {ENDPOINT_TYPE}\")\n",
    "    print(f\"   Input: ARRAY<FLOAT64> (30 features)\")\n",
    "    print(f\"   Output fields: {len([x for x in output_schema.split(',') if x.strip()])}\")\n",
    "    \n",
    "    print(f\"\\nüí° View in BigQuery Console:\")\n",
    "    print(f\"   https://console.cloud.google.com/bigquery?project={PROJECT_ID}&ws=!1m5!1m4!4m3!1s{PROJECT_ID}!2s{BQ_DATASET}!3s{REMOTE_MODEL_NAME.split('.')[-1]}\")\n",
    "else:\n",
    "    print(f\"‚ùå Model creation failed\")\n",
    "    print(f\"   State: {job.state}\")\n",
    "    if job.errors:\n",
    "        print(f\"   Errors: {job.errors}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "test_remote_model",
   "metadata": {},
   "source": [
    "---\n",
    "## Test Remote Model with ML.PREDICT\n",
    "\n",
    "Now that the remote model is created, test it with SQL using `ML.PREDICT()`.\n",
    "\n",
    "We'll demonstrate three progressively complex examples:\n",
    "1. **Simple prediction**: Get full prediction output\n",
    "2. **Extract specific fields**: Work with STRUCT to get individual values\n",
    "3. **Apply business logic**: Use predictions for decision-making"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "example1_simple",
   "metadata": {},
   "source": [
    "### Example 1: Simple Prediction\n",
    "\n",
    "Get predictions for a single test record with full output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "example1_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running simple ML.PREDICT query...\n",
      "\n",
      "‚úÖ Prediction successful!\n",
      "\n",
      "üìä Result columns (15 total):\n",
      "   ['denormalized_MAE', 'denormalized_RMSE', 'denormalized_MSE', 'denormalized_MSLE', 'normalized_MAE', 'normalized_RMSE', 'normalized_MSE', 'normalized_MSLE', 'encoded', 'normalized_reconstruction', 'normalized_reconstruction_errors', 'denormalized_reconstruction', 'denormalized_reconstruction_errors', 'remote_model_status', 'input_features']\n",
      "\n",
      "First prediction:\n",
      "   denormalized_MAE  denormalized_RMSE  denormalized_MSE  denormalized_MSLE  normalized_MAE  normalized_RMSE  normalized_MSE  normalized_MSLE encoded normalized_reconstruction normalized_reconstruction_errors denormalized_reconstruction denormalized_reconstruction_errors       remote_model_status                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            input_features\n",
      "0               NaN                NaN               NaN                NaN             NaN              NaN             NaN              NaN      []                        []                               []                          []                                 []  INTERNAL error occurred.  [122959.0, -1.3272968090323798, 0.422904408477484, 1.61750487263453, 2.29119608568876, 2.37505507675092, 0.411734608148703, 0.213516912086352, 0.42474305658616796, -1.8096243240512202, 0.563424163577428, -0.215715613427969, 0.255745374601212, 0.14372086809316198, 0.0664174538762353, -2.19269325676012, 1.5000437460373202, -1.48305134433897, -0.0973547276746839, -1.36598161864692, 0.0685717012577449, -0.337965984584885, -1.4619590513124, 0.19260414415618501, 0.0682811106065862, -0.24572504736969603, -0.697654195893893, 0.0382157166420934, 0.15005927811162398, 0.0]\n",
      "\n",
      "üîµ Pre-built Container Output:\n",
      "   Anomaly Score (MAE): nan\n",
      "   RMSE: nan\n",
      "   MSE: nan\n",
      "   Encoded (latent): []...\n",
      "\n",
      "üí° Notice: BigQuery wraps array in 'instances' key automatically for Vertex AI endpoints\n"
     ]
    }
   ],
   "source": [
    "print(f\"Running simple ML.PREDICT query...\\n\")\n",
    "\n",
    "# Build feature array - combine all features into a single ARRAY column\n",
    "feature_list = \", \".join(input_features)\n",
    "\n",
    "query = f\"\"\"\n",
    "SELECT *\n",
    "FROM ML.PREDICT(\n",
    "    MODEL `{REMOTE_MODEL_NAME}`,\n",
    "    (\n",
    "        SELECT ARRAY[{feature_list}] AS input_features\n",
    "        FROM `{PROJECT_ID}.{BQ_DATASET}.{BQ_SOURCE_TABLE}`\n",
    "        WHERE splits = 'TEST'\n",
    "        LIMIT 1\n",
    "    )\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "result_df = bq.query(query).to_dataframe()\n",
    "\n",
    "print(f\"‚úÖ Prediction successful!\")\n",
    "print(f\"\\nüìä Result columns ({len(result_df.columns)} total):\")\n",
    "print(f\"   {list(result_df.columns)}\\n\")\n",
    "\n",
    "# Show first row\n",
    "print(f\"First prediction:\")\n",
    "print(result_df.head(1).to_string())\n",
    "\n",
    "# Extract key prediction fields based on endpoint type\n",
    "if ENDPOINT_TYPE == 'prebuilt':\n",
    "    print(f\"\\nüîµ Pre-built Container Output:\")\n",
    "    print(f\"   Anomaly Score (MAE): {result_df['denormalized_MAE'].iloc[0]:.2f}\")\n",
    "    print(f\"   RMSE: {result_df['denormalized_RMSE'].iloc[0]:.2f}\")\n",
    "    print(f\"   MSE: {result_df['denormalized_MSE'].iloc[0]:.2f}\")\n",
    "    print(f\"   Encoded (latent): {result_df['encoded'].iloc[0][:4]}...\")\n",
    "elif ENDPOINT_TYPE == 'custom':\n",
    "    print(f\"\\nüü¢ Custom Container Output:\")\n",
    "    print(f\"   Anomaly Score: {result_df['anomaly_score'].iloc[0]:.2f}\")\n",
    "    print(f\"   Encoded: {result_df['encoded'].iloc[0][:4]}...\")\n",
    "\n",
    "print(f\"\\nüí° Notice: BigQuery wraps array in 'instances' key automatically for Vertex AI endpoints\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "example2_extract",
   "metadata": {},
   "source": [
    "### Example 2: Extract Specific Fields\n",
    "\n",
    "In practice, you often only need specific prediction fields. Show how to extract just the anomaly score and a few key metrics.\n",
    "\n",
    "**Why this matters:**\n",
    "- Cleaner query results (fewer columns)\n",
    "- Better performance (less data transferred)\n",
    "- Easier to join with other tables\n",
    "- Simpler downstream processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "example2_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Running ML.PREDICT with field extraction...\\n\")\n",
    "\n",
    "# Build feature array - combine all features into a single ARRAY column\n",
    "feature_list = \", \".join(input_features)\n",
    "\n",
    "# Query varies by endpoint type\n",
    "if ENDPOINT_TYPE == 'prebuilt':\n",
    "    query = f\"\"\"\n",
    "    SELECT\n",
    "        -- Extracted prediction fields\n",
    "        denormalized_MAE as anomaly_score,\n",
    "        denormalized_RMSE as rmse,\n",
    "        denormalized_MSE as mse,\n",
    "        encoded as latent_encoding\n",
    "    FROM ML.PREDICT(\n",
    "        MODEL `{REMOTE_MODEL_NAME}`,\n",
    "        (\n",
    "            SELECT ARRAY[{feature_list}] AS input_features\n",
    "            FROM `{PROJECT_ID}.{BQ_DATASET}.{BQ_SOURCE_TABLE}`\n",
    "            WHERE splits = 'TEST'\n",
    "            LIMIT 5\n",
    "        )\n",
    "    )\n",
    "    \"\"\"\n",
    "elif ENDPOINT_TYPE == 'custom':\n",
    "    query = f\"\"\"\n",
    "    SELECT\n",
    "        -- Extracted prediction fields\n",
    "        anomaly_score,\n",
    "        encoded as latent_encoding\n",
    "    FROM ML.PREDICT(\n",
    "        MODEL `{REMOTE_MODEL_NAME}`,\n",
    "        (\n",
    "            SELECT ARRAY[{feature_list}] AS input_features\n",
    "            FROM `{PROJECT_ID}.{BQ_DATASET}.{BQ_SOURCE_TABLE}`\n",
    "            WHERE splits = 'TEST'\n",
    "            LIMIT 5\n",
    "        )\n",
    "    )\n",
    "    \"\"\"\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è  Skipping example - unknown endpoint type\")\n",
    "    query = None\n",
    "\n",
    "if query:\n",
    "    result_df = bq.query(query).to_dataframe()\n",
    "\n",
    "    print(f\"‚úÖ Prediction successful!\")\n",
    "    print(f\"\\nüìä Extracted fields ({len(result_df.columns)} columns):\")\n",
    "    print(f\"   {list(result_df.columns)}\\n\")\n",
    "\n",
    "    print(f\"Results:\")\n",
    "    print(result_df.to_string())\n",
    "\n",
    "    print(f\"\\nüí° Benefits of field extraction:\")\n",
    "    print(f\"   ‚Ä¢ Only get data you need\")\n",
    "    print(f\"   ‚Ä¢ Rename fields for clarity (denormalized_MAE ‚Üí anomaly_score)\")\n",
    "    print(f\"   ‚Ä¢ Easier to use in downstream queries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "example3_logic",
   "metadata": {},
   "source": [
    "### Example 3: Apply Business Logic\n",
    "\n",
    "Use predictions to make decisions in SQL. This example:\n",
    "- Classifies transactions as normal or anomaly based on threshold\n",
    "- Adds risk levels (low, medium, high)\n",
    "- Shows how to use predictions for filtering and grouping\n",
    "\n",
    "**Real-world use case:**\n",
    "- Fraud detection: Flag transactions above anomaly threshold\n",
    "- Quality control: Identify defective products\n",
    "- Customer segmentation: Group by latent encoding similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "example3_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Running ML.PREDICT with business logic...\\n\")\n",
    "\n",
    "# Define anomaly thresholds (these would be tuned based on validation data)\n",
    "THRESHOLD_MEDIUM = 100.0  # Medium risk\n",
    "THRESHOLD_HIGH = 200.0    # High risk\n",
    "\n",
    "print(f\"Anomaly thresholds:\")\n",
    "print(f\"  Medium risk: > {THRESHOLD_MEDIUM}\")\n",
    "print(f\"  High risk: > {THRESHOLD_HIGH}\\n\")\n",
    "\n",
    "# Build feature array - combine all features into a single ARRAY column\n",
    "feature_list = \", \".join(input_features)\n",
    "\n",
    "# Query varies by endpoint type\n",
    "if ENDPOINT_TYPE == 'prebuilt':\n",
    "    anomaly_field = 'denormalized_MAE'\n",
    "elif ENDPOINT_TYPE == 'custom':\n",
    "    anomaly_field = 'anomaly_score'\n",
    "else:\n",
    "    anomaly_field = 'denormalized_MAE'  # Default\n",
    "\n",
    "query = f\"\"\"\n",
    "SELECT\n",
    "    {anomaly_field} as anomaly_score,\n",
    "\n",
    "    -- Business logic: Classify transaction\n",
    "    CASE\n",
    "        WHEN {anomaly_field} > {THRESHOLD_HIGH} THEN 'ANOMALY - HIGH RISK'\n",
    "        WHEN {anomaly_field} > {THRESHOLD_MEDIUM} THEN 'ANOMALY - MEDIUM RISK'\n",
    "        ELSE 'NORMAL'\n",
    "    END as risk_category,\n",
    "\n",
    "    -- Binary flag for filtering\n",
    "    CASE WHEN {anomaly_field} > {THRESHOLD_MEDIUM} THEN 1 ELSE 0 END as is_anomaly,\n",
    "\n",
    "    -- Risk score (0-100)\n",
    "    LEAST(100, CAST({anomaly_field} / {THRESHOLD_HIGH} * 100 AS INT64)) as risk_score\n",
    "\n",
    "FROM ML.PREDICT(\n",
    "    MODEL `{REMOTE_MODEL_NAME}`,\n",
    "    (\n",
    "        SELECT ARRAY[{feature_list}] AS input_features\n",
    "        FROM `{PROJECT_ID}.{BQ_DATASET}.{BQ_SOURCE_TABLE}`\n",
    "        WHERE splits = 'TEST'\n",
    "        LIMIT 100\n",
    "    )\n",
    ")\n",
    "ORDER BY anomaly_score DESC\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "\n",
    "result_df = bq.query(query).to_dataframe()\n",
    "\n",
    "print(f\"‚úÖ Prediction with business logic successful!\")\n",
    "print(f\"\\nüìä Top 10 highest risk transactions:\\n\")\n",
    "print(result_df.to_string())\n",
    "\n",
    "# Summary statistics\n",
    "print(f\"\\nüìà Risk Distribution (from 100 test transactions):\")\n",
    "summary_query = f\"\"\"\n",
    "SELECT\n",
    "    CASE\n",
    "        WHEN {anomaly_field} > {THRESHOLD_HIGH} THEN 'ANOMALY - HIGH RISK'\n",
    "        WHEN {anomaly_field} > {THRESHOLD_MEDIUM} THEN 'ANOMALY - MEDIUM RISK'\n",
    "        ELSE 'NORMAL'\n",
    "    END as risk_category,\n",
    "    COUNT(*) as count,\n",
    "    ROUND(AVG({anomaly_field}), 2) as avg_anomaly_score,\n",
    "    ROUND(MIN({anomaly_field}), 2) as min_anomaly_score,\n",
    "    ROUND(MAX({anomaly_field}), 2) as max_anomaly_score\n",
    "FROM ML.PREDICT(\n",
    "    MODEL `{REMOTE_MODEL_NAME}`,\n",
    "    (\n",
    "        SELECT ARRAY[{feature_list}] AS input_features\n",
    "        FROM `{PROJECT_ID}.{BQ_DATASET}.{BQ_SOURCE_TABLE}`\n",
    "        WHERE splits = 'TEST'\n",
    "        LIMIT 100\n",
    "    )\n",
    ")\n",
    "GROUP BY risk_category\n",
    "ORDER BY avg_anomaly_score DESC\n",
    "\"\"\"\n",
    "\n",
    "summary_df = bq.query(summary_query).to_dataframe()\n",
    "print(summary_df.to_string())\n",
    "\n",
    "print(f\"\\nüí° Use cases for business logic:\")\n",
    "print(f\"   ‚Ä¢ Filter: WHERE is_anomaly = 1 (only anomalies)\")\n",
    "print(f\"   ‚Ä¢ Route: Send high-risk transactions to manual review\")\n",
    "print(f\"   ‚Ä¢ Alert: Trigger notifications for ANOMALY - HIGH RISK\")\n",
    "print(f\"   ‚Ä¢ Dashboard: GROUP BY risk_category for visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "batch_scoring",
   "metadata": {},
   "source": [
    "---\n",
    "## Batch Scoring: Create Enriched Table\n",
    "\n",
    "One of the most powerful use cases for BigQuery ML remote models is **batch scoring** - scoring large datasets and persisting results to tables.\n",
    "\n",
    "**Why create prediction tables:**\n",
    "- **Performance**: Pre-compute predictions once, query many times\n",
    "- **Cost**: Avoid repeated endpoint calls for the same data\n",
    "- **Joining**: Easily join predictions with other tables\n",
    "- **Tracking**: Keep historical predictions for analysis\n",
    "- **Sharing**: Give SQL analysts access to predictions\n",
    "\n",
    "**This section will:**\n",
    "1. Score all TEST transactions (could be millions of rows)\n",
    "2. Create enriched table with predictions + business logic\n",
    "3. Show how to query the predictions table\n",
    "4. Demonstrate joining predictions with original data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "create_predictions_table",
   "metadata": {},
   "source": [
    "### Create Predictions Table\n",
    "\n",
    "This creates a table with:\n",
    "- Original transaction features\n",
    "- Anomaly scores from the model\n",
    "- Business logic classifications (risk levels)\n",
    "- Timestamp for tracking when predictions were made"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create_predictions_table_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Creating predictions table with batch scoring...\")\n",
    "print(f\"  Source: {PROJECT_ID}.{BQ_DATASET}.{BQ_SOURCE_TABLE} (TEST split)\")\n",
    "print(f\"  Target: {PROJECT_ID}.{BQ_DATASET}.{BQ_PREDICTIONS_TABLE}\\n\")\n",
    "\n",
    "# Build feature array - combine all features into a single ARRAY column\n",
    "feature_list = \", \".join(input_features)\n",
    "\n",
    "# Query varies by endpoint type\n",
    "if ENDPOINT_TYPE == 'prebuilt':\n",
    "    anomaly_field = 'denormalized_MAE'\n",
    "    extra_fields = ''',\n",
    "        pred.denormalized_RMSE as rmse,\n",
    "        pred.denormalized_MSE as mse,\n",
    "        pred.denormalized_MSLE as msle,\n",
    "        pred.encoded as latent_encoding'''\n",
    "elif ENDPOINT_TYPE == 'custom':\n",
    "    anomaly_field = 'anomaly_score'\n",
    "    extra_fields = ''',\n",
    "        pred.encoded as latent_encoding'''\n",
    "else:\n",
    "    anomaly_field = 'denormalized_MAE'\n",
    "    extra_fields = ''\n",
    "\n",
    "create_table_query = f\"\"\"\n",
    "CREATE OR REPLACE TABLE `{PROJECT_ID}.{BQ_DATASET}.{BQ_PREDICTIONS_TABLE}` AS\n",
    "SELECT\n",
    "    -- Original data (with transaction_id and Class for validation)\n",
    "    orig.transaction_id,\n",
    "    orig.Class as actual_label,\n",
    "    orig.Time,\n",
    "    orig.Amount,\n",
    "    orig.V1, orig.V2, orig.V3, orig.V4, orig.V5,\n",
    "\n",
    "    -- Predictions\n",
    "    pred.{anomaly_field} as anomaly_score{extra_fields},\n",
    "\n",
    "    -- Business logic\n",
    "    CASE\n",
    "        WHEN pred.{anomaly_field} > {THRESHOLD_HIGH} THEN 'ANOMALY - HIGH RISK'\n",
    "        WHEN pred.{anomaly_field} > {THRESHOLD_MEDIUM} THEN 'ANOMALY - MEDIUM RISK'\n",
    "        ELSE 'NORMAL'\n",
    "    END as risk_category,\n",
    "\n",
    "    CASE WHEN pred.{anomaly_field} > {THRESHOLD_MEDIUM} THEN 1 ELSE 0 END as is_anomaly,\n",
    "\n",
    "    LEAST(100, CAST(pred.{anomaly_field} / {THRESHOLD_HIGH} * 100 AS INT64)) as risk_score,\n",
    "\n",
    "    -- Metadata\n",
    "    CURRENT_TIMESTAMP() as prediction_timestamp,\n",
    "    '{ENDPOINT_TYPE}' as endpoint_type,\n",
    "    '{REMOTE_MODEL_NAME}' as model_name\n",
    "\n",
    "FROM (\n",
    "    -- Get predictions from remote model\n",
    "    SELECT *\n",
    "    FROM ML.PREDICT(\n",
    "        MODEL `{REMOTE_MODEL_NAME}`,\n",
    "        (\n",
    "            SELECT ARRAY[{feature_list}] AS input_features\n",
    "            FROM `{PROJECT_ID}.{BQ_DATASET}.{BQ_SOURCE_TABLE}`\n",
    "            WHERE splits = 'TEST'\n",
    "        )\n",
    "    )\n",
    ") pred\n",
    "JOIN (\n",
    "    -- Join back original data to get transaction_id and Class\n",
    "    SELECT *\n",
    "    FROM `{PROJECT_ID}.{BQ_DATASET}.{BQ_SOURCE_TABLE}`\n",
    "    WHERE splits = 'TEST'\n",
    ") orig\n",
    "ON pred.input_features[OFFSET(0)] = orig.Time  -- Match on first element (Time)\n",
    "   AND pred.input_features[OFFSET(29)] = orig.Amount  -- Match on last element (Amount)\n",
    "\"\"\"\n",
    "\n",
    "print(f\"Executing CREATE TABLE query...\")\n",
    "print(f\"‚è±Ô∏è  This may take a few minutes depending on data size...\\n\")\n",
    "\n",
    "job = bq.query(query=create_table_query)\n",
    "job.result()\n",
    "\n",
    "if job.state == 'DONE':\n",
    "    print(f\"‚úÖ Predictions table created successfully!\")\n",
    "\n",
    "    # Get table stats\n",
    "    table = bq.get_table(f\"{PROJECT_ID}.{BQ_DATASET}.{BQ_PREDICTIONS_TABLE}\")\n",
    "\n",
    "    print(f\"\\nüìä Table details:\")\n",
    "    print(f\"   Table: {PROJECT_ID}.{BQ_DATASET}.{BQ_PREDICTIONS_TABLE}\")\n",
    "    print(f\"   Rows: {table.num_rows:,}\")\n",
    "    print(f\"   Size: {table.num_bytes / 1024 / 1024:.2f} MB\")\n",
    "    print(f\"   Columns: {len(table.schema)}\")\n",
    "\n",
    "    print(f\"\\nüí° View in BigQuery Console:\")\n",
    "    print(f\"   https://console.cloud.google.com/bigquery?project={PROJECT_ID}&ws=!1m5!1m4!4m3!1s{PROJECT_ID}!2s{BQ_DATASET}!3s{BQ_PREDICTIONS_TABLE}\")\n",
    "else:\n",
    "    print(f\"‚ùå Table creation failed\")\n",
    "    print(f\"   State: {job.state}\")\n",
    "    if job.errors:\n",
    "        print(f\"   Errors: {job.errors}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "query_predictions",
   "metadata": {},
   "source": [
    "### Query Predictions Table\n",
    "\n",
    "Now that predictions are in a table, you can query them like any other BigQuery table - no endpoint calls needed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "query_predictions_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Querying predictions table...\\n\")\n",
    "\n",
    "# Example 1: Top anomalies\n",
    "query = f\"\"\"\n",
    "SELECT \n",
    "    transaction_id,\n",
    "    actual_label,\n",
    "    anomaly_score,\n",
    "    risk_category,\n",
    "    risk_score,\n",
    "    Amount,\n",
    "    prediction_timestamp\n",
    "FROM `{PROJECT_ID}.{BQ_DATASET}.{BQ_PREDICTIONS_TABLE}`\n",
    "ORDER BY anomaly_score DESC\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "\n",
    "top_anomalies = bq.query(query).to_dataframe()\n",
    "print(f\"üìä Top 10 Anomalies:\\n\")\n",
    "display(top_anomalies)\n",
    "\n",
    "# Example 2: Risk distribution\n",
    "print(f\"\\nüìà Risk Category Distribution:\\n\")\n",
    "query = f\"\"\"\n",
    "SELECT \n",
    "    risk_category,\n",
    "    COUNT(*) as count,\n",
    "    ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER(), 2) as percentage,\n",
    "    ROUND(AVG(anomaly_score), 2) as avg_anomaly_score\n",
    "FROM `{PROJECT_ID}.{BQ_DATASET}.{BQ_PREDICTIONS_TABLE}`\n",
    "GROUP BY risk_category\n",
    "ORDER BY avg_anomaly_score DESC\n",
    "\"\"\"\n",
    "\n",
    "risk_dist = bq.query(query).to_dataframe()\n",
    "display(risk_dist)\n",
    "\n",
    "# Example 3: Model performance check (actual vs predicted)\n",
    "print(f\"\\nüéØ Model Performance (Actual Fraud vs Predicted Anomaly):\\n\")\n",
    "query = f\"\"\"\n",
    "SELECT \n",
    "    actual_label,\n",
    "    is_anomaly,\n",
    "    COUNT(*) as count\n",
    "FROM `{PROJECT_ID}.{BQ_DATASET}.{BQ_PREDICTIONS_TABLE}`\n",
    "GROUP BY actual_label, is_anomaly\n",
    "ORDER BY actual_label, is_anomaly\n",
    "\"\"\"\n",
    "\n",
    "performance = bq.query(query).to_dataframe()\n",
    "display(performance)\n",
    "\n",
    "print(f\"\\nüí° Benefits of predictions table:\")\n",
    "print(f\"   ‚ö° Fast queries (no endpoint calls)\")\n",
    "print(f\"   üí∞ Cost-effective (predictions cached)\")\n",
    "print(f\"   üîÑ Joinable with other tables\")\n",
    "print(f\"   üìä Ready for BI tools and dashboards\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "schedule_hint",
   "metadata": {},
   "source": [
    "### Scheduling Batch Predictions\n",
    "\n",
    "For production use cases, you can schedule this table creation to run automatically:\n",
    "\n",
    "**Option 1: BigQuery Scheduled Queries**\n",
    "1. Go to BigQuery Console\n",
    "2. Click \"Schedule\" button after writing query\n",
    "3. Set frequency (hourly, daily, weekly)\n",
    "4. Predictions refresh automatically\n",
    "\n",
    "**Option 2: Cloud Scheduler + Workflows**\n",
    "1. Create workflow that runs CREATE TABLE query\n",
    "2. Schedule with Cloud Scheduler\n",
    "3. Add error handling and notifications\n",
    "\n",
    "**Example use case:**\n",
    "```sql\n",
    "-- Daily scheduled query (runs at 2 AM)\n",
    "CREATE OR REPLACE TABLE `predictions.daily_fraud_scores` AS\n",
    "SELECT *\n",
    "FROM ML.PREDICT(\n",
    "    MODEL `models.fraud_detector`,\n",
    "    (SELECT * FROM `transactions.daily` WHERE date = CURRENT_DATE())\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advanced_topics",
   "metadata": {},
   "source": [
    "---\n",
    "## Advanced Topics\n",
    "\n",
    "This section discusses advanced capabilities and optimizations for BigQuery ML remote models. These topics are presented conceptually without code implementation.\n",
    "\n",
    "### Model Explainability with ML.EXPLAIN\n",
    "\n",
    "BigQuery ML supports **model explainability** for some remote models using `ML.EXPLAIN()`:\n",
    "\n",
    "**What it provides:**\n",
    "- Feature importance scores for each prediction\n",
    "- Understand which features drove the anomaly score\n",
    "- Identify patterns in high-risk transactions\n",
    "\n",
    "**Limitations:**\n",
    "- Not all remote models support `ML.EXPLAIN()`\n",
    "- Requires endpoint to return feature attributions\n",
    "- May increase prediction latency\n",
    "\n",
    "**Example use case:**\n",
    "```sql\n",
    "-- Get feature importance for each prediction\n",
    "SELECT *\n",
    "FROM ML.EXPLAIN_PREDICT(\n",
    "    MODEL `dataset.remote_model`,\n",
    "    (SELECT * FROM `dataset.transactions` LIMIT 10),\n",
    "    STRUCT(3 AS top_k_features)\n",
    ")\n",
    "```\n",
    "\n",
    "**When to use:**\n",
    "- Regulatory compliance (explain decisions)\n",
    "- Model debugging (understand failures)\n",
    "- Feature engineering (identify important features)\n",
    "\n",
    "### Scheduled Queries for Automated Predictions\n",
    "\n",
    "**BigQuery Scheduled Queries** enable automatic, recurring batch predictions:\n",
    "\n",
    "**Setup steps:**\n",
    "1. Write `CREATE OR REPLACE TABLE` query with `ML.PREDICT()`\n",
    "2. Click \"Schedule\" in BigQuery Console\n",
    "3. Set frequency: hourly, daily, weekly, monthly, or custom cron\n",
    "4. Configure notifications for failures\n",
    "\n",
    "**Advanced features:**\n",
    "- **Partitioning**: Create date-partitioned tables for time-series predictions\n",
    "- **Incremental updates**: Only score new data since last run\n",
    "- **Error handling**: Retry failed queries automatically\n",
    "- **Cost controls**: Set maximum bytes billed\n",
    "\n",
    "**Example: Daily fraud scoring**\n",
    "```sql\n",
    "-- Scheduled to run daily at 2 AM\n",
    "CREATE OR REPLACE TABLE `fraud.daily_scores`\n",
    "PARTITION BY DATE(transaction_date) AS\n",
    "SELECT \n",
    "    transaction_date,\n",
    "    transaction_id,\n",
    "    anomaly_score,\n",
    "    risk_category\n",
    "FROM ML.PREDICT(\n",
    "    MODEL `models.fraud_detector`,\n",
    "    (SELECT * FROM `transactions.raw` WHERE transaction_date = CURRENT_DATE())\n",
    ")\n",
    "```\n",
    "\n",
    "**Monitoring:**\n",
    "- View execution history in Scheduled Queries page\n",
    "- Set up email/Slack notifications for failures\n",
    "- Track query costs and performance over time\n",
    "\n",
    "### Materialized Views with Predictions\n",
    "\n",
    "**Materialized views** provide pre-computed, auto-refreshing prediction results:\n",
    "\n",
    "**Benefits:**\n",
    "- ‚ö° **Fast queries**: Results pre-computed and cached\n",
    "- üîÑ **Auto-refresh**: Updates automatically when source data changes\n",
    "- üí∞ **Cost-effective**: Predictions cached, not recomputed every query\n",
    "- üéØ **Always current**: Automatically stays in sync with data\n",
    "\n",
    "**Example:**\n",
    "```sql\n",
    "-- Create materialized view with predictions\n",
    "CREATE MATERIALIZED VIEW `dataset.fraud_scores_mv` AS\n",
    "SELECT \n",
    "    transaction_id,\n",
    "    timestamp,\n",
    "    anomaly_score,\n",
    "    risk_category\n",
    "FROM ML.PREDICT(\n",
    "    MODEL `models.fraud_detector`,\n",
    "    (SELECT * FROM `transactions.live`)\n",
    ")\n",
    "WHERE anomaly_score > 100  -- Only high-risk transactions\n",
    "```\n",
    "\n",
    "**Use cases:**\n",
    "- **Real-time dashboards**: BI tools query materialized view (fast!)\n",
    "- **Alert systems**: Monitor view for new high-risk predictions\n",
    "- **Filtered predictions**: Pre-filter to only anomalies\n",
    "\n",
    "**Limitations:**\n",
    "- Refresh frequency controlled by BigQuery (not guaranteed immediate)\n",
    "- Additional storage costs for materialized results\n",
    "- Not all queries supported in materialized views\n",
    "\n",
    "### Performance Optimization\n",
    "\n",
    "**Best practices for production remote models:**\n",
    "\n",
    "**1. Batch Size Optimization**\n",
    "- BigQuery automatically batches requests to endpoint\n",
    "- Larger batches = fewer endpoint calls = lower cost\n",
    "- Balance: batch size vs. query timeout (10 min default)\n",
    "\n",
    "**2. Endpoint Configuration**\n",
    "- Use **autoscaling** for variable workloads\n",
    "- Set **min replicas > 0** to avoid cold starts\n",
    "- Consider **GPU instances** for large models\n",
    "- Monitor **endpoint metrics** (latency, throughput)\n",
    "\n",
    "**3. Query Optimization**\n",
    "- **Filter early**: Apply WHERE before ML.PREDICT()\n",
    "- **Project only needed columns**: Reduce data transfer\n",
    "- **Use partitioning**: Scan less data\n",
    "- **Cache results**: CREATE TABLE instead of repeated predictions\n",
    "\n",
    "**4. Cost Management**\n",
    "- **BigQuery costs**: Query bytes scanned + ML.PREDICT calls\n",
    "- **Vertex AI costs**: Endpoint uptime + prediction requests\n",
    "- **Optimization**: Score once, query many times (use tables)\n",
    "- **Monitoring**: Set up budget alerts\n",
    "\n",
    "**Example: Optimized query**\n",
    "```sql\n",
    "-- Inefficient: Predicts all, then filters\n",
    "SELECT * FROM ML.PREDICT(...)\n",
    "WHERE date = '2024-01-01'  -- Filters AFTER prediction\n",
    "\n",
    "-- Efficient: Filters first, then predicts\n",
    "SELECT * FROM ML.PREDICT(\n",
    "    MODEL ...,\n",
    "    (SELECT * FROM table WHERE date = '2024-01-01')  -- Filters BEFORE\n",
    ")\n",
    "```\n",
    "\n",
    "### Integration with Other GCP Services\n",
    "\n",
    "Remote models integrate seamlessly with the broader GCP ecosystem:\n",
    "\n",
    "**Cloud Workflows:**\n",
    "- Orchestrate complex prediction pipelines\n",
    "- Chain multiple remote models\n",
    "- Add data preprocessing and postprocessing\n",
    "- Handle errors and retries\n",
    "\n",
    "**Pub/Sub + Cloud Functions:**\n",
    "- Trigger predictions on new data arrival\n",
    "- Real-time scoring as data streams in\n",
    "- Fan-out to multiple consumers\n",
    "\n",
    "**Looker/Data Studio:**\n",
    "- Visualize predictions in dashboards\n",
    "- Query predictions table directly\n",
    "- Create alerts on anomaly thresholds\n",
    "\n",
    "**Vertex AI Pipelines:**\n",
    "- Incorporate batch scoring in ML pipelines\n",
    "- Automate model retraining based on prediction drift\n",
    "- End-to-end MLOps workflows\n",
    "\n",
    "### Monitoring and Observability\n",
    "\n",
    "**Key metrics to track:**\n",
    "\n",
    "**BigQuery Side:**\n",
    "- Query execution time\n",
    "- Bytes scanned\n",
    "- ML.PREDICT call count\n",
    "- Failed queries\n",
    "\n",
    "**Vertex AI Endpoint:**\n",
    "- Prediction latency (p50, p95, p99)\n",
    "- Request count\n",
    "- Error rate\n",
    "- Resource utilization (CPU, memory)\n",
    "\n",
    "**Model Quality:**\n",
    "- Prediction distribution over time (drift detection)\n",
    "- Anomaly rate trends\n",
    "- Feature importance changes\n",
    "- Actual vs. predicted (if labels available)\n",
    "\n",
    "**Tools:**\n",
    "- **Cloud Monitoring**: Endpoint metrics and alerts\n",
    "- **BigQuery Information Schema**: Query statistics\n",
    "- **Cloud Logging**: Detailed logs for debugging\n",
    "- **Vertex AI Model Monitoring**: Automated drift detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cleanup",
   "metadata": {},
   "source": [
    "---\n",
    "## Cleanup\n",
    "\n",
    "To avoid incurring unnecessary charges, clean up resources created in this notebook.\n",
    "\n",
    "**‚ö†Ô∏è Warning:** The commands below are commented out to prevent accidental deletion. Uncomment only what you want to remove.\n",
    "\n",
    "**What gets deleted:**\n",
    "- BigQuery ML remote model (does NOT delete the endpoint)\n",
    "- BigQuery predictions table\n",
    "- BigQuery Cloud Resource Connection\n",
    "\n",
    "**What is NOT deleted:**\n",
    "- Vertex AI Endpoint (you may want to keep this for other uses)\n",
    "- Source data table (`frameworks` dataset)\n",
    "- To delete endpoint: Use the endpoint cleanup notebook or Console"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cleanup_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to delete BigQuery ML remote model\n",
    "# print(f\"Deleting remote model: {REMOTE_MODEL_NAME}\")\n",
    "# bq.delete_model(REMOTE_MODEL_NAME, not_found_ok=True)\n",
    "# print(f\"‚úÖ Remote model deleted\")\n",
    "\n",
    "# Uncomment to delete predictions table\n",
    "# print(f\"Deleting predictions table: {PROJECT_ID}.{BQ_DATASET}.{BQ_PREDICTIONS_TABLE}\")\n",
    "# bq.delete_table(f\"{PROJECT_ID}.{BQ_DATASET}.{BQ_PREDICTIONS_TABLE}\", not_found_ok=True)\n",
    "# print(f\"‚úÖ Predictions table deleted\")\n",
    "\n",
    "# Uncomment to delete BigQuery connection\n",
    "# print(f\"Deleting BigQuery connection: {BQ_CONNECTION_NAME}\")\n",
    "# connection_name = f\"projects/{PROJECT_ID}/locations/{BQ_REGION}/connections/{BQ_CONNECTION_NAME}\"\n",
    "# try:\n",
    "#     connection_client.delete_connection(name=connection_name)\n",
    "#     print(f\"‚úÖ Connection deleted\")\n",
    "# except Exception as e:\n",
    "#     print(f\"‚ùå Failed to delete connection: {e}\")\n",
    "\n",
    "# Uncomment to remove IAM policy binding for connection service account\n",
    "# Note: Only remove if you're sure no other remote models use this connection\n",
    "# print(f\"Removing Vertex AI User role from service account: {service_account}\")\n",
    "# result = subprocess.run(\n",
    "#     ['gcloud', 'projects', 'remove-iam-policy-binding', PROJECT_ID,\n",
    "#      f'--member=serviceAccount:{service_account}',\n",
    "#      '--role=roles/aiplatform.user'],\n",
    "#     capture_output=True,\n",
    "#     text=True\n",
    "# )\n",
    "# if result.returncode == 0:\n",
    "#     print(f\"‚úÖ IAM policy binding removed\")\n",
    "# else:\n",
    "#     print(f\"‚ùå Failed to remove binding: {result.stderr}\")\n",
    "\n",
    "print(f\"\\nüí° To delete the Vertex AI Endpoint:\")\n",
    "print(f\"   1. Use the endpoint deployment notebook's cleanup section, OR\")\n",
    "print(f\"   2. Go to Vertex AI Console > Endpoints > {endpoint.display_name} > Delete\")\n",
    "print(f\"\\n‚ö†Ô∏è  Deleting endpoint will affect ALL remote models using it!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "Congratulations! You've successfully created and used a BigQuery ML remote model to call a Vertex AI endpoint from SQL.\n",
    "\n",
    "### What You Accomplished\n",
    "\n",
    "‚úÖ **Prerequisites & Testing**\n",
    "- Verified deployed Vertex AI endpoint\n",
    "- Tested endpoint with Python SDK\n",
    "- Understood response signature differences (prebuilt vs custom)\n",
    "\n",
    "‚úÖ **Infrastructure Setup**\n",
    "- Created BigQuery Cloud Resource Connection\n",
    "- Granted IAM permissions for Vertex AI access\n",
    "- Configured secure service-to-service communication\n",
    "\n",
    "‚úÖ **Remote Model Creation**\n",
    "- Registered Vertex AI endpoint as BQML model\n",
    "- Defined input/output schemas for both endpoint types\n",
    "- Handled endpoint-specific response structures\n",
    "\n",
    "‚úÖ **SQL-Based Inference**\n",
    "- Made simple predictions with `ML.PREDICT()`\n",
    "- Extracted specific fields from prediction output\n",
    "- Applied business logic (risk categorization)\n",
    "- Created enriched tables with batch predictions\n",
    "\n",
    "‚úÖ **Production Patterns**\n",
    "- Batch scored large datasets\n",
    "- Persisted predictions to queryable tables\n",
    "- Demonstrated joining predictions with source data\n",
    "- Explored advanced topics (scheduling, materialized views, optimization)\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "**When to Use BigQuery ML Remote Models:**\n",
    "- ‚úÖ Batch scoring large datasets in BigQuery\n",
    "- ‚úÖ SQL analysts need predictions without Python\n",
    "- ‚úÖ Scheduled/recurring inference jobs\n",
    "- ‚úÖ Models already deployed on Vertex AI endpoints\n",
    "- ‚úÖ Integration with BigQuery data warehouse\n",
    "\n",
    "**Benefits Over Direct SDK Calls:**\n",
    "- üí™ **Scalability**: Petabyte-scale batch predictions\n",
    "- üéØ **Simplicity**: Pure SQL, no Python required\n",
    "- üîÑ **Automation**: Scheduled queries for recurring predictions\n",
    "- üí∞ **Cost-effective**: Cache predictions in tables\n",
    "- ü§ù **Collaboration**: SQL accessible to broader teams\n",
    "\n",
    "**Performance Tips:**\n",
    "- Filter data BEFORE calling `ML.PREDICT()` (reduce endpoint calls)\n",
    "- Create prediction tables for repeated queries (cache results)\n",
    "- Use endpoint autoscaling for variable workloads\n",
    "- Monitor both BigQuery and Vertex AI metrics\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "**Explore Related Notebooks:**\n",
    "- [vertex-ai-endpoint-prebuilt-container.ipynb](./vertex-ai-endpoint-prebuilt-container.ipynb) - Deploy PyTorch models with TorchServe\n",
    "- [vertex-ai-endpoint-custom-container.ipynb](./vertex-ai-endpoint-custom-container.ipynb) - Custom FastAPI serving containers\n",
    "- [scaling-vertex-ai-endpoint.ipynb](./scaling-vertex-ai-endpoint.ipynb) - Performance testing and autoscaling\n",
    "- [torchserve-cloud-run.ipynb](./torchserve-cloud-run.ipynb) - Serverless alternative with Cloud Run\n",
    "\n",
    "**Production Enhancements:**\n",
    "1. **Set up scheduled queries** for daily/hourly predictions\n",
    "2. **Create materialized views** for fast dashboard queries\n",
    "3. **Add monitoring** for prediction drift and data quality\n",
    "4. **Implement alerting** for high-risk predictions\n",
    "5. **Optimize costs** with partitioning and clustering\n",
    "\n",
    "**Advanced Topics to Explore:**\n",
    "- Model explainability with `ML.EXPLAIN_PREDICT()`\n",
    "- Multi-model ensembles (combine predictions from multiple endpoints)\n",
    "- A/B testing (compare prebuilt vs custom endpoint predictions)\n",
    "- Integration with Vertex AI Pipelines for end-to-end MLOps\n",
    "\n",
    "### Resources\n",
    "\n",
    "**Documentation:**\n",
    "- [BigQuery ML Remote Models](https://cloud.google.com/bigquery/docs/reference/standard-sql/bigqueryml-syntax-create-remote-model)\n",
    "- [ML.PREDICT() Function](https://cloud.google.com/bigquery/docs/reference/standard-sql/bigqueryml-syntax-predict)\n",
    "- [BigQuery Cloud Resource Connections](https://cloud.google.com/bigquery/docs/create-cloud-resource-connection)\n",
    "- [Vertex AI Endpoints](https://cloud.google.com/vertex-ai/docs/predictions/getting-predictions)\n",
    "\n",
    "**Tutorials:**\n",
    "- [Make predictions with remote models on Vertex AI](https://cloud.google.com/bigquery/docs/bigquery-ml-remote-model-tutorial)\n",
    "- [Schedule queries](https://cloud.google.com/bigquery/docs/scheduling-queries)\n",
    "- [Create materialized views](https://cloud.google.com/bigquery/docs/materialized-views-intro)\n",
    "\n",
    "**Related Repositories:**\n",
    "- [vertex-ai-mlops](https://github.com/statmike/vertex-ai-mlops) - This repository with more examples\n",
    "- [Vertex AI Samples](https://github.com/GoogleCloudPlatform/vertex-ai-samples)\n",
    "\n",
    "---\n",
    "\n",
    "**Questions or feedback?** \n",
    "- Open an issue: [GitHub Issues](https://github.com/statmike/vertex-ai-mlops/issues)\n",
    "- Connect: [LinkedIn](https://www.linkedin.com/in/statmike) | [Twitter/X](https://x.com/statmike) | [BlueSky](https://bsky.app/profile/statmike.bsky.social)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (frameworks-pytorch)",
   "language": "python",
   "name": "frameworks-pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
