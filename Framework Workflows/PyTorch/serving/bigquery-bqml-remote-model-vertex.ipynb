{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "![tracker](https://us-central1-vertex-ai-mlops-369716.cloudfunctions.net/pixel-tracking?path=statmike%2Fvertex-ai-mlops%2FFramework+Workflows%2FPyTorch%2Fserving&file=bigquery-bqml-remote-model-vertex.ipynb)\n",
    "<!--- header table --->\n",
    "<table align=\"left\">\n",
    "<tr>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%20Workflows/PyTorch/serving/bigquery-bqml-remote-model-vertex.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.svgrepo.com/download/217753/github.svg\" alt=\"GitHub logo\">\n",
    "      <br>View on<br>GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://colab.research.google.com/github/statmike/vertex-ai-mlops/blob/main/Framework%20Workflows/PyTorch/serving/bigquery-bqml-remote-model-vertex.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.gstatic.com/pantheon/images/bigquery/welcome_page/colab-logo.svg\" alt=\"Google Colaboratory logo\">\n",
    "      <br>Run in<br>Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https%3A%2F%2Fraw.githubusercontent.com%2Fstatmike%2Fvertex-ai-mlops%2Fmain%2FFramework%2520Workflows%2FPyTorch%2Fserving%2Fbigquery-bqml-remote-model-vertex.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\">\n",
    "      <br>Run in<br>Colab Enterprise\n",
    "    </a>\n",
    "  </td>      \n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/bigquery/import?url=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%20Workflows/PyTorch/serving/bigquery-bqml-remote-model-vertex.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.gstatic.com/images/branding/gcpiconscolors/bigquery/v1/32px.svg\" alt=\"BigQuery logo\">\n",
    "      <br>Open in<br>BigQuery Studio\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/statmike/vertex-ai-mlops/main/Framework%20Workflows/PyTorch/serving/bigquery-bqml-remote-model-vertex.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.gstatic.com/images/branding/gcpiconscolors/vertexai/v1/32px.svg\" alt=\"Vertex AI logo\">\n",
    "      <br>Open in<br>Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td colspan=\"5\" style=\"text-align: right\">\n",
    "    <b>Share This On: </b> \n",
    "    <a href=\"https://www.linkedin.com/sharing/share-offsite/?url=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%2520Workflows/PyTorch/serving/bigquery-bqml-remote-model-vertex.ipynb\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"Linkedin Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://reddit.com/submit?url=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%2520Workflows/PyTorch/serving/bigquery-bqml-remote-model-vertex.ipynb\"><img src=\"https://redditinc.com/hubfs/Reddit%20Inc/Brand/Reddit_Logo.png\" alt=\"Reddit Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://bsky.app/intent/compose?text=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%2520Workflows/PyTorch/serving/bigquery-bqml-remote-model-vertex.ipynb\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"BlueSky Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://twitter.com/intent/tweet?url=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%2520Workflows/PyTorch/serving/bigquery-bqml-remote-model-vertex.ipynb\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/5/5a/X_icon_2.svg\" alt=\"X (Twitter) Logo\" width=\"20px\"></a> \n",
    "  </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td colspan=\"5\" style=\"text-align: right\">\n",
    "    <b>Connect With Author On: </b> \n",
    "    <a href=\"https://www.linkedin.com/in/statmike\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"Linkedin Logo\" width=\"20px\"></a>\n",
    "    <a href=\"https://www.github.com/statmike\"><img src=\"https://www.svgrepo.com/download/217753/github.svg\" alt=\"GitHub Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://www.youtube.com/@statmike-channel\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/f/fd/YouTube_full-color_icon_%282024%29.svg\" alt=\"YouTube Logo\" width=\"20px\"></a>\n",
    "    <a href=\"https://bsky.app/profile/statmike.bsky.social\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"BlueSky Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://x.com/statmike\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/5/5a/X_icon_2.svg\" alt=\"X (Twitter) Logo\" width=\"20px\"></a>\n",
    "  </td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overview",
   "metadata": {},
   "source": [
    "# BigQuery ML Remote Model Inference with Vertex AI Endpoints\n",
    "\n",
    "This notebook demonstrates how to use **BigQuery ML Remote Models** to call Vertex AI Endpoints directly from SQL using `ML.PREDICT()`. This powerful integration enables SQL-based batch scoring without needing Python or model deployment infrastructure.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "This workflow covers:\n",
    "\n",
    "1. **Prerequisites Check**: Verify deployed Vertex AI endpoint\n",
    "2. **Test Endpoint Locally**: Validate endpoint with Python SDK\n",
    "3. **Create BigQuery Connection**: Setup Cloud Resource Connection for remote inference\n",
    "4. **Grant Permissions**: Configure IAM for connection service account\n",
    "5. **Create Remote Model**: Register endpoint as BQML model\n",
    "6. **Simple Predictions**: Use ML.PREDICT() with test data\n",
    "7. **Extract Prediction Fields**: Work with STRUCT output\n",
    "8. **Apply Business Logic**: Use predictions for decision-making\n",
    "9. **Batch Scoring**: Create enriched tables with predictions\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "This notebook requires an existing Vertex AI Endpoint with a deployed PyTorch model.\n",
    "\n",
    "**Create an endpoint using either:**\n",
    "- [vertex-ai-endpoint-prebuilt-container.ipynb](./vertex-ai-endpoint-prebuilt-container.ipynb) - Pre-built TorchServe container (recommended)\n",
    "- [vertex-ai-endpoint-custom-container.ipynb](./vertex-ai-endpoint-custom-container.ipynb) - Custom FastAPI container\n",
    "\n",
    "Both endpoints work with this notebook, but have different response signatures (see below).\n",
    "\n",
    "## Core Concepts\n",
    "\n",
    "### BigQuery ML Remote Models\n",
    "\n",
    "Remote models allow BigQuery to call external prediction endpoints (like Vertex AI) during query execution:\n",
    "\n",
    "**Benefits:**\n",
    "- ‚úÖ **SQL-based inference**: No Python code needed for batch scoring\n",
    "- ‚úÖ **Data warehouse integration**: Predictions join seamlessly with BigQuery tables\n",
    "- ‚úÖ **Scheduled scoring**: Use BigQuery scheduled queries for automated predictions\n",
    "- ‚úÖ **Leverage existing endpoints**: Reuse models already deployed for online serving\n",
    "- ‚úÖ **Large-scale batch processing**: Process millions of rows efficiently\n",
    "\n",
    "**When to use:**\n",
    "- Batch scoring large datasets already in BigQuery\n",
    "- SQL analysts need predictions without Python knowledge\n",
    "- Scheduled/recurring inference jobs\n",
    "- Models too large to import into BigQuery ML\n",
    "\n",
    "### Architecture\n",
    "\n",
    "```\n",
    "BigQuery Table ‚Üí ML.PREDICT() ‚Üí Cloud Resource Connection ‚Üí Vertex AI Endpoint ‚Üí Predictions ‚Üí BigQuery Results\n",
    "```\n",
    "\n",
    "**How it works:**\n",
    "1. SQL query with `ML.PREDICT()` sends rows to remote model\n",
    "2. BigQuery Connection forwards requests to Vertex AI endpoint\n",
    "3. Endpoint returns predictions (as STRUCT)\n",
    "4. Predictions merge back into query results\n",
    "\n",
    "### Endpoint Response Signatures\n",
    "\n",
    "The two endpoint types return different prediction structures:\n",
    "\n",
    "**Pre-built Container** (`vertex-ai-endpoint-prebuilt-container.ipynb`):\n",
    "- Returns **13 fields** (full model output)\n",
    "- Includes all error metrics, latent encodings, and reconstructions\n",
    "- Example fields: `denormalized_MAE`, `denormalized_RMSE`, `encoded`, `normalized_reconstruction`, etc.\n",
    "- **Use when**: You need full model diagnostics and multiple metrics\n",
    "\n",
    "**Custom Container** (`vertex-ai-endpoint-custom-container.ipynb`):\n",
    "- Returns **2 fields** (simplified output)\n",
    "- Only `anomaly_score` and `encoded` (latent representation)\n",
    "- Reduced response size (~70% smaller)\n",
    "- **Use when**: You only need anomaly scores for decision-making\n",
    "\n",
    "**Impact on BQML Remote Model:**\n",
    "- Pre-built: More fields to extract and analyze in SQL\n",
    "- Custom: Simpler SQL queries, faster response times\n",
    "- This notebook demonstrates **both** (you choose which endpoint to use)\n",
    "\n",
    "### BigQuery Cloud Resource Connection\n",
    "\n",
    "Connections enable BigQuery to securely call external services:\n",
    "\n",
    "- Creates a dedicated service account\n",
    "- Service account needs `Vertex AI User` role\n",
    "- Connection tied to specific region (must match endpoint region)\n",
    "- Reusable across multiple remote models\n",
    "\n",
    "## What's Different from Direct SDK Calls?\n",
    "\n",
    "| Aspect | Python SDK (Direct) | BQML Remote Model |\n",
    "|--------|-------------------|------------------|\n",
    "| **Language** | Python | SQL |\n",
    "| **Data Source** | Manual loading | BigQuery tables |\n",
    "| **Scale** | Limited by memory | Petabyte-scale |\n",
    "| **Scheduling** | Custom code | Scheduled queries |\n",
    "| **Result Storage** | Manual export | Directly to tables |\n",
    "| **Who can use** | Data scientists | SQL analysts |\n",
    "\n",
    "**Example comparison:**\n",
    "\n",
    "```python\n",
    "# Python SDK - Manual batching required\n",
    "predictions = []\n",
    "for batch in chunks(data, batch_size=1000):\n",
    "    pred = endpoint.predict(instances=batch)\n",
    "    predictions.extend(pred.predictions)\n",
    "```\n",
    "\n",
    "```sql\n",
    "-- BQML - Automatic batching, millions of rows\n",
    "SELECT * FROM ML.PREDICT(\n",
    "  MODEL `dataset.remote_model`,\n",
    "  (SELECT * FROM `dataset.transactions` WHERE date = CURRENT_DATE())\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "env_setup",
   "metadata": {},
   "source": [
    "---\n",
    "## Environment Setup\n",
    "\n",
    "This section will authenticate your session, enable required Google Cloud APIs, and install necessary Python packages.\n",
    "\n",
    "**Package Installation Options (`REQ_TYPE`):**\n",
    "- `PRIMARY`: Installs only the main packages. Faster, but pip resolves sub-dependencies which may result in different versions than development.\n",
    "- `ALL` (Default): Installs exact versions of all packages and dependencies. Best for perfectly reproducing the development environment.\n",
    "- `COLAB`: Installs a Colab-optimized list that excludes pre-installed packages like `ipython` and `ipykernel`.\n",
    "\n",
    "**Installation Tool Options (`INSTALL_TOOL`):**\n",
    "- `pip` (Default): Uses pip for package installation. Standard Python package installer.\n",
    "- `uv`: Modern, fast Python package installer. Must be installed separately. See: https://github.com/astral-sh/uv\n",
    "- `poetry`: Dependency management tool. Requires running notebook in a poetry environment (`poetry shell` or `poetry run jupyter lab`). Uses `pyproject.toml` instead of requirements.txt.\n",
    "\n",
    "> **Note:** If running in Google Colab, the script will automatically detect this and set `REQ_TYPE = 'COLAB'` to prevent package conflicts, overriding any manual setting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "set_project",
   "metadata": {},
   "source": [
    "### Set Your Project ID\n",
    "\n",
    "‚ö†Ô∏è **Action Required:** Replace the `PROJECT_ID` value below with your Google Cloud project ID before running this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "project_config",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = 'statmike-mlops-349915' # replace with GCP project ID\n",
    "REQ_TYPE = 'ALL' # Specify PRIMARY or ALL or COLAB\n",
    "INSTALL_TOOL = 'poetry' # Specify pip, uv, or poetry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config_header",
   "metadata": {},
   "source": [
    "### Configuration\n",
    "\n",
    "This cell defines the requirements files and Google Cloud APIs needed for this notebook. Run as-is without modification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": [
    "REQUIREMENTS_URL = 'https://raw.githubusercontent.com/statmike/vertex-ai-mlops/refs/heads/main/Framework%20Workflows/PyTorch/requirements.txt'\n",
    "\n",
    "REQUIRED_APIS = [\n",
    "    \"aiplatform.googleapis.com\",\n",
    "    \"bigquery.googleapis.com\",\n",
    "    \"bigqueryconnection.googleapis.com\",  # Required for remote models\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "run_setup_header",
   "metadata": {},
   "source": [
    "### Run Setup\n",
    "\n",
    "This cell downloads the centralized setup code and configures your environment. It will:\n",
    "- Authenticate your session with Google Cloud\n",
    "- Enable required APIs for this notebook (including BigQuery Connection API)\n",
    "- Install necessary Python packages\n",
    "- Display a setup summary with your project information\n",
    "\n",
    "> **Note:** In Colab, if packages are installed, the kernel will automatically restart. After restart, continue from the next cell without re-running earlier cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "run_setup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "PYTHON GCP ENVIRONMENT SETUP\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "AUTHENTICATION\n",
      "==================================================\n",
      "Checking for existing ADC...\n",
      "‚úÖ Existing ADC found.\n",
      "‚úÖ Project is correctly set to 'statmike-mlops-349915'.\n",
      "\n",
      "==================================================\n",
      "API CHECK & ENABLE\n",
      "==================================================\n",
      "‚úÖ aiplatform.googleapis.com is already enabled.\n",
      "‚úÖ bigquery.googleapis.com is already enabled.\n",
      "‚úÖ bigqueryconnection.googleapis.com is already enabled.\n",
      "\n",
      "==================================================\n",
      "PACKAGE MANAGEMENT\n",
      "==================================================\n",
      "Installation Tool: poetry\n",
      "‚úÖ Found poetry at: /usr/local/google/home/statmike/.local/bin/poetry\n",
      "‚úÖ Running in poetry environment: /usr/local/google/home/statmike/.cache/pypoetry/virtualenvs/frameworks-pytorch-0KVJlKeQ-py3.13\n",
      "‚ÑπÔ∏è  Poetry mode: Installing from pyproject.toml (REQUIREMENTS_URL ignored)\n",
      "‚úÖ Found pyproject.toml at: /usr/local/google/home/statmike/Git/vertex-ai-mlops/Framework Workflows/PyTorch/pyproject.toml\n",
      "   Changed working directory to: /usr/local/google/home/statmike/Git/vertex-ai-mlops/Framework Workflows/PyTorch\n",
      "Running poetry install...\n",
      "   Restored working directory to: /usr/local/google/home/statmike/Git/vertex-ai-mlops/Framework Workflows/PyTorch/serving\n",
      "‚úÖ All packages are already installed and up to date.\n",
      "\n",
      "==================================================\n",
      "Google Cloud Project Information\n",
      "==================================================\n",
      "PROJECT_ID     = statmike-mlops-349915\n",
      "PROJECT_NUMBER = 1026793852137\n",
      "==================================================\n",
      "\n",
      "\n",
      "==================================================\n",
      "SETUP SUMMARY\n",
      "==================================================\n",
      "‚úÖ Authentication:    Success\n",
      "‚úÖ API Configuration: Success\n",
      "‚úÖ Package Install:   Already up to date\n",
      "‚úÖ Installation Tool: poetry\n",
      "‚úÖ Project ID:        statmike-mlops-349915\n",
      "‚úÖ Project Number:    1026793852137\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os, urllib.request\n",
    "\n",
    "# Download and import setup code\n",
    "url = 'https://raw.githubusercontent.com/statmike/vertex-ai-mlops/refs/heads/main/core/notebook-template/python_setup.py'\n",
    "urllib.request.urlretrieve(url, 'python_setup_local.py')\n",
    "import python_setup_local as python_setup\n",
    "os.remove('python_setup_local.py')\n",
    "\n",
    "# Run setup\n",
    "setup_info = python_setup.setup_environment(PROJECT_ID, REQ_TYPE, REQUIREMENTS_URL, REQUIRED_APIS, INSTALL_TOOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "python_setup",
   "metadata": {},
   "source": [
    "---\n",
    "## Python Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imports_header",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "imports",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/google/home/statmike/.cache/pypoetry/virtualenvs/frameworks-pytorch-0KVJlKeQ-py3.13/lib/python3.13/site-packages/google/cloud/aiplatform/models.py:52: FutureWarning: Support for google-cloud-storage < 3.0.0 will be removed in a future version of google-cloud-aiplatform. Please upgrade to google-cloud-storage >= 3.0.0.\n",
      "  from google.cloud.aiplatform.utils import gcs_utils\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import json\n",
    "import pandas as pd\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud import bigquery\n",
    "from google.cloud.bigquery_connection_v1 import ConnectionServiceClient\n",
    "from google.cloud.bigquery_connection_v1.types import Connection, CloudResourceProperties"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vars_user",
   "metadata": {},
   "source": [
    "### Variables - User Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "vars_user_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION = 'us-central1'  # Vertex AI endpoint region\n",
    "SERIES = 'frameworks'\n",
    "EXPERIMENT = 'pytorch-bqml-remote-model'\n",
    "\n",
    "# Endpoint to use (choose one)\n",
    "# Options: 'pytorch-autoencoder' (prebuilt) or 'pytorch-autoencoder-custom' (custom)\n",
    "ENDPOINT_DISPLAY_NAME = 'pytorch-autoencoder-endpoint'  # Change to 'pytorch-autoencoder-custom-endpoint' to use custom container endpoint\n",
    "\n",
    "# BigQuery configuration\n",
    "BQ_REGION = 'US'  # BigQuery connection region (must match dataset location)\n",
    "BQ_DATASET = SERIES.replace('-', '_')  # 'frameworks'\n",
    "BQ_SOURCE_TABLE = SERIES  # Source data table\n",
    "BQ_PREDICTIONS_TABLE = 'bqml_remote_model_predictions'  # Will be created\n",
    "\n",
    "# Connection name for BigQuery remote model\n",
    "BQ_CONNECTION_NAME = f\"{SERIES.replace('-', '_')}_{EXPERIMENT.replace('-', '_')}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vars_auto",
   "metadata": {},
   "source": [
    "### Variables - Auto Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "vars_auto_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Google Cloud Project Information\n",
      "==================================================\n",
      "PROJECT_ID     = statmike-mlops-349915\n",
      "PROJECT_NUMBER = 1026793852137\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "PROJECT_ID = subprocess.run(['gcloud', 'config', 'get-value', 'project'], capture_output=True, text=True, check=True).stdout.strip()\n",
    "PROJECT_NUMBER = subprocess.run(['gcloud', 'projects', 'describe', PROJECT_ID, '--format=value(projectNumber)'], capture_output=True, text=True, check=True).stdout.strip()\n",
    "\n",
    "print(f\"\\n{'='*50}\\nGoogle Cloud Project Information\\n{'='*50}\\nPROJECT_ID     = {PROJECT_ID}\\nPROJECT_NUMBER = {PROJECT_NUMBER}\\n{'='*50}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "init_clients",
   "metadata": {},
   "source": [
    "### Initialize Clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "init_clients_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Clients initialized:\n",
      "   Vertex AI: statmike-mlops-349915 in us-central1\n",
      "   BigQuery: statmike-mlops-349915\n",
      "   BigQuery Connection: statmike-mlops-349915\n"
     ]
    }
   ],
   "source": [
    "# Initialize Vertex AI SDK\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION)\n",
    "\n",
    "# Initialize BigQuery client\n",
    "bq = bigquery.Client(project=PROJECT_ID)\n",
    "\n",
    "# Initialize BigQuery Connection client\n",
    "connection_client = ConnectionServiceClient()\n",
    "\n",
    "print(f\"‚úÖ Clients initialized:\")\n",
    "print(f\"   Vertex AI: {PROJECT_ID} in {REGION}\")\n",
    "print(f\"   BigQuery: {PROJECT_ID}\")\n",
    "print(f\"   BigQuery Connection: {PROJECT_ID}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prereq_check",
   "metadata": {},
   "source": [
    "---\n",
    "## Prerequisites: Verify Deployed Endpoint\n",
    "\n",
    "This notebook requires a deployed Vertex AI Endpoint with the PyTorch autoencoder model.\n",
    "\n",
    "**If you haven't deployed an endpoint yet:**\n",
    "- [vertex-ai-endpoint-prebuilt-container.ipynb](./vertex-ai-endpoint-prebuilt-container.ipynb) - Pre-built TorchServe container (recommended)\n",
    "- [vertex-ai-endpoint-custom-container.ipynb](./vertex-ai-endpoint-custom-container.ipynb) - Custom FastAPI container\n",
    "\n",
    "**This section will:**\n",
    "1. Find the endpoint by display name\n",
    "2. Verify it has a deployed model\n",
    "3. Test predictions using the Python SDK\n",
    "4. Understand the response signature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "find_endpoint",
   "metadata": {},
   "source": [
    "### Find Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "find_endpoint_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Found endpoint: pytorch-autoencoder-endpoint\n",
      "\n",
      "Endpoint Details:\n",
      "  Resource name: projects/1026793852137/locations/us-central1/endpoints/5971323405637517312\n",
      "  Display name: pytorch-autoencoder-endpoint\n",
      "  Created: 2025-11-10 13:22:27.236300+00:00\n"
     ]
    }
   ],
   "source": [
    "# Find endpoint by display name\n",
    "endpoints = aiplatform.Endpoint.list(filter=f\"display_name={ENDPOINT_DISPLAY_NAME}\")\n",
    "\n",
    "if not endpoints:\n",
    "    print(f\"‚ùå No endpoint found with display name: {ENDPOINT_DISPLAY_NAME}\")\n",
    "    print(f\"\\nAvailable endpoints:\")\n",
    "    all_endpoints = aiplatform.Endpoint.list()\n",
    "    if all_endpoints:\n",
    "        for ep in all_endpoints:\n",
    "            print(f\"  - {ep.display_name} ({ep.resource_name})\")\n",
    "    else:\n",
    "        print(f\"  No endpoints found in project {PROJECT_ID}\")\n",
    "    \n",
    "    print(f\"\\n‚ö†Ô∏è  Please deploy an endpoint first using one of these notebooks:\")\n",
    "    print(f\"   ‚Ä¢ vertex-ai-endpoint-prebuilt-container.ipynb (recommended)\")\n",
    "    print(f\"   ‚Ä¢ vertex-ai-endpoint-custom-container.ipynb\")\n",
    "    raise ValueError(f\"Endpoint not found: {ENDPOINT_DISPLAY_NAME}\")\n",
    "\n",
    "endpoint = endpoints[0]\n",
    "print(f\"‚úÖ Found endpoint: {endpoint.display_name}\")\n",
    "\n",
    "if len(endpoints) > 1:\n",
    "    print(f\"   ‚ö†Ô∏è  Multiple endpoints found with this name. Using first one.\")\n",
    "\n",
    "print(f\"\\nEndpoint Details:\")\n",
    "print(f\"  Resource name: {endpoint.resource_name}\")\n",
    "print(f\"  Display name: {endpoint.display_name}\")\n",
    "print(f\"  Created: {endpoint.create_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inspect_models",
   "metadata": {},
   "source": [
    "### Inspect Deployed Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "inspect_models_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deployed Models: 1\n",
      "\n",
      "  Model: pytorch-autoencoder\n",
      "  ID: 4732525644854853632\n",
      "  Machine type: n1-standard-4\n",
      "  Replicas: 1 - 4\n",
      "\n",
      "‚úÖ Endpoint is ready for remote model creation\n"
     ]
    }
   ],
   "source": [
    "# Get deployed models on this endpoint\n",
    "deployed_models = endpoint.list_models()\n",
    "\n",
    "if not deployed_models:\n",
    "    print(f\"‚ùå No models deployed to endpoint: {endpoint.display_name}\")\n",
    "    print(f\"\\n‚ö†Ô∏è  Please deploy a model to this endpoint first.\")\n",
    "    raise ValueError(f\"No models deployed to endpoint\")\n",
    "\n",
    "print(f\"Deployed Models: {len(deployed_models)}\")\n",
    "for dm in deployed_models:\n",
    "    print(f\"\\n  Model: {dm.display_name}\")\n",
    "    print(f\"  ID: {dm.id}\")\n",
    "    \n",
    "    # Access machine type through dedicated_resources\n",
    "    if hasattr(dm, 'dedicated_resources') and dm.dedicated_resources:\n",
    "        machine_type = dm.dedicated_resources.machine_spec.machine_type\n",
    "        min_replicas = dm.dedicated_resources.min_replica_count\n",
    "        max_replicas = dm.dedicated_resources.max_replica_count\n",
    "        \n",
    "        print(f\"  Machine type: {machine_type}\")\n",
    "        print(f\"  Replicas: {min_replicas} - {max_replicas}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Endpoint is ready for remote model creation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "test_endpoint",
   "metadata": {},
   "source": [
    "### Test Endpoint with SDK\n",
    "\n",
    "Before creating the BigQuery remote model, validate that the endpoint works and understand its response signature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "get_test_data",
   "metadata": {},
   "source": [
    "#### Get Test Data from BigQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "get_test_data_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 3 test instances\n",
      "\n",
      "Features: ['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount']\n",
      "\n",
      "First instance (30 features):\n",
      "[122959.0, -1.3272968090323798, 0.422904408477484, 1.61750487263453, 2.29119608568876] ...\n"
     ]
    }
   ],
   "source": [
    "# Retrieve a few test records\n",
    "query = f\"\"\"\n",
    "SELECT * EXCEPT(splits, transaction_id, Class)\n",
    "FROM `{PROJECT_ID}.{BQ_DATASET}.{BQ_SOURCE_TABLE}`\n",
    "WHERE splits='TEST'\n",
    "LIMIT 3\n",
    "\"\"\"\n",
    "\n",
    "test_df = bq.query(query).to_dataframe()\n",
    "print(f\"Retrieved {len(test_df)} test instances\")\n",
    "print(f\"\\nFeatures: {list(test_df.columns)}\")\n",
    "\n",
    "# Convert to format expected by endpoint (list of lists)\n",
    "test_instances = test_df.values.tolist()\n",
    "print(f\"\\nFirst instance (30 features):\")\n",
    "print(test_instances[0][:5], \"...\")  # Show first 5 features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "test_prediction",
   "metadata": {},
   "source": [
    "#### Make Test Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "test_prediction_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing endpoint with SDK...\n",
      "\n",
      "‚úÖ Prediction successful!\n",
      "\n",
      "Response structure:\n",
      "  Type: <class 'dict'>\n",
      "  Keys: ['normalized_reconstruction_errors', 'denormalized_RMSE', 'normalized_RMSE', 'denormalized_MSE', 'normalized_MSLE', 'normalized_MSE', 'denormalized_reconstruction_errors', 'normalized_MAE', 'normalized_reconstruction', 'denormalized_MAE', 'encoded', 'denormalized_MSLE', 'denormalized_reconstruction']\n",
      "  Number of fields: 13\n",
      "\n",
      "üìä Prediction output:\n",
      "\n",
      "  üîµ Pre-built Container Response (13 fields)\n",
      "     Anomaly Score (MAE): 150.21\n",
      "     RMSE: 816.0109252929688\n",
      "     MSE: 665873.8125\n",
      "     Encoded (latent): [0.0, 0.0, 0.2779858708381653, 0.0]\n",
      "\n",
      "üí° This endpoint type: prebuilt\n",
      "   Response will have different fields in BigQuery remote model\n"
     ]
    }
   ],
   "source": [
    "# Test with a single instance\n",
    "print(f\"Testing endpoint with SDK...\")\n",
    "response = endpoint.predict(instances=[test_instances[0]])\n",
    "prediction = response.predictions[0]\n",
    "\n",
    "print(f\"\\n‚úÖ Prediction successful!\")\n",
    "print(f\"\\nResponse structure:\")\n",
    "print(f\"  Type: {type(prediction)}\")\n",
    "\n",
    "if isinstance(prediction, dict):\n",
    "    print(f\"  Keys: {list(prediction.keys())}\")\n",
    "    print(f\"  Number of fields: {len(prediction)}\")\n",
    "    \n",
    "    # Show prediction details based on endpoint type\n",
    "    print(f\"\\nüìä Prediction output:\")\n",
    "    \n",
    "    # Check if this is prebuilt (13 fields) or custom (2 fields)\n",
    "    if 'denormalized_MAE' in prediction:\n",
    "        print(f\"\\n  üîµ Pre-built Container Response (13 fields)\")\n",
    "        print(f\"     Anomaly Score (MAE): {prediction['denormalized_MAE']:.2f}\")\n",
    "        print(f\"     RMSE: {prediction.get('denormalized_RMSE', 'N/A')}\")\n",
    "        print(f\"     MSE: {prediction.get('denormalized_MSE', 'N/A')}\")\n",
    "        if 'encoded' in prediction:\n",
    "            print(f\"     Encoded (latent): {prediction['encoded']}\")\n",
    "        ENDPOINT_TYPE = 'prebuilt'\n",
    "    elif 'anomaly_score' in prediction:\n",
    "        print(f\"\\n  üü¢ Custom Container Response (2 fields)\")\n",
    "        print(f\"     Anomaly Score: {prediction['anomaly_score']:.2f}\")\n",
    "        print(f\"     Encoded: {prediction['encoded']}\")\n",
    "        ENDPOINT_TYPE = 'custom'\n",
    "    else:\n",
    "        print(f\"  Unknown response format: {prediction}\")\n",
    "        ENDPOINT_TYPE = 'unknown'\n",
    "    \n",
    "    print(f\"\\nüí° This endpoint type: {ENDPOINT_TYPE}\")\n",
    "    print(f\"   Response will have different fields in BigQuery remote model\")\n",
    "else:\n",
    "    print(f\"  Value: {prediction}\")\n",
    "    ENDPOINT_TYPE = 'unknown'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bq_connection",
   "metadata": {},
   "source": [
    "---\n",
    "## Create BigQuery Cloud Resource Connection\n",
    "\n",
    "BigQuery needs a **Cloud Resource Connection** to securely call Vertex AI endpoints.\n",
    "\n",
    "**How it works:**\n",
    "1. Create connection in BigQuery (tied to specific region)\n",
    "2. Connection gets a dedicated service account\n",
    "3. Grant service account `Vertex AI User` role\n",
    "4. Use connection when creating remote model\n",
    "\n",
    "**Regional Compatibility:**\n",
    "- **Vertex AI Endpoint**: `us-central1` (regional location)\n",
    "- **BigQuery Dataset**: `US` (multi-region location)\n",
    "- **BigQuery Connection**: `US` (multi-region - compatible with both!)\n",
    "\n",
    "The `US` multi-region is compatible with `us-central1` endpoints, so we create the connection in `US` to match the dataset location.\n",
    "\n",
    "**Security:**\n",
    "- Service account has minimal permissions (only what's granted)\n",
    "- Connection can be reused across multiple remote models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "check_connection",
   "metadata": {},
   "source": [
    "### Check for Existing Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "check_connection_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for existing connections in US...\n",
      "\n",
      "Found 8 connection(s) in US:\n",
      "  - applied-genai_bq-advisor\n",
      "    Service Account: bqcx-1026793852137-dyw1@gcp-sa-bigquery-condel.iam.gserviceaccount.com\n",
      "  - applied-genai_bq-metadata\n",
      "    Service Account: bqcx-1026793852137-tqpc@gcp-sa-bigquery-condel.iam.gserviceaccount.com\n",
      "  - applied-genai_embed-feature-classifier\n",
      "    Service Account: bqcx-1026793852137-pdxa@gcp-sa-bigquery-condel.iam.gserviceaccount.com\n",
      "  - bqml_2024\n",
      "    Service Account: bqcx-1026793852137-d2h9@gcp-sa-bigquery-condel.iam.gserviceaccount.com\n",
      "  - bqml_genai-sql\n",
      "    Service Account: bqcx-1026793852137-bmph@gcp-sa-bigquery-condel.iam.gserviceaccount.com\n",
      "  - document-processing\n",
      "    Service Account: bqcx-1026793852137-vk6u@gcp-sa-bigquery-condel.iam.gserviceaccount.com\n",
      "  - frameworks_pytorch_bqml_remote_model\n",
      "    Service Account: bqcx-1026793852137-546t@gcp-sa-bigquery-condel.iam.gserviceaccount.com\n",
      "  - working-with-docai_from-bigquery\n",
      "    Service Account: bqcx-1026793852137-te86@gcp-sa-bigquery-condel.iam.gserviceaccount.com\n",
      "\n",
      "‚úÖ Connection already exists: frameworks_pytorch_bqml_remote_model\n"
     ]
    }
   ],
   "source": [
    "# List existing connections in BigQuery region\n",
    "print(f\"Checking for existing connections in {BQ_REGION}...\\n\")\n",
    "\n",
    "# Construct parent path for listing connections\n",
    "parent = f\"projects/{PROJECT_ID}/locations/{BQ_REGION}\"\n",
    "\n",
    "# List all connections in this location\n",
    "try:\n",
    "    connections = connection_client.list_connections(parent=parent)\n",
    "    \n",
    "    connection_list = list(connections)\n",
    "    \n",
    "    if connection_list:\n",
    "        print(f\"Found {len(connection_list)} connection(s) in {BQ_REGION}:\")\n",
    "        for conn in connection_list:\n",
    "            # Extract connection ID from full resource name\n",
    "            conn_id = conn.name.split('/')[-1]\n",
    "            print(f\"  - {conn_id}\")\n",
    "            print(f\"    Service Account: {conn.cloud_resource.service_account_id}\")\n",
    "    else:\n",
    "        print(f\"No connections found in {BQ_REGION}\")\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    # Check if our connection already exists\n",
    "    connection_exists = any(conn.name.endswith(BQ_CONNECTION_NAME) for conn in connection_list)\n",
    "    \n",
    "    if connection_exists:\n",
    "        print(f\"‚úÖ Connection already exists: {BQ_CONNECTION_NAME}\")\n",
    "    else:\n",
    "        print(f\"Connection '{BQ_CONNECTION_NAME}' not found - will create it\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error listing connections: {e}\")\n",
    "    connection_exists = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "create_connection",
   "metadata": {},
   "source": [
    "### Create Connection (if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "create_connection_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Using existing connection\n"
     ]
    }
   ],
   "source": [
    "if not connection_exists:\n",
    "    print(f\"Creating BigQuery Cloud Resource Connection...\")\n",
    "    print(f\"  Name: {BQ_CONNECTION_NAME}\")\n",
    "    print(f\"  Location: {BQ_REGION}\")\n",
    "    print(f\"  Type: CLOUD_RESOURCE\\n\")\n",
    "    \n",
    "    # Construct parent path\n",
    "    parent = f\"projects/{PROJECT_ID}/locations/{BQ_REGION}\"\n",
    "    \n",
    "    # Create connection object\n",
    "    connection = Connection()\n",
    "    connection.cloud_resource = CloudResourceProperties()\n",
    "    \n",
    "    # Create the connection\n",
    "    try:\n",
    "        created_connection = connection_client.create_connection(\n",
    "            parent=parent,\n",
    "            connection=connection,\n",
    "            connection_id=BQ_CONNECTION_NAME\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úÖ Connection created successfully!\")\n",
    "        print(f\"   Full name: {created_connection.name}\")\n",
    "        print(f\"   Service Account: {created_connection.cloud_resource.service_account_id}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to create connection: {e}\")\n",
    "        raise Exception(f\"Connection creation failed: {e}\")\n",
    "else:\n",
    "    print(f\"‚úÖ Using existing connection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "get_service_account",
   "metadata": {},
   "source": [
    "### Get Connection Service Account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "get_service_account_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving connection service account...\n",
      "\n",
      "‚úÖ Connection details:\n",
      "   Connection: statmike-mlops-349915.US.frameworks_pytorch_bqml_remote_model\n",
      "   Full resource name: projects/1026793852137/locations/us/connections/frameworks_pytorch_bqml_remote_model\n",
      "   Service Account: bqcx-1026793852137-546t@gcp-sa-bigquery-condel.iam.gserviceaccount.com\n",
      "   Creation time: 1762727929877\n",
      "\n",
      "üí° This service account needs Vertex AI User role to call endpoints\n"
     ]
    }
   ],
   "source": [
    "# Get connection details to find service account\n",
    "print(f\"Retrieving connection service account...\\n\")\n",
    "\n",
    "# Construct connection name\n",
    "connection_name = f\"projects/{PROJECT_ID}/locations/{BQ_REGION}/connections/{BQ_CONNECTION_NAME}\"\n",
    "\n",
    "try:\n",
    "    # Get the connection\n",
    "    connection = connection_client.get_connection(name=connection_name)\n",
    "    \n",
    "    # Extract service account\n",
    "    service_account = connection.cloud_resource.service_account_id\n",
    "    \n",
    "    print(f\"‚úÖ Connection details:\")\n",
    "    print(f\"   Connection: {PROJECT_ID}.{BQ_REGION}.{BQ_CONNECTION_NAME}\")\n",
    "    print(f\"   Full resource name: {connection.name}\")\n",
    "    print(f\"   Service Account: {service_account}\")\n",
    "    print(f\"   Creation time: {connection.creation_time}\")\n",
    "    print(f\"\\nüí° This service account needs Vertex AI User role to call endpoints\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to retrieve connection: {e}\")\n",
    "    raise Exception(f\"Could not get service account: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grant_permissions",
   "metadata": {},
   "source": [
    "### Grant Vertex AI User Role\n",
    "\n",
    "The connection's service account needs permission to invoke Vertex AI endpoints.\n",
    "\n",
    "**Required role:** `roles/aiplatform.user`\n",
    "\n",
    "**What this allows:**\n",
    "- Call Vertex AI endpoints for predictions\n",
    "- Access deployed models\n",
    "\n",
    "**What this does NOT allow:**\n",
    "- Create/delete endpoints or models\n",
    "- Modify deployment configurations\n",
    "- Access other Vertex AI resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "grant_permissions_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking if service account has Vertex AI User role...\n",
      "  Service Account: bqcx-1026793852137-546t@gcp-sa-bigquery-condel.iam.gserviceaccount.com\n",
      "  Role: roles/aiplatform.user\n",
      "  Project: statmike-mlops-349915\n",
      "\n",
      "‚úÖ Service account already has Vertex AI User role\n",
      "\n",
      "üí° Service account can:\n",
      "   ‚Ä¢ Call Vertex AI endpoints for predictions\n",
      "   ‚Ä¢ Access deployed models\n",
      "\n",
      "‚ö†Ô∏è  Note: This permission is project-wide, not endpoint-specific\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "print(f\"Checking if service account has Vertex AI User role...\")\n",
    "print(f\"  Service Account: {service_account}\")\n",
    "print(f\"  Role: roles/aiplatform.user\")\n",
    "print(f\"  Project: {PROJECT_ID}\\n\")\n",
    "\n",
    "# Check current IAM policy for this service account\n",
    "check_result = subprocess.run(\n",
    "    ['gcloud', 'projects', 'get-iam-policy', PROJECT_ID,\n",
    "     '--flatten=bindings[].members',\n",
    "     '--format=table(bindings.role)',\n",
    "     f'--filter=bindings.members:serviceAccount:{service_account}'],\n",
    "    capture_output=True,\n",
    "    text=True\n",
    ")\n",
    "\n",
    "# Check if the role is already granted\n",
    "has_role = 'roles/aiplatform.user' in check_result.stdout\n",
    "\n",
    "if has_role:\n",
    "    print(f\"‚úÖ Service account already has Vertex AI User role\")\n",
    "    print(f\"\\nüí° Service account can:\")\n",
    "    print(f\"   ‚Ä¢ Call Vertex AI endpoints for predictions\")\n",
    "    print(f\"   ‚Ä¢ Access deployed models\")\n",
    "    print(f\"\\n‚ö†Ô∏è  Note: This permission is project-wide, not endpoint-specific\")\n",
    "else:\n",
    "    print(f\"Role not found. Granting Vertex AI User role...\\n\")\n",
    "    \n",
    "    # Service accounts may take a few seconds to propagate\n",
    "    # Retry up to 3 times with delays\n",
    "    max_retries = 3\n",
    "    retry_delay = 10  # seconds\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        if attempt > 0:\n",
    "            print(f\"‚è≥ Waiting {retry_delay} seconds for service account to propagate...\")\n",
    "            time.sleep(retry_delay)\n",
    "            print(f\"   Retry attempt {attempt + 1}/{max_retries}...\\n\")\n",
    "        \n",
    "        result = subprocess.run(\n",
    "            ['gcloud', 'projects', 'add-iam-policy-binding', PROJECT_ID,\n",
    "             f'--member=serviceAccount:{service_account}',\n",
    "             '--role=roles/aiplatform.user',\n",
    "             '--condition=None'],  # No conditions on this binding\n",
    "            capture_output=True,\n",
    "            text=True\n",
    "        )\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            print(f\"‚úÖ Granted Vertex AI User role\")\n",
    "            print(f\"\\nüí° Service account can now:\")\n",
    "            print(f\"   ‚Ä¢ Call Vertex AI endpoints for predictions\")\n",
    "            print(f\"   ‚Ä¢ Access deployed models\")\n",
    "            print(f\"\\n‚ö†Ô∏è  Note: This permission is project-wide, not endpoint-specific\")\n",
    "            break\n",
    "        elif \"already exists\" in result.stderr or \"Policy already contains role\" in result.stderr:\n",
    "            print(f\"‚úÖ Service account already has Vertex AI User role (verified during grant)\")\n",
    "            break\n",
    "        elif \"does not exist\" in result.stderr and attempt < max_retries - 1:\n",
    "            # Service account not yet propagated, continue to retry\n",
    "            continue\n",
    "        else:\n",
    "            # Final attempt failed\n",
    "            print(f\"‚ùå Failed to grant role: {result.stderr}\")\n",
    "            print(f\"\\nüí° You may need to grant this manually:\")\n",
    "            print(f\"   1. Go to Cloud Console > IAM\")\n",
    "            print(f\"   2. Click '+ Grant Access'\")\n",
    "            print(f\"   3. Principal: {service_account}\")\n",
    "            print(f\"   4. Role: Vertex AI User\")\n",
    "            print(f\"   5. Click Save\")\n",
    "            \n",
    "            # Don't raise exception, allow user to grant manually and continue\n",
    "            print(f\"\\n‚ö†Ô∏è  Continuing anyway - you may need to grant permission manually before creating remote model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "create_remote_model",
   "metadata": {},
   "source": [
    "---\n",
    "## Create BigQuery ML Remote Model\n",
    "\n",
    "Now that the connection is configured, create the BigQuery ML remote model that points to the Vertex AI endpoint.\n",
    "\n",
    "**Endpoint Type Handling:**\n",
    "\n",
    "The two endpoint types have different response signatures, so the remote model needs different input/output specifications:\n",
    "\n",
    "**Pre-built Container** (13 output fields):\n",
    "```sql\n",
    "OUTPUT (\n",
    "  denormalized_MAE FLOAT64,\n",
    "  denormalized_RMSE FLOAT64,\n",
    "  denormalized_MSE FLOAT64,\n",
    "  denormalized_MSLE FLOAT64,\n",
    "  normalized_MAE FLOAT64,\n",
    "  ...\n",
    ")\n",
    "```\n",
    "\n",
    "**Custom Container** (2 output fields):\n",
    "```sql\n",
    "OUTPUT (\n",
    "  anomaly_score FLOAT64,\n",
    "  encoded ARRAY<FLOAT64>\n",
    ")\n",
    "```\n",
    "\n",
    "This section will:\n",
    "1. Detect which endpoint type you're using\n",
    "2. Build the appropriate CREATE MODEL statement\n",
    "3. Create the remote model with correct input/output schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "define_model_schema",
   "metadata": {},
   "source": [
    "### Define Model Input/Output Schema\n",
    "\n",
    "Based on the endpoint type detected earlier, define the appropriate schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "define_model_schema_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input schema (30 features):\n",
      "  Time FLOAT64, V1 FLOAT64, V2 FLOAT64, V3 FLOAT64, V4 FLOAT64, V5 FLOAT64, V6 FLOAT64, V7 FLOAT64, V8...\n",
      "\n",
      "üîµ Detected Pre-built Container Endpoint\n",
      "   Output: 13 fields (full model diagnostics)\n",
      "\n",
      "Output schema:\n",
      "\n",
      "        denormalized_MAE FLOAT64,\n",
      "        denormalized_RMSE FLOAT64,\n",
      "        denormalized_MSE FLOAT64,\n",
      "        denormalized_MSLE FLOAT64,\n",
      "        normalized_MAE FLOAT64,\n",
      "        normalized_RMSE FLOAT64,\n",
      "        normalized_MSE FLOAT64,\n",
      "        normalized_MSLE FLOAT64,\n",
      "        encoded ARRAY<FLOAT64>,\n",
      "        normalized_reconstruction ARRAY<FLOAT64>,\n",
      "        normalized_reconstruction_errors ARRAY<FLOAT64>,\n",
      "        denormalized_reconstruction ARRAY<FLOAT64>,\n",
      "        denormalized_reconstruction_errors ARRAY<FLOAT64>\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "# Ensure ENDPOINT_TYPE is defined (in case user skipped the test endpoint section)\n",
    "try:\n",
    "    ENDPOINT_TYPE\n",
    "except NameError:\n",
    "    print(f\"‚ö†Ô∏è  ENDPOINT_TYPE not set. Auto-detecting from endpoint...\")\n",
    "    # Make a quick test prediction to detect endpoint type\n",
    "    test_response = endpoint.predict(instances=[[0.0] * 30])\n",
    "    test_pred = test_response.predictions[0]\n",
    "    \n",
    "    if 'denormalized_MAE' in test_pred:\n",
    "        ENDPOINT_TYPE = 'prebuilt'\n",
    "        print(f\"   Detected: Pre-built container (13 fields)\")\n",
    "    elif 'anomaly_score' in test_pred:\n",
    "        ENDPOINT_TYPE = 'custom'\n",
    "        print(f\"   Detected: Custom container (2 fields)\")\n",
    "    else:\n",
    "        ENDPOINT_TYPE = 'prebuilt'\n",
    "        print(f\"   Could not detect - defaulting to prebuilt\")\n",
    "\n",
    "# Input schema (same for both endpoint types - 30 features)\n",
    "input_features = list(test_df.columns)\n",
    "input_schema = \", \".join([f\"{col} FLOAT64\" for col in input_features])\n",
    "\n",
    "print(f\"Input schema ({len(input_features)} features):\")\n",
    "print(f\"  {input_schema[:100]}...\\n\")\n",
    "\n",
    "# Output schema depends on endpoint type\n",
    "if ENDPOINT_TYPE == 'prebuilt':\n",
    "    print(f\"üîµ Detected Pre-built Container Endpoint\")\n",
    "    print(f\"   Output: 13 fields (full model diagnostics)\\n\")\n",
    "    \n",
    "    output_schema = \"\"\"\n",
    "        denormalized_MAE FLOAT64,\n",
    "        denormalized_RMSE FLOAT64,\n",
    "        denormalized_MSE FLOAT64,\n",
    "        denormalized_MSLE FLOAT64,\n",
    "        normalized_MAE FLOAT64,\n",
    "        normalized_RMSE FLOAT64,\n",
    "        normalized_MSE FLOAT64,\n",
    "        normalized_MSLE FLOAT64,\n",
    "        encoded ARRAY<FLOAT64>,\n",
    "        normalized_reconstruction ARRAY<FLOAT64>,\n",
    "        normalized_reconstruction_errors ARRAY<FLOAT64>,\n",
    "        denormalized_reconstruction ARRAY<FLOAT64>,\n",
    "        denormalized_reconstruction_errors ARRAY<FLOAT64>\n",
    "    \"\"\"\n",
    "    \n",
    "elif ENDPOINT_TYPE == 'custom':\n",
    "    print(f\"üü¢ Detected Custom Container Endpoint\")\n",
    "    print(f\"   Output: 2 fields (simplified)\\n\")\n",
    "    \n",
    "    output_schema = \"\"\"\n",
    "        anomaly_score FLOAT64,\n",
    "        encoded ARRAY<FLOAT64>\n",
    "    \"\"\"\n",
    "    \n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è  Unknown endpoint type: {ENDPOINT_TYPE}\")\n",
    "    print(f\"   Defaulting to prebuilt schema (13 fields)\")\n",
    "    print(f\"\\nüí° If predictions fail, check the endpoint response structure\")\n",
    "    \n",
    "    # Default to prebuilt schema\n",
    "    output_schema = \"\"\"\n",
    "        denormalized_MAE FLOAT64,\n",
    "        denormalized_RMSE FLOAT64,\n",
    "        denormalized_MSE FLOAT64,\n",
    "        denormalized_MSLE FLOAT64,\n",
    "        normalized_MAE FLOAT64,\n",
    "        normalized_RMSE FLOAT64,\n",
    "        normalized_MSE FLOAT64,\n",
    "        normalized_MSLE FLOAT64,\n",
    "        encoded ARRAY<FLOAT64>,\n",
    "        normalized_reconstruction ARRAY<FLOAT64>,\n",
    "        normalized_reconstruction_errors ARRAY<FLOAT64>,\n",
    "        denormalized_reconstruction ARRAY<FLOAT64>,\n",
    "        denormalized_reconstruction_errors ARRAY<FLOAT64>\n",
    "    \"\"\"\n",
    "\n",
    "print(f\"Output schema:\")\n",
    "print(output_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "create_model_sql",
   "metadata": {},
   "source": [
    "### Create Remote Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "create_model_sql_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating BigQuery ML remote model...\n",
      "  Model: frameworks.frameworks_pytorch_bqml_remote_model\n",
      "  Connection: statmike-mlops-349915.US.frameworks_pytorch_bqml_remote_model\n",
      "  Endpoint: projects/1026793852137/locations/us-central1/endpoints/5971323405637517312\n",
      "  Endpoint region: us-central1\n",
      "  Connection region: US\n",
      "\n",
      "üí° Model uses individual feature columns as input\n",
      "   BigQuery ML will automatically construct instances array for Vertex AI endpoint\n",
      "\n",
      "‚úÖ Remote model created successfully!\n",
      "\n",
      "üìä Model details:\n",
      "   Full name: frameworks.frameworks_pytorch_bqml_remote_model\n",
      "   Endpoint type: prebuilt\n",
      "   Input: 30 feature columns\n",
      "   Output fields: 13\n",
      "\n",
      "üí° View in BigQuery Console:\n",
      "   https://console.cloud.google.com/bigquery?project=statmike-mlops-349915&ws=!1m5!1m4!4m3!1sstatmike-mlops-349915!2sframeworks!3sframeworks_pytorch_bqml_remote_model\n"
     ]
    }
   ],
   "source": [
    "# Remote model name\n",
    "REMOTE_MODEL_NAME = f\"{BQ_DATASET}.{SERIES.replace('-', '_')}_{EXPERIMENT.replace('-', '_')}\"\n",
    "\n",
    "print(f\"Creating BigQuery ML remote model...\")\n",
    "print(f\"  Model: {REMOTE_MODEL_NAME}\")\n",
    "print(f\"  Connection: {PROJECT_ID}.{BQ_REGION}.{BQ_CONNECTION_NAME}\")\n",
    "print(f\"  Endpoint: {endpoint.resource_name}\")\n",
    "print(f\"  Endpoint region: {REGION}\")\n",
    "print(f\"  Connection region: {BQ_REGION}\\n\")\n",
    "\n",
    "# Define input schema with individual feature columns\n",
    "# BigQuery ML will automatically construct the instances array for Vertex AI\n",
    "create_model_query = f\"\"\"\n",
    "CREATE OR REPLACE MODEL `{REMOTE_MODEL_NAME}`\n",
    "    INPUT ({input_schema})\n",
    "    OUTPUT ({output_schema})\n",
    "    REMOTE WITH CONNECTION `{PROJECT_ID}.{BQ_REGION}.{BQ_CONNECTION_NAME}`\n",
    "    OPTIONS(\n",
    "        endpoint = 'https://{REGION}-aiplatform.googleapis.com/v1/{endpoint.resource_name}'\n",
    "    )\n",
    "\"\"\"\n",
    "\n",
    "print(f\"üí° Model uses individual feature columns as input\")\n",
    "print(f\"   BigQuery ML will automatically construct instances array for Vertex AI endpoint\\n\")\n",
    "\n",
    "# Execute query\n",
    "job = bq.query(query=create_model_query)\n",
    "job.result()\n",
    "\n",
    "if job.state == 'DONE':\n",
    "    print(f\"‚úÖ Remote model created successfully!\")\n",
    "    print(f\"\\nüìä Model details:\")\n",
    "    print(f\"   Full name: {REMOTE_MODEL_NAME}\")\n",
    "    print(f\"   Endpoint type: {ENDPOINT_TYPE}\")\n",
    "    print(f\"   Input: {len(input_features)} feature columns\")\n",
    "    print(f\"   Output fields: {len([x for x in output_schema.split(',') if x.strip()])}\")\n",
    "    \n",
    "    print(f\"\\nüí° View in BigQuery Console:\")\n",
    "    print(f\"   https://console.cloud.google.com/bigquery?project={PROJECT_ID}&ws=!1m5!1m4!4m3!1s{PROJECT_ID}!2s{BQ_DATASET}!3s{REMOTE_MODEL_NAME.split('.')[-1]}\")\n",
    "else:\n",
    "    print(f\"‚ùå Model creation failed\")\n",
    "    print(f\"   State: {job.state}\")\n",
    "    if job.errors:\n",
    "        print(f\"   Errors: {job.errors}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "test_remote_model",
   "metadata": {},
   "source": [
    "---\n",
    "## Test Remote Model with ML.PREDICT\n",
    "\n",
    "Now that the remote model is created, test it with SQL using `ML.PREDICT()`.\n",
    "\n",
    "We'll demonstrate three progressively complex examples:\n",
    "1. **Simple prediction**: Get full prediction output\n",
    "2. **Extract specific fields**: Work with STRUCT to get individual values\n",
    "3. **Apply business logic**: Use predictions for decision-making"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "example1_simple",
   "metadata": {},
   "source": [
    "### Example 1: Simple Prediction\n",
    "\n",
    "Get predictions for a single test record with full output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "example1_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running simple ML.PREDICT query...\n",
      "\n",
      "SQL Query:\n",
      "\n",
      "SELECT *\n",
      "FROM ML.PREDICT(\n",
      "    MODEL `frameworks.frameworks_pytorch_bqml_remote_model`,\n",
      "    (\n",
      "        SELECT Time, V1, V2, V3, V4, V5, V6, V7, V8, V9, V10, V11, V12, V13, V14, V15, V16, V17, V18, V19, V20, V21, V22, V23, V24, V25, V26, V27, V28, Amount\n",
      "        FROM `statmike-mlops-349915.frameworks.frameworks`\n",
      "        WHERE splits = 'TEST'\n",
      "        LIMIT 1\n",
      "    )\n",
      ")\n",
      "\n",
      "\n",
      "‚úÖ Prediction query executed!\n",
      "\n",
      "üìä Result columns (44 total):\n",
      "   ['denormalized_MAE', 'denormalized_RMSE', 'denormalized_MSE', 'denormalized_MSLE', 'normalized_MAE', 'normalized_RMSE', 'normalized_MSE', 'normalized_MSLE', 'encoded', 'normalized_reconstruction', 'normalized_reconstruction_errors', 'denormalized_reconstruction', 'denormalized_reconstruction_errors', 'remote_model_status', 'Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount']\n",
      "\n",
      "First prediction:\n",
      "   denormalized_MAE  denormalized_RMSE  denormalized_MSE  denormalized_MSLE  normalized_MAE  normalized_RMSE  normalized_MSE  normalized_MSLE                              encoded                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 normalized_reconstruction                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  normalized_reconstruction_errors                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           denormalized_reconstruction                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          denormalized_reconstruction_errors remote_model_status      Time        V1        V2        V3        V4        V5        V6        V7        V8        V9       V10       V11       V12       V13       V14       V15       V16       V17       V18       V19       V20       V21       V22       V23       V24       V25       V26       V27       V28  Amount\n",
      "0        150.206619         816.010925       665873.8125           0.406234        0.728919         1.027713        1.056194         0.115544  [0.0, 0.0, 0.2779858708381653, 0.0]  [0.496685802936554, -0.1663977950811386, 0.3092061579227448, -0.1292876303195953, -0.2710019052028656, 0.2666797637939453, -0.3458425104618073, 0.3410252332687378, -0.01824184879660606, -0.1457267850637436, -0.2188219875097275, -0.1709446161985397, 0.07083847373723984, -0.02429202198982239, 0.06421135365962982, -0.07479119300842285, 0.1612516939640045, -0.2596517503261566, 0.06413978338241577, -0.03770392760634422, -0.06590954214334488, -0.02667352557182312, -0.006648033857345581, 0.00349387526512146, 0.04215648397803307, -0.2747231423854828, 0.02514052763581276, 0.01440841518342495, 0.04091453552246094, -0.2871989011764526]  [0.094093918800354, -0.5257938504219055, -0.04655492305755615, 1.229093194007874, 1.914798140525818, 1.474638223648071, 0.6522809863090515, -0.1688978523015976, 0.3831289708614349, -1.520434617996216, 0.75139981508255, -0.03715351223945618, 0.1887734830379486, 0.1684851795434952, -0.004654370248317719, -2.320516586303711, 1.607786893844604, -1.744550108909607, -0.1875995397567749, -1.644443988800049, 0.1549994051456451, -0.4425396025180817, -2.014584064483643, 0.3060042560100555, 0.06971314549446106, -0.195423424243927, -1.470040440559387, 0.08220881223678589, 0.4061190187931061, -0.06085675954818726]  [118489.5546875, -0.3120456039905548, 0.4992930889129639, -0.1774093806743622, -0.387037605047226, 0.3680939078330994, -0.4575689435005188, 0.4135843813419342, -0.02196572721004486, -0.1536415815353394, -0.2177727967500687, -0.1784695535898209, 0.07708073407411575, -0.02406852692365646, 0.07058779150247574, -0.06948549300432205, 0.1438623368740082, -0.1822962909936905, 0.05712198466062546, -0.03122163191437721, -0.05218391120433807, -0.02036896347999573, -0.004269406199455261, 0.002231955761089921, 0.02611784636974335, -0.1438862681388855, 0.01171431876718998, 0.005227732937783003, 0.01374295633286238, 15.46217060089111]  [4469.4453125, -1.015251278877258, -0.07638868689537048, 1.794914245605469, 2.678233623504639, 2.006961107254028, 0.8693035840988159, -0.200067475438118, 0.4467087984085083, -1.655982732772827, 0.7811969518661499, -0.03724606335163116, 0.1786646544933319, 0.1677893996238708, -0.004170335829257965, -2.123207807540894, 1.356181383132935, -1.300755023956299, -0.1544767022132874, -1.334759950637817, 0.1207556128501892, -0.3175970315933228, -1.457689642906189, 0.1903721839189529, 0.04216326773166656, -0.1018387824296951, -0.7093685269355774, 0.03298798203468323, 0.136316329240799, -15.46217060089111]                None  122959.0 -1.327297  0.422904  1.617505  2.291196  2.375055  0.411735  0.213517  0.424743 -1.809624  0.563424 -0.215716  0.255745  0.143721  0.066417 -2.192693  1.500044 -1.483051 -0.097355 -1.365982  0.068572 -0.337966 -1.461959  0.192604  0.068281 -0.245725 -0.697654  0.038216  0.150059     0.0\n",
      "\n",
      "‚úÖ Pre-built Container Output:\n",
      "   Anomaly Score (MAE): 150.21\n",
      "   RMSE: 816.01\n",
      "   MSE: 665873.81\n",
      "   Encoded (latent): [0.         0.         0.27798587 0.        ]...\n",
      "\n",
      "üí° BigQuery ML automatically wraps features in instances array for Vertex AI\n"
     ]
    }
   ],
   "source": [
    "print(f\"Running simple ML.PREDICT query...\\n\")\n",
    "\n",
    "# Query selects individual feature columns\n",
    "# BigQuery ML automatically constructs the instances array for Vertex AI\n",
    "query = f\"\"\"\n",
    "SELECT *\n",
    "FROM ML.PREDICT(\n",
    "    MODEL `{REMOTE_MODEL_NAME}`,\n",
    "    (\n",
    "        SELECT {\", \".join(input_features)}\n",
    "        FROM `{PROJECT_ID}.{BQ_DATASET}.{BQ_SOURCE_TABLE}`\n",
    "        WHERE splits = 'TEST'\n",
    "        LIMIT 1\n",
    "    )\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "print(f\"SQL Query:\")\n",
    "print(query)\n",
    "print()\n",
    "\n",
    "result_df = bq.query(query).to_dataframe()\n",
    "\n",
    "print(f\"‚úÖ Prediction query executed!\")\n",
    "print(f\"\\nüìä Result columns ({len(result_df.columns)} total):\")\n",
    "print(f\"   {list(result_df.columns)}\\n\")\n",
    "\n",
    "# Show first row\n",
    "print(f\"First prediction:\")\n",
    "print(result_df.head(1).to_string())\n",
    "\n",
    "# Extract key prediction fields based on endpoint type\n",
    "if ENDPOINT_TYPE == 'prebuilt':\n",
    "    if not pd.isna(result_df['denormalized_MAE'].iloc[0]):\n",
    "        print(f\"\\n‚úÖ Pre-built Container Output:\")\n",
    "        print(f\"   Anomaly Score (MAE): {result_df['denormalized_MAE'].iloc[0]:.2f}\")\n",
    "        print(f\"   RMSE: {result_df['denormalized_RMSE'].iloc[0]:.2f}\")\n",
    "        print(f\"   MSE: {result_df['denormalized_MSE'].iloc[0]:.2f}\")\n",
    "        print(f\"   Encoded (latent): {result_df['encoded'].iloc[0][:4]}...\")\n",
    "    else:\n",
    "        print(f\"\\n‚ö†Ô∏è  Got NaN predictions - check remote_model_status column\")\n",
    "        if 'remote_model_status' in result_df.columns:\n",
    "            print(f\"   Status: {result_df['remote_model_status'].iloc[0]}\")\n",
    "elif ENDPOINT_TYPE == 'custom':\n",
    "    if not pd.isna(result_df['anomaly_score'].iloc[0]):\n",
    "        print(f\"\\n‚úÖ Custom Container Output:\")\n",
    "        print(f\"   Anomaly Score: {result_df['anomaly_score'].iloc[0]:.2f}\")\n",
    "        print(f\"   Encoded: {result_df['encoded'].iloc[0][:4]}...\")\n",
    "    else:\n",
    "        print(f\"\\n‚ö†Ô∏è  Got NaN predictions - check remote_model_status column\")\n",
    "        if 'remote_model_status' in result_df.columns:\n",
    "            print(f\"   Status: {result_df['remote_model_status'].iloc[0]}\")\n",
    "\n",
    "print(f\"\\nüí° BigQuery ML automatically wraps features in instances array for Vertex AI\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "example2_extract",
   "metadata": {},
   "source": [
    "### Example 2: Extract Specific Fields\n",
    "\n",
    "In practice, you often only need specific prediction fields. Show how to extract just the anomaly score and a few key metrics.\n",
    "\n",
    "**Why this matters:**\n",
    "- Cleaner query results (fewer columns)\n",
    "- Better performance (less data transferred)\n",
    "- Easier to join with other tables\n",
    "- Simpler downstream processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "example2_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running ML.PREDICT with field extraction...\n",
      "\n",
      "‚úÖ Prediction successful!\n",
      "\n",
      "üìä Extracted fields (4 columns):\n",
      "   ['anomaly_score', 'rmse', 'mse', 'latent_encoding']\n",
      "\n",
      "Results:\n",
      "   anomaly_score         rmse           mse                      latent_encoding\n",
      "0     150.206619   816.010925  6.658738e+05  [0.0, 0.0, 0.2779858708381653, 0.0]\n",
      "1      78.623901   423.065216  1.789842e+05  [0.0, 0.0, 0.2594718337059021, 0.0]\n",
      "2     553.305237  3023.149170  9.139431e+06  [0.0, 0.0, 0.1517185717821121, 0.0]\n",
      "3     700.801941  3830.523193  1.467291e+07  [0.0, 0.0, 0.1383998543024063, 0.0]\n",
      "4     818.971375  4477.672363  2.004955e+07   [0.0, 0.0, 13.52433013916016, 0.0]\n",
      "\n",
      "üí° Benefits of field extraction:\n",
      "   ‚Ä¢ Only get data you need\n",
      "   ‚Ä¢ Rename fields for clarity (denormalized_MAE ‚Üí anomaly_score)\n",
      "   ‚Ä¢ Easier to use in downstream queries\n"
     ]
    }
   ],
   "source": [
    "print(f\"Running ML.PREDICT with field extraction...\\n\")\n",
    "\n",
    "# Query varies by endpoint type\n",
    "if ENDPOINT_TYPE == 'prebuilt':\n",
    "    query = f\"\"\"\n",
    "    SELECT\n",
    "        -- Extracted prediction fields\n",
    "        denormalized_MAE as anomaly_score,\n",
    "        denormalized_RMSE as rmse,\n",
    "        denormalized_MSE as mse,\n",
    "        encoded as latent_encoding\n",
    "    FROM ML.PREDICT(\n",
    "        MODEL `{REMOTE_MODEL_NAME}`,\n",
    "        (\n",
    "            SELECT {\", \".join(input_features)}\n",
    "            FROM `{PROJECT_ID}.{BQ_DATASET}.{BQ_SOURCE_TABLE}`\n",
    "            WHERE splits = 'TEST'\n",
    "            LIMIT 5\n",
    "        )\n",
    "    )\n",
    "    \"\"\"\n",
    "elif ENDPOINT_TYPE == 'custom':\n",
    "    query = f\"\"\"\n",
    "    SELECT\n",
    "        -- Extracted prediction fields\n",
    "        anomaly_score,\n",
    "        encoded as latent_encoding\n",
    "    FROM ML.PREDICT(\n",
    "        MODEL `{REMOTE_MODEL_NAME}`,\n",
    "        (\n",
    "            SELECT {\", \".join(input_features)}\n",
    "            FROM `{PROJECT_ID}.{BQ_DATASET}.{BQ_SOURCE_TABLE}`\n",
    "            WHERE splits = 'TEST'\n",
    "            LIMIT 5\n",
    "        )\n",
    "    )\n",
    "    \"\"\"\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è  Skipping example - unknown endpoint type\")\n",
    "    query = None\n",
    "\n",
    "if query:\n",
    "    result_df = bq.query(query).to_dataframe()\n",
    "\n",
    "    print(f\"‚úÖ Prediction successful!\")\n",
    "    print(f\"\\nüìä Extracted fields ({len(result_df.columns)} columns):\")\n",
    "    print(f\"   {list(result_df.columns)}\\n\")\n",
    "\n",
    "    print(f\"Results:\")\n",
    "    print(result_df.to_string())\n",
    "\n",
    "    print(f\"\\nüí° Benefits of field extraction:\")\n",
    "    print(f\"   ‚Ä¢ Only get data you need\")\n",
    "    print(f\"   ‚Ä¢ Rename fields for clarity (denormalized_MAE ‚Üí anomaly_score)\")\n",
    "    print(f\"   ‚Ä¢ Easier to use in downstream queries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "example3_logic",
   "metadata": {},
   "source": [
    "### Example 3: Apply Business Logic\n",
    "\n",
    "Use predictions to make decisions in SQL. This example:\n",
    "- Classifies transactions as normal or anomaly based on threshold\n",
    "- Adds risk levels (low, medium, high)\n",
    "- Shows how to use predictions for filtering and grouping\n",
    "\n",
    "**Real-world use case:**\n",
    "- Fraud detection: Flag transactions above anomaly threshold\n",
    "- Quality control: Identify defective products\n",
    "- Customer segmentation: Group by latent encoding similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "example3_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running ML.PREDICT with business logic...\n",
      "\n",
      "Anomaly thresholds:\n",
      "  Medium risk: > 100.0\n",
      "  High risk: > 200.0\n",
      "\n",
      "‚úÖ Prediction with business logic successful!\n",
      "\n",
      "üìä Top 10 highest risk transactions:\n",
      "\n",
      "   anomaly_score        risk_category  is_anomaly  risk_score\n",
      "0    3096.734375  ANOMALY - HIGH RISK           1         100\n",
      "1    2524.846191  ANOMALY - HIGH RISK           1         100\n",
      "2    2359.387695  ANOMALY - HIGH RISK           1         100\n",
      "3    2286.602295  ANOMALY - HIGH RISK           1         100\n",
      "4    2014.056885  ANOMALY - HIGH RISK           1         100\n",
      "5    1982.533569  ANOMALY - HIGH RISK           1         100\n",
      "6    1919.844116  ANOMALY - HIGH RISK           1         100\n",
      "7    1831.535522  ANOMALY - HIGH RISK           1         100\n",
      "8    1798.430176  ANOMALY - HIGH RISK           1         100\n",
      "9    1792.919434  ANOMALY - HIGH RISK           1         100\n",
      "\n",
      "üìà Risk Distribution (from 100 test transactions):\n",
      "           risk_category  count  avg_anomaly_score  min_anomaly_score  max_anomaly_score\n",
      "0    ANOMALY - HIGH RISK     77             898.86             207.45            3096.73\n",
      "1  ANOMALY - MEDIUM RISK      9             137.57             105.93             183.15\n",
      "2                 NORMAL     14              42.74               7.19              83.30\n",
      "\n",
      "üí° Use cases for business logic:\n",
      "   ‚Ä¢ Filter: WHERE is_anomaly = 1 (only anomalies)\n",
      "   ‚Ä¢ Route: Send high-risk transactions to manual review\n",
      "   ‚Ä¢ Alert: Trigger notifications for ANOMALY - HIGH RISK\n",
      "   ‚Ä¢ Dashboard: GROUP BY risk_category for visualization\n"
     ]
    }
   ],
   "source": [
    "print(f\"Running ML.PREDICT with business logic...\\n\")\n",
    "\n",
    "# Define anomaly thresholds (these would be tuned based on validation data)\n",
    "THRESHOLD_MEDIUM = 100.0  # Medium risk\n",
    "THRESHOLD_HIGH = 200.0    # High risk\n",
    "\n",
    "print(f\"Anomaly thresholds:\")\n",
    "print(f\"  Medium risk: > {THRESHOLD_MEDIUM}\")\n",
    "print(f\"  High risk: > {THRESHOLD_HIGH}\\n\")\n",
    "\n",
    "# Query varies by endpoint type\n",
    "if ENDPOINT_TYPE == 'prebuilt':\n",
    "    anomaly_field = 'denormalized_MAE'\n",
    "elif ENDPOINT_TYPE == 'custom':\n",
    "    anomaly_field = 'anomaly_score'\n",
    "else:\n",
    "    anomaly_field = 'denormalized_MAE'  # Default\n",
    "\n",
    "query = f\"\"\"\n",
    "SELECT\n",
    "    {anomaly_field} as anomaly_score,\n",
    "\n",
    "    -- Business logic: Classify transaction\n",
    "    CASE\n",
    "        WHEN {anomaly_field} > {THRESHOLD_HIGH} THEN 'ANOMALY - HIGH RISK'\n",
    "        WHEN {anomaly_field} > {THRESHOLD_MEDIUM} THEN 'ANOMALY - MEDIUM RISK'\n",
    "        ELSE 'NORMAL'\n",
    "    END as risk_category,\n",
    "\n",
    "    -- Binary flag for filtering\n",
    "    CASE WHEN {anomaly_field} > {THRESHOLD_MEDIUM} THEN 1 ELSE 0 END as is_anomaly,\n",
    "\n",
    "    -- Risk score (0-100)\n",
    "    LEAST(100, CAST({anomaly_field} / {THRESHOLD_HIGH} * 100 AS INT64)) as risk_score\n",
    "\n",
    "FROM ML.PREDICT(\n",
    "    MODEL `{REMOTE_MODEL_NAME}`,\n",
    "    (\n",
    "        SELECT {\", \".join(input_features)}\n",
    "        FROM `{PROJECT_ID}.{BQ_DATASET}.{BQ_SOURCE_TABLE}`\n",
    "        WHERE splits = 'TEST'\n",
    "        LIMIT 100\n",
    "    )\n",
    ")\n",
    "ORDER BY anomaly_score DESC\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "\n",
    "result_df = bq.query(query).to_dataframe()\n",
    "\n",
    "print(f\"‚úÖ Prediction with business logic successful!\")\n",
    "print(f\"\\nüìä Top 10 highest risk transactions:\\n\")\n",
    "print(result_df.to_string())\n",
    "\n",
    "# Summary statistics\n",
    "print(f\"\\nüìà Risk Distribution (from 100 test transactions):\")\n",
    "summary_query = f\"\"\"\n",
    "SELECT\n",
    "    CASE\n",
    "        WHEN {anomaly_field} > {THRESHOLD_HIGH} THEN 'ANOMALY - HIGH RISK'\n",
    "        WHEN {anomaly_field} > {THRESHOLD_MEDIUM} THEN 'ANOMALY - MEDIUM RISK'\n",
    "        ELSE 'NORMAL'\n",
    "    END as risk_category,\n",
    "    COUNT(*) as count,\n",
    "    ROUND(AVG({anomaly_field}), 2) as avg_anomaly_score,\n",
    "    ROUND(MIN({anomaly_field}), 2) as min_anomaly_score,\n",
    "    ROUND(MAX({anomaly_field}), 2) as max_anomaly_score\n",
    "FROM ML.PREDICT(\n",
    "    MODEL `{REMOTE_MODEL_NAME}`,\n",
    "    (\n",
    "        SELECT {\", \".join(input_features)}\n",
    "        FROM `{PROJECT_ID}.{BQ_DATASET}.{BQ_SOURCE_TABLE}`\n",
    "        WHERE splits = 'TEST'\n",
    "        LIMIT 100\n",
    "    )\n",
    ")\n",
    "GROUP BY risk_category\n",
    "ORDER BY avg_anomaly_score DESC\n",
    "\"\"\"\n",
    "\n",
    "summary_df = bq.query(summary_query).to_dataframe()\n",
    "print(summary_df.to_string())\n",
    "\n",
    "print(f\"\\nüí° Use cases for business logic:\")\n",
    "print(f\"   ‚Ä¢ Filter: WHERE is_anomaly = 1 (only anomalies)\")\n",
    "print(f\"   ‚Ä¢ Route: Send high-risk transactions to manual review\")\n",
    "print(f\"   ‚Ä¢ Alert: Trigger notifications for ANOMALY - HIGH RISK\")\n",
    "print(f\"   ‚Ä¢ Dashboard: GROUP BY risk_category for visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "batch_scoring",
   "metadata": {},
   "source": [
    "---\n",
    "## Batch Scoring: Create Enriched Table\n",
    "\n",
    "One of the most powerful use cases for BigQuery ML remote models is **batch scoring** - scoring large datasets and persisting results to tables.\n",
    "\n",
    "**Why create prediction tables:**\n",
    "- **Performance**: Pre-compute predictions once, query many times\n",
    "- **Cost**: Avoid repeated endpoint calls for the same data\n",
    "- **Joining**: Easily join predictions with other tables\n",
    "- **Tracking**: Keep historical predictions for analysis\n",
    "- **Sharing**: Give SQL analysts access to predictions\n",
    "\n",
    "**This section will:**\n",
    "1. Score all TEST transactions (could be millions of rows)\n",
    "2. Create enriched table with predictions + business logic\n",
    "3. Show how to query the predictions table\n",
    "4. Demonstrate joining predictions with original data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "create_predictions_table",
   "metadata": {},
   "source": [
    "### Create Predictions Table\n",
    "\n",
    "This creates a table with:\n",
    "- Original transaction features\n",
    "- Anomaly scores from the model\n",
    "- Business logic classifications (risk levels)\n",
    "- Timestamp for tracking when predictions were made"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "create_predictions_table_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating predictions table with batch scoring...\n",
      "  Source: statmike-mlops-349915.frameworks.frameworks (TEST split)\n",
      "  Target: statmike-mlops-349915.frameworks.bqml_remote_model_predictions\n",
      "\n",
      "Executing CREATE TABLE query...\n",
      "‚è±Ô∏è  This may take a few minutes depending on data size...\n",
      "\n",
      "‚úÖ Predictions table created successfully!\n",
      "\n",
      "üìä Table details:\n",
      "   Table: statmike-mlops-349915.frameworks.bqml_remote_model_predictions\n",
      "   Rows: 28,588\n",
      "   Size: 7.33 MB\n",
      "   Columns: 20\n",
      "\n",
      "üí° View in BigQuery Console:\n",
      "   https://console.cloud.google.com/bigquery?project=statmike-mlops-349915&ws=!1m5!1m4!4m3!1sstatmike-mlops-349915!2sframeworks!3sbqml_remote_model_predictions\n"
     ]
    }
   ],
   "source": [
    "print(f\"Creating predictions table with batch scoring...\")\n",
    "print(f\"  Source: {PROJECT_ID}.{BQ_DATASET}.{BQ_SOURCE_TABLE} (TEST split)\")\n",
    "print(f\"  Target: {PROJECT_ID}.{BQ_DATASET}.{BQ_PREDICTIONS_TABLE}\\n\")\n",
    "\n",
    "# Query varies by endpoint type\n",
    "if ENDPOINT_TYPE == 'prebuilt':\n",
    "    anomaly_field = 'denormalized_MAE'\n",
    "    extra_fields = ''',\n",
    "        pred.denormalized_RMSE as rmse,\n",
    "        pred.denormalized_MSE as mse,\n",
    "        pred.denormalized_MSLE as msle,\n",
    "        pred.encoded as latent_encoding'''\n",
    "elif ENDPOINT_TYPE == 'custom':\n",
    "    anomaly_field = 'anomaly_score'\n",
    "    extra_fields = ''',\n",
    "        pred.encoded as latent_encoding'''\n",
    "else:\n",
    "    anomaly_field = 'denormalized_MAE'\n",
    "    extra_fields = ''\n",
    "\n",
    "create_table_query = f\"\"\"\n",
    "CREATE OR REPLACE TABLE `{PROJECT_ID}.{BQ_DATASET}.{BQ_PREDICTIONS_TABLE}` AS\n",
    "SELECT\n",
    "    -- Original data (with transaction_id and Class for validation)\n",
    "    orig.transaction_id,\n",
    "    orig.Class as actual_label,\n",
    "    orig.Time,\n",
    "    orig.Amount,\n",
    "    orig.V1, orig.V2, orig.V3, orig.V4, orig.V5,\n",
    "\n",
    "    -- Predictions\n",
    "    pred.{anomaly_field} as anomaly_score{extra_fields},\n",
    "\n",
    "    -- Business logic\n",
    "    CASE\n",
    "        WHEN pred.{anomaly_field} > {THRESHOLD_HIGH} THEN 'ANOMALY - HIGH RISK'\n",
    "        WHEN pred.{anomaly_field} > {THRESHOLD_MEDIUM} THEN 'ANOMALY - MEDIUM RISK'\n",
    "        ELSE 'NORMAL'\n",
    "    END as risk_category,\n",
    "\n",
    "    CASE WHEN pred.{anomaly_field} > {THRESHOLD_MEDIUM} THEN 1 ELSE 0 END as is_anomaly,\n",
    "\n",
    "    LEAST(100, CAST(pred.{anomaly_field} / {THRESHOLD_HIGH} * 100 AS INT64)) as risk_score,\n",
    "\n",
    "    -- Metadata\n",
    "    CURRENT_TIMESTAMP() as prediction_timestamp,\n",
    "    '{ENDPOINT_TYPE}' as endpoint_type,\n",
    "    '{REMOTE_MODEL_NAME}' as model_name\n",
    "\n",
    "FROM (\n",
    "    -- Get predictions from remote model\n",
    "    SELECT *\n",
    "    FROM ML.PREDICT(\n",
    "        MODEL `{REMOTE_MODEL_NAME}`,\n",
    "        (\n",
    "            SELECT {\", \".join(input_features)}\n",
    "            FROM `{PROJECT_ID}.{BQ_DATASET}.{BQ_SOURCE_TABLE}`\n",
    "            WHERE splits = 'TEST'\n",
    "        )\n",
    "    )\n",
    ") pred\n",
    "JOIN (\n",
    "    -- Join back original data to get transaction_id and Class\n",
    "    SELECT *\n",
    "    FROM `{PROJECT_ID}.{BQ_DATASET}.{BQ_SOURCE_TABLE}`\n",
    "    WHERE splits = 'TEST'\n",
    ") orig\n",
    "ON pred.Time = orig.Time AND pred.Amount = orig.Amount  -- Match on Time and Amount\n",
    "\"\"\"\n",
    "\n",
    "print(f\"Executing CREATE TABLE query...\")\n",
    "print(f\"‚è±Ô∏è  This may take a few minutes depending on data size...\\n\")\n",
    "\n",
    "job = bq.query(query=create_table_query)\n",
    "job.result()\n",
    "\n",
    "if job.state == 'DONE':\n",
    "    print(f\"‚úÖ Predictions table created successfully!\")\n",
    "\n",
    "    # Get table stats\n",
    "    table = bq.get_table(f\"{PROJECT_ID}.{BQ_DATASET}.{BQ_PREDICTIONS_TABLE}\")\n",
    "\n",
    "    print(f\"\\nüìä Table details:\")\n",
    "    print(f\"   Table: {PROJECT_ID}.{BQ_DATASET}.{BQ_PREDICTIONS_TABLE}\")\n",
    "    print(f\"   Rows: {table.num_rows:,}\")\n",
    "    print(f\"   Size: {table.num_bytes / 1024 / 1024:.2f} MB\")\n",
    "    print(f\"   Columns: {len(table.schema)}\")\n",
    "\n",
    "    print(f\"\\nüí° View in BigQuery Console:\")\n",
    "    print(f\"   https://console.cloud.google.com/bigquery?project={PROJECT_ID}&ws=!1m5!1m4!4m3!1s{PROJECT_ID}!2s{BQ_DATASET}!3s{BQ_PREDICTIONS_TABLE}\")\n",
    "else:\n",
    "    print(f\"‚ùå Table creation failed\")\n",
    "    print(f\"   State: {job.state}\")\n",
    "    if job.errors:\n",
    "        print(f\"   Errors: {job.errors}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "query_predictions",
   "metadata": {},
   "source": [
    "### Query Predictions Table\n",
    "\n",
    "Now that predictions are in a table, you can query them like any other BigQuery table - no endpoint calls needed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "query_predictions_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Querying predictions table...\n",
      "\n",
      "üìä Top 10 Anomalies:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transaction_id</th>\n",
       "      <th>actual_label</th>\n",
       "      <th>anomaly_score</th>\n",
       "      <th>risk_category</th>\n",
       "      <th>risk_score</th>\n",
       "      <th>Amount</th>\n",
       "      <th>prediction_timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>02c93fd4-e981-4e92-80cf-e3fa9a8a8681</td>\n",
       "      <td>0</td>\n",
       "      <td>4079.226807</td>\n",
       "      <td>ANOMALY - HIGH RISK</td>\n",
       "      <td>100</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2025-11-10 18:15:28.391792+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>40679e68-f1d6-4120-a3ee-3b388fbedfa4</td>\n",
       "      <td>0</td>\n",
       "      <td>3836.285645</td>\n",
       "      <td>ANOMALY - HIGH RISK</td>\n",
       "      <td>100</td>\n",
       "      <td>32.76</td>\n",
       "      <td>2025-11-10 18:15:28.391792+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>f78e4a10-4e09-4c64-aff1-c582d3168660</td>\n",
       "      <td>0</td>\n",
       "      <td>3602.212158</td>\n",
       "      <td>ANOMALY - HIGH RISK</td>\n",
       "      <td>100</td>\n",
       "      <td>1264.35</td>\n",
       "      <td>2025-11-10 18:15:28.391792+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>345a37d5-1601-4e5d-a742-ff83e4ea8603</td>\n",
       "      <td>0</td>\n",
       "      <td>3594.519043</td>\n",
       "      <td>ANOMALY - HIGH RISK</td>\n",
       "      <td>100</td>\n",
       "      <td>79.99</td>\n",
       "      <td>2025-11-10 18:15:28.391792+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>69da5434-65a8-4213-a491-7683f19a3ca0</td>\n",
       "      <td>0</td>\n",
       "      <td>3563.286377</td>\n",
       "      <td>ANOMALY - HIGH RISK</td>\n",
       "      <td>100</td>\n",
       "      <td>96.70</td>\n",
       "      <td>2025-11-10 18:15:28.391792+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>f4a60a12-7058-4e87-9756-c4feddacee40</td>\n",
       "      <td>0</td>\n",
       "      <td>3523.909424</td>\n",
       "      <td>ANOMALY - HIGH RISK</td>\n",
       "      <td>100</td>\n",
       "      <td>10.00</td>\n",
       "      <td>2025-11-10 18:15:28.391792+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>953dcad0-e85c-44c0-82c2-65da23a71603</td>\n",
       "      <td>0</td>\n",
       "      <td>3501.584473</td>\n",
       "      <td>ANOMALY - HIGH RISK</td>\n",
       "      <td>100</td>\n",
       "      <td>4.50</td>\n",
       "      <td>2025-11-10 18:15:28.391792+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0090a7df-9c1a-471c-8dc5-edba9cf7c772</td>\n",
       "      <td>0</td>\n",
       "      <td>3457.641113</td>\n",
       "      <td>ANOMALY - HIGH RISK</td>\n",
       "      <td>100</td>\n",
       "      <td>47.60</td>\n",
       "      <td>2025-11-10 18:15:28.391792+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>d9b30262-cc54-488f-83a6-4214522ad8df</td>\n",
       "      <td>0</td>\n",
       "      <td>3456.861572</td>\n",
       "      <td>ANOMALY - HIGH RISK</td>\n",
       "      <td>100</td>\n",
       "      <td>11.85</td>\n",
       "      <td>2025-11-10 18:15:28.391792+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>a7531840-0efe-4356-898d-dbfb0b39567d</td>\n",
       "      <td>0</td>\n",
       "      <td>3430.818604</td>\n",
       "      <td>ANOMALY - HIGH RISK</td>\n",
       "      <td>100</td>\n",
       "      <td>70.00</td>\n",
       "      <td>2025-11-10 18:15:28.391792+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         transaction_id  actual_label  anomaly_score  \\\n",
       "0  02c93fd4-e981-4e92-80cf-e3fa9a8a8681             0    4079.226807   \n",
       "1  40679e68-f1d6-4120-a3ee-3b388fbedfa4             0    3836.285645   \n",
       "2  f78e4a10-4e09-4c64-aff1-c582d3168660             0    3602.212158   \n",
       "3  345a37d5-1601-4e5d-a742-ff83e4ea8603             0    3594.519043   \n",
       "4  69da5434-65a8-4213-a491-7683f19a3ca0             0    3563.286377   \n",
       "5  f4a60a12-7058-4e87-9756-c4feddacee40             0    3523.909424   \n",
       "6  953dcad0-e85c-44c0-82c2-65da23a71603             0    3501.584473   \n",
       "7  0090a7df-9c1a-471c-8dc5-edba9cf7c772             0    3457.641113   \n",
       "8  d9b30262-cc54-488f-83a6-4214522ad8df             0    3456.861572   \n",
       "9  a7531840-0efe-4356-898d-dbfb0b39567d             0    3430.818604   \n",
       "\n",
       "         risk_category  risk_score   Amount             prediction_timestamp  \n",
       "0  ANOMALY - HIGH RISK         100     1.00 2025-11-10 18:15:28.391792+00:00  \n",
       "1  ANOMALY - HIGH RISK         100    32.76 2025-11-10 18:15:28.391792+00:00  \n",
       "2  ANOMALY - HIGH RISK         100  1264.35 2025-11-10 18:15:28.391792+00:00  \n",
       "3  ANOMALY - HIGH RISK         100    79.99 2025-11-10 18:15:28.391792+00:00  \n",
       "4  ANOMALY - HIGH RISK         100    96.70 2025-11-10 18:15:28.391792+00:00  \n",
       "5  ANOMALY - HIGH RISK         100    10.00 2025-11-10 18:15:28.391792+00:00  \n",
       "6  ANOMALY - HIGH RISK         100     4.50 2025-11-10 18:15:28.391792+00:00  \n",
       "7  ANOMALY - HIGH RISK         100    47.60 2025-11-10 18:15:28.391792+00:00  \n",
       "8  ANOMALY - HIGH RISK         100    11.85 2025-11-10 18:15:28.391792+00:00  \n",
       "9  ANOMALY - HIGH RISK         100    70.00 2025-11-10 18:15:28.391792+00:00  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìà Risk Category Distribution:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>risk_category</th>\n",
       "      <th>count</th>\n",
       "      <th>percentage</th>\n",
       "      <th>avg_anomaly_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ANOMALY - HIGH RISK</td>\n",
       "      <td>23440</td>\n",
       "      <td>81.99</td>\n",
       "      <td>891.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ANOMALY - MEDIUM RISK</td>\n",
       "      <td>2591</td>\n",
       "      <td>9.06</td>\n",
       "      <td>151.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NORMAL</td>\n",
       "      <td>2557</td>\n",
       "      <td>8.94</td>\n",
       "      <td>51.17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           risk_category  count  percentage  avg_anomaly_score\n",
       "0    ANOMALY - HIGH RISK  23440       81.99             891.65\n",
       "1  ANOMALY - MEDIUM RISK   2591        9.06             151.25\n",
       "2                 NORMAL   2557        8.94              51.17"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üéØ Model Performance (Actual Fraud vs Predicted Anomaly):\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual_label</th>\n",
       "      <th>is_anomaly</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>25982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   actual_label  is_anomaly  count\n",
       "0             0           0   2556\n",
       "1             0           1  25982\n",
       "2             1           0      1\n",
       "3             1           1     49"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üí° Benefits of predictions table:\n",
      "   ‚ö° Fast queries (no endpoint calls)\n",
      "   üí∞ Cost-effective (predictions cached)\n",
      "   üîÑ Joinable with other tables\n",
      "   üìä Ready for BI tools and dashboards\n"
     ]
    }
   ],
   "source": [
    "print(f\"Querying predictions table...\\n\")\n",
    "\n",
    "# Example 1: Top anomalies\n",
    "query = f\"\"\"\n",
    "SELECT \n",
    "    transaction_id,\n",
    "    actual_label,\n",
    "    anomaly_score,\n",
    "    risk_category,\n",
    "    risk_score,\n",
    "    Amount,\n",
    "    prediction_timestamp\n",
    "FROM `{PROJECT_ID}.{BQ_DATASET}.{BQ_PREDICTIONS_TABLE}`\n",
    "ORDER BY anomaly_score DESC\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "\n",
    "top_anomalies = bq.query(query).to_dataframe()\n",
    "print(f\"üìä Top 10 Anomalies:\\n\")\n",
    "display(top_anomalies)\n",
    "\n",
    "# Example 2: Risk distribution\n",
    "print(f\"\\nüìà Risk Category Distribution:\\n\")\n",
    "query = f\"\"\"\n",
    "SELECT \n",
    "    risk_category,\n",
    "    COUNT(*) as count,\n",
    "    ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER(), 2) as percentage,\n",
    "    ROUND(AVG(anomaly_score), 2) as avg_anomaly_score\n",
    "FROM `{PROJECT_ID}.{BQ_DATASET}.{BQ_PREDICTIONS_TABLE}`\n",
    "GROUP BY risk_category\n",
    "ORDER BY avg_anomaly_score DESC\n",
    "\"\"\"\n",
    "\n",
    "risk_dist = bq.query(query).to_dataframe()\n",
    "display(risk_dist)\n",
    "\n",
    "# Example 3: Model performance check (actual vs predicted)\n",
    "print(f\"\\nüéØ Model Performance (Actual Fraud vs Predicted Anomaly):\\n\")\n",
    "query = f\"\"\"\n",
    "SELECT \n",
    "    actual_label,\n",
    "    is_anomaly,\n",
    "    COUNT(*) as count\n",
    "FROM `{PROJECT_ID}.{BQ_DATASET}.{BQ_PREDICTIONS_TABLE}`\n",
    "GROUP BY actual_label, is_anomaly\n",
    "ORDER BY actual_label, is_anomaly\n",
    "\"\"\"\n",
    "\n",
    "performance = bq.query(query).to_dataframe()\n",
    "display(performance)\n",
    "\n",
    "print(f\"\\nüí° Benefits of predictions table:\")\n",
    "print(f\"   ‚ö° Fast queries (no endpoint calls)\")\n",
    "print(f\"   üí∞ Cost-effective (predictions cached)\")\n",
    "print(f\"   üîÑ Joinable with other tables\")\n",
    "print(f\"   üìä Ready for BI tools and dashboards\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "schedule_hint",
   "metadata": {},
   "source": [
    "### Scheduling Batch Predictions\n",
    "\n",
    "For production use cases, you can schedule this table creation to run automatically:\n",
    "\n",
    "**Option 1: BigQuery Scheduled Queries**\n",
    "1. Go to BigQuery Console\n",
    "2. Click \"Schedule\" button after writing query\n",
    "3. Set frequency (hourly, daily, weekly)\n",
    "4. Predictions refresh automatically\n",
    "\n",
    "**Option 2: Cloud Scheduler + Workflows**\n",
    "1. Create workflow that runs CREATE TABLE query\n",
    "2. Schedule with Cloud Scheduler\n",
    "3. Add error handling and notifications\n",
    "\n",
    "**Example use case:**\n",
    "```sql\n",
    "-- Daily scheduled query (runs at 2 AM)\n",
    "CREATE OR REPLACE TABLE `predictions.daily_fraud_scores` AS\n",
    "SELECT *\n",
    "FROM ML.PREDICT(\n",
    "    MODEL `models.fraud_detector`,\n",
    "    (SELECT * FROM `transactions.daily` WHERE date = CURRENT_DATE())\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advanced_topics",
   "metadata": {},
   "source": [
    "---\n",
    "## Advanced Topics\n",
    "\n",
    "This section discusses advanced capabilities and optimizations for BigQuery ML remote models. These topics are presented conceptually without code implementation.\n",
    "\n",
    "### Model Explainability with ML.EXPLAIN\n",
    "\n",
    "BigQuery ML supports **model explainability** for some remote models using `ML.EXPLAIN()`:\n",
    "\n",
    "**What it provides:**\n",
    "- Feature importance scores for each prediction\n",
    "- Understand which features drove the anomaly score\n",
    "- Identify patterns in high-risk transactions\n",
    "\n",
    "**Limitations:**\n",
    "- Not all remote models support `ML.EXPLAIN()`\n",
    "- Requires endpoint to return feature attributions\n",
    "- May increase prediction latency\n",
    "\n",
    "**Example use case:**\n",
    "```sql\n",
    "-- Get feature importance for each prediction\n",
    "SELECT *\n",
    "FROM ML.EXPLAIN_PREDICT(\n",
    "    MODEL `dataset.remote_model`,\n",
    "    (SELECT * FROM `dataset.transactions` LIMIT 10),\n",
    "    STRUCT(3 AS top_k_features)\n",
    ")\n",
    "```\n",
    "\n",
    "**When to use:**\n",
    "- Regulatory compliance (explain decisions)\n",
    "- Model debugging (understand failures)\n",
    "- Feature engineering (identify important features)\n",
    "\n",
    "### Scheduled Queries for Automated Predictions\n",
    "\n",
    "**BigQuery Scheduled Queries** enable automatic, recurring batch predictions:\n",
    "\n",
    "**Setup steps:**\n",
    "1. Write `CREATE OR REPLACE TABLE` query with `ML.PREDICT()`\n",
    "2. Click \"Schedule\" in BigQuery Console\n",
    "3. Set frequency: hourly, daily, weekly, monthly, or custom cron\n",
    "4. Configure notifications for failures\n",
    "\n",
    "**Advanced features:**\n",
    "- **Partitioning**: Create date-partitioned tables for time-series predictions\n",
    "- **Incremental updates**: Only score new data since last run\n",
    "- **Error handling**: Retry failed queries automatically\n",
    "- **Cost controls**: Set maximum bytes billed\n",
    "\n",
    "**Example: Daily fraud scoring**\n",
    "```sql\n",
    "-- Scheduled to run daily at 2 AM\n",
    "CREATE OR REPLACE TABLE `fraud.daily_scores`\n",
    "PARTITION BY DATE(transaction_date) AS\n",
    "SELECT \n",
    "    transaction_date,\n",
    "    transaction_id,\n",
    "    anomaly_score,\n",
    "    risk_category\n",
    "FROM ML.PREDICT(\n",
    "    MODEL `models.fraud_detector`,\n",
    "    (SELECT * FROM `transactions.raw` WHERE transaction_date = CURRENT_DATE())\n",
    ")\n",
    "```\n",
    "\n",
    "**Monitoring:**\n",
    "- View execution history in Scheduled Queries page\n",
    "- Set up email/Slack notifications for failures\n",
    "- Track query costs and performance over time\n",
    "\n",
    "### Continuous Queries for Real-Time Predictions\n",
    "\n",
    "**BigQuery Continuous Queries** enable near-real-time ML inference on streaming data as it arrives in BigQuery tables.\n",
    "\n",
    "**What are Continuous Queries?**\n",
    "\n",
    "Continuous queries automatically run `ML.PREDICT()` on new data as it's inserted into BigQuery tables, providing low-latency predictions without manual scheduling:\n",
    "\n",
    "- **Automated execution**: Triggers automatically when new rows arrive\n",
    "- **Low latency**: Predictions generated within seconds of data insertion\n",
    "- **Incremental processing**: Only scores new data, not entire table\n",
    "- **Streaming integration**: Works with BigQuery streaming inserts and Storage Write API\n",
    "\n",
    "**Key Differences:**\n",
    "\n",
    "| Feature | Scheduled Queries | Continuous Queries |\n",
    "|---------|-------------------|-------------------|\n",
    "| **Trigger** | Time-based (cron) | Data arrival |\n",
    "| **Latency** | Minutes to hours | Seconds |\n",
    "| **Processing** | Batch (full or filtered) | Incremental (new rows only) |\n",
    "| **Use Case** | Periodic batch scoring | Near-real-time inference |\n",
    "| **Cost** | Lower (less frequent) | Higher (runs continuously) |\n",
    "\n",
    "**How ML.PREDICT() Works with Continuous Queries:**\n",
    "\n",
    "```sql\n",
    "-- Continuous query that scores new transactions as they arrive\n",
    "CREATE OR REPLACE TABLE `fraud.continuous_scores` AS (\n",
    "  SELECT\n",
    "    transaction_id,\n",
    "    timestamp,\n",
    "    anomaly_score,\n",
    "    risk_category,\n",
    "    _PARTITIONTIME as processing_time\n",
    "  FROM ML.PREDICT(\n",
    "    MODEL `models.fraud_detector`,\n",
    "    TABLE `transactions.streaming_input`\n",
    "  )\n",
    ")\n",
    "OPTIONS (\n",
    "  continuous = TRUE,\n",
    "  max_staleness = INTERVAL 30 SECOND  -- Maximum delay before processing\n",
    ");\n",
    "```\n",
    "\n",
    "**Benefits for ML Inference:**\n",
    "\n",
    "‚úÖ **Near-real-time anomaly detection**\n",
    "- Fraud transactions flagged seconds after occurrence\n",
    "- Immediate risk scoring for decision-making\n",
    "- Low-latency predictions for operational dashboards\n",
    "\n",
    "‚úÖ **Automatic incremental processing**\n",
    "- Only new rows trigger predictions (cost-efficient)\n",
    "- No need to track \"last processed\" timestamps\n",
    "- Prevents redundant scoring of existing data\n",
    "\n",
    "‚úÖ **Simplified architecture**\n",
    "- No external orchestration needed (Pub/Sub, Cloud Functions, etc.)\n",
    "- Single SQL query handles streaming and prediction\n",
    "- Reduced operational complexity\n",
    "\n",
    "‚úÖ **Integrated with BigQuery streaming**\n",
    "- Works seamlessly with streaming inserts\n",
    "- Compatible with Storage Write API\n",
    "- Native integration with Dataflow and other streaming sources\n",
    "\n",
    "**When to Use Continuous Queries:**\n",
    "\n",
    "‚úÖ **Real-time fraud detection**\n",
    "- Score transactions immediately upon insertion\n",
    "- Flag high-risk activities for instant review\n",
    "- Trigger alerts within seconds of anomaly detection\n",
    "\n",
    "‚úÖ **Live dashboards and monitoring**\n",
    "- Keep prediction tables always up-to-date\n",
    "- Power real-time BI dashboards\n",
    "- Monitor prediction distributions continuously\n",
    "\n",
    "‚úÖ **Event-driven workflows**\n",
    "- Trigger downstream actions based on predictions\n",
    "- Integrate with Pub/Sub for notification pipelines\n",
    "- Feed predictions to real-time decision engines\n",
    "\n",
    "**Limitations and Considerations:**\n",
    "\n",
    "‚ö†Ô∏è **Higher costs**\n",
    "- Continuous execution vs. periodic scheduled runs\n",
    "- More endpoint API calls (pay-per-prediction)\n",
    "- Monitor usage carefully with budget alerts\n",
    "\n",
    "‚ö†Ô∏è **Latency vs. batch efficiency trade-off**\n",
    "- Lower latency = smaller batches = less efficient batching\n",
    "- Vertex AI endpoints optimize for larger batch sizes\n",
    "- Consider `max_staleness` setting for batching window\n",
    "\n",
    "‚ö†Ô∏è **Endpoint capacity planning**\n",
    "- Ensure endpoint autoscaling can handle continuous traffic\n",
    "- Set appropriate `min_replicas` to avoid cold starts\n",
    "- Monitor endpoint metrics (latency, throughput, errors)\n",
    "\n",
    "**Example: Real-Time Fraud Scoring Pipeline**\n",
    "\n",
    "```sql\n",
    "-- Step 1: Continuous query scores new transactions\n",
    "CREATE OR REPLACE TABLE `fraud.realtime_scores` AS (\n",
    "  SELECT\n",
    "    transaction_id,\n",
    "    customer_id,\n",
    "    amount,\n",
    "    timestamp,\n",
    "    pred.denormalized_MAE as anomaly_score,\n",
    "    CASE\n",
    "      WHEN pred.denormalized_MAE > 200 THEN 'HIGH_RISK'\n",
    "      WHEN pred.denormalized_MAE > 100 THEN 'MEDIUM_RISK'\n",
    "      ELSE 'LOW_RISK'\n",
    "    END as risk_level,\n",
    "    CURRENT_TIMESTAMP() as scored_at\n",
    "  FROM ML.PREDICT(\n",
    "    MODEL `models.fraud_remote_model`,\n",
    "    TABLE `transactions.streaming_inserts`\n",
    "  ) AS pred\n",
    ")\n",
    "OPTIONS (\n",
    "  continuous = TRUE,\n",
    "  max_staleness = INTERVAL 10 SECOND\n",
    ");\n",
    "\n",
    "-- Step 2: Create view for high-risk transactions only\n",
    "CREATE OR REPLACE VIEW `fraud.high_risk_alerts` AS\n",
    "SELECT *\n",
    "FROM `fraud.realtime_scores`\n",
    "WHERE risk_level = 'HIGH_RISK'\n",
    "  AND scored_at > TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 1 HOUR);\n",
    "\n",
    "-- Step 3: Downstream systems query the view for alerts\n",
    "```\n",
    "\n",
    "**Combining Continuous and Scheduled Queries:**\n",
    "\n",
    "For comprehensive ML inference pipelines, use both together:\n",
    "\n",
    "**Continuous Queries** ‚Üí Real-time scoring of incoming data\n",
    "**Scheduled Queries** ‚Üí Periodic reprocessing and batch updates\n",
    "\n",
    "```sql\n",
    "-- Continuous: Score new data immediately\n",
    "CREATE OR REPLACE TABLE `fraud.live_scores` AS (...)\n",
    "OPTIONS (continuous = TRUE);\n",
    "\n",
    "-- Scheduled: Daily reprocessing for model drift correction\n",
    "CREATE OR REPLACE TABLE `fraud.daily_reprocessed_scores` AS\n",
    "SELECT * FROM ML.PREDICT(...)  -- Rescore last 7 days\n",
    "WHERE transaction_date >= DATE_SUB(CURRENT_DATE(), INTERVAL 7 DAY);\n",
    "-- Scheduled daily at 2 AM\n",
    "```\n",
    "\n",
    "**Resources:**\n",
    "- [BigQuery Continuous Queries](https://cloud.google.com/bigquery/docs/continuous-queries)\n",
    "- [Streaming into BigQuery](https://cloud.google.com/bigquery/docs/streaming-data-into-bigquery)\n",
    "- [BigQuery ML with Streaming Data](https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-syntax-create-model#continuous_training)\n",
    "\n",
    "### Materialized Views with Predictions\n",
    "\n",
    "**Materialized views** provide pre-computed, auto-refreshing prediction results:\n",
    "\n",
    "**Benefits:**\n",
    "- ‚ö° **Fast queries**: Results pre-computed and cached\n",
    "- üîÑ **Auto-refresh**: Updates automatically when source data changes\n",
    "- üí∞ **Cost-effective**: Predictions cached, not recomputed every query\n",
    "- üéØ **Always current**: Automatically stays in sync with data\n",
    "\n",
    "**Example:**\n",
    "```sql\n",
    "-- Create materialized view with predictions\n",
    "CREATE MATERIALIZED VIEW `dataset.fraud_scores_mv` AS\n",
    "SELECT \n",
    "    transaction_id,\n",
    "    timestamp,\n",
    "    anomaly_score,\n",
    "    risk_category\n",
    "FROM ML.PREDICT(\n",
    "    MODEL `models.fraud_detector`,\n",
    "    (SELECT * FROM `transactions.live`)\n",
    ")\n",
    "WHERE anomaly_score > 100  -- Only high-risk transactions\n",
    "```\n",
    "\n",
    "**Use cases:**\n",
    "- **Real-time dashboards**: BI tools query materialized view (fast!)\n",
    "- **Alert systems**: Monitor view for new high-risk predictions\n",
    "- **Filtered predictions**: Pre-filter to only anomalies\n",
    "\n",
    "**Limitations:**\n",
    "- Refresh frequency controlled by BigQuery (not guaranteed immediate)\n",
    "- Additional storage costs for materialized results\n",
    "- Not all queries supported in materialized views\n",
    "\n",
    "### Performance Optimization\n",
    "\n",
    "**Best practices for production remote models:**\n",
    "\n",
    "**1. Batch Size Optimization**\n",
    "- BigQuery automatically batches requests to endpoint\n",
    "- Larger batches = fewer endpoint calls = lower cost\n",
    "- Balance: batch size vs. query timeout (10 min default)\n",
    "\n",
    "**2. Endpoint Configuration**\n",
    "- Use **autoscaling** for variable workloads\n",
    "- Set **min replicas > 0** to avoid cold starts\n",
    "- Consider **GPU instances** for large models\n",
    "- Monitor **endpoint metrics** (latency, throughput)\n",
    "\n",
    "**3. Query Optimization**\n",
    "- **Filter early**: Apply WHERE before ML.PREDICT()\n",
    "- **Project only needed columns**: Reduce data transfer\n",
    "- **Use partitioning**: Scan less data\n",
    "- **Cache results**: CREATE TABLE instead of repeated predictions\n",
    "\n",
    "**4. Cost Management**\n",
    "- **BigQuery costs**: Query bytes scanned + ML.PREDICT calls\n",
    "- **Vertex AI costs**: Endpoint uptime + prediction requests\n",
    "- **Optimization**: Score once, query many times (use tables)\n",
    "- **Monitoring**: Set up budget alerts\n",
    "\n",
    "**Example: Optimized query**\n",
    "```sql\n",
    "-- Inefficient: Predicts all, then filters\n",
    "SELECT * FROM ML.PREDICT(...)\n",
    "WHERE date = '2024-01-01'  -- Filters AFTER prediction\n",
    "\n",
    "-- Efficient: Filters first, then predicts\n",
    "SELECT * FROM ML.PREDICT(\n",
    "    MODEL ...,\n",
    "    (SELECT * FROM table WHERE date = '2024-01-01')  -- Filters BEFORE\n",
    ")\n",
    "```\n",
    "\n",
    "### Integration with Other GCP Services\n",
    "\n",
    "Remote models integrate seamlessly with the broader GCP ecosystem:\n",
    "\n",
    "**Cloud Workflows:**\n",
    "- Orchestrate complex prediction pipelines\n",
    "- Chain multiple remote models\n",
    "- Add data preprocessing and postprocessing\n",
    "- Handle errors and retries\n",
    "\n",
    "**Pub/Sub + Cloud Functions:**\n",
    "- Trigger predictions on new data arrival\n",
    "- Real-time scoring as data streams in\n",
    "- Fan-out to multiple consumers\n",
    "\n",
    "**Looker/Data Studio:**\n",
    "- Visualize predictions in dashboards\n",
    "- Query predictions table directly\n",
    "- Create alerts on anomaly thresholds\n",
    "\n",
    "**Vertex AI Pipelines:**\n",
    "- Incorporate batch scoring in ML pipelines\n",
    "- Automate model retraining based on prediction drift\n",
    "- End-to-end MLOps workflows\n",
    "\n",
    "### Monitoring and Observability\n",
    "\n",
    "**Key metrics to track:**\n",
    "\n",
    "**BigQuery Side:**\n",
    "- Query execution time\n",
    "- Bytes scanned\n",
    "- ML.PREDICT call count\n",
    "- Failed queries\n",
    "\n",
    "**Vertex AI Endpoint:**\n",
    "- Prediction latency (p50, p95, p99)\n",
    "- Request count\n",
    "- Error rate\n",
    "- Resource utilization (CPU, memory)\n",
    "\n",
    "**Model Quality:**\n",
    "- Prediction distribution over time (drift detection)\n",
    "- Anomaly rate trends\n",
    "- Feature importance changes\n",
    "- Actual vs. predicted (if labels available)\n",
    "\n",
    "**Tools:**\n",
    "- **Cloud Monitoring**: Endpoint metrics and alerts\n",
    "- **BigQuery Information Schema**: Query statistics\n",
    "- **Cloud Logging**: Detailed logs for debugging\n",
    "- **Vertex AI Model Monitoring**: Automated drift detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cleanup",
   "metadata": {},
   "source": [
    "---\n",
    "## Cleanup\n",
    "\n",
    "To avoid incurring unnecessary charges, clean up resources created in this notebook.\n",
    "\n",
    "**‚ö†Ô∏è Warning:** The commands below are commented out to prevent accidental deletion. Uncomment only what you want to remove.\n",
    "\n",
    "**What gets deleted:**\n",
    "- BigQuery ML remote model (does NOT delete the endpoint)\n",
    "- BigQuery predictions table\n",
    "- BigQuery Cloud Resource Connection\n",
    "\n",
    "**What is NOT deleted:**\n",
    "- Vertex AI Endpoint (you may want to keep this for other uses)\n",
    "- Source data table (`frameworks` dataset)\n",
    "- To delete endpoint: Use the endpoint cleanup notebook or Console"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cleanup_code",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Uncomment to delete BigQuery ML remote model\n",
    "# print(f\"Deleting remote model: {REMOTE_MODEL_NAME}\")\n",
    "# bq.delete_model(REMOTE_MODEL_NAME, not_found_ok=True)\n",
    "# print(f\"‚úÖ Remote model deleted\")\n",
    "\n",
    "# Uncomment to delete predictions table\n",
    "# print(f\"Deleting predictions table: {PROJECT_ID}.{BQ_DATASET}.{BQ_PREDICTIONS_TABLE}\")\n",
    "# bq.delete_table(f\"{PROJECT_ID}.{BQ_DATASET}.{BQ_PREDICTIONS_TABLE}\", not_found_ok=True)\n",
    "# print(f\"‚úÖ Predictions table deleted\")\n",
    "\n",
    "# Uncomment to delete BigQuery connection\n",
    "# print(f\"Deleting BigQuery connection: {BQ_CONNECTION_NAME}\")\n",
    "# connection_name = f\"projects/{PROJECT_ID}/locations/{BQ_REGION}/connections/{BQ_CONNECTION_NAME}\"\n",
    "# try:\n",
    "#     connection_client.delete_connection(name=connection_name)\n",
    "#     print(f\"‚úÖ Connection deleted\")\n",
    "# except Exception as e:\n",
    "#     print(f\"‚ùå Failed to delete connection: {e}\")\n",
    "\n",
    "# Uncomment to remove IAM policy binding for connection service account\n",
    "# Note: Only remove if you're sure no other remote models use this connection\n",
    "# print(f\"Removing Vertex AI User role from service account: {service_account}\")\n",
    "# result = subprocess.run(\n",
    "#     ['gcloud', 'projects', 'remove-iam-policy-binding', PROJECT_ID,\n",
    "#      f'--member=serviceAccount:{service_account}',\n",
    "#      '--role=roles/aiplatform.user'],\n",
    "#     capture_output=True,\n",
    "#     text=True\n",
    "# )\n",
    "# if result.returncode == 0:\n",
    "#     print(f\"‚úÖ IAM policy binding removed\")\n",
    "# else:\n",
    "#     print(f\"‚ùå Failed to remove binding: {result.stderr}\")\n",
    "\n",
    "print(f\"\\nüí° To delete the Vertex AI Endpoint:\")\n",
    "print(f\"   1. Use the endpoint deployment notebook's cleanup section, OR\")\n",
    "print(f\"   2. Go to Vertex AI Console > Endpoints > {endpoint.display_name} > Delete\")\n",
    "print(f\"\\n‚ö†Ô∏è  Deleting endpoint will affect ALL remote models using it!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "Congratulations! You've successfully created and used a BigQuery ML remote model to call a Vertex AI endpoint from SQL.\n",
    "\n",
    "### What You Accomplished\n",
    "\n",
    "‚úÖ **Prerequisites & Testing**\n",
    "- Verified deployed Vertex AI endpoint\n",
    "- Tested endpoint with Python SDK\n",
    "- Understood response signature differences (prebuilt vs custom)\n",
    "\n",
    "‚úÖ **Infrastructure Setup**\n",
    "- Created BigQuery Cloud Resource Connection\n",
    "- Granted IAM permissions for Vertex AI access\n",
    "- Configured secure service-to-service communication\n",
    "\n",
    "‚úÖ **Remote Model Creation**\n",
    "- Registered Vertex AI endpoint as BQML model\n",
    "- Defined input/output schemas for both endpoint types\n",
    "- Handled endpoint-specific response structures\n",
    "\n",
    "‚úÖ **SQL-Based Inference**\n",
    "- Made simple predictions with `ML.PREDICT()`\n",
    "- Extracted specific fields from prediction output\n",
    "- Applied business logic (risk categorization)\n",
    "- Created enriched tables with batch predictions\n",
    "\n",
    "‚úÖ **Production Patterns**\n",
    "- Batch scored large datasets\n",
    "- Persisted predictions to queryable tables\n",
    "- Demonstrated joining predictions with source data\n",
    "- Explored advanced topics (scheduling, materialized views, optimization)\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "**When to Use BigQuery ML Remote Models:**\n",
    "- ‚úÖ Batch scoring large datasets in BigQuery\n",
    "- ‚úÖ SQL analysts need predictions without Python\n",
    "- ‚úÖ Scheduled/recurring inference jobs\n",
    "- ‚úÖ Models already deployed on Vertex AI endpoints\n",
    "- ‚úÖ Integration with BigQuery data warehouse\n",
    "\n",
    "**Benefits Over Direct SDK Calls:**\n",
    "- üí™ **Scalability**: Petabyte-scale batch predictions\n",
    "- üéØ **Simplicity**: Pure SQL, no Python required\n",
    "- üîÑ **Automation**: Scheduled queries for recurring predictions\n",
    "- üí∞ **Cost-effective**: Cache predictions in tables\n",
    "- ü§ù **Collaboration**: SQL accessible to broader teams\n",
    "\n",
    "**Performance Tips:**\n",
    "- Filter data BEFORE calling `ML.PREDICT()` (reduce endpoint calls)\n",
    "- Create prediction tables for repeated queries (cache results)\n",
    "- Use endpoint autoscaling for variable workloads\n",
    "- Monitor both BigQuery and Vertex AI metrics\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "**Explore Related Notebooks:**\n",
    "- [vertex-ai-endpoint-prebuilt-container.ipynb](./vertex-ai-endpoint-prebuilt-container.ipynb) - Deploy PyTorch models with TorchServe\n",
    "- [vertex-ai-endpoint-custom-container.ipynb](./vertex-ai-endpoint-custom-container.ipynb) - Custom FastAPI serving containers\n",
    "- [scaling-vertex-ai-endpoint.ipynb](./scaling-vertex-ai-endpoint.ipynb) - Performance testing and autoscaling\n",
    "- [torchserve-cloud-run.ipynb](./torchserve-cloud-run.ipynb) - Serverless alternative with Cloud Run\n",
    "\n",
    "**Production Enhancements:**\n",
    "1. **Set up scheduled queries** for daily/hourly predictions\n",
    "2. **Create materialized views** for fast dashboard queries\n",
    "3. **Add monitoring** for prediction drift and data quality\n",
    "4. **Implement alerting** for high-risk predictions\n",
    "5. **Optimize costs** with partitioning and clustering\n",
    "\n",
    "**Advanced Topics to Explore:**\n",
    "- Model explainability with `ML.EXPLAIN_PREDICT()`\n",
    "- Multi-model ensembles (combine predictions from multiple endpoints)\n",
    "- A/B testing (compare prebuilt vs custom endpoint predictions)\n",
    "- Integration with Vertex AI Pipelines for end-to-end MLOps\n",
    "\n",
    "### Resources\n",
    "\n",
    "**Documentation:**\n",
    "- [BigQuery ML Remote Models](https://cloud.google.com/bigquery/docs/reference/standard-sql/bigqueryml-syntax-create-remote-model)\n",
    "- [ML.PREDICT() Function](https://cloud.google.com/bigquery/docs/reference/standard-sql/bigqueryml-syntax-predict)\n",
    "- [BigQuery Cloud Resource Connections](https://cloud.google.com/bigquery/docs/create-cloud-resource-connection)\n",
    "- [Vertex AI Endpoints](https://cloud.google.com/vertex-ai/docs/predictions/getting-predictions)\n",
    "\n",
    "**Tutorials:**\n",
    "- [Make predictions with remote models on Vertex AI](https://cloud.google.com/bigquery/docs/bigquery-ml-remote-model-tutorial)\n",
    "- [Schedule queries](https://cloud.google.com/bigquery/docs/scheduling-queries)\n",
    "- [Create materialized views](https://cloud.google.com/bigquery/docs/materialized-views-intro)\n",
    "\n",
    "**Related Repositories:**\n",
    "- [vertex-ai-mlops](https://github.com/statmike/vertex-ai-mlops) - This repository with more examples\n",
    "- [Vertex AI Samples](https://github.com/GoogleCloudPlatform/vertex-ai-samples)\n",
    "\n",
    "---\n",
    "\n",
    "**Questions or feedback?** \n",
    "- Open an issue: [GitHub Issues](https://github.com/statmike/vertex-ai-mlops/issues)\n",
    "- Connect: [LinkedIn](https://www.linkedin.com/in/statmike) | [Twitter/X](https://x.com/statmike) | [BlueSky](https://bsky.app/profile/statmike.bsky.social)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (frameworks-pytorch)",
   "language": "python",
   "name": "frameworks-pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
