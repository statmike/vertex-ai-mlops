{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "![tracker](https://us-central1-vertex-ai-mlops-369716.cloudfunctions.net/pixel-tracking?path=statmike%2Fvertex-ai-mlops%2FFramework+Workflows%2FPyTorch%2Fserving&file=alloydb-vertex-ai-endpoint.ipynb)\n",
    "<!--- header table --->\n",
    "<table align=\"left\">\n",
    "<tr>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%20Workflows/PyTorch/serving/alloydb-vertex-ai-endpoint.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.svgrepo.com/download/217753/github.svg\" alt=\"GitHub logo\">\n",
    "      <br>View on<br>GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://colab.research.google.com/github/statmike/vertex-ai-mlops/blob/main/Framework%20Workflows/PyTorch/serving/alloydb-vertex-ai-endpoint.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.gstatic.com/pantheon/images/bigquery/welcome_page/colab-logo.svg\" alt=\"Google Colaboratory logo\">\n",
    "      <br>Run in<br>Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https%3A%2F%2Fraw.githubusercontent.com%2Fstatmike%2Fvertex-ai-mlops%2Fmain%2FFramework%2520Workflows%2FPyTorch%2Fserving%2Falloydb-vertex-ai-endpoint.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\">\n",
    "      <br>Run in<br>Colab Enterprise\n",
    "    </a>\n",
    "  </td>      \n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/bigquery/import?url=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%20Workflows/PyTorch/serving/alloydb-vertex-ai-endpoint.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.gstatic.com/images/branding/gcpiconscolors/bigquery/v1/32px.svg\" alt=\"BigQuery logo\">\n",
    "      <br>Open in<br>BigQuery Studio\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/statmike/vertex-ai-mlops/main/Framework%20Workflows/PyTorch/serving/alloydb-vertex-ai-endpoint.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.gstatic.com/images/branding/gcpiconscolors/vertexai/v1/32px.svg\" alt=\"Vertex AI logo\">\n",
    "      <br>Open in<br>Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td colspan=\"5\" style=\"text-align: right\">\n",
    "    <b>Share This On: </b> \n",
    "    <a href=\"https://www.linkedin.com/sharing/share-offsite/?url=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%2520Workflows/PyTorch/serving/alloydb-vertex-ai-endpoint.ipynb\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"Linkedin Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://reddit.com/submit?url=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%2520Workflows/PyTorch/serving/alloydb-vertex-ai-endpoint.ipynb\"><img src=\"https://redditinc.com/hubfs/Reddit%20Inc/Brand/Reddit_Logo.png\" alt=\"Reddit Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://bsky.app/intent/compose?text=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%2520Workflows/PyTorch/serving/alloydb-vertex-ai-endpoint.ipynb\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"BlueSky Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://twitter.com/intent/tweet?url=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%2520Workflows/PyTorch/serving/alloydb-vertex-ai-endpoint.ipynb\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/5/5a/X_icon_2.svg\" alt=\"X (Twitter) Logo\" width=\"20px\"></a> \n",
    "  </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td colspan=\"5\" style=\"text-align: right\">\n",
    "    <b>Connect With Author On: </b> \n",
    "    <a href=\"https://www.linkedin.com/in/statmike\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"Linkedin Logo\" width=\"20px\"></a>\n",
    "    <a href=\"https://www.github.com/statmike\"><img src=\"https://www.svgrepo.com/download/217753/github.svg\" alt=\"GitHub Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://www.youtube.com/@statmike-channel\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/f/fd/YouTube_full-color_icon_%282024%29.svg\" alt=\"YouTube Logo\" width=\"20px\"></a>\n",
    "    <a href=\"https://bsky.app/profile/statmike.bsky.social\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"BlueSky Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://x.com/statmike\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/5/5a/X_icon_2.svg\" alt=\"X (Twitter) Logo\" width=\"20px\"></a>\n",
    "  </td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overview",
   "metadata": {},
   "source": [
    "# AlloyDB SQL-Based ML Inference with Vertex AI Endpoints\n",
    "\n",
    "This notebook demonstrates how to use **AlloyDB for PostgreSQL** to call Vertex AI endpoints directly from SQL for ML inference. This powerful integration enables database-native predictions without Python code, bringing AI capabilities into your PostgreSQL queries.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "This workflow covers:\n",
    "\n",
    "1. **Prerequisites Check**: Verify deployed Vertex AI endpoint and test predictions\n",
    "2. **AlloyDB Infrastructure**: Create minimal cluster and instance for demo\n",
    "3. **Database Setup**: Create database, tables, and enable ML integration\n",
    "4. **Load Test Data**: Import transaction data from BigQuery into AlloyDB\n",
    "5. **Register Vertex AI Model**: Use `google_ml.create_model()` to register endpoint\n",
    "6. **SQL-Based Predictions**: Call models directly from SQL queries\n",
    "7. **Multi-Stage Inference**: Simple predictions ‚Üí Field extraction ‚Üí Business logic ‚Üí Batch scoring\n",
    "8. **Cleanup**: Remove all AlloyDB resources to avoid charges\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "This notebook requires an existing Vertex AI Endpoint with a deployed PyTorch model.\n",
    "\n",
    "**Create an endpoint using either:**\n",
    "- [vertex-ai-endpoint-prebuilt-container.ipynb](./vertex-ai-endpoint-prebuilt-container.ipynb) - Pre-built TorchServe container (recommended)\n",
    "- [vertex-ai-endpoint-custom-container.ipynb](./vertex-ai-endpoint-custom-container.ipynb) - Custom FastAPI container\n",
    "\n",
    "Both endpoints work with this notebook. The first section will find and test your deployed endpoint.\n",
    "\n",
    "## What is AlloyDB for PostgreSQL?\n",
    "\n",
    "[**AlloyDB for PostgreSQL**](https://cloud.google.com/alloydb) is Google Cloud's fully managed, PostgreSQL-compatible database service designed for demanding enterprise workloads. It delivers exceptional performance, enterprise-grade reliability, and native integration with Google Cloud AI services.\n",
    "\n",
    "### AlloyDB Architecture\n",
    "\n",
    "AlloyDB is organized in a hierarchical structure:\n",
    "\n",
    "```\n",
    "Cluster (regional resource)\n",
    "  ‚îî‚îÄ‚îÄ Instance (compute + storage)\n",
    "      ‚îî‚îÄ‚îÄ Database\n",
    "          ‚îî‚îÄ‚îÄ Schema (default: public)\n",
    "              ‚îî‚îÄ‚îÄ Tables\n",
    "```\n",
    "\n",
    "**Cluster**: Regional resource that contains instances and defines networking\n",
    "- Multi-zone high availability support\n",
    "- Automated backups and point-in-time recovery\n",
    "- VPC network configuration\n",
    "- User authentication settings\n",
    "\n",
    "**Instance**: Compute resource that runs PostgreSQL (multiple types available)\n",
    "- **PRIMARY**: Read/write instance (required, one per cluster)\n",
    "- **READ POOL**: Read-only replicas for scaling reads (optional, multiple allowed)\n",
    "- CPU and memory configuration (from 2 to 128 vCPUs)\n",
    "- Availability type: ZONAL (single zone) or REGIONAL (multi-zone HA)\n",
    "\n",
    "**Database**: PostgreSQL database within an instance\n",
    "- Multiple databases per instance\n",
    "- Standard PostgreSQL database features\n",
    "- Extensions and configuration options\n",
    "\n",
    "### AlloyDB Capabilities\n",
    "\n",
    "**Performance:**\n",
    "- ‚úÖ **4x faster** than standard PostgreSQL for transactional workloads\n",
    "- ‚úÖ **100x faster** for analytical queries (columnar engine)\n",
    "- ‚úÖ Intelligent caching and query optimization\n",
    "- ‚úÖ Sub-millisecond latency for point queries\n",
    "\n",
    "**AI & ML Integration:**\n",
    "- ‚úÖ **Built-in Vertex AI integration** for ML predictions from SQL\n",
    "- ‚úÖ **Vector search** with pgvector and ScaNN extensions\n",
    "- ‚úÖ **Embedding generation** directly in database\n",
    "- ‚úÖ Call any Vertex AI endpoint or deployed model\n",
    "\n",
    "**Scalability:**\n",
    "- ‚úÖ Scale compute independently (2-128 vCPUs)\n",
    "- ‚úÖ Scale storage automatically (up to 64 TB)\n",
    "- ‚úÖ Read pool replicas for horizontal read scaling\n",
    "- ‚úÖ Connection pooling with PgBouncer\n",
    "\n",
    "**High Availability:**\n",
    "- ‚úÖ Multi-zone REGIONAL instances (99.99% SLA)\n",
    "- ‚úÖ Automatic failover (typically < 60 seconds)\n",
    "- ‚úÖ Continuous backups and point-in-time recovery\n",
    "- ‚úÖ Cross-region replication (optional)\n",
    "\n",
    "**PostgreSQL Compatibility:**\n",
    "- ‚úÖ 100% compatible with PostgreSQL\n",
    "- ‚úÖ Support for PostgreSQL extensions (pgvector, PostGIS, etc.)\n",
    "- ‚úÖ Standard SQL and PostgreSQL tools work seamlessly\n",
    "- ‚úÖ Easy migration from existing PostgreSQL databases\n",
    "\n",
    "### Scaling AlloyDB\n",
    "\n",
    "AlloyDB offers flexible scaling options for different workload needs:\n",
    "\n",
    "**Vertical Scaling (Compute):**\n",
    "- **Development/Test**: 2 vCPUs, 16 GB RAM\n",
    "- **Small Production**: 4-8 vCPUs, 32-64 GB RAM\n",
    "- **Medium Production**: 16-32 vCPUs, 128-256 GB RAM\n",
    "- **Large Production**: 64-128 vCPUs, 512-864 GB RAM\n",
    "- Scale up/down with minimal downtime\n",
    "\n",
    "**Horizontal Scaling (Read Replicas):**\n",
    "- Add **READ POOL instances** to distribute read traffic\n",
    "- Each replica can have different CPU/memory configurations\n",
    "- Automatic load balancing across replicas\n",
    "- Replicas stay in sync (sub-second replication lag)\n",
    "\n",
    "**Storage Scaling:**\n",
    "- Automatically grows as needed (up to 64 TB)\n",
    "- No manual provisioning required\n",
    "- Instant expansion without downtime\n",
    "\n",
    "### Minimal Setup for This Notebook\n",
    "\n",
    "To keep costs low while demonstrating AlloyDB's ML capabilities, this notebook uses a **minimal configuration**:\n",
    "\n",
    "- **Instance Type**: PRIMARY (required for read/write)\n",
    "- **Availability**: ZONAL (single zone, not high availability)\n",
    "- **Compute**: 2 vCPUs, 16 GB RAM (smallest available)\n",
    "- **Storage**: Auto-scaling from minimal size\n",
    "- **Read Replicas**: None (not needed for demo)\n",
    "\n",
    "**Cost Estimate** (approximate, `us-central1` region):\n",
    "- ~$0.30/hour when running (billed per second)\n",
    "- ~$7.20/day if left running 24/7\n",
    "- ~$216/month if not deleted\n",
    "\n",
    "**Important**: The cleanup section at the end of this notebook will delete all resources to avoid ongoing charges.\n",
    "\n",
    "### When to Use AlloyDB for ML Inference\n",
    "\n",
    "AlloyDB is ideal when:\n",
    "- ‚úÖ **Your data is already in PostgreSQL**: Avoid data movement\n",
    "- ‚úÖ **Transactional + analytical workloads**: OLTP + OLAP in one database\n",
    "- ‚úÖ **SQL-based ML inference**: Analysts can make predictions without Python\n",
    "- ‚úÖ **Low-latency predictions**: Sub-millisecond database queries + ML inference\n",
    "- ‚úÖ **Hybrid vector + relational**: Combine vector search with structured data\n",
    "- ‚úÖ **Enterprise features needed**: HA, backup/recovery, security compliance\n",
    "\n",
    "**Comparison with BigQuery ML Remote Models:**\n",
    "\n",
    "| Feature | AlloyDB | BigQuery |\n",
    "|---------|---------|----------|\n",
    "| **Workload** | Transactional (OLTP) + Analytical | Analytical only (OLAP) |\n",
    "| **Latency** | Sub-millisecond queries | Seconds for aggregations |\n",
    "| **Data Scale** | Up to 64 TB | Petabytes |\n",
    "| **Use Case** | Operational databases with ML | Data warehouse batch scoring |\n",
    "| **Cost Model** | Per-second compute | Per-query bytes scanned |\n",
    "| **ML Integration** | `google_ml.predict_row()` | `ML.PREDICT()` |\n",
    "\n",
    "Both support calling Vertex AI endpoints from SQL, but serve different use cases.\n",
    "\n",
    "### AlloyDB AI Integration\n",
    "\n",
    "AlloyDB's **Vertex AI integration** enables SQL-based ML inference through the `google_ml_integration` extension:\n",
    "\n",
    "**Key Functions:**\n",
    "- `google_ml.create_model()` - Register Vertex AI endpoint or deployed model\n",
    "- `google_ml.predict_row()` - Make predictions for a single row\n",
    "- `google_ml.predict_batch()` - Batch predictions for multiple rows\n",
    "\n",
    "**Example:**\n",
    "```sql\n",
    "-- Register Vertex AI endpoint\n",
    "CALL google_ml.create_model(\n",
    "    model_id => 'my_model',\n",
    "    model_type => 'vertex_ai_endpoint',\n",
    "    endpoint_id => 'projects/.../endpoints/123'\n",
    ");\n",
    "\n",
    "-- Make predictions in SQL\n",
    "SELECT \n",
    "    transaction_id,\n",
    "    amount,\n",
    "    google_ml.predict_row('my_model', JSON_BUILD_OBJECT(\n",
    "        'instances', ARRAY[ARRAY[time, v1, v2, amount]]\n",
    "    )) AS prediction\n",
    "FROM transactions;\n",
    "```\n",
    "\n",
    "This notebook will walk through the complete setup and demonstrate all inference patterns.\n",
    "\n",
    "## Resources\n",
    "\n",
    "**Documentation:**\n",
    "- [AlloyDB Overview](https://cloud.google.com/alloydb/docs/overview)\n",
    "- [AlloyDB AI Capabilities](https://cloud.google.com/alloydb/docs/ai)\n",
    "- [Configure Vertex AI Integration](https://cloud.google.com/alloydb/docs/ai/configure-vertex-ai)\n",
    "- [Register Model Endpoints](https://cloud.google.com/alloydb/docs/ai/register-model-endpoint)\n",
    "- [Invoke Predictions](https://cloud.google.com/alloydb/docs/ai/invoke-predictions)\n",
    "- [AlloyDB Pricing](https://cloud.google.com/alloydb/pricing)\n",
    "\n",
    "**Related Notebooks:**\n",
    "- [vertex-ai-endpoint-prebuilt-container.ipynb](./vertex-ai-endpoint-prebuilt-container.ipynb) - Deploy endpoint with pre-built container\n",
    "- [vertex-ai-endpoint-custom-container.ipynb](./vertex-ai-endpoint-custom-container.ipynb) - Deploy endpoint with custom container  \n",
    "- [bigquery-bqml-remote-model-vertex.ipynb](./bigquery-bqml-remote-model-vertex.ipynb) - Compare with BigQuery ML approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "env_setup",
   "metadata": {},
   "source": [
    "---\n",
    "## Environment Setup\n",
    "\n",
    "This section will authenticate your session, enable required Google Cloud APIs, and install necessary Python packages.\n",
    "\n",
    "**Package Installation Options (`REQ_TYPE`):**\n",
    "- `PRIMARY`: Installs only the main packages. Faster, but pip resolves sub-dependencies which may result in different versions than development.\n",
    "- `ALL` (Default): Installs exact versions of all packages and dependencies. Best for perfectly reproducing the development environment.\n",
    "- `COLAB`: Installs a Colab-optimized list that excludes pre-installed packages like `ipython` and `ipykernel`.\n",
    "\n",
    "**Installation Tool Options (`INSTALL_TOOL`):**\n",
    "- `pip` (Default): Uses pip for package installation. Standard Python package installer.\n",
    "- `uv`: Modern, fast Python package installer. Must be installed separately. See: https://github.com/astral-sh/uv\n",
    "- `poetry`: Dependency management tool. Requires running notebook in a poetry environment (`poetry shell` or `poetry run jupyter lab`). Uses `pyproject.toml` instead of requirements.txt.\n",
    "\n",
    "> **Note:** If running in Google Colab, the script will automatically detect this and set `REQ_TYPE = 'COLAB'` to prevent package conflicts, overriding any manual setting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "set_project",
   "metadata": {},
   "source": [
    "### Set Your Project ID\n",
    "\n",
    "‚ö†Ô∏è **Action Required:** Replace the `PROJECT_ID` value below with your Google Cloud project ID before running this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "project_config",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = 'statmike-mlops-349915' # replace with GCP project ID\n",
    "REQ_TYPE = 'ALL' # Specify PRIMARY or ALL or COLAB\n",
    "INSTALL_TOOL = 'poetry' # Specify pip, uv, or poetry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config_header",
   "metadata": {},
   "source": [
    "### Configuration\n",
    "\n",
    "This cell defines the requirements files and Google Cloud APIs needed for this notebook. Run as-is without modification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": [
    "REQUIREMENTS_URL = 'https://raw.githubusercontent.com/statmike/vertex-ai-mlops/refs/heads/main/Framework%20Workflows/PyTorch/requirements.txt'\n",
    "\n",
    "REQUIRED_APIS = [\n",
    "    \"aiplatform.googleapis.com\",\n",
    "    \"alloydb.googleapis.com\",\n",
    "    \"bigquery.googleapis.com\",\n",
    "    \"compute.googleapis.com\",  # Required for AlloyDB networking\n",
    "    \"servicenetworking.googleapis.com\",  # Required for AlloyDB private IP\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "run_setup_header",
   "metadata": {},
   "source": [
    "### Run Setup\n",
    "\n",
    "This cell downloads the centralized setup code and configures your environment. It will:\n",
    "- Authenticate your session with Google Cloud\n",
    "- Enable required APIs for this notebook (including AlloyDB)\n",
    "- Install necessary Python packages\n",
    "- Display a setup summary with your project information\n",
    "\n",
    "> **Note:** In Colab, if packages are installed, the kernel will automatically restart. After restart, continue from the next cell without re-running earlier cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "run_setup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "PYTHON GCP ENVIRONMENT SETUP\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "AUTHENTICATION\n",
      "==================================================\n",
      "Checking for existing ADC...\n",
      "‚úÖ Existing ADC found.\n",
      "‚úÖ Project is correctly set to 'statmike-mlops-349915'.\n",
      "\n",
      "==================================================\n",
      "API CHECK & ENABLE\n",
      "==================================================\n",
      "‚úÖ aiplatform.googleapis.com is already enabled.\n",
      "‚úÖ alloydb.googleapis.com is already enabled.\n",
      "‚úÖ bigquery.googleapis.com is already enabled.\n",
      "‚úÖ compute.googleapis.com is already enabled.\n",
      "‚úÖ servicenetworking.googleapis.com is already enabled.\n",
      "\n",
      "==================================================\n",
      "PACKAGE MANAGEMENT\n",
      "==================================================\n",
      "Installation Tool: poetry\n",
      "‚úÖ Found poetry at: /usr/local/google/home/statmike/.local/bin/poetry\n",
      "‚úÖ Running in poetry environment: /usr/local/google/home/statmike/.cache/pypoetry/virtualenvs/frameworks-pytorch-0KVJlKeQ-py3.13\n",
      "‚ÑπÔ∏è  Poetry mode: Installing from pyproject.toml (REQUIREMENTS_URL ignored)\n",
      "‚úÖ Found pyproject.toml at: /usr/local/google/home/statmike/Git/vertex-ai-mlops/Framework Workflows/PyTorch/pyproject.toml\n",
      "   Changed working directory to: /usr/local/google/home/statmike/Git/vertex-ai-mlops/Framework Workflows/PyTorch\n",
      "Running poetry install...\n",
      "   Restored working directory to: /usr/local/google/home/statmike/Git/vertex-ai-mlops/Framework Workflows/PyTorch/serving\n",
      "‚úÖ All packages are already installed and up to date.\n",
      "\n",
      "==================================================\n",
      "Google Cloud Project Information\n",
      "==================================================\n",
      "PROJECT_ID     = statmike-mlops-349915\n",
      "PROJECT_NUMBER = 1026793852137\n",
      "==================================================\n",
      "\n",
      "\n",
      "==================================================\n",
      "SETUP SUMMARY\n",
      "==================================================\n",
      "‚úÖ Authentication:    Success\n",
      "‚úÖ API Configuration: Success\n",
      "‚úÖ Package Install:   Already up to date\n",
      "‚úÖ Installation Tool: poetry\n",
      "‚úÖ Project ID:        statmike-mlops-349915\n",
      "‚úÖ Project Number:    1026793852137\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os, urllib.request\n",
    "\n",
    "# Download and import setup code\n",
    "url = 'https://raw.githubusercontent.com/statmike/vertex-ai-mlops/refs/heads/main/core/notebook-template/python_setup.py'\n",
    "urllib.request.urlretrieve(url, 'python_setup_local.py')\n",
    "import python_setup_local as python_setup\n",
    "os.remove('python_setup_local.py')\n",
    "\n",
    "# Run setup\n",
    "setup_info = python_setup.setup_environment(PROJECT_ID, REQ_TYPE, REQUIREMENTS_URL, REQUIRED_APIS, INSTALL_TOOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xmuqvlaobd",
   "metadata": {},
   "source": [
    "---\n",
    "## Python Setup\n",
    "\n",
    "Import necessary libraries and initialize clients for Vertex AI, BigQuery, and AlloyDB."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "w873drglvoe",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "t9ry7g6clyt",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/google/home/statmike/.cache/pypoetry/virtualenvs/frameworks-pytorch-0KVJlKeQ-py3.13/lib/python3.13/site-packages/google/cloud/aiplatform/models.py:52: FutureWarning: Support for google-cloud-storage < 3.0.0 will be removed in a future version of google-cloud-aiplatform. Please upgrade to google-cloud-storage >= 3.0.0.\n",
      "  from google.cloud.aiplatform.utils import gcs_utils\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import json\n",
    "import asyncio\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Vertex AI\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "# BigQuery\n",
    "from google.cloud import bigquery\n",
    "\n",
    "# AlloyDB\n",
    "from google.cloud import alloydb\n",
    "import google.cloud.alloydb.connector\n",
    "from google.cloud.alloydb.connector import IPTypes\n",
    "import sqlalchemy\n",
    "import asyncpg\n",
    "import sqlalchemy.ext.asyncio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rii95x6fgw",
   "metadata": {},
   "source": [
    "### Variables - User Set\n",
    "\n",
    "Configure AlloyDB and Vertex AI endpoint settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31j5a476sie",
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION = 'us-central1'\n",
    "SERIES = 'frameworks'\n",
    "EXPERIMENT = 'pytorch-autoencoder-alloydb'\n",
    "\n",
    "# Vertex AI Endpoint (must already be deployed)\n",
    "ENDPOINT_DISPLAY_NAME = 'pytorch-autoencoder-endpoint'\n",
    "\n",
    "# AlloyDB Configuration\n",
    "# Note: AlloyDB cluster/instance names must use hyphens, not underscores\n",
    "ALLOYDB_CLUSTER_NAME = PROJECT_ID  # Keep hyphens from PROJECT_ID\n",
    "ALLOYDB_INSTANCE_NAME = PROJECT_ID  # Keep hyphens from PROJECT_ID\n",
    "ALLOYDB_DATABASE_NAME = SERIES\n",
    "ALLOYDB_TABLE_NAME = EXPERIMENT.replace('-', '_')\n",
    "ALLOYDB_USER = 'postgres'\n",
    "ALLOYDB_PASSWORD = 'alloydb-demo-password'  # For demo purposes only"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0i6636krk5up",
   "metadata": {},
   "source": [
    "### Variables - Auto Set\n",
    "\n",
    "Retrieve project information from the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fm2i5r7bz9t",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project ID: statmike-mlops-349915\n",
      "Project Number: 1026793852137\n",
      "Region: us-central1\n"
     ]
    }
   ],
   "source": [
    "# Get project information\n",
    "if not PROJECT_ID:\n",
    "    shell_output = subprocess.run(['gcloud', 'config', 'list', '--format', 'value(core.project)'], capture_output=True)\n",
    "    PROJECT_ID = shell_output.stdout.decode('utf-8').strip()\n",
    "\n",
    "shell_output = subprocess.run(['gcloud', 'projects', 'describe', PROJECT_ID, '--format', 'value(projectNumber)'], capture_output=True)\n",
    "PROJECT_NUMBER = shell_output.stdout.decode('utf-8').strip()\n",
    "\n",
    "print(f'Project ID: {PROJECT_ID}')\n",
    "print(f'Project Number: {PROJECT_NUMBER}')\n",
    "print(f'Region: {REGION}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9zfe8lscwt",
   "metadata": {},
   "source": [
    "### Initialize Clients\n",
    "\n",
    "Create clients for Vertex AI, BigQuery, and AlloyDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bgjuyhs3mt9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Vertex AI\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION)\n",
    "\n",
    "# Initialize BigQuery\n",
    "bq = bigquery.Client(project=PROJECT_ID)\n",
    "\n",
    "# Initialize AlloyDB Admin Client\n",
    "alloydb_client = alloydb.AlloyDBAdminClient()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kudjgwpoxm",
   "metadata": {},
   "source": [
    "---\n",
    "## Prerequisites: Verify Deployed Endpoint\n",
    "\n",
    "This notebook requires a deployed Vertex AI Endpoint. Use either:\n",
    "- [vertex-ai-endpoint-prebuilt-container.ipynb](./vertex-ai-endpoint-prebuilt-container.ipynb) - Pre-built TorchServe container\n",
    "- [vertex-ai-endpoint-custom-container.ipynb](./vertex-ai-endpoint-custom-container.ipynb) - Custom FastAPI container\n",
    "\n",
    "This section will find the endpoint and test predictions to verify it's working."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tc40vpe6h5m",
   "metadata": {},
   "source": [
    "### Find Endpoint\n",
    "\n",
    "Search for the deployed endpoint by display name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "up4klncxdn",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found endpoint: pytorch-autoencoder-endpoint\n",
      "Resource name: projects/1026793852137/locations/us-central1/endpoints/5971323405637517312\n",
      "Endpoint ID: 5971323405637517312\n"
     ]
    }
   ],
   "source": [
    "# Find endpoint by display name\n",
    "endpoints = aiplatform.Endpoint.list(\n",
    "    filter=f'display_name=\"{ENDPOINT_DISPLAY_NAME}\"',\n",
    "    order_by='create_time desc'\n",
    ")\n",
    "\n",
    "if not endpoints:\n",
    "    raise ValueError(\n",
    "        f\"No endpoint found with display_name='{ENDPOINT_DISPLAY_NAME}'. \"\n",
    "        f\"Please deploy an endpoint first using one of the prerequisite notebooks.\"\n",
    "    )\n",
    "\n",
    "endpoint = endpoints[0]\n",
    "print(f'Found endpoint: {endpoint.display_name}')\n",
    "print(f'Resource name: {endpoint.resource_name}')\n",
    "print(f'Endpoint ID: {endpoint.name.split(\"/\")[-1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "srljbebyyb9",
   "metadata": {},
   "source": [
    "### Test Predictions\n",
    "\n",
    "Make a test prediction to verify the endpoint is working and detect its type (pre-built or custom container)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dp31mvaoncu",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test prediction successful!\n",
      "Prediction type: <class 'google.cloud.aiplatform.models.Prediction'>\n",
      "Prediction structure: <class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "# Sample test instance (30 features matching the autoencoder model)\n",
    "test_instance = [\n",
    "    92.35, -0.26, 0.13, -1.15, 0.93, -0.60, -0.20, 0.57, \n",
    "    0.82, 0.30, -0.58, 0.07, -0.18, 0.42, -0.22, 0.43, \n",
    "    -0.24, -0.03, 0.15, 0.06, -0.14, 0.24, -0.05, 0.29, \n",
    "    0.54, -0.33, 0.21, 0.09, -0.02, 15.47\n",
    "]\n",
    "\n",
    "# Make prediction\n",
    "prediction = endpoint.predict(instances=[test_instance])\n",
    "\n",
    "print('Test prediction successful!')\n",
    "print(f'Prediction type: {type(prediction)}')\n",
    "print(f'Prediction structure: {type(prediction.predictions[0])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sukm6kx9m2",
   "metadata": {},
   "source": [
    "### Detect Endpoint Type\n",
    "\n",
    "Determine if this is a pre-built container (13 output fields) or custom container (2 output fields)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4wdit11upue",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Detected: Pre-built Container Endpoint\n",
      "  - Output fields: ['denormalized_reconstruction_errors', 'normalized_reconstruction_errors', 'denormalized_MSE', 'normalized_reconstruction', 'denormalized_RMSE', 'normalized_MSE', 'denormalized_MSLE', 'denormalized_reconstruction', 'encoded', 'denormalized_MAE', 'normalized_MSLE', 'normalized_MAE', 'normalized_RMSE']\n",
      "  - Denormalized MAE: 2413.40673828125\n"
     ]
    }
   ],
   "source": [
    "# Detect endpoint type by checking response structure\n",
    "first_prediction = prediction.predictions[0]\n",
    "\n",
    "if isinstance(first_prediction, dict):\n",
    "    # Custom container - returns dict with specific fields\n",
    "    if 'anomaly_score' in first_prediction and 'encoded' in first_prediction:\n",
    "        ENDPOINT_TYPE = 'custom'\n",
    "        print('‚úì Detected: Custom Container Endpoint')\n",
    "        print(f'  - Output fields: {list(first_prediction.keys())}')\n",
    "        print(f'  - Anomaly score: {first_prediction[\"anomaly_score\"]:.2f}')\n",
    "        print(f'  - Encoded representation: {first_prediction[\"encoded\"][:2]}...')\n",
    "    else:\n",
    "        # Pre-built container - returns dict with 13 fields\n",
    "        ENDPOINT_TYPE = 'prebuilt'\n",
    "        print('‚úì Detected: Pre-built Container Endpoint')\n",
    "        print(f'  - Output fields: {list(first_prediction.keys())}')\n",
    "        print(f'  - Denormalized MAE: {first_prediction.get(\"denormalized_MAE\", \"N/A\")}')\n",
    "else:\n",
    "    raise ValueError(f'Unexpected prediction format: {type(first_prediction)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fhdl2frewfa",
   "metadata": {},
   "source": [
    "---\n",
    "## AlloyDB Infrastructure Setup\n",
    "\n",
    "Create minimal AlloyDB infrastructure for this demonstration:\n",
    "- **Cluster**: Regional container for instances\n",
    "- **Instance**: PRIMARY instance (2 vCPU, ZONAL availability)\n",
    "- **Database**: PostgreSQL database for storing data\n",
    "- **Extensions**: Enable `google_ml_integration` for Vertex AI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1p6ntg2t",
   "metadata": {},
   "source": [
    "### Create AlloyDB Cluster\n",
    "\n",
    "Create a cluster with **Private IP connectivity** for local development.\n",
    "\n",
    "**Important: AlloyDB Connectivity is Either/Or**\n",
    "\n",
    "AlloyDB supports two mutually exclusive connectivity modes:\n",
    "\n",
    "1. **Private IP (VPC-based)**: \n",
    "   - Accessible from VPN-connected workstations\n",
    "   - Requires VPC network configuration\n",
    "   - Lower latency (direct VPC routing)\n",
    "   - **This is what we're using** ‚úì\n",
    "\n",
    "2. **Private Service Connect (PSC)**:\n",
    "   - Accessible from Google Cloud environments only (Cloud Shell, Vertex AI, GCE)\n",
    "   - Does NOT work from local workstations\n",
    "   - No VPC configuration needed\n",
    "\n",
    "**You cannot have both on the same cluster.** Since this notebook needs to run locally, we're using Private IP connectivity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aova9axta3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Found existing cluster: projects/statmike-mlops-349915/locations/us-central1/clusters/statmike-mlops-349915\n",
      "  Network: projects/1026793852137/global/networks/default\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    alloydb_cluster = alloydb_client.get_cluster(\n",
    "        name=f'projects/{PROJECT_ID}/locations/{REGION}/clusters/{ALLOYDB_CLUSTER_NAME}'\n",
    "    )\n",
    "    print(f'‚úì Found existing cluster: {alloydb_cluster.name}')\n",
    "    print(f'  Network: {alloydb_cluster.network if alloydb_cluster.network else \"NOT SET\"}')\n",
    "except Exception:\n",
    "    print(f'Creating AlloyDB cluster: {ALLOYDB_CLUSTER_NAME}')\n",
    "    print('  Using Private IP connectivity for local development...')\n",
    "    print('  This may take 10-15 minutes...')\n",
    "    \n",
    "    create_cluster = alloydb_client.create_cluster(\n",
    "        parent=f'projects/{PROJECT_ID}/locations/{REGION}',\n",
    "        cluster_id=ALLOYDB_CLUSTER_NAME,\n",
    "        cluster=alloydb.Cluster(\n",
    "            # Enable private IP connectivity (for local development via VPN)\n",
    "            network=f'projects/{PROJECT_ID}/global/networks/default',\n",
    "            # Set initial user credentials\n",
    "            initial_user=alloydb.UserPassword(\n",
    "                user=ALLOYDB_USER,\n",
    "                password=ALLOYDB_PASSWORD\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    alloydb_cluster = create_cluster.result()\n",
    "    print(f'‚úì Created cluster: {alloydb_cluster.name}')\n",
    "    print(f'  Network: {alloydb_cluster.network}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vqvzo8o8z3t",
   "metadata": {},
   "source": [
    "### Create AlloyDB Instance\n",
    "\n",
    "Create a PRIMARY instance with minimal configuration for local development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "tysrbkzo4y",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Found existing instance: projects/statmike-mlops-349915/locations/us-central1/clusters/statmike-mlops-349915/instances/statmike-mlops-349915\n",
      "\n",
      "Instance details:\n",
      "  Private IP: 172.16.0.31\n",
      "  State: READY\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    alloydb_instance = alloydb_client.get_instance(\n",
    "        name=f'{alloydb_cluster.name}/instances/{ALLOYDB_INSTANCE_NAME}'\n",
    "    )\n",
    "    print(f'‚úì Found existing instance: {alloydb_instance.name}')\n",
    "except Exception:\n",
    "    print(f'Creating AlloyDB instance: {ALLOYDB_INSTANCE_NAME}')\n",
    "    print('  This may take 10-15 minutes...')\n",
    "    \n",
    "    create_instance = alloydb_client.create_instance(\n",
    "        parent=alloydb_cluster.name,\n",
    "        instance_id=ALLOYDB_INSTANCE_NAME,\n",
    "        instance=alloydb.Instance(\n",
    "            instance_type=alloydb.Instance.InstanceType.PRIMARY,\n",
    "            machine_config=alloydb.Instance.MachineConfig(cpu_count=2),\n",
    "            availability_type=alloydb.Instance.AvailabilityType.ZONAL,\n",
    "            gce_zone=f'{REGION}-a'\n",
    "        )\n",
    "    )\n",
    "    alloydb_instance = create_instance.result()\n",
    "    print(f'‚úì Created instance: {alloydb_instance.name}')\n",
    "\n",
    "# Display connection information\n",
    "print(f'\\nInstance details:')\n",
    "print(f'  Private IP: {alloydb_instance.ip_address}')\n",
    "print(f'  State: {alloydb_instance.state.name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "test_connectivity_header",
   "metadata": {},
   "source": [
    "### Test Connectivity & Setup Bastion (if needed)\n",
    "\n",
    "This cell automatically detects if your environment can connect to AlloyDB's private IP.\n",
    "\n",
    "**Auto-Detection:**\n",
    "- ‚úÖ **Direct connectivity** (Cloud Shell, GCE, VPN): Uses private IP directly\n",
    "- üîß **No connectivity** (local workstation): Creates bastion VM with SSH tunnel\n",
    "\n",
    "**Bastion Setup (automatic if needed):**\n",
    "- Creates minimal `e2-micro` VM (~$0.01/hour)\n",
    "- Installs AlloyDB Auth Proxy\n",
    "- Establishes SSH tunnel\n",
    "- Transparently redirects connections through localhost\n",
    "- Auto-deleted in cleanup section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "test_and_create_bastion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing connectivity to AlloyDB at 172.16.0.31...\n",
      "‚ö†Ô∏è  Cannot reach AlloyDB private IP directly\n",
      "   Setting up bastion VM with SSH tunnel...\n",
      "\n",
      "‚úì Found existing bastion VM: statmike-mlops-349915-bastion\n",
      "  Status: RUNNING\n",
      "  ‚úì Bastion VM is already running with proper scopes\n",
      "\n",
      "Creating SSH tunnel from localhost:5432 ‚Üí bastion:5432...\n",
      "  This may take 10-15 seconds to establish...\n",
      "  Waiting for tunnel to stabilize...\n",
      "‚úÖ SSH tunnel established successfully!\n",
      "   AlloyDB is now accessible via localhost:5432\n",
      "   Connections will be tunneled through bastion VM\n"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "import time\n",
    "\n",
    "# Test if we can reach AlloyDB private IP\n",
    "print(f'Testing connectivity to AlloyDB at {alloydb_instance.ip_address}...')\n",
    "\n",
    "# Initialize bastion tracking variables\n",
    "BASTION_NEEDED = False\n",
    "BASTION_VM_NAME = None\n",
    "BASTION_ZONE = None\n",
    "SSH_TUNNEL_PROCESS = None\n",
    "\n",
    "try:\n",
    "    # Try to create a socket connection to AlloyDB private IP\n",
    "    sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "    sock.settimeout(5)  # 5 second timeout\n",
    "    result = sock.connect_ex((alloydb_instance.ip_address, 5432))\n",
    "    sock.close()\n",
    "    \n",
    "    if result == 0:\n",
    "        print('‚úÖ Direct connectivity successful!')\n",
    "        print('   Environment has network access to AlloyDB private IP')\n",
    "        print('   No bastion needed - will connect directly')\n",
    "        BASTION_NEEDED = False\n",
    "    else:\n",
    "        print('‚ö†Ô∏è  Cannot reach AlloyDB private IP directly')\n",
    "        print('   Setting up bastion VM with SSH tunnel...')\n",
    "        BASTION_NEEDED = True\n",
    "except Exception as e:\n",
    "    print(f'‚ö†Ô∏è  Connection test failed: {e}')\n",
    "    print('   Setting up bastion VM with SSH tunnel...')\n",
    "    BASTION_NEEDED = True\n",
    "\n",
    "if BASTION_NEEDED:\n",
    "    from google.cloud import compute_v1\n",
    "    \n",
    "    # Bastion VM configuration\n",
    "    BASTION_VM_NAME = f'{ALLOYDB_CLUSTER_NAME}-bastion'\n",
    "    BASTION_ZONE = f'{REGION}-a'\n",
    "    \n",
    "    # Set CONNECTION_NAME here (normally defined later, but needed for bastion)\n",
    "    CONNECTION_NAME = alloydb_instance.name\n",
    "    \n",
    "    compute_client = compute_v1.InstancesClient()\n",
    "    \n",
    "    # Check if bastion already exists\n",
    "    vm_exists = False\n",
    "    try:\n",
    "        existing_vm = compute_client.get(\n",
    "            project=PROJECT_ID,\n",
    "            zone=BASTION_ZONE,\n",
    "            instance=BASTION_VM_NAME\n",
    "        )\n",
    "        print(f'\\n‚úì Found existing bastion VM: {BASTION_VM_NAME}')\n",
    "        print(f'  Status: {existing_vm.status}')\n",
    "        \n",
    "        # Check if it has proper scopes - if not, we need to recreate it\n",
    "        has_proper_scopes = False\n",
    "        for sa in existing_vm.service_accounts:\n",
    "            if 'https://www.googleapis.com/auth/cloud-platform' in sa.scopes:\n",
    "                has_proper_scopes = True\n",
    "                break\n",
    "        \n",
    "        if not has_proper_scopes:\n",
    "            print('  ‚ö†Ô∏è  VM lacks required API access scopes')\n",
    "            print('  Deleting and recreating with proper configuration...')\n",
    "            delete_op = compute_client.delete(\n",
    "                project=PROJECT_ID,\n",
    "                zone=BASTION_ZONE,\n",
    "                instance=BASTION_VM_NAME\n",
    "            )\n",
    "            delete_op.result()\n",
    "            print('  ‚úì Old VM deleted')\n",
    "            vm_exists = False\n",
    "        else:\n",
    "            vm_exists = True\n",
    "            # If VM is not running, start it\n",
    "            if existing_vm.status != 'RUNNING':\n",
    "                print(f'  Starting bastion VM...')\n",
    "                start_op = compute_client.start(\n",
    "                    project=PROJECT_ID,\n",
    "                    zone=BASTION_ZONE,\n",
    "                    instance=BASTION_VM_NAME\n",
    "                )\n",
    "                start_op.result()\n",
    "                print(f'  ‚úì Bastion VM started')\n",
    "                # Give it time to start proxy\n",
    "                time.sleep(30)\n",
    "            else:\n",
    "                print(f'  ‚úì Bastion VM is already running with proper scopes')\n",
    "            \n",
    "    except Exception:\n",
    "        vm_exists = False\n",
    "    \n",
    "    if not vm_exists:\n",
    "        print(f'\\nCreating bastion VM: {BASTION_VM_NAME}')\n",
    "        print('  Machine type: e2-micro (0.25 vCPU, 1 GB RAM)')\n",
    "        print('  Cost: ~$0.01/hour (~$7/month if left running)')\n",
    "        print('  This will take 1-2 minutes...')\n",
    "        \n",
    "        # Create e2-micro VM\n",
    "        machine_type = f'zones/{BASTION_ZONE}/machineTypes/e2-micro'\n",
    "        \n",
    "        # Startup script to install AlloyDB Auth Proxy\n",
    "        startup_script = f'''#!/bin/bash\n",
    "# Download AlloyDB Auth Proxy\n",
    "wget -q https://storage.googleapis.com/alloydb-auth-proxy/v1.11.1/alloydb-auth-proxy.linux.amd64 -O /usr/local/bin/alloydb-auth-proxy\n",
    "chmod +x /usr/local/bin/alloydb-auth-proxy\n",
    "\n",
    "# Start AlloyDB Auth Proxy in background\n",
    "nohup /usr/local/bin/alloydb-auth-proxy \"{CONNECTION_NAME}\" --address 0.0.0.0 --port 5432 > /var/log/alloydb-proxy.log 2>&1 &\n",
    "\n",
    "echo \"AlloyDB Auth Proxy started\"\n",
    "'''\n",
    "        \n",
    "        instance_config = compute_v1.Instance(\n",
    "            name=BASTION_VM_NAME,\n",
    "            machine_type=machine_type,\n",
    "            disks=[\n",
    "                compute_v1.AttachedDisk(\n",
    "                    auto_delete=True,\n",
    "                    boot=True,\n",
    "                    initialize_params=compute_v1.AttachedDiskInitializeParams(\n",
    "                        source_image='projects/debian-cloud/global/images/family/debian-12',\n",
    "                        disk_size_gb=10\n",
    "                    )\n",
    "                )\n",
    "            ],\n",
    "            network_interfaces=[\n",
    "                compute_v1.NetworkInterface(\n",
    "                    network=f'projects/{PROJECT_ID}/global/networks/default',\n",
    "                    access_configs=[compute_v1.AccessConfig(name='External NAT')]\n",
    "                )\n",
    "            ],\n",
    "            # IMPORTANT: Add proper service account with full cloud-platform scope\n",
    "            service_accounts=[\n",
    "                compute_v1.ServiceAccount(\n",
    "                    email='default',\n",
    "                    scopes=['https://www.googleapis.com/auth/cloud-platform']\n",
    "                )\n",
    "            ],\n",
    "            metadata=compute_v1.Metadata(\n",
    "                items=[\n",
    "                    compute_v1.Items(key='startup-script', value=startup_script)\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        operation = compute_client.insert(\n",
    "            project=PROJECT_ID,\n",
    "            zone=BASTION_ZONE,\n",
    "            instance_resource=instance_config\n",
    "        )\n",
    "        \n",
    "        # Wait for VM creation\n",
    "        operation.result()\n",
    "        print(f'‚úì Created bastion VM: {BASTION_VM_NAME}')\n",
    "        \n",
    "        # Give proxy time to start\n",
    "        print('  Waiting for AlloyDB Auth Proxy to start (30 seconds)...')\n",
    "        time.sleep(30)\n",
    "    \n",
    "    # Create SSH tunnel\n",
    "    print('\\nCreating SSH tunnel from localhost:5432 ‚Üí bastion:5432...')\n",
    "    print('  This may take 10-15 seconds to establish...')\n",
    "    import subprocess\n",
    "    \n",
    "    ssh_command = [\n",
    "        'gcloud', 'compute', 'ssh', BASTION_VM_NAME,\n",
    "        '--project', PROJECT_ID,\n",
    "        '--zone', BASTION_ZONE,\n",
    "        '--ssh-flag=-N',  # No remote command\n",
    "        '--ssh-flag=-L 5432:localhost:5432',  # Forward localhost:5432 ‚Üí bastion:5432\n",
    "        '--tunnel-through-iap',  # Use IAP for SSH\n",
    "        '--quiet'  # Suppress SSH output\n",
    "    ]\n",
    "    \n",
    "    # Start tunnel in background\n",
    "    SSH_TUNNEL_PROCESS = subprocess.Popen(\n",
    "        ssh_command,\n",
    "        stdout=subprocess.PIPE,\n",
    "        stderr=subprocess.PIPE\n",
    "    )\n",
    "    \n",
    "    # Give tunnel more time to establish\n",
    "    print('  Waiting for tunnel to stabilize...')\n",
    "    time.sleep(10)\n",
    "    \n",
    "    # Check if tunnel is still running\n",
    "    if SSH_TUNNEL_PROCESS.poll() is None:\n",
    "        # Tunnel process is running, test if port is actually listening\n",
    "        try:\n",
    "            test_sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "            test_sock.settimeout(2)\n",
    "            test_result = test_sock.connect_ex(('localhost', 5432))\n",
    "            test_sock.close()\n",
    "            \n",
    "            if test_result == 0:\n",
    "                print('‚úÖ SSH tunnel established successfully!')\n",
    "                print('   AlloyDB is now accessible via localhost:5432')\n",
    "                print('   Connections will be tunneled through bastion VM')\n",
    "            else:\n",
    "                print('‚ö†Ô∏è  SSH tunnel process is running but port 5432 is not accessible')\n",
    "                print('   Waiting additional 10 seconds for tunnel to fully establish...')\n",
    "                time.sleep(10)\n",
    "                # Test again\n",
    "                test_sock2 = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "                test_sock2.settimeout(2)\n",
    "                test_result2 = test_sock2.connect_ex(('localhost', 5432))\n",
    "                test_sock2.close()\n",
    "                if test_result2 == 0:\n",
    "                    print('‚úÖ SSH tunnel now working!')\n",
    "                else:\n",
    "                    print('‚ùå Port 5432 still not accessible after 20 seconds total')\n",
    "                    stdout, stderr = SSH_TUNNEL_PROCESS.communicate(timeout=1)\n",
    "                    print(f'   SSH stdout: {stdout.decode() if stdout else \"(empty)\"}')\n",
    "                    print(f'   SSH stderr: {stderr.decode() if stderr else \"(empty)\"}')\n",
    "                    raise Exception('SSH tunnel failed to establish connection')\n",
    "        except socket.error as e:\n",
    "            print(f'‚ùå SSH tunnel test failed: {e}')\n",
    "            raise Exception('SSH tunnel established but connection test failed')\n",
    "    else:\n",
    "        print('‚ùå SSH tunnel process died')\n",
    "        stdout, stderr = SSH_TUNNEL_PROCESS.communicate()\n",
    "        print(f'   Exit code: {SSH_TUNNEL_PROCESS.returncode}')\n",
    "        print(f'   SSH stdout: {stdout.decode() if stdout else \"(empty)\"}')\n",
    "        print(f'   SSH stderr: {stderr.decode() if stderr else \"(empty)\"}')\n",
    "        raise Exception('SSH tunnel setup failed - process terminated')\n",
    "else:\n",
    "    print('\\n‚úÖ Ready to connect to AlloyDB directly')\n",
    "    # Set CONNECTION_NAME for direct connections too\n",
    "    CONNECTION_NAME = alloydb_instance.name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jy56otmmg3",
   "metadata": {},
   "source": [
    "---\n",
    "## Database Connection Setup\n",
    "\n",
    "AlloyDB instances are **private by default** and require proper network configuration. This section will:\n",
    "1. Detect your execution environment\n",
    "2. Test connectivity to the AlloyDB instance\n",
    "3. Enable public IP if needed (for local/Colab environments)\n",
    "4. Create database connections using the AlloyDB Python Connector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nckriimskql",
   "metadata": {},
   "source": [
    "### Create Connector and Connection Pool\n",
    "\n",
    "Set up synchronous and asynchronous connection pools using the AlloyDB connector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "mjj2b5zgu6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using bastion tunnel connection mode (localhost:5432)\n",
      "‚úì Connection functions defined\n"
     ]
    }
   ],
   "source": [
    "# Connection functions adapt based on whether bastion is being used\n",
    "\n",
    "if BASTION_NEEDED:\n",
    "    # When using bastion + SSH tunnel, connect to localhost:5432\n",
    "    print('Using bastion tunnel connection mode (localhost:5432)')\n",
    "    \n",
    "    def get_sync_conn(connector, db):\n",
    "        def getconn():\n",
    "            import pg8000\n",
    "            conn = pg8000.connect(\n",
    "                host='localhost',\n",
    "                port=5432,\n",
    "                user=ALLOYDB_USER,\n",
    "                password=ALLOYDB_PASSWORD,\n",
    "                database=db\n",
    "            )\n",
    "            return conn\n",
    "        return getconn\n",
    "    \n",
    "    async def get_async_conn(connector, db):\n",
    "        async def getconn():\n",
    "            conn = await asyncpg.connect(\n",
    "                host='localhost',\n",
    "                port=5432,\n",
    "                user=ALLOYDB_USER,\n",
    "                password=ALLOYDB_PASSWORD,\n",
    "                database=db\n",
    "            )\n",
    "            return conn\n",
    "        return getconn\n",
    "else:\n",
    "    # When connecting directly, use AlloyDB connector\n",
    "    print('Using AlloyDB connector (direct private IP)')\n",
    "    \n",
    "    def get_sync_conn(connector, db):\n",
    "        def getconn():\n",
    "            conn = connector.connect(\n",
    "                CONNECTION_NAME,\n",
    "                \"pg8000\",\n",
    "                user=ALLOYDB_USER,\n",
    "                password=ALLOYDB_PASSWORD,\n",
    "                db=db,\n",
    "                ip_type=IPTypes.PRIVATE\n",
    "            )\n",
    "            return conn\n",
    "        return getconn\n",
    "    \n",
    "    async def get_async_conn(connector, db):\n",
    "        async def getconn():\n",
    "            conn = await connector.connect(\n",
    "                CONNECTION_NAME,\n",
    "                \"asyncpg\",\n",
    "                user=ALLOYDB_USER,\n",
    "                password=ALLOYDB_PASSWORD,\n",
    "                db=db,\n",
    "                ip_type=IPTypes.PRIVATE\n",
    "            )\n",
    "            return conn\n",
    "        return getconn\n",
    "\n",
    "# Create connection pools (same for both modes)\n",
    "def get_sync_pool(connector, db):\n",
    "    pool = sqlalchemy.create_engine(\n",
    "        \"postgresql+pg8000://\",\n",
    "        creator=get_sync_conn(connector, db)\n",
    "    )\n",
    "    pool.dialect.description_encoding = None\n",
    "    pool.execution_options(isolation_level=\"AUTOCOMMIT\")\n",
    "    return pool\n",
    "\n",
    "async def get_async_pool(connector, db):\n",
    "    pool = sqlalchemy.ext.asyncio.create_async_engine(\n",
    "        \"postgresql+asyncpg://\",\n",
    "        async_creator=await get_async_conn(connector, db)\n",
    "    )\n",
    "    pool.dialect.description_encoding = None\n",
    "    pool.execution_options(isolation_level=\"AUTOCOMMIT\")\n",
    "    return pool\n",
    "\n",
    "print('‚úì Connection functions defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pk0ttc6wlim",
   "metadata": {},
   "source": [
    "### Initialize Connections\n",
    "\n",
    "Create connector and pool connected to the default `postgres` database."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kwrp8ie0c1l",
   "metadata": {},
   "source": [
    "### Understanding AlloyDB Private IP Connectivity\n",
    "\n",
    "This notebook creates an AlloyDB cluster with **Private IP connectivity** using VPC networking.\n",
    "\n",
    "**How Private IP Works:**\n",
    "\n",
    "- AlloyDB instance gets a private IP address in your VPC network\n",
    "- Accessible from resources connected to the same VPC network\n",
    "- **From local workstations**: Requires VPN or Cloud Interconnect to the VPC\n",
    "- **From Google Cloud**: Cloud Shell, GCE VMs, Cloud Run (in same VPC)\n",
    "- Uses standard PostgreSQL protocol over private network\n",
    "\n",
    "**Connection Requirements:**\n",
    "\n",
    "To connect from your local workstation, you need:\n",
    "1. **VPN or Cloud Interconnect** to your Google Cloud VPC\n",
    "2. **Network firewall rules** allowing PostgreSQL traffic (port 5432)\n",
    "3. **AlloyDB Python Connector** (handles authentication automatically)\n",
    "\n",
    "**For this demo:**\n",
    "- The connector auto-detects if you're on a VPN-connected workstation\n",
    "- Uses `IPTypes.PRIVATE` for VPC-based connectivity\n",
    "- If you get connection errors, verify your VPN is connected to the project's VPC\n",
    "\n",
    "**Security:**\n",
    "- IAM authentication required\n",
    "- TLS encryption enforced\n",
    "- No public internet exposure\n",
    "- Network-level isolation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dh7lhff5gah",
   "metadata": {},
   "source": [
    "### Set Connection String\n",
    "\n",
    "Prepare the AlloyDB instance URI for the connector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ajp5ndjfjh",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection string: projects/statmike-mlops-349915/locations/us-central1/clusters/statmike-mlops-349915/instances/statmike-mlops-349915\n",
      "Instance private IP: 172.16.0.31\n",
      "Instance state: READY\n",
      "\n",
      "‚úì Using bastion tunnel mode\n",
      "  Bastion VM: statmike-mlops-349915-bastion\n",
      "  Connections route through: localhost:5432 ‚Üí bastion ‚Üí AlloyDB\n"
     ]
    }
   ],
   "source": [
    "# CONNECTION_NAME is already set in the connectivity test cell\n",
    "# Just display the connection information\n",
    "\n",
    "print(f'Connection string: {CONNECTION_NAME}')\n",
    "print(f'Instance private IP: {alloydb_instance.ip_address}')\n",
    "print(f'Instance state: {alloydb_instance.state.name}')\n",
    "\n",
    "if BASTION_NEEDED:\n",
    "    print(f'\\n‚úì Using bastion tunnel mode')\n",
    "    print(f'  Bastion VM: {BASTION_VM_NAME}')\n",
    "    print(f'  Connections route through: localhost:5432 ‚Üí bastion ‚Üí AlloyDB')\n",
    "else:\n",
    "    print(f'\\n‚úì Using direct connection mode')\n",
    "    print(f'  Connections route directly to: {alloydb_instance.ip_address}:5432')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qvvd2e5axyr",
   "metadata": {},
   "source": [
    "### Create Database Connections\n",
    "\n",
    "Create the AlloyDB connector and connection pools. The connector will automatically use the public IP endpoint if it was enabled above, or the private IP if you're running in the same VPC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "sjaxx4uzikf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Database connection pools created\n"
     ]
    }
   ],
   "source": [
    "# Create connectors\n",
    "sync_connector = google.cloud.alloydb.connector.Connector()\n",
    "async_connector = google.cloud.alloydb.connector.AsyncConnector()\n",
    "\n",
    "# Create connection pools to postgres database\n",
    "sync_pool = get_sync_pool(sync_connector, 'postgres')\n",
    "async_pool = await get_async_pool(async_connector, 'postgres')\n",
    "\n",
    "print('‚úì Database connection pools created')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ra1t3rd0q",
   "metadata": {},
   "source": [
    "### Query Helper Functions\n",
    "\n",
    "Create helper functions to execute SQL queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "yllycv60ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_query(query, pool=None):\n",
    "    \"\"\"Execute a synchronous SQL query.\"\"\"\n",
    "    if pool is None:\n",
    "        pool = sync_pool\n",
    "    \n",
    "    with pool.connect().execution_options(isolation_level=\"AUTOCOMMIT\") as connection:\n",
    "        result = connection.execute(query)\n",
    "    \n",
    "    # Prepare response\n",
    "    rows = []\n",
    "    try:\n",
    "        for row in result:\n",
    "            rows.append(dict(zip(result.keys(), row)))\n",
    "    except Exception:\n",
    "        pass\n",
    "    \n",
    "    return rows[0] if len(rows) == 1 else rows\n",
    "\n",
    "async def async_run_query(query, pool=None):\n",
    "    \"\"\"Execute an asynchronous SQL query.\"\"\"\n",
    "    if pool is None:\n",
    "        pool = async_pool\n",
    "    \n",
    "    async with pool.connect() as connection:\n",
    "        result = await connection.execute(query)\n",
    "        await connection.commit()\n",
    "    \n",
    "    # Prepare response\n",
    "    rows = []\n",
    "    try:\n",
    "        for row in result:\n",
    "            rows.append(dict(zip(result.keys(), row)))\n",
    "    except Exception:\n",
    "        pass\n",
    "    \n",
    "    return rows[0] if len(rows) == 1 else rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eixcb6el2p4",
   "metadata": {},
   "source": [
    "### Test Connection\n",
    "\n",
    "Execute a simple query to verify the connection works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1hv3f03ldf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection successful!\n",
      "Current database: postgres\n"
     ]
    }
   ],
   "source": [
    "# Test synchronous query\n",
    "result = run_query(sqlalchemy.text(\"SELECT 'Connection successful!' as status\"))\n",
    "print(result['status'])\n",
    "\n",
    "# Test asynchronous query\n",
    "result_async = await async_run_query(sqlalchemy.text(\"SELECT current_database() as db\"))\n",
    "print(f'Current database: {result_async[\"db\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff657ctt95c",
   "metadata": {},
   "source": [
    "---\n",
    "## Database Setup\n",
    "\n",
    "Create a database and enable the Vertex AI integration extension."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "il0ke03ggv",
   "metadata": {},
   "source": [
    "### Create Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4uycxnov9bg",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Found existing database: frameworks\n"
     ]
    }
   ],
   "source": [
    "# Check if database exists\n",
    "result = run_query(\n",
    "    sqlalchemy.text(f\"SELECT datname FROM pg_database WHERE datname = '{ALLOYDB_DATABASE_NAME}'\")\n",
    ")\n",
    "\n",
    "if not result:\n",
    "    # Create database\n",
    "    run_query(sqlalchemy.text(f'CREATE DATABASE \"{ALLOYDB_DATABASE_NAME}\"'))\n",
    "    print(f'‚úì Created database: {ALLOYDB_DATABASE_NAME}')\n",
    "else:\n",
    "    print(f'‚úì Found existing database: {ALLOYDB_DATABASE_NAME}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vmrohjbh1m",
   "metadata": {},
   "source": [
    "### Reconnect to New Database\n",
    "\n",
    "Close existing connections and reconnect to the newly created database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6f99616y37k",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Connected to database: frameworks\n"
     ]
    }
   ],
   "source": [
    "# Close existing pools\n",
    "sync_pool.dispose()\n",
    "sync_connector.close()\n",
    "await async_pool.dispose()\n",
    "await async_connector.close()\n",
    "\n",
    "# Create new connectors and pools for the new database\n",
    "sync_connector = google.cloud.alloydb.connector.Connector()\n",
    "async_connector = google.cloud.alloydb.connector.AsyncConnector()\n",
    "sync_pool = get_sync_pool(sync_connector, ALLOYDB_DATABASE_NAME)\n",
    "async_pool = await get_async_pool(async_connector, ALLOYDB_DATABASE_NAME)\n",
    "\n",
    "# Verify connection to new database\n",
    "result = run_query(sqlalchemy.text('SELECT current_database() as db'))\n",
    "print(f'‚úì Connected to database: {result[\"db\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "i9yxzjsxj4i",
   "metadata": {},
   "source": [
    "### Enable Vertex AI Integration Extension\n",
    "\n",
    "Enable the `google_ml_integration` extension for SQL-based ML inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5wzbybwv4y",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Enabled google_ml_integration extension\n",
      "Extension status: google_ml_integration\n"
     ]
    }
   ],
   "source": [
    "# Enable the Vertex AI integration extension\n",
    "run_query(sqlalchemy.text('CREATE EXTENSION IF NOT EXISTS google_ml_integration CASCADE'))\n",
    "print('‚úì Enabled google_ml_integration extension')\n",
    "\n",
    "# Verify extension is installed\n",
    "result = run_query(\n",
    "    sqlalchemy.text(\"SELECT extname FROM pg_extension WHERE extname = 'google_ml_integration'\")\n",
    ")\n",
    "print(f'Extension status: {result[\"extname\"] if result else \"Not found\"}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kbfwzvskz6",
   "metadata": {},
   "source": [
    "---\n",
    "## Load Test Data\n",
    "\n",
    "Load transaction data from BigQuery into AlloyDB for inference testing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rud3x17i6al",
   "metadata": {},
   "source": [
    "### Query Test Data from BigQuery\n",
    "\n",
    "Retrieve a sample of transactions for testing predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6knkvl70waw",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Retrieved 1000 test transactions from BigQuery\n",
      "  Columns: 33\n"
     ]
    }
   ],
   "source": [
    "# Query sample data from BigQuery\n",
    "query = f\"\"\"\n",
    "SELECT *\n",
    "FROM `{PROJECT_ID}.{SERIES}.{SERIES}`\n",
    "WHERE splits = 'TEST'\n",
    "LIMIT 1000\n",
    "\"\"\"\n",
    "\n",
    "df = bq.query(query).to_dataframe()\n",
    "print(f'‚úì Retrieved {len(df)} test transactions from BigQuery')\n",
    "print(f'  Columns: {df.shape[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "msn4orfk7ws",
   "metadata": {},
   "source": [
    "### Review Data Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5fbimeklg7r",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "      <th>transaction_id</th>\n",
       "      <th>splits</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>122959.0</td>\n",
       "      <td>-1.327297</td>\n",
       "      <td>0.422904</td>\n",
       "      <td>1.617505</td>\n",
       "      <td>2.291196</td>\n",
       "      <td>2.375055</td>\n",
       "      <td>0.411735</td>\n",
       "      <td>0.213517</td>\n",
       "      <td>0.424743</td>\n",
       "      <td>-1.809624</td>\n",
       "      <td>...</td>\n",
       "      <td>0.192604</td>\n",
       "      <td>0.068281</td>\n",
       "      <td>-0.245725</td>\n",
       "      <td>-0.697654</td>\n",
       "      <td>0.038216</td>\n",
       "      <td>0.150059</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>c97b6e2f-603a-4dbe-9bca-0add881f2084</td>\n",
       "      <td>TEST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>122312.0</td>\n",
       "      <td>-1.988557</td>\n",
       "      <td>-0.720301</td>\n",
       "      <td>0.863204</td>\n",
       "      <td>3.114494</td>\n",
       "      <td>1.847474</td>\n",
       "      <td>0.255881</td>\n",
       "      <td>0.580362</td>\n",
       "      <td>-0.083756</td>\n",
       "      <td>-0.939044</td>\n",
       "      <td>...</td>\n",
       "      <td>1.564951</td>\n",
       "      <td>0.546312</td>\n",
       "      <td>-0.548531</td>\n",
       "      <td>-0.746620</td>\n",
       "      <td>-0.748016</td>\n",
       "      <td>0.410640</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>791e403e-d59f-491d-b0b7-d8f8710c07fb</td>\n",
       "      <td>TEST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>119592.0</td>\n",
       "      <td>2.139741</td>\n",
       "      <td>0.245651</td>\n",
       "      <td>-2.654856</td>\n",
       "      <td>0.178287</td>\n",
       "      <td>1.336991</td>\n",
       "      <td>-0.724664</td>\n",
       "      <td>0.906032</td>\n",
       "      <td>-0.436125</td>\n",
       "      <td>-0.528015</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.216033</td>\n",
       "      <td>0.345316</td>\n",
       "      <td>0.747103</td>\n",
       "      <td>0.700184</td>\n",
       "      <td>-0.123739</td>\n",
       "      <td>-0.099989</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>d9e720c5-311d-4cf7-95cb-2256823803ba</td>\n",
       "      <td>TEST</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows √ó 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Time        V1        V2        V3        V4        V5        V6  \\\n",
       "0  122959.0 -1.327297  0.422904  1.617505  2.291196  2.375055  0.411735   \n",
       "1  122312.0 -1.988557 -0.720301  0.863204  3.114494  1.847474  0.255881   \n",
       "2  119592.0  2.139741  0.245651 -2.654856  0.178287  1.336991 -0.724664   \n",
       "\n",
       "         V7        V8        V9  ...       V23       V24       V25       V26  \\\n",
       "0  0.213517  0.424743 -1.809624  ...  0.192604  0.068281 -0.245725 -0.697654   \n",
       "1  0.580362 -0.083756 -0.939044  ...  1.564951  0.546312 -0.548531 -0.746620   \n",
       "2  0.906032 -0.436125 -0.528015  ... -0.216033  0.345316  0.747103  0.700184   \n",
       "\n",
       "        V27       V28  Amount  Class                        transaction_id  \\\n",
       "0  0.038216  0.150059     0.0      0  c97b6e2f-603a-4dbe-9bca-0add881f2084   \n",
       "1 -0.748016  0.410640     0.0      0  791e403e-d59f-491d-b0b7-d8f8710c07fb   \n",
       "2 -0.123739 -0.099989     0.0      0  d9e720c5-311d-4cf7-95cb-2256823803ba   \n",
       "\n",
       "   splits  \n",
       "0    TEST  \n",
       "1    TEST  \n",
       "2    TEST  \n",
       "\n",
       "[3 rows x 33 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display sample rows\n",
    "print('Sample data:')\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cle3ddp5ak9",
   "metadata": {},
   "source": [
    "### Create Table in AlloyDB\n",
    "\n",
    "Create a table matching the BigQuery schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "186lba0huyl",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating table with schema:\n",
      "\n",
      "CREATE TABLE \"pytorch_autoencoder_alloydb\" (\n",
      "    \"Time\" FLOAT,\n",
      "    \"V1\" FLOAT,\n",
      "    \"V2\" FLOAT,\n",
      "    \"V3\" FLOAT,\n",
      "    \"V4\" FLOAT,\n",
      "    \"V5\" FLOAT,\n",
      "    \"V6\" FLOAT,\n",
      "    \"V7\" FLOAT,\n",
      "    \"V8\" FLOAT,\n",
      "    \"V9\" FLOAT,\n",
      "    \"V10\" FLOAT,\n",
      "    \"V11\" FLOAT,\n",
      "    \"V12\" FLOAT,\n",
      "    \"V13\" FLOAT,\n",
      "    \"V14\" FLOAT,\n",
      "    \"V15\" FLOAT,\n",
      "    \"V16\" FLOAT,\n",
      "    \"V17\" FLOAT,\n",
      "    \"V18\" FLOAT,\n",
      "    \"V19\" FLOAT,\n",
      "    \"V20\" FLOAT,\n",
      "    \"V21\" FLOAT,\n",
      "    \"V22\" FLOAT,\n",
      "    \"V23\" FLOAT,\n",
      "    \"V24\" FLOAT,\n",
      "    \"V25\" FLOAT,\n",
      "    \"V26\" FLOAT,\n",
      "    \"V27\" FLOAT,\n",
      "    \"V28\" FLOAT,\n",
      "    \"Amount\" FLOAT,\n",
      "    \"Class\" INTEGER,\n",
      "    \"transaction_id\" TEXT,\n",
      "    \"splits\" TEXT\n",
      ")\n",
      "\n",
      "\n",
      "‚úì Created table: pytorch_autoencoder_alloydb\n"
     ]
    }
   ],
   "source": [
    "# Drop table if exists (for demo purposes)\n",
    "run_query(sqlalchemy.text(f'DROP TABLE IF EXISTS \"{ALLOYDB_TABLE_NAME}\"'))\n",
    "\n",
    "# Create table dynamically from DataFrame schema\n",
    "# Map pandas dtypes to PostgreSQL types\n",
    "dtype_mapping = {\n",
    "    'int64': 'INTEGER',\n",
    "    'Int64': 'INTEGER',\n",
    "    'float64': 'FLOAT',\n",
    "    'object': 'TEXT',\n",
    "    'bool': 'BOOLEAN',\n",
    "    'datetime64[ns]': 'TIMESTAMP',\n",
    "}\n",
    "\n",
    "columns = []\n",
    "for col, dtype in df.dtypes.items():\n",
    "    pg_type = dtype_mapping.get(str(dtype), 'TEXT')\n",
    "    # Quote column names to preserve case sensitivity\n",
    "    columns.append(f'    \"{col}\" {pg_type}')\n",
    "\n",
    "create_table_sql = f\"\"\"\n",
    "CREATE TABLE \"{ALLOYDB_TABLE_NAME}\" (\n",
    "{',\\n'.join(columns)}\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "print(f'Creating table with schema:')\n",
    "print(create_table_sql)\n",
    "\n",
    "run_query(sqlalchemy.text(create_table_sql))\n",
    "print(f'\\n‚úì Created table: {ALLOYDB_TABLE_NAME}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jv8ceo7zad",
   "metadata": {},
   "source": [
    "### Load Data into AlloyDB\n",
    "\n",
    "Use asynchronous inserts for efficient data loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "i5xy43vstai",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserting 1000 rows...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Loaded 1000 rows into AlloyDB\n"
     ]
    }
   ],
   "source": [
    "# Use pandas to_sql for efficient bulk loading\n",
    "print(f'Inserting {len(df)} rows...')\n",
    "\n",
    "# Create a fresh connection\n",
    "temp_pool = sqlalchemy.create_engine(\n",
    "    \"postgresql+pg8000://\",\n",
    "    creator=get_sync_conn(sync_connector, ALLOYDB_DATABASE_NAME)\n",
    ")\n",
    "\n",
    "try:\n",
    "    # Use pandas to_sql with the temporary connection\n",
    "    # This preserves exact column names and order from BigQuery\n",
    "    df.to_sql(\n",
    "        name=ALLOYDB_TABLE_NAME,\n",
    "        con=temp_pool,\n",
    "        if_exists='append',  # Append to existing table\n",
    "        index=False,\n",
    "        method='multi',  # Use multi-row INSERT statements\n",
    "        chunksize=100  # Insert in batches of 100 rows\n",
    "    )\n",
    "    print(f'‚úì Loaded {len(df)} rows into AlloyDB')\n",
    "except Exception as e:\n",
    "    print(f'Error during insert: {e}')\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "finally:\n",
    "    # Clean up temp pool\n",
    "    temp_pool.dispose()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1kvfq2hht3w",
   "metadata": {},
   "source": [
    "### Verify Data Load\n",
    "\n",
    "Check that data was loaded successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "edmr7yae5r9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row count: 1000\n",
      "\\nSample rows: 3\n"
     ]
    }
   ],
   "source": [
    "# Count rows\n",
    "result = run_query(\n",
    "    sqlalchemy.text(f'SELECT COUNT(*) as count FROM \"{ALLOYDB_TABLE_NAME}\"')\n",
    ")\n",
    "print(f'Row count: {result[\"count\"]}')\n",
    "\n",
    "# Preview data\n",
    "result = run_query(\n",
    "    sqlalchemy.text(f'SELECT * FROM \"{ALLOYDB_TABLE_NAME}\" LIMIT 3')\n",
    ")\n",
    "print(f'\\\\nSample rows: {len(result) if isinstance(result, list) else 1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3fc8mna7t",
   "metadata": {},
   "source": [
    "---\n",
    "## Register Vertex AI Model in AlloyDB\n",
    "\n",
    "Use `google_ml.create_model()` to register the Vertex AI endpoint for SQL-based inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "btr5i9yfl5n",
   "metadata": {},
   "source": [
    "### Create Model Registration\n",
    "\n",
    "Register the endpoint using the `google_ml.create_model()` stored procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "rwk2qewd77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped existing model registration\n",
      "Registering model with Google provider and alloydb_service_agent_iam auth...\n",
      "‚úì Registered Vertex AI endpoint as model: pytorch-autoencoder-endpoint\n",
      "  Endpoint: projects/1026793852137/locations/us-central1/endpoints/5971323405637517312\n",
      "  Prediction URL: https://us-central1-aiplatform.googleapis.com/v1/projects/1026793852137/locations/us-central1/endpoints/5971323405637517312:predict\n",
      "  Provider: google\n",
      "  Auth type: alloydb_service_agent_iam\n"
     ]
    }
   ],
   "source": [
    "# Drop model if exists (for demo purposes)\n",
    "try:\n",
    "    run_query(sqlalchemy.text(f\"CALL google_ml.drop_model('{ENDPOINT_DISPLAY_NAME}')\"))\n",
    "    print('Dropped existing model registration')\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# Register the Vertex AI endpoint as a model WITH EXPLICIT PROVIDER AND AUTHENTICATION\n",
    "# Construct the prediction URL from the endpoint resource name\n",
    "endpoint_id = endpoint.resource_name.split('/')[-1]\n",
    "prediction_url = f'https://{REGION}-aiplatform.googleapis.com/v1/{endpoint.resource_name}:predict'\n",
    "\n",
    "# Must specify model_provider as 'google' to use alloydb_service_agent_iam auth\n",
    "register_model_sql = f\"\"\"\n",
    "CALL google_ml.create_model(\n",
    "    model_id => '{ENDPOINT_DISPLAY_NAME}'::VARCHAR,\n",
    "    model_request_url => '{prediction_url}'::VARCHAR,\n",
    "    model_provider => 'google'::google_ml.model_provider,\n",
    "    model_auth_type => 'alloydb_service_agent_iam'::google_ml.auth_type\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "print(f'Registering model with Google provider and alloydb_service_agent_iam auth...')\n",
    "run_query(sqlalchemy.text(register_model_sql))\n",
    "print(f'‚úì Registered Vertex AI endpoint as model: {ENDPOINT_DISPLAY_NAME}')\n",
    "print(f'  Endpoint: {endpoint.resource_name}')\n",
    "print(f'  Prediction URL: {prediction_url}')\n",
    "print(f'  Provider: google')\n",
    "print(f'  Auth type: alloydb_service_agent_iam')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3zdgdg6228o",
   "metadata": {},
   "source": [
    "### Grant IAM Permissions\n",
    "\n",
    "Grant the AlloyDB service account permission to invoke Vertex AI endpoints. This is required for `google_ml.predict_row()` to authenticate with Vertex AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9s2etxhvuce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Granting Vertex AI permissions to AlloyDB service account...\n",
      "Service account: service-1026793852137@gcp-sa-alloydb.iam.gserviceaccount.com\n",
      "‚úì Granted roles/aiplatform.user to AlloyDB service account\n",
      "  This allows AlloyDB to invoke Vertex AI endpoints\n",
      "\n",
      "Granting permissions to bastion VM service account...\n",
      "Service account: 1026793852137-compute@developer.gserviceaccount.com\n",
      "‚úì Granted roles/aiplatform.user to compute service account\n"
     ]
    }
   ],
   "source": [
    "# Grant AlloyDB service account permission to call Vertex AI\n",
    "# The AlloyDB service account needs aiplatform.user role to invoke endpoints\n",
    "\n",
    "ALLOYDB_SERVICE_ACCOUNT = f'service-{PROJECT_NUMBER}@gcp-sa-alloydb.iam.gserviceaccount.com'\n",
    "\n",
    "print(f'Granting Vertex AI permissions to AlloyDB service account...')\n",
    "print(f'Service account: {ALLOYDB_SERVICE_ACCOUNT}')\n",
    "\n",
    "# Grant the aiplatform.user role at project level\n",
    "grant_command = [\n",
    "    'gcloud', 'projects', 'add-iam-policy-binding', PROJECT_ID,\n",
    "    '--member', f'serviceAccount:{ALLOYDB_SERVICE_ACCOUNT}',\n",
    "    '--role', 'roles/aiplatform.user',\n",
    "    '--condition', 'None'\n",
    "]\n",
    "\n",
    "result = subprocess.run(grant_command, capture_output=True, text=True)\n",
    "\n",
    "if result.returncode == 0:\n",
    "    print('‚úì Granted roles/aiplatform.user to AlloyDB service account')\n",
    "    print('  This allows AlloyDB to invoke Vertex AI endpoints')\n",
    "else:\n",
    "    print(f'Warning: Could not grant IAM role')\n",
    "    print(f'  stdout: {result.stdout}')\n",
    "    print(f'  stderr: {result.stderr}')\n",
    "\n",
    "# ALSO grant to the bastion VM's default compute service account (needed for SSH tunnel setup)\n",
    "if BASTION_NEEDED:\n",
    "    COMPUTE_SERVICE_ACCOUNT = f'{PROJECT_NUMBER}-compute@developer.gserviceaccount.com'\n",
    "    print(f'\\nGranting permissions to bastion VM service account...')\n",
    "    print(f'Service account: {COMPUTE_SERVICE_ACCOUNT}')\n",
    "    \n",
    "    grant_command_compute = [\n",
    "        'gcloud', 'projects', 'add-iam-policy-binding', PROJECT_ID,\n",
    "        '--member', f'serviceAccount:{COMPUTE_SERVICE_ACCOUNT}',\n",
    "        '--role', 'roles/aiplatform.user',\n",
    "        '--condition', 'None'\n",
    "    ]\n",
    "    \n",
    "    result_compute = subprocess.run(grant_command_compute, capture_output=True, text=True)\n",
    "    \n",
    "    if result_compute.returncode == 0:\n",
    "        print('‚úì Granted roles/aiplatform.user to compute service account')\n",
    "    else:\n",
    "        print(f'Warning: Could not grant IAM role to compute SA')\n",
    "        print(f'  stderr: {result_compute.stderr}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "y092dwbilo8",
   "metadata": {},
   "source": [
    "### Verify Model Registration\n",
    "\n",
    "Check that the model was registered successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "kqow295958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model registration details:\n",
      "  model_id: pytorch-autoencoder-endpoint\n",
      "  model_request_url: https://us-central1-aiplatform.googleapis.com/v1/projects/1026793852137/locations/us-central1/endpoints/5971323405637517312:predict\n",
      "  model_provider: google\n",
      "  model_type: generic\n",
      "  model_qualified_name: None\n",
      "  model_auth_type: alloydb_service_agent_iam\n",
      "  model_auth_id: None\n",
      "  header_gen_fn: None\n",
      "  input_transform_fn: None\n",
      "  output_transform_fn: None\n",
      "  input_batch_transform_fn: None\n",
      "  output_batch_transform_fn: None\n"
     ]
    }
   ],
   "source": [
    "# List registered models using the model_info_view\n",
    "result = run_query(\n",
    "    sqlalchemy.text(f\"SELECT * FROM google_ml.model_info_view WHERE model_id = '{ENDPOINT_DISPLAY_NAME}'\")\n",
    ")\n",
    "\n",
    "if result:\n",
    "    print('Model registration details:')\n",
    "    # Handle both single result (dict) and multiple results (list)\n",
    "    model = result if isinstance(result, dict) else result[0]\n",
    "    for key, value in model.items():\n",
    "        # Skip very verbose fields\n",
    "        if key not in ['model_options']:\n",
    "            print(f'  {key}: {value}')\n",
    "else:\n",
    "    print('Model not found in google_ml.model_info_view')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cso1ghcfhl",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking available authentication types...\n",
      "Available auth_type values:\n",
      "  - alloydb_service_agent_iam\n",
      "  - secret_manager\n"
     ]
    }
   ],
   "source": [
    "# Check what authentication types are available\n",
    "print('Checking available authentication types...')\n",
    "auth_types = run_query(sqlalchemy.text(\"\"\"\n",
    "    SELECT e.enumlabel\n",
    "    FROM pg_enum e\n",
    "    JOIN pg_type t ON e.enumtypid = t.oid\n",
    "    JOIN pg_namespace n ON t.typnamespace = n.oid\n",
    "    WHERE n.nspname = 'google_ml' AND t.typname = 'auth_type'\n",
    "    ORDER BY e.enumsortorder\n",
    "\"\"\"))\n",
    "\n",
    "print('Available auth_type values:')\n",
    "for auth_type in auth_types:\n",
    "    print(f\"  - {auth_type['enumlabel']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wfzecas37v",
   "metadata": {},
   "source": [
    "---\n",
    "## SQL-Based ML Inference\n",
    "\n",
    "Make predictions directly from SQL using `google_ml.predict_row()`. This section demonstrates a multi-stage progression from simple predictions to complex business logic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "au2gbe0hgbi",
   "source": "### Reconnect to Database (if needed)\n\nIf you restarted the kernel or the SSH tunnel died, run this cell to re-establish connections before making predictions.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "jay1gra6e0c",
   "source": "# Test if connections are still alive\nprint('Testing database connection...')\ntry:\n    test_result = run_query(sqlalchemy.text('SELECT 1 as test'))\n    print('‚úì Database connection is active')\nexcept Exception as e:\n    print(f'‚ùå Connection failed: {e}')\n    print('\\nReconnecting...')\n    \n    # Close existing pools if they exist\n    try:\n        sync_pool.dispose()\n        sync_connector.close()\n        await async_pool.dispose()\n        await async_connector.close()\n    except Exception:\n        pass\n    \n    # Recreate connectors and pools\n    sync_connector = google.cloud.alloydb.connector.Connector()\n    async_connector = google.cloud.alloydb.connector.AsyncConnector()\n    sync_pool = get_sync_pool(sync_connector, ALLOYDB_DATABASE_NAME)\n    async_pool = await get_async_pool(async_connector, ALLOYDB_DATABASE_NAME)\n    \n    # Test again\n    test_result = run_query(sqlalchemy.text('SELECT 1 as test'))\n    print('‚úì Reconnected successfully')\n\n# Also verify SSH tunnel if using bastion\nif BASTION_NEEDED:\n    if SSH_TUNNEL_PROCESS and SSH_TUNNEL_PROCESS.poll() is None:\n        print('‚úì SSH tunnel is active')\n    else:\n        print('‚ùå SSH tunnel has died')\n        print('   Please re-run the \"Test Connectivity & Setup Bastion\" cell to recreate the tunnel')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "vkbc4wfffv8",
   "metadata": {},
   "source": [
    "### Stage 1: Simple Prediction\n",
    "\n",
    "Make a basic prediction using `google_ml.predict_row()` with a JSON-formatted input array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "34vmhwx6tj8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing database connection...\n",
      "‚úì Database connection is alive: {'test': 1}\n",
      "\n",
      "Testing simple SELECT...\n",
      "‚úì Simple SELECT works: Time=122959.0, Amount=0.0, Class=0\n",
      "\n",
      "Making prediction (this may take 10-30 seconds)...\n",
      "Simple prediction result:\n",
      "  Time: 122959.0\n",
      "  Amount: 0.0\n",
      "  Actual class: 0\n",
      "  Prediction: {\n",
      "  \"predictions\": [\n",
      "    {\n",
      "      \"encoded\": [\n",
      "        0,\n",
      "        0,\n",
      "        0.2779858708381653,\n",
      "        0\n",
      "      ],\n",
      "      \"denormalized_reconstruction\": [\n",
      "        118489.5546875,\n",
      "        -0.3120456039905548,\n",
      "        0.4992930889129639,\n",
      "        -0.1774093806743622,\n",
      "        -0.387037605047226,\n",
      "        0.3680939078330994,\n",
      "        -0.4575689435005188,\n",
      "        0.4135843813419342,\n",
      "        -0.02196572721004486,\n",
      "        -0.1536415815353394,\n",
      "        -0.2177727967500687,\n",
      "        -0.1784695535898209,\n",
      "      ...\n"
     ]
    }
   ],
   "source": [
    "# First, let's test if the connection is still alive\n",
    "print('Testing database connection...')\n",
    "try:\n",
    "    test_result = run_query(sqlalchemy.text('SELECT 1 as test'))\n",
    "    print(f'‚úì Database connection is alive: {test_result}')\n",
    "except Exception as e:\n",
    "    print(f'‚ùå Database connection failed: {e}')\n",
    "    print('\\nThe SSH tunnel may have died. You may need to:')\n",
    "    print('1. Restart the notebook kernel')\n",
    "    print('2. Re-run the bastion setup cells')\n",
    "    raise\n",
    "\n",
    "# Now try a simple query without prediction first\n",
    "print('\\nTesting simple SELECT...')\n",
    "simple_select = f'SELECT \"Time\", \"Amount\", \"Class\" FROM \"{ALLOYDB_TABLE_NAME}\" LIMIT 1'\n",
    "result = run_query(sqlalchemy.text(simple_select))\n",
    "print(f'‚úì Simple SELECT works: Time={result[\"Time\"]}, Amount={result[\"Amount\"]}, Class={result[\"Class\"]}')\n",
    "\n",
    "# Now try the prediction (this might take 10-30 seconds per row due to endpoint latency)\n",
    "print('\\nMaking prediction (this may take 10-30 seconds)...')\n",
    "simple_prediction_sql = f\"\"\"\n",
    "SELECT\n",
    "    \"Time\",\n",
    "    \"Amount\",\n",
    "    \"Class\",\n",
    "    google_ml.predict_row(\n",
    "        '{ENDPOINT_DISPLAY_NAME}',\n",
    "        JSON_BUILD_OBJECT(\n",
    "            'instances', ARRAY[\n",
    "                ARRAY[\n",
    "                    \"Time\", \"V1\", \"V2\", \"V3\", \"V4\", \"V5\", \"V6\", \"V7\", \"V8\", \"V9\", \"V10\",\n",
    "                    \"V11\", \"V12\", \"V13\", \"V14\", \"V15\", \"V16\", \"V17\", \"V18\", \"V19\", \"V20\",\n",
    "                    \"V21\", \"V22\", \"V23\", \"V24\", \"V25\", \"V26\", \"V27\", \"V28\", \"Amount\"\n",
    "                ]\n",
    "            ]\n",
    "        )\n",
    "    ) AS prediction\n",
    "FROM \"{ALLOYDB_TABLE_NAME}\"\n",
    "LIMIT 1\n",
    "\"\"\"\n",
    "\n",
    "result = run_query(sqlalchemy.text(simple_prediction_sql))\n",
    "print('Simple prediction result:')\n",
    "print(f'  Time: {result[\"Time\"]}')\n",
    "print(f'  Amount: {result[\"Amount\"]}')\n",
    "print(f'  Actual class: {result[\"Class\"]}')\n",
    "print(f'  Prediction: {json.dumps(result[\"prediction\"], indent=2)[:500]}...')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dvteplscper",
   "metadata": {},
   "source": [
    "### Stage 2: Extract Specific Fields\n",
    "\n",
    "Extract the anomaly score and encoded representation from the prediction response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9nr45eqidvs",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted fields from 5 predictions:\n",
      "\n",
      "Row 1:\n",
      "  Amount: $0.00\n",
      "  Anomaly score: 150.21\n",
      "  Encoded: [0, 0, 0.2779858708381653, 0]...\n",
      "\n",
      "Row 2:\n",
      "  Amount: $0.00\n",
      "  Anomaly score: 78.62\n",
      "  Encoded: [0, 0, 0.2594718337059021, 0]...\n",
      "\n",
      "Row 3:\n",
      "  Amount: $0.00\n",
      "  Anomaly score: 553.31\n",
      "  Encoded: [0, 0, 0.1517185717821121, 0]...\n"
     ]
    }
   ],
   "source": [
    "# Extract specific fields based on endpoint type\n",
    "if ENDPOINT_TYPE == 'custom':\n",
    "    # Custom container returns: {anomaly_score, encoded}\n",
    "    extract_fields_sql = f\"\"\"\n",
    "    SELECT\n",
    "        \"Time\",\n",
    "        \"Amount\",\n",
    "        \"Class\",\n",
    "        (google_ml.predict_row(\n",
    "            '{ENDPOINT_DISPLAY_NAME}',\n",
    "            JSON_BUILD_OBJECT(\n",
    "                'instances', ARRAY[\n",
    "                    ARRAY[\n",
    "                        \"Time\", \"V1\", \"V2\", \"V3\", \"V4\", \"V5\", \"V6\", \"V7\", \"V8\", \"V9\", \"V10\",\n",
    "                        \"V11\", \"V12\", \"V13\", \"V14\", \"V15\", \"V16\", \"V17\", \"V18\", \"V19\", \"V20\",\n",
    "                        \"V21\", \"V22\", \"V23\", \"V24\", \"V25\", \"V26\", \"V27\", \"V28\", \"Amount\"\n",
    "                    ]\n",
    "                ]\n",
    "            )\n",
    "        )::jsonb->'predictions'->0->>'anomaly_score')::FLOAT AS anomaly_score,\n",
    "        (google_ml.predict_row(\n",
    "            '{ENDPOINT_DISPLAY_NAME}',\n",
    "            JSON_BUILD_OBJECT(\n",
    "                'instances', ARRAY[\n",
    "                    ARRAY[\n",
    "                        \"Time\", \"V1\", \"V2\", \"V3\", \"V4\", \"V5\", \"V6\", \"V7\", \"V8\", \"V9\", \"V10\",\n",
    "                        \"V11\", \"V12\", \"V13\", \"V14\", \"V15\", \"V16\", \"V17\", \"V18\", \"V19\", \"V20\",\n",
    "                        \"V21\", \"V22\", \"V23\", \"V24\", \"V25\", \"V26\", \"V27\", \"V28\", \"Amount\"\n",
    "                    ]\n",
    "                ]\n",
    "            )\n",
    "        )::jsonb->'predictions'->0->'encoded') AS encoded\n",
    "    FROM \"{ALLOYDB_TABLE_NAME}\"\n",
    "    LIMIT 5\n",
    "    \"\"\"\n",
    "else:\n",
    "    # Pre-built container returns: {denormalized_MAE, encoded, ...}\n",
    "    extract_fields_sql = f\"\"\"\n",
    "    SELECT\n",
    "        \"Time\",\n",
    "        \"Amount\",\n",
    "        \"Class\",\n",
    "        (google_ml.predict_row(\n",
    "            '{ENDPOINT_DISPLAY_NAME}',\n",
    "            JSON_BUILD_OBJECT(\n",
    "                'instances', ARRAY[\n",
    "                    ARRAY[\n",
    "                        \"Time\", \"V1\", \"V2\", \"V3\", \"V4\", \"V5\", \"V6\", \"V7\", \"V8\", \"V9\", \"V10\",\n",
    "                        \"V11\", \"V12\", \"V13\", \"V14\", \"V15\", \"V16\", \"V17\", \"V18\", \"V19\", \"V20\",\n",
    "                        \"V21\", \"V22\", \"V23\", \"V24\", \"V25\", \"V26\", \"V27\", \"V28\", \"Amount\"\n",
    "                    ]\n",
    "                ]\n",
    "            )\n",
    "        )::jsonb->'predictions'->0->>'denormalized_MAE')::FLOAT AS anomaly_score,\n",
    "        (google_ml.predict_row(\n",
    "            '{ENDPOINT_DISPLAY_NAME}',\n",
    "            JSON_BUILD_OBJECT(\n",
    "                'instances', ARRAY[\n",
    "                    ARRAY[\n",
    "                        \"Time\", \"V1\", \"V2\", \"V3\", \"V4\", \"V5\", \"V6\", \"V7\", \"V8\", \"V9\", \"V10\",\n",
    "                        \"V11\", \"V12\", \"V13\", \"V14\", \"V15\", \"V16\", \"V17\", \"V18\", \"V19\", \"V20\",\n",
    "                        \"V21\", \"V22\", \"V23\", \"V24\", \"V25\", \"V26\", \"V27\", \"V28\", \"Amount\"\n",
    "                    ]\n",
    "                ]\n",
    "            )\n",
    "        )::jsonb->'predictions'->0->'encoded') AS encoded\n",
    "    FROM \"{ALLOYDB_TABLE_NAME}\"\n",
    "    LIMIT 5\n",
    "    \"\"\"\n",
    "\n",
    "results = run_query(sqlalchemy.text(extract_fields_sql))\n",
    "print(f'Extracted fields from {len(results)} predictions:')\n",
    "for i, result in enumerate(results[:3]):\n",
    "    print(f'\\nRow {i+1}:')\n",
    "    print(f'  Amount: ${result[\"Amount\"]:.2f}')\n",
    "    print(f'  Anomaly score: {result[\"anomaly_score\"]:.2f}')\n",
    "    print(f'  Encoded: {str(result[\"encoded\"])[:50]}...')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4o43w538392",
   "metadata": {},
   "source": [
    "### Stage 3: Apply Business Logic\n",
    "\n",
    "Add CASE statements to classify transactions based on anomaly scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0zohfxbs9q3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Business logic applied to 10 predictions:\n",
      "\n",
      "Top 5 by anomaly score:\n",
      "1. Amount: $0.00 | Score: 2014.06 | Risk: HIGH_RISK | Actual: Legitimate\n",
      "2. Amount: $0.00 | Score: 1831.54 | Risk: HIGH_RISK | Actual: Legitimate\n",
      "3. Amount: $0.00 | Score: 818.97 | Risk: HIGH_RISK | Actual: Legitimate\n",
      "4. Amount: $0.00 | Score: 700.80 | Risk: HIGH_RISK | Actual: Legitimate\n",
      "5. Amount: $0.00 | Score: 553.31 | Risk: HIGH_RISK | Actual: Legitimate\n"
     ]
    }
   ],
   "source": [
    "# Apply business logic based on endpoint type\n",
    "field_name = 'anomaly_score' if ENDPOINT_TYPE == 'custom' else 'denormalized_MAE'\n",
    "\n",
    "business_logic_sql = f\"\"\"\n",
    "WITH predictions AS (\n",
    "    SELECT\n",
    "        \"Time\",\n",
    "        \"Amount\",\n",
    "        \"Class\",\n",
    "        (google_ml.predict_row(\n",
    "            '{ENDPOINT_DISPLAY_NAME}',\n",
    "            JSON_BUILD_OBJECT(\n",
    "                'instances', ARRAY[\n",
    "                    ARRAY[\n",
    "                        \"Time\", \"V1\", \"V2\", \"V3\", \"V4\", \"V5\", \"V6\", \"V7\", \"V8\", \"V9\", \"V10\",\n",
    "                        \"V11\", \"V12\", \"V13\", \"V14\", \"V15\", \"V16\", \"V17\", \"V18\", \"V19\", \"V20\",\n",
    "                        \"V21\", \"V22\", \"V23\", \"V24\", \"V25\", \"V26\", \"V27\", \"V28\", \"Amount\"\n",
    "                    ]\n",
    "                ]\n",
    "            )\n",
    "        )::jsonb->'predictions'->0->>'{field_name}')::FLOAT AS anomaly_score\n",
    "    FROM \"{ALLOYDB_TABLE_NAME}\"\n",
    "    LIMIT 10\n",
    ")\n",
    "SELECT\n",
    "    \"Time\",\n",
    "    \"Amount\",\n",
    "    \"Class\",\n",
    "    anomaly_score,\n",
    "    CASE\n",
    "        WHEN anomaly_score > 200 THEN 'HIGH_RISK'\n",
    "        WHEN anomaly_score > 100 THEN 'MEDIUM_RISK'\n",
    "        ELSE 'NORMAL'\n",
    "    END AS risk_category,\n",
    "    CASE\n",
    "        WHEN \"Class\" = 1 THEN 'Fraud'\n",
    "        ELSE 'Legitimate'\n",
    "    END AS actual_label\n",
    "FROM predictions\n",
    "ORDER BY anomaly_score DESC\n",
    "\"\"\"\n",
    "\n",
    "results = run_query(sqlalchemy.text(business_logic_sql))\n",
    "print(f'Business logic applied to {len(results)} predictions:')\n",
    "print('\\nTop 5 by anomaly score:')\n",
    "for i, result in enumerate(results[:5]):\n",
    "    print(f'{i+1}. Amount: ${result[\"Amount\"]:.2f} | '\n",
    "          f'Score: {result[\"anomaly_score\"]:.2f} | '\n",
    "          f'Risk: {result[\"risk_category\"]} | '\n",
    "          f'Actual: {result[\"actual_label\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oci3ls1dpr",
   "metadata": {},
   "source": [
    "### Stage 4: Batch Scoring with Results Table\n",
    "\n",
    "Create a table to store prediction results for all transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "n2ekyojvzb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating batch predictions table...\n",
      "\n",
      "‚úì Created predictions table: pytorch_autoencoder_alloydb_predictions\n",
      "  Total predictions: 100\n",
      "  High risk: 77\n",
      "  Medium risk: 9\n",
      "  Normal: 14\n",
      "  Actual fraud cases: 3\n"
     ]
    }
   ],
   "source": [
    "# Create results table\n",
    "results_table_name = f'{ALLOYDB_TABLE_NAME}_predictions'\n",
    "\n",
    "run_query(sqlalchemy.text(f'DROP TABLE IF EXISTS \"{results_table_name}\"'))\n",
    "\n",
    "field_name = 'anomaly_score' if ENDPOINT_TYPE == 'custom' else 'denormalized_MAE'\n",
    "\n",
    "# Create table from prediction query\n",
    "batch_scoring_sql = f\"\"\"\n",
    "CREATE TABLE \"{results_table_name}\" AS\n",
    "WITH predictions AS (\n",
    "    SELECT\n",
    "        \"Time\",\n",
    "        \"Amount\",\n",
    "        \"Class\",\n",
    "        (google_ml.predict_row(\n",
    "            '{ENDPOINT_DISPLAY_NAME}',\n",
    "            JSON_BUILD_OBJECT(\n",
    "                'instances', ARRAY[\n",
    "                    ARRAY[\n",
    "                        \"Time\", \"V1\", \"V2\", \"V3\", \"V4\", \"V5\", \"V6\", \"V7\", \"V8\", \"V9\", \"V10\",\n",
    "                        \"V11\", \"V12\", \"V13\", \"V14\", \"V15\", \"V16\", \"V17\", \"V18\", \"V19\", \"V20\",\n",
    "                        \"V21\", \"V22\", \"V23\", \"V24\", \"V25\", \"V26\", \"V27\", \"V28\", \"Amount\"\n",
    "                    ]\n",
    "                ]\n",
    "            )\n",
    "        )::jsonb->'predictions'->0->>'{field_name}')::FLOAT AS anomaly_score\n",
    "    FROM \"{ALLOYDB_TABLE_NAME}\"\n",
    "    LIMIT 100\n",
    ")\n",
    "SELECT\n",
    "    \"Time\",\n",
    "    \"Amount\",\n",
    "    \"Class\",\n",
    "    anomaly_score,\n",
    "    CASE\n",
    "        WHEN anomaly_score > 200 THEN 'HIGH_RISK'\n",
    "        WHEN anomaly_score > 100 THEN 'MEDIUM_RISK'\n",
    "        ELSE 'NORMAL'\n",
    "    END AS risk_category,\n",
    "    CASE\n",
    "        WHEN \"Class\" = 1 THEN 'Fraud'\n",
    "        ELSE 'Legitimate'\n",
    "    END AS actual_label\n",
    "FROM predictions\n",
    "\"\"\"\n",
    "\n",
    "print('Creating batch predictions table...')\n",
    "run_query(sqlalchemy.text(batch_scoring_sql))\n",
    "\n",
    "# Get statistics\n",
    "stats = run_query(sqlalchemy.text(f\"\"\"\n",
    "    SELECT\n",
    "        COUNT(*) as total_predictions,\n",
    "        SUM(CASE WHEN risk_category = 'HIGH_RISK' THEN 1 ELSE 0 END) as high_risk_count,\n",
    "        SUM(CASE WHEN risk_category = 'MEDIUM_RISK' THEN 1 ELSE 0 END) as medium_risk_count,\n",
    "        SUM(CASE WHEN risk_category = 'NORMAL' THEN 1 ELSE 0 END) as normal_count,\n",
    "        SUM(CASE WHEN \"Class\" = 1 THEN 1 ELSE 0 END) as actual_fraud_count\n",
    "    FROM \"{results_table_name}\"\n",
    "\"\"\"))\n",
    "\n",
    "print(f'\\n‚úì Created predictions table: {results_table_name}')\n",
    "print(f'  Total predictions: {stats[\"total_predictions\"]}')\n",
    "print(f'  High risk: {stats[\"high_risk_count\"]}')\n",
    "print(f'  Medium risk: {stats[\"medium_risk_count\"]}')\n",
    "print(f'  Normal: {stats[\"normal_count\"]}')\n",
    "print(f'  Actual fraud cases: {stats[\"actual_fraud_count\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71zdiwk2sl8",
   "metadata": {},
   "source": [
    "---\n",
    "## Cleanup\n",
    "\n",
    "Remove all AlloyDB resources to avoid ongoing charges. This will:\n",
    "1. Close SSH tunnel (if bastion was used)\n",
    "2. Delete bastion VM (if created)\n",
    "3. Drop the model registration\n",
    "4. Drop tables\n",
    "5. Delete the database\n",
    "6. Delete the AlloyDB instance\n",
    "7. Delete the AlloyDB cluster\n",
    "\n",
    "**Warning**: This will permanently delete all data. Make sure you've saved any results you need."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4k4mgqhjk0y",
   "metadata": {},
   "source": [
    "### Enable Cleanup\n",
    "\n",
    "**IMPORTANT:** Cleanup is disabled by default to prevent accidental deletion.\n",
    "\n",
    "To enable cleanup and delete all resources, change `ENABLE_CLEANUP` to `True` in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ok5hj1jpz3s",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è  CLEANUP IS DISABLED\n",
      "   To delete resources, change ENABLE_CLEANUP to True above and re-run this cell.\n",
      "   Then proceed with the cleanup cells below.\n"
     ]
    }
   ],
   "source": [
    "# Change to True to delete all resources\n",
    "ENABLE_CLEANUP = False\n",
    "\n",
    "# Safety check - do not modify below this line\n",
    "if not ENABLE_CLEANUP:\n",
    "    print('‚ö†Ô∏è  CLEANUP IS DISABLED')\n",
    "    print('   To delete resources, change ENABLE_CLEANUP to True above and re-run this cell.')\n",
    "    print('   Then proceed with the cleanup cells below.')\n",
    "else:\n",
    "    print('‚úì CLEANUP IS ENABLED')\n",
    "    print('  All cleanup cells will execute and delete resources.')\n",
    "    print('  Estimated resources to be deleted:')\n",
    "    if BASTION_NEEDED and BASTION_VM_NAME:\n",
    "        print(f'    - Bastion VM: {BASTION_VM_NAME}')\n",
    "    print(f'    - AlloyDB Instance: {ALLOYDB_INSTANCE_NAME}')\n",
    "    print(f'    - AlloyDB Cluster: {ALLOYDB_CLUSTER_NAME}')\n",
    "    print(f'    - Database: {ALLOYDB_DATABASE_NAME}')\n",
    "    print(f'    - Tables: {ALLOYDB_TABLE_NAME}, {ALLOYDB_TABLE_NAME}_predictions')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cleanup_bastion_header",
   "metadata": {},
   "source": [
    "### Close SSH Tunnel and Delete Bastion VM\n",
    "\n",
    "If a bastion VM was created for connectivity, clean it up first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "cleanup_bastion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è  Skipping: Cleanup is disabled. Set ENABLE_CLEANUP = True to proceed.\n"
     ]
    }
   ],
   "source": [
    "if not ENABLE_CLEANUP:\n",
    "    print('‚ö†Ô∏è  Skipping: Cleanup is disabled. Set ENABLE_CLEANUP = True to proceed.')\n",
    "elif BASTION_NEEDED and BASTION_VM_NAME:\n",
    "    # Close SSH tunnel\n",
    "    if SSH_TUNNEL_PROCESS and SSH_TUNNEL_PROCESS.poll() is None:\n",
    "        print('Closing SSH tunnel...')\n",
    "        SSH_TUNNEL_PROCESS.terminate()\n",
    "        try:\n",
    "            SSH_TUNNEL_PROCESS.wait(timeout=10)\n",
    "            print('‚úì SSH tunnel closed')\n",
    "        except subprocess.TimeoutExpired:\n",
    "            SSH_TUNNEL_PROCESS.kill()\n",
    "            print('‚úì SSH tunnel killed')\n",
    "    \n",
    "    # Delete bastion VM\n",
    "    try:\n",
    "        from google.cloud import compute_v1\n",
    "        compute_client = compute_v1.InstancesClient()\n",
    "        \n",
    "        print(f'Deleting bastion VM: {BASTION_VM_NAME}...')\n",
    "        operation = compute_client.delete(\n",
    "            project=PROJECT_ID,\n",
    "            zone=BASTION_ZONE,\n",
    "            instance=BASTION_VM_NAME\n",
    "        )\n",
    "        operation.result()\n",
    "        print(f'‚úì Deleted bastion VM: {BASTION_VM_NAME}')\n",
    "    except Exception as e:\n",
    "        print(f'Bastion VM deletion skipped: {e}')\n",
    "else:\n",
    "    print('No bastion VM to clean up')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "meywbhj43u",
   "metadata": {},
   "source": [
    "### Drop Model and Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "prlxspjfmm",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è  Skipping: Cleanup is disabled. Set ENABLE_CLEANUP = True to proceed.\n"
     ]
    }
   ],
   "source": [
    "if not ENABLE_CLEANUP:\n",
    "    print('‚ö†Ô∏è  Skipping: Cleanup is disabled. Set ENABLE_CLEANUP = True to proceed.')\n",
    "else:\n",
    "    # Drop model registration\n",
    "    try:\n",
    "        run_query(sqlalchemy.text(f\"CALL google_ml.drop_model('{ENDPOINT_DISPLAY_NAME}')\"))\n",
    "        print(f'‚úì Dropped model: {ENDPOINT_DISPLAY_NAME}')\n",
    "    except Exception as e:\n",
    "        print(f'Model drop skipped: {e}')\n",
    "\n",
    "    # Drop tables\n",
    "    for table in [ALLOYDB_TABLE_NAME, f'{ALLOYDB_TABLE_NAME}_predictions']:\n",
    "        try:\n",
    "            run_query(sqlalchemy.text(f'DROP TABLE IF EXISTS \"{table}\"'))\n",
    "            print(f'‚úì Dropped table: {table}')\n",
    "        except Exception as e:\n",
    "            print(f'Table drop skipped: {e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rr4jz91ka0c",
   "metadata": {},
   "source": [
    "### Close Database Connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3063x31o2ng",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è  Skipping: Cleanup is disabled. Set ENABLE_CLEANUP = True to proceed.\n"
     ]
    }
   ],
   "source": [
    "if not ENABLE_CLEANUP:\n",
    "    print('‚ö†Ô∏è  Skipping: Cleanup is disabled. Set ENABLE_CLEANUP = True to proceed.')\n",
    "else:\n",
    "    # Close connection pools and connectors\n",
    "    sync_pool.dispose()\n",
    "    sync_connector.close()\n",
    "    await async_pool.dispose()\n",
    "    await async_connector.close()\n",
    "\n",
    "    print('‚úì Closed database connections')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pjzgdzix3x",
   "metadata": {},
   "source": [
    "### Delete AlloyDB Instance\n",
    "\n",
    "Delete the AlloyDB instance (this may take 5-10 minutes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "hhoaxdszss",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è  Skipping: Cleanup is disabled. Set ENABLE_CLEANUP = True to proceed.\n"
     ]
    }
   ],
   "source": [
    "if not ENABLE_CLEANUP:\n",
    "    print('‚ö†Ô∏è  Skipping: Cleanup is disabled. Set ENABLE_CLEANUP = True to proceed.')\n",
    "else:\n",
    "    print(f'Deleting AlloyDB instance: {ALLOYDB_INSTANCE_NAME}')\n",
    "    print('This may take 5-10 minutes...')\n",
    "\n",
    "    delete_instance = alloydb_client.delete_instance(\n",
    "        name=alloydb_instance.name\n",
    "    )\n",
    "    delete_instance.result()\n",
    "\n",
    "    print(f'‚úì Deleted instance: {ALLOYDB_INSTANCE_NAME}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tjzhres2r5q",
   "metadata": {},
   "source": [
    "### Delete AlloyDB Cluster\n",
    "\n",
    "Delete the AlloyDB cluster (this may take 5-10 minutes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "x0btj2op4vg",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è  Skipping: Cleanup is disabled. Set ENABLE_CLEANUP = True to proceed.\n"
     ]
    }
   ],
   "source": [
    "if not ENABLE_CLEANUP:\n",
    "    print('‚ö†Ô∏è  Skipping: Cleanup is disabled. Set ENABLE_CLEANUP = True to proceed.')\n",
    "else:\n",
    "    print(f'Deleting AlloyDB cluster: {ALLOYDB_CLUSTER_NAME}')\n",
    "    print('This may take 5-10 minutes...')\n",
    "\n",
    "    delete_cluster = alloydb_client.delete_cluster(\n",
    "        request=dict(\n",
    "            name=alloydb_cluster.name,\n",
    "            force=True  # Force delete even if backups exist\n",
    "        )\n",
    "    )\n",
    "    delete_cluster.result()\n",
    "\n",
    "    print(f'‚úì Deleted cluster: {ALLOYDB_CLUSTER_NAME}')\n",
    "    print('\\nAll AlloyDB resources have been deleted!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d917cbe2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (frameworks-pytorch)",
   "language": "python",
   "name": "frameworks-pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}