{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "![tracker](https://us-central1-vertex-ai-mlops-369716.cloudfunctions.net/pixel-tracking?path=statmike%2Fvertex-ai-mlops%2FFramework+Workflows%2FPyTorch%2Fserving&file=dataflow-cleanup.ipynb)\n",
    "<!--- header table --->\n",
    "<table align=\"left\">\n",
    "<tr>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%20Workflows/PyTorch/serving/dataflow-cleanup.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.svgrepo.com/download/217753/github.svg\" alt=\"GitHub logo\">\n",
    "      <br>View on<br>GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://colab.research.google.com/github/statmike/vertex-ai-mlops/blob/main/Framework%20Workflows/PyTorch/serving/dataflow-cleanup.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.gstatic.com/pantheon/images/bigquery/welcome_page/colab-logo.svg\" alt=\"Google Colaboratory logo\">\n",
    "      <br>Run in<br>Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https%3A%2F%2Fraw.githubusercontent.com%2Fstatmike%2Fvertex-ai-mlops%2Fmain%2FFramework%2520Workflows%2FPyTorch%2Fserving%2Fdataflow-cleanup.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\">\n",
    "      <br>Run in<br>Colab Enterprise\n",
    "    </a>\n",
    "  </td>      \n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/bigquery/import?url=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%20Workflows/PyTorch/serving/dataflow-cleanup.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.gstatic.com/images/branding/gcpiconscolors/bigquery/v1/32px.svg\" alt=\"BigQuery logo\">\n",
    "      <br>Open in<br>BigQuery Studio\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/statmike/vertex-ai-mlops/main/Framework%20Workflows/PyTorch/serving/dataflow-cleanup.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.gstatic.com/images/branding/gcpiconscolors/vertexai/v1/32px.svg\" alt=\"Vertex AI logo\">\n",
    "      <br>Open in<br>Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td colspan=\"5\" style=\"text-align: right\">\n",
    "    <b>Share This On: </b> \n",
    "    <a href=\"https://www.linkedin.com/sharing/share-offsite/?url=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%2520Workflows/PyTorch/serving/dataflow-cleanup.ipynb\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"Linkedin Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://reddit.com/submit?url=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%2520Workflows/PyTorch/serving/dataflow-cleanup.ipynb\"><img src=\"https://redditinc.com/hubfs/Reddit%20Inc/Brand/Reddit_Logo.png\" alt=\"Reddit Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://bsky.app/intent/compose?text=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%2520Workflows/PyTorch/serving/dataflow-cleanup.ipynb\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"BlueSky Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://twitter.com/intent/tweet?url=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%2520Workflows/PyTorch/serving/dataflow-cleanup.ipynb\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/5/5a/X_icon_2.svg\" alt=\"X (Twitter) Logo\" width=\"20px\"></a> \n",
    "  </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td colspan=\"5\" style=\"text-align: right\">\n",
    "    <b>Connect With Author On: </b> \n",
    "    <a href=\"https://www.linkedin.com/in/statmike\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"Linkedin Logo\" width=\"20px\"></a>\n",
    "    <a href=\"https://www.github.com/statmike\"><img src=\"https://www.svgrepo.com/download/217753/github.svg\" alt=\"GitHub Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://www.youtube.com/@statmike-channel\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/f/fd/YouTube_full-color_icon_%282024%29.svg\" alt=\"YouTube Logo\" width=\"20px\"></a>\n",
    "    <a href=\"https://bsky.app/profile/statmike.bsky.social\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"BlueSky Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://x.com/statmike\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/5/5a/X_icon_2.svg\" alt=\"X (Twitter) Logo\" width=\"20px\"></a>\n",
    "  </td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overview",
   "metadata": {},
   "source": "# Dataflow Infrastructure Cleanup\n\nThis notebook provides centralized cleanup for all Dataflow inference workflows.\n\n## What This Notebook Does\n\nSafely cleans up resources created by Dataflow notebooks:\n\n1. **Stop Dataflow Jobs**: Cancel running streaming/batch jobs\n2. **Clean BigQuery Tables**: Truncate or delete result tables\n3. **Clean Pub/Sub Resources**: Delete topics and subscriptions\n4. **Clean GCS Files**: Delete model files uploaded for Dataflow\n\n## Safety Features\n\n- **Confirmation prompts**: Review what will be deleted before proceeding\n- **Safety checks**: Warns if deleting resources with active jobs\n- **Granular control**: Choose exactly what to clean up\n- **Flexible options**: Truncate tables without deleting schema\n\n## When to Use This Notebook\n\n**After testing:**\n- Stop streaming jobs that are still running\n- Clear test data from BigQuery tables\n- Remove temporary Pub/Sub topics\n\n**Before re-running workflows:**\n- Truncate BigQuery tables to start fresh\n- Ensure no old jobs are consuming resources\n\n**Complete teardown:**\n- Delete all resources to avoid ongoing costs\n- Prepare for fresh setup via dataflow-setup.ipynb\n\n## Resources Managed\n\n**Dataflow Jobs:**\n- `pytorch-streaming-*` (local model streaming jobs)\n- `pytorch-streaming-vertex-*` (vertex endpoint streaming jobs)\n- `pytorch-batch-*` (local model batch jobs)\n- `pytorch-batch-vertex-*` (vertex endpoint batch jobs)\n\n**BigQuery Tables:**\n- `pytorch_autoencoder_batch_results` (local model batch)\n- `pytorch_autoencoder_streaming_results` (local model streaming)\n- `pytorch_autoencoder_batch_results_vertex` (vertex endpoint batch)\n- `pytorch_autoencoder_streaming_results_vertex` (vertex endpoint streaming)\n\n**Pub/Sub Resources:**\n- Input topics (completely isolated):\n  - `pytorch-autoencoder-input-local` (local model pipeline)\n  - `pytorch-autoencoder-input-vertex` (vertex endpoint pipeline)\n- Output topics (completely isolated):\n  - `pytorch-autoencoder-output-local` (local model pipeline)\n  - `pytorch-autoencoder-output-vertex` (vertex endpoint pipeline)\n- Subscriptions:\n  - `pytorch-autoencoder-input-sub-local` (local model input)\n  - `pytorch-autoencoder-input-sub-vertex` (vertex endpoint input)\n  - `pytorch-autoencoder-output-sub-local` (local model output)\n  - `pytorch-autoencoder-output-sub-vertex` (vertex endpoint output)\n\n**GCS Files:**\n- `gs://{PROJECT_ID}/frameworks/pytorch-autoencoder/dataflow/`"
  },
  {
   "cell_type": "markdown",
   "id": "45352812",
   "metadata": {},
   "source": [
    "---\n",
    "## Environment Setup\n",
    "\n",
    "This section will authenticate your session, enable required Google Cloud APIs, and install necessary Python packages.\n",
    "\n",
    "**Package Installation Options (`REQ_TYPE`):**\n",
    "- `PRIMARY`: Installs only the main packages\n",
    "- `ALL` (Default): Installs exact versions of all packages and dependencies\n",
    "- `COLAB`: Installs a Colab-optimized list\n",
    "\n",
    "**Installation Tool Options (`INSTALL_TOOL`):**\n",
    "- `pip` (Default): Standard Python package installer\n",
    "- `uv`: Modern, fast Python package installer\n",
    "- `poetry`: Dependency management tool\n",
    "\n",
    "> **Note:** If running in Google Colab, the script will automatically set `REQ_TYPE = 'COLAB' to prevent package conflicts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0081ca",
   "metadata": {},
   "source": [
    "### Set Your Project ID\n",
    "\n",
    "⚠️ **Action Required:** Replace the `PROJECT_ID` value below with your Google Cloud project ID before running this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89aa0e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = 'statmike-mlops-349915' # replace with GCP project ID\n",
    "REQ_TYPE = 'ALL' # Specify PRIMARY or ALL or COLAB\n",
    "INSTALL_TOOL = 'poetry' # Specify pip, uv, or poetry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac172d3",
   "metadata": {},
   "source": [
    "### Configuration\n",
    "\n",
    "This cell defines the requirements files and Google Cloud APIs needed for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85ad481d",
   "metadata": {},
   "outputs": [],
   "source": [
    "REQUIREMENTS_URL = 'https://raw.githubusercontent.com/statmike/vertex-ai-mlops/refs/heads/main/Framework%20Workflows/PyTorch/requirements.txt'\n",
    "\n",
    "REQUIRED_APIS = [\n",
    "    \"dataflow.googleapis.com\",\n",
    "    \"bigquery.googleapis.com\",\n",
    "    \"pubsub.googleapis.com\",\n",
    "    \"storage.googleapis.com\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f55c22",
   "metadata": {},
   "source": [
    "### Run Setup\n",
    "\n",
    "This cell downloads the centralized setup code and configures your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f86b053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "PYTHON GCP ENVIRONMENT SETUP\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "AUTHENTICATION\n",
      "==================================================\n",
      "Checking for existing ADC...\n",
      "✅ Existing ADC found.\n",
      "✅ Project is correctly set to 'statmike-mlops-349915'.\n",
      "\n",
      "==================================================\n",
      "API CHECK & ENABLE\n",
      "==================================================\n",
      "✅ dataflow.googleapis.com is already enabled.\n",
      "✅ bigquery.googleapis.com is already enabled.\n",
      "✅ pubsub.googleapis.com is already enabled.\n",
      "✅ storage.googleapis.com is already enabled.\n",
      "\n",
      "==================================================\n",
      "PACKAGE MANAGEMENT\n",
      "==================================================\n",
      "Installation Tool: poetry\n",
      "✅ Found poetry at: /usr/local/google/home/statmike/.local/bin/poetry\n",
      "✅ Running in poetry environment: /usr/local/google/home/statmike/.cache/pypoetry/virtualenvs/frameworks-pytorch-0KVJlKeQ-py3.13\n",
      "ℹ️  Poetry mode: Installing from pyproject.toml (REQUIREMENTS_URL ignored)\n",
      "✅ Found pyproject.toml at: /usr/local/google/home/statmike/Git/vertex-ai-mlops/Framework Workflows/PyTorch/pyproject.toml\n",
      "   Changed working directory to: /usr/local/google/home/statmike/Git/vertex-ai-mlops/Framework Workflows/PyTorch\n",
      "Running poetry install...\n",
      "   Restored working directory to: /usr/local/google/home/statmike/Git/vertex-ai-mlops/Framework Workflows/PyTorch/serving\n",
      "✅ All packages are already installed and up to date.\n",
      "\n",
      "==================================================\n",
      "Google Cloud Project Information\n",
      "==================================================\n",
      "PROJECT_ID     = statmike-mlops-349915\n",
      "PROJECT_NUMBER = 1026793852137\n",
      "==================================================\n",
      "\n",
      "\n",
      "==================================================\n",
      "SETUP SUMMARY\n",
      "==================================================\n",
      "✅ Authentication:    Success\n",
      "✅ API Configuration: Success\n",
      "✅ Package Install:   Already up to date\n",
      "✅ Installation Tool: poetry\n",
      "✅ Project ID:        statmike-mlops-349915\n",
      "✅ Project Number:    1026793852137\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os, urllib.request\n",
    "\n",
    "# Download and import setup code\n",
    "url = 'https://raw.githubusercontent.com/statmike/vertex-ai-mlops/refs/heads/main/core/notebook-template/python_setup.py'\n",
    "urllib.request.urlretrieve(url, 'python_setup_local.py')\n",
    "import python_setup_local as python_setup\n",
    "os.remove('python_setup_local.py')\n",
    "\n",
    "# Run setup\n",
    "setup_info = python_setup.setup_environment(PROJECT_ID, REQ_TYPE, REQUIREMENTS_URL, REQUIRED_APIS, INSTALL_TOOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "py",
   "metadata": {},
   "source": [
    "---\n",
    "## Python Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import storage\n",
    "from google.cloud import pubsub_v1\n",
    "from google.cloud import dataflow_v1beta3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "vars",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project: statmike-mlops-349915\n",
      "Region: us-central1\n",
      "GCS Dataflow Directory: gs://statmike-mlops-349915/frameworks/pytorch-autoencoder/dataflow\n",
      "BigQuery Dataset: frameworks\n"
     ]
    }
   ],
   "source": [
    "PROJECT_ID = subprocess.run([\"gcloud\", \"config\", \"get-value\", \"project\"], capture_output=True, text=True, check=True).stdout.strip()\n",
    "\n",
    "# Configuration\n",
    "REGION = \"us-central1\"\n",
    "SERIES = \"frameworks\"\n",
    "EXPERIMENT = \"pytorch-autoencoder\"\n",
    "\n",
    "# GCS configuration\n",
    "BUCKET_NAME = PROJECT_ID\n",
    "BUCKET_URI = f\"gs://{BUCKET_NAME}\"\n",
    "GCS_DATAFLOW_DIR = f\"{BUCKET_URI}/{SERIES}/{EXPERIMENT}/dataflow\"\n",
    "\n",
    "# BigQuery configuration\n",
    "BQ_DATASET = SERIES.replace(\"-\", \"_\")\n",
    "\n",
    "print(f\"Project: {PROJECT_ID}\")\n",
    "print(f\"Region: {REGION}\")\n",
    "print(f\"GCS Dataflow Directory: {GCS_DATAFLOW_DIR}\")\n",
    "print(f\"BigQuery Dataset: {BQ_DATASET}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config_header",
   "metadata": {},
   "source": [
    "---\n",
    "## Cleanup Configuration\n",
    "\n",
    "Set flags to control what gets cleaned up. Each section can be enabled/disabled independently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "config",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CLEANUP CONFIGURATION\n",
      "============================================================\n",
      "\n",
      "Dataflow Jobs:\n",
      "  Stop streaming jobs: True\n",
      "  Stop batch jobs: False\n",
      "  Stop local model jobs: True\n",
      "  Stop vertex endpoint jobs: True\n",
      "\n",
      "BigQuery Tables:\n",
      "  Delete tables: True\n",
      "  Truncate tables: True\n",
      "  Clean local model tables: True\n",
      "  Clean vertex endpoint tables: True\n",
      "  Clean batch tables: True\n",
      "  Clean streaming tables: True\n",
      "\n",
      "Pub/Sub:\n",
      "  Delete topics: True\n",
      "  Delete subscriptions: True\n",
      "\n",
      "GCS:\n",
      "  Delete files: True\n",
      "\n",
      "Confirmation required: True\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# DATAFLOW JOBS CLEANUP\n",
    "# ========================================\n",
    "# Stop running Dataflow jobs to avoid ongoing costs\n",
    "\n",
    "STOP_STREAMING_JOBS = True      # Stop streaming jobs (run continuously until cancelled)\n",
    "STOP_BATCH_JOBS = False         # Stop batch jobs (usually complete automatically)\n",
    "\n",
    "# Job filtering options\n",
    "STOP_LOCAL_MODEL_JOBS = True    # Jobs with local PyTorch model (pytorch-streaming, pytorch-batch)\n",
    "STOP_VERTEX_ENDPOINT_JOBS = True # Jobs calling Vertex AI endpoints (pytorch-streaming-vertex, pytorch-batch-vertex)\n",
    "\n",
    "# ========================================\n",
    "# BIGQUERY TABLES CLEANUP\n",
    "# ========================================\n",
    "# Manage result tables created by dataflow-setup.ipynb\n",
    "\n",
    "DELETE_BIGQUERY_TABLES = True  # Completely delete tables (removes schema)\n",
    "TRUNCATE_BIGQUERY_TABLES = True # Empty tables but keep schema (recommended for testing)\n",
    "\n",
    "# Table filtering options\n",
    "CLEAN_LOCAL_MODEL_TABLES = True # Tables for local model results\n",
    "CLEAN_VERTEX_ENDPOINT_TABLES = True # Tables for Vertex endpoint results\n",
    "CLEAN_BATCH_TABLES = True       # Batch result tables\n",
    "CLEAN_STREAMING_TABLES = True   # Streaming result tables\n",
    "\n",
    "# ========================================\n",
    "# PUB/SUB CLEANUP\n",
    "# ========================================\n",
    "# Remove Pub/Sub topics and subscriptions\n",
    "\n",
    "DELETE_PUBSUB_TOPICS = True       # Delete topics (removes message queues)\n",
    "DELETE_PUBSUB_SUBSCRIPTIONS = True  # Delete subscriptions (removes subscribers)\n",
    "\n",
    "# ========================================\n",
    "# GCS FILES CLEANUP\n",
    "# ========================================\n",
    "# Remove model files uploaded for Dataflow\n",
    "\n",
    "DELETE_GCS_FILES = True        # Delete all files in dataflow directory\n",
    "\n",
    "# ========================================\n",
    "# CONFIRMATION SETTINGS\n",
    "# ========================================\n",
    "\n",
    "REQUIRE_CONFIRMATION = True     # Show what will be deleted and ask for confirmation\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"CLEANUP CONFIGURATION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nDataflow Jobs:\")\n",
    "print(f\"  Stop streaming jobs: {STOP_STREAMING_JOBS}\")\n",
    "print(f\"  Stop batch jobs: {STOP_BATCH_JOBS}\")\n",
    "print(f\"  Stop local model jobs: {STOP_LOCAL_MODEL_JOBS}\")\n",
    "print(f\"  Stop vertex endpoint jobs: {STOP_VERTEX_ENDPOINT_JOBS}\")\n",
    "print(f\"\\nBigQuery Tables:\")\n",
    "print(f\"  Delete tables: {DELETE_BIGQUERY_TABLES}\")\n",
    "print(f\"  Truncate tables: {TRUNCATE_BIGQUERY_TABLES}\")\n",
    "print(f\"  Clean local model tables: {CLEAN_LOCAL_MODEL_TABLES}\")\n",
    "print(f\"  Clean vertex endpoint tables: {CLEAN_VERTEX_ENDPOINT_TABLES}\")\n",
    "print(f\"  Clean batch tables: {CLEAN_BATCH_TABLES}\")\n",
    "print(f\"  Clean streaming tables: {CLEAN_STREAMING_TABLES}\")\n",
    "print(f\"\\nPub/Sub:\")\n",
    "print(f\"  Delete topics: {DELETE_PUBSUB_TOPICS}\")\n",
    "print(f\"  Delete subscriptions: {DELETE_PUBSUB_SUBSCRIPTIONS}\")\n",
    "print(f\"\\nGCS:\")\n",
    "print(f\"  Delete files: {DELETE_GCS_FILES}\")\n",
    "print(f\"\\nConfirmation required: {REQUIRE_CONFIRMATION}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "init_clients",
   "metadata": {},
   "source": [
    "---\n",
    "## Initialize Clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "clients",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Clients initialized\n"
     ]
    }
   ],
   "source": [
    "# Initialize clients\n",
    "dataflow_client = dataflow_v1beta3.JobsV1Beta3Client()\n",
    "bq_client = bigquery.Client(project=PROJECT_ID)\n",
    "storage_client = storage.Client(project=PROJECT_ID)\n",
    "pubsub_publisher = pubsub_v1.PublisherClient()\n",
    "pubsub_subscriber = pubsub_v1.SubscriberClient()\n",
    "\n",
    "print(\"✅ Clients initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "discover",
   "metadata": {},
   "source": [
    "---\n",
    "## Discover Resources\n",
    "\n",
    "Scan for all resources that match cleanup criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "discover_code",
   "metadata": {},
   "outputs": [],
   "source": "print(\"=\" * 60)\nprint(\"DISCOVERING RESOURCES\")\nprint(\"=\" * 60)\n\n# ========================================\n# Discover Dataflow Jobs\n# ========================================\nprint(\"\\n1. Dataflow Jobs:\")\nprint(\"-\" * 60)\n\nrequest = dataflow_v1beta3.ListJobsRequest(\n    project_id=PROJECT_ID,\n    location=REGION\n)\n\nall_jobs = list(dataflow_client.list_jobs(request=request))\n\n# Filter active jobs\nactive_jobs = [\n    job for job in all_jobs \n    if job.current_state in [\n        dataflow_v1beta3.JobState.JOB_STATE_RUNNING,\n        dataflow_v1beta3.JobState.JOB_STATE_PENDING\n    ]\n]\n\n# Categorize jobs\njobs_to_stop = []\n\nfor job in active_jobs:\n    # Check job type\n    is_streaming = job.name.startswith(\"pytorch-streaming\")\n    is_batch = job.name.startswith(\"pytorch-batch\")\n    is_vertex = \"-vertex-\" in job.name\n    is_local = not is_vertex\n    \n    # Apply filters\n    should_stop = False\n    \n    if is_streaming and STOP_STREAMING_JOBS:\n        if (is_local and STOP_LOCAL_MODEL_JOBS) or (is_vertex and STOP_VERTEX_ENDPOINT_JOBS):\n            should_stop = True\n    \n    if is_batch and STOP_BATCH_JOBS:\n        if (is_local and STOP_LOCAL_MODEL_JOBS) or (is_vertex and STOP_VERTEX_ENDPOINT_JOBS):\n            should_stop = True\n    \n    if should_stop:\n        jobs_to_stop.append(job)\n\nif jobs_to_stop:\n    print(f\"Found {len(jobs_to_stop)} job(s) to stop:\")\n    for job in jobs_to_stop:\n        job_type = \"streaming\" if \"streaming\" in job.name else \"batch\"\n        model_type = \"vertex\" if \"vertex\" in job.name else \"local\"\n        print(f\"  - {job.name} ({job_type}/{model_type})\")\n        print(f\"    State: {job.current_state.name}\")\n        print(f\"    Created: {job.create_time}\")\nelse:\n    print(\"No jobs to stop.\")\n\n# ========================================\n# Discover BigQuery Tables\n# ========================================\nprint(\"\\n2. BigQuery Tables:\")\nprint(\"-\" * 60)\n\n# Define all result tables\nall_tables = {\n    \"pytorch_autoencoder_batch_results\": {\"type\": \"batch\", \"model\": \"local\"},\n    \"pytorch_autoencoder_streaming_results\": {\"type\": \"streaming\", \"model\": \"local\"},\n    \"pytorch_autoencoder_batch_results_vertex\": {\"type\": \"batch\", \"model\": \"vertex\"},\n    \"pytorch_autoencoder_streaming_results_vertex\": {\"type\": \"streaming\", \"model\": \"vertex\"},\n}\n\ntables_to_clean = []\ndataset_id = f\"{PROJECT_ID}.{BQ_DATASET}\"\n\nfor table_name, metadata in all_tables.items():\n    table_id = f\"{dataset_id}.{table_name}\"\n    \n    # Check if table exists\n    try:\n        table = bq_client.get_table(table_id)\n        \n        # Apply filters\n        should_clean = False\n        \n        if metadata[\"model\"] == \"local\" and CLEAN_LOCAL_MODEL_TABLES:\n            if (metadata[\"type\"] == \"batch\" and CLEAN_BATCH_TABLES) or \\\n               (metadata[\"type\"] == \"streaming\" and CLEAN_STREAMING_TABLES):\n                should_clean = True\n        \n        if metadata[\"model\"] == \"vertex\" and CLEAN_VERTEX_ENDPOINT_TABLES:\n            if (metadata[\"type\"] == \"batch\" and CLEAN_BATCH_TABLES) or \\\n               (metadata[\"type\"] == \"streaming\" and CLEAN_STREAMING_TABLES):\n                should_clean = True\n        \n        if should_clean:\n            tables_to_clean.append({\"table\": table, \"name\": table_name, \"metadata\": metadata})\n    \n    except Exception:\n        pass  # Table doesn't exist\n\nif tables_to_clean:\n    action = \"delete\" if DELETE_BIGQUERY_TABLES else \"truncate\"\n    print(f\"Found {len(tables_to_clean)} table(s) to {action}:\")\n    for item in tables_to_clean:\n        table = item[\"table\"]\n        metadata = item[\"metadata\"]\n        print(f\"  - {item['name']} ({metadata['type']}/{metadata['model']})\")\n        print(f\"    Rows: {table.num_rows:,}\")\n        print(f\"    Size: {table.num_bytes / 1024 / 1024:.2f} MB\")\nelse:\n    print(\"No tables to clean.\")\n\n# ========================================\n# Discover Pub/Sub Resources\n# ========================================\nprint(\"\\n3. Pub/Sub Resources:\")\nprint(\"-\" * 60)\n\n# Topics - includes both input and output topics for each pipeline\ntopics_to_delete = []\nif DELETE_PUBSUB_TOPICS:\n    topic_names = [\n        f\"{EXPERIMENT}-input-local\",    # Local model input topic\n        f\"{EXPERIMENT}-input-vertex\",   # Vertex endpoint input topic\n        f\"{EXPERIMENT}-output-local\",   # Local model output topic\n        f\"{EXPERIMENT}-output-vertex\"   # Vertex endpoint output topic\n    ]\n    for topic_name in topic_names:\n        topic_path = pubsub_publisher.topic_path(PROJECT_ID, topic_name)\n        try:\n            pubsub_publisher.get_topic(request={\"topic\": topic_path})\n            topics_to_delete.append(topic_path)\n        except Exception:\n            pass  # Topic doesn't exist\n\nif topics_to_delete:\n    print(f\"Found {len(topics_to_delete)} topic(s) to delete:\")\n    for topic_path in topics_to_delete:\n        print(f\"  - {topic_path.split('/')[-1]}\")\nelse:\n    print(\"No topics to delete.\")\n\n# Subscriptions - includes both input and output subscriptions for each pipeline\nsubscriptions_to_delete = []\nif DELETE_PUBSUB_SUBSCRIPTIONS:\n    sub_names = [\n        f\"{EXPERIMENT}-input-sub-local\",   # Local model input subscription\n        f\"{EXPERIMENT}-input-sub-vertex\",  # Vertex endpoint input subscription\n        f\"{EXPERIMENT}-output-sub-local\",  # Local model output subscription\n        f\"{EXPERIMENT}-output-sub-vertex\"  # Vertex endpoint output subscription\n    ]\n    for sub_name in sub_names:\n        sub_path = pubsub_subscriber.subscription_path(PROJECT_ID, sub_name)\n        try:\n            pubsub_subscriber.get_subscription(request={\"subscription\": sub_path})\n            subscriptions_to_delete.append(sub_path)\n        except Exception:\n            pass  # Subscription doesn't exist\n\nif subscriptions_to_delete:\n    print(f\"Found {len(subscriptions_to_delete)} subscription(s) to delete:\")\n    for sub_path in subscriptions_to_delete:\n        print(f\"  - {sub_path.split('/')[-1]}\")\nelse:\n    print(\"No subscriptions to delete.\")\n\n# ========================================\n# Discover GCS Files\n# ========================================\nprint(\"\\n4. GCS Files:\")\nprint(\"-\" * 60)\n\nfiles_to_delete = []\nif DELETE_GCS_FILES:\n    bucket = storage_client.bucket(BUCKET_NAME)\n    prefix = f\"{SERIES}/{EXPERIMENT}/dataflow/\"\n    blobs = list(bucket.list_blobs(prefix=prefix))\n    files_to_delete = blobs\n\nif files_to_delete:\n    total_size = sum(blob.size for blob in files_to_delete)\n    print(f\"Found {len(files_to_delete)} file(s) to delete:\")\n    for blob in files_to_delete:\n        print(f\"  - {blob.name} ({blob.size / 1024:.2f} KB)\")\n    print(f\"  Total size: {total_size / 1024 / 1024:.2f} MB\")\nelse:\n    print(\"No files to delete.\")\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"DISCOVERY COMPLETE\")\nprint(\"=\" * 60)"
  },
  {
   "cell_type": "markdown",
   "id": "safety_checks",
   "metadata": {},
   "source": [
    "---\n",
    "## Safety Checks\n",
    "\n",
    "Verify it's safe to proceed with cleanup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "safety",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "SAFETY CHECKS\n",
      "============================================================\n",
      "\n",
      "Safety warnings detected:\n",
      "\n",
      "⚠️  WARNING: You're about to DELETE (not truncate) tables with 30 total rows.\n",
      "    This will remove the table schema. Consider using TRUNCATE instead.\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"SAFETY CHECKS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "warnings = []\n",
    "\n",
    "# Check 1: Active jobs using resources we want to delete\n",
    "if (DELETE_BIGQUERY_TABLES or DELETE_PUBSUB_TOPICS or DELETE_PUBSUB_SUBSCRIPTIONS or DELETE_GCS_FILES):\n",
    "    # Get all active jobs (not just ones we're stopping)\n",
    "    active_dataflow_jobs = [\n",
    "        job for job in all_jobs \n",
    "        if job.current_state in [\n",
    "            dataflow_v1beta3.JobState.JOB_STATE_RUNNING,\n",
    "            dataflow_v1beta3.JobState.JOB_STATE_PENDING\n",
    "        ]\n",
    "        and (job.name.startswith(\"pytorch-streaming\") or job.name.startswith(\"pytorch-batch\"))\n",
    "    ]\n",
    "    \n",
    "    jobs_not_being_stopped = [\n",
    "        job for job in active_dataflow_jobs if job not in jobs_to_stop\n",
    "    ]\n",
    "    \n",
    "    if jobs_not_being_stopped:\n",
    "        warnings.append(\n",
    "            f\"⚠️  WARNING: {len(jobs_not_being_stopped)} active Dataflow job(s) may be using resources you're about to delete:\\n\" +\n",
    "            \"\\n\".join([f\"    - {job.name}\" for job in jobs_not_being_stopped]) +\n",
    "            \"\\n    Consider stopping these jobs first or they may fail.\"\n",
    "        )\n",
    "\n",
    "# Check 2: Deleting tables with data\n",
    "if DELETE_BIGQUERY_TABLES:\n",
    "    tables_with_data = [item for item in tables_to_clean if item[\"table\"].num_rows > 0]\n",
    "    if tables_with_data:\n",
    "        total_rows = sum(item[\"table\"].num_rows for item in tables_with_data)\n",
    "        warnings.append(\n",
    "            f\"⚠️  WARNING: You're about to DELETE (not truncate) tables with {total_rows:,} total rows.\\n\"\n",
    "            f\"    This will remove the table schema. Consider using TRUNCATE instead.\"\n",
    "        )\n",
    "\n",
    "# Check 3: Deleting Pub/Sub topics with subscriptions\n",
    "if DELETE_PUBSUB_TOPICS and not DELETE_PUBSUB_SUBSCRIPTIONS:\n",
    "    if topics_to_delete and subscriptions_to_delete:\n",
    "        warnings.append(\n",
    "            f\"⚠️  WARNING: Deleting topics but keeping subscriptions.\\n\"\n",
    "            f\"    Subscriptions will fail without their topics. Consider deleting both.\"\n",
    "        )\n",
    "\n",
    "# Display warnings or all clear\n",
    "if warnings:\n",
    "    print(\"\\nSafety warnings detected:\\n\")\n",
    "    for warning in warnings:\n",
    "        print(warning)\n",
    "        print()\n",
    "else:\n",
    "    print(\"\\n✅ No safety concerns detected.\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confirm",
   "metadata": {},
   "source": [
    "---\n",
    "## Confirm Cleanup\n",
    "\n",
    "Review what will be cleaned up and confirm before proceeding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "confirm_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CLEANUP SUMMARY\n",
      "============================================================\n",
      "\n",
      "The following actions will be performed:\n",
      "\n",
      "1. Stop 2 Dataflow job(s)\n",
      "2. DELETE 4 BigQuery table(s)\n",
      "3. Delete 2 Pub/Sub topic(s)\n",
      "4. Delete 2 Pub/Sub subscription(s)\n",
      "5. Delete 2 GCS file(s)\n",
      "\n",
      "============================================================\n",
      "\n",
      "⚠️  CONFIRMATION REQUIRED\n",
      "To proceed with cleanup, manually set PROCEED_WITH_CLEANUP = True in the next cell.\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"CLEANUP SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "actions = []\n",
    "\n",
    "if jobs_to_stop:\n",
    "    actions.append(f\"Stop {len(jobs_to_stop)} Dataflow job(s)\")\n",
    "\n",
    "if tables_to_clean:\n",
    "    if DELETE_BIGQUERY_TABLES:\n",
    "        actions.append(f\"DELETE {len(tables_to_clean)} BigQuery table(s)\")\n",
    "    elif TRUNCATE_BIGQUERY_TABLES:\n",
    "        actions.append(f\"TRUNCATE {len(tables_to_clean)} BigQuery table(s)\")\n",
    "\n",
    "if topics_to_delete:\n",
    "    actions.append(f\"Delete {len(topics_to_delete)} Pub/Sub topic(s)\")\n",
    "\n",
    "if subscriptions_to_delete:\n",
    "    actions.append(f\"Delete {len(subscriptions_to_delete)} Pub/Sub subscription(s)\")\n",
    "\n",
    "if files_to_delete:\n",
    "    actions.append(f\"Delete {len(files_to_delete)} GCS file(s)\")\n",
    "\n",
    "if actions:\n",
    "    print(\"\\nThe following actions will be performed:\\n\")\n",
    "    for i, action in enumerate(actions, 1):\n",
    "        print(f\"{i}. {action}\")\n",
    "    print()\n",
    "else:\n",
    "    print(\"\\nNo cleanup actions to perform.\")\n",
    "    print(\"Either no resources found or all cleanup flags are set to False.\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Set flag for next cell\n",
    "PROCEED_WITH_CLEANUP = False\n",
    "\n",
    "if actions:\n",
    "    if REQUIRE_CONFIRMATION:\n",
    "        print(\"\\n⚠️  CONFIRMATION REQUIRED\")\n",
    "        print(\"To proceed with cleanup, manually set PROCEED_WITH_CLEANUP = True in the next cell.\")\n",
    "    else:\n",
    "        print(\"\\n⚠️  Auto-proceeding (REQUIRE_CONFIRMATION = False)\")\n",
    "        PROCEED_WITH_CLEANUP = True\n",
    "else:\n",
    "    print(\"\\n✅ Nothing to clean up.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confirm_manual",
   "metadata": {},
   "source": [
    "### Manual Confirmation\n",
    "\n",
    "**⚠️ Set `PROCEED_WITH_CLEANUP = True` to proceed with cleanup.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "confirm_flag",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Confirmation received. Proceeding with cleanup...\n"
     ]
    }
   ],
   "source": [
    "# Set this to True to proceed with cleanup\n",
    "PROCEED_WITH_CLEANUP = True\n",
    "\n",
    "if PROCEED_WITH_CLEANUP:\n",
    "    print(\"✅ Confirmation received. Proceeding with cleanup...\")\n",
    "else:\n",
    "    print(\"⚠️  Cleanup NOT confirmed. Set PROCEED_WITH_CLEANUP = True to proceed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "execute",
   "metadata": {},
   "source": [
    "---\n",
    "## Execute Cleanup\n",
    "\n",
    "Perform the actual cleanup operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "execute_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "EXECUTING CLEANUP\n",
      "============================================================\n",
      "\n",
      "1. Stopping Dataflow Jobs:\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Stopped: pytorch-streaming-vertex-20251112-160443\n",
      "✅ Stopped: pytorch-streaming-20251112-150859\n",
      "\n",
      "2. Cleaning BigQuery Tables:\n",
      "------------------------------------------------------------\n",
      "✅ Deleted: pytorch_autoencoder_batch_results\n",
      "✅ Deleted: pytorch_autoencoder_streaming_results\n",
      "✅ Deleted: pytorch_autoencoder_batch_results_vertex\n",
      "✅ Deleted: pytorch_autoencoder_streaming_results_vertex\n",
      "\n",
      "3. Deleting Pub/Sub Subscriptions:\n",
      "------------------------------------------------------------\n",
      "✅ Deleted: pytorch-autoencoder-input-sub\n",
      "✅ Deleted: pytorch-autoencoder-output-sub\n",
      "\n",
      "4. Deleting Pub/Sub Topics:\n",
      "------------------------------------------------------------\n",
      "✅ Deleted: pytorch-autoencoder-input\n",
      "✅ Deleted: pytorch-autoencoder-output\n",
      "\n",
      "5. Deleting GCS Files:\n",
      "------------------------------------------------------------\n",
      "✅ Deleted: frameworks/pytorch-autoencoder/dataflow/final_model_traced.pt\n",
      "✅ Deleted: frameworks/pytorch-autoencoder/dataflow/pytorch_autoencoder.mar\n",
      "\n",
      "============================================================\n",
      "CLEANUP COMPLETE\n",
      "============================================================\n",
      "\n",
      "✅ Results:\n",
      "  - Dataflow jobs stopped: 2\n",
      "  - BigQuery tables cleaned: 4\n",
      "  - Pub/Sub topics deleted: 2\n",
      "  - Pub/Sub subscriptions deleted: 2\n",
      "  - GCS files deleted: 2\n",
      "\n",
      "✅ No errors encountered.\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "if not PROCEED_WITH_CLEANUP:\n",
    "    print(\"⚠️  Cleanup cancelled. Set PROCEED_WITH_CLEANUP = True to proceed.\")\n",
    "else:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"EXECUTING CLEANUP\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    cleanup_results = {\n",
    "        \"jobs_stopped\": 0,\n",
    "        \"tables_cleaned\": 0,\n",
    "        \"topics_deleted\": 0,\n",
    "        \"subscriptions_deleted\": 0,\n",
    "        \"files_deleted\": 0,\n",
    "        \"errors\": []\n",
    "    }\n",
    "    \n",
    "    # ========================================\n",
    "    # Stop Dataflow Jobs\n",
    "    # ========================================\n",
    "    if jobs_to_stop:\n",
    "        print(\"\\n1. Stopping Dataflow Jobs:\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        for job in jobs_to_stop:\n",
    "            try:\n",
    "                cancel_request = dataflow_v1beta3.UpdateJobRequest(\n",
    "                    project_id=PROJECT_ID,\n",
    "                    location=REGION,\n",
    "                    job_id=job.id,\n",
    "                    job=dataflow_v1beta3.Job(\n",
    "                        requested_state=dataflow_v1beta3.JobState.JOB_STATE_CANCELLED\n",
    "                    )\n",
    "                )\n",
    "                \n",
    "                dataflow_client.update_job(request=cancel_request)\n",
    "                print(f\"✅ Stopped: {job.name}\")\n",
    "                cleanup_results[\"jobs_stopped\"] += 1\n",
    "            \n",
    "            except Exception as e:\n",
    "                error_msg = f\"Failed to stop job {job.name}: {e}\"\n",
    "                print(f\"❌ {error_msg}\")\n",
    "                cleanup_results[\"errors\"].append(error_msg)\n",
    "    \n",
    "    # ========================================\n",
    "    # Clean BigQuery Tables\n",
    "    # ========================================\n",
    "    if tables_to_clean:\n",
    "        print(\"\\n2. Cleaning BigQuery Tables:\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        for item in tables_to_clean:\n",
    "            table = item[\"table\"]\n",
    "            table_name = item[\"name\"]\n",
    "            \n",
    "            try:\n",
    "                if DELETE_BIGQUERY_TABLES:\n",
    "                    bq_client.delete_table(table, not_found_ok=True)\n",
    "                    print(f\"✅ Deleted: {table_name}\")\n",
    "                elif TRUNCATE_BIGQUERY_TABLES:\n",
    "                    # Truncate by deleting all rows\n",
    "                    query = f\"DELETE FROM `{table.project}.{table.dataset_id}.{table.table_id}` WHERE TRUE\"\n",
    "                    bq_client.query(query).result()\n",
    "                    print(f\"✅ Truncated: {table_name} ({table.num_rows:,} rows removed)\")\n",
    "                \n",
    "                cleanup_results[\"tables_cleaned\"] += 1\n",
    "            \n",
    "            except Exception as e:\n",
    "                error_msg = f\"Failed to clean table {table_name}: {e}\"\n",
    "                print(f\"❌ {error_msg}\")\n",
    "                cleanup_results[\"errors\"].append(error_msg)\n",
    "    \n",
    "    # ========================================\n",
    "    # Delete Pub/Sub Subscriptions (before topics)\n",
    "    # ========================================\n",
    "    if subscriptions_to_delete:\n",
    "        print(\"\\n3. Deleting Pub/Sub Subscriptions:\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        for sub_path in subscriptions_to_delete:\n",
    "            try:\n",
    "                pubsub_subscriber.delete_subscription(request={\"subscription\": sub_path})\n",
    "                sub_name = sub_path.split('/')[-1]\n",
    "                print(f\"✅ Deleted: {sub_name}\")\n",
    "                cleanup_results[\"subscriptions_deleted\"] += 1\n",
    "            \n",
    "            except Exception as e:\n",
    "                error_msg = f\"Failed to delete subscription {sub_path}: {e}\"\n",
    "                print(f\"❌ {error_msg}\")\n",
    "                cleanup_results[\"errors\"].append(error_msg)\n",
    "    \n",
    "    # ========================================\n",
    "    # Delete Pub/Sub Topics\n",
    "    # ========================================\n",
    "    if topics_to_delete:\n",
    "        print(\"\\n4. Deleting Pub/Sub Topics:\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        for topic_path in topics_to_delete:\n",
    "            try:\n",
    "                pubsub_publisher.delete_topic(request={\"topic\": topic_path})\n",
    "                topic_name = topic_path.split('/')[-1]\n",
    "                print(f\"✅ Deleted: {topic_name}\")\n",
    "                cleanup_results[\"topics_deleted\"] += 1\n",
    "            \n",
    "            except Exception as e:\n",
    "                error_msg = f\"Failed to delete topic {topic_path}: {e}\"\n",
    "                print(f\"❌ {error_msg}\")\n",
    "                cleanup_results[\"errors\"].append(error_msg)\n",
    "    \n",
    "    # ========================================\n",
    "    # Delete GCS Files\n",
    "    # ========================================\n",
    "    if files_to_delete:\n",
    "        print(\"\\n5. Deleting GCS Files:\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        for blob in files_to_delete:\n",
    "            try:\n",
    "                blob.delete()\n",
    "                print(f\"✅ Deleted: {blob.name}\")\n",
    "                cleanup_results[\"files_deleted\"] += 1\n",
    "            \n",
    "            except Exception as e:\n",
    "                error_msg = f\"Failed to delete file {blob.name}: {e}\"\n",
    "                print(f\"❌ {error_msg}\")\n",
    "                cleanup_results[\"errors\"].append(error_msg)\n",
    "    \n",
    "    # ========================================\n",
    "    # Cleanup Summary\n",
    "    # ========================================\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"CLEANUP COMPLETE\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"\\n✅ Results:\")\n",
    "    print(f\"  - Dataflow jobs stopped: {cleanup_results['jobs_stopped']}\")\n",
    "    print(f\"  - BigQuery tables cleaned: {cleanup_results['tables_cleaned']}\")\n",
    "    print(f\"  - Pub/Sub topics deleted: {cleanup_results['topics_deleted']}\")\n",
    "    print(f\"  - Pub/Sub subscriptions deleted: {cleanup_results['subscriptions_deleted']}\")\n",
    "    print(f\"  - GCS files deleted: {cleanup_results['files_deleted']}\")\n",
    "    \n",
    "    if cleanup_results[\"errors\"]:\n",
    "        print(f\"\\n❌ Errors encountered: {len(cleanup_results['errors'])}\")\n",
    "        for error in cleanup_results[\"errors\"]:\n",
    "            print(f\"  - {error}\")\n",
    "    else:\n",
    "        print(f\"\\n✅ No errors encountered.\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "This notebook provides centralized cleanup for Dataflow inference workflows.\n",
    "\n",
    "### Key Features\n",
    "\n",
    "✅ **Granular Control**: Choose exactly what to clean up\n",
    "\n",
    "✅ **Safety Checks**: Warnings for risky operations\n",
    "\n",
    "✅ **Confirmation Prompts**: Review before executing\n",
    "\n",
    "✅ **Flexible Options**: Truncate vs delete for BigQuery tables\n",
    "\n",
    "✅ **Error Handling**: Continue cleanup even if some operations fail\n",
    "\n",
    "### Common Cleanup Scenarios\n",
    "\n",
    "**Scenario 1: Stop running streaming jobs only**\n",
    "```python\n",
    "STOP_STREAMING_JOBS = True\n",
    "STOP_BATCH_JOBS = False\n",
    "# All other flags = False\n",
    "```\n",
    "\n",
    "**Scenario 2: Clear test data but keep infrastructure**\n",
    "```python\n",
    "TRUNCATE_BIGQUERY_TABLES = True\n",
    "# All other flags = False\n",
    "```\n",
    "\n",
    "**Scenario 3: Complete teardown**\n",
    "```python\n",
    "STOP_STREAMING_JOBS = True\n",
    "STOP_BATCH_JOBS = True\n",
    "DELETE_BIGQUERY_TABLES = True\n",
    "DELETE_PUBSUB_TOPICS = True\n",
    "DELETE_PUBSUB_SUBSCRIPTIONS = True\n",
    "DELETE_GCS_FILES = True\n",
    "```\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "**After cleanup:**\n",
    "- Verify jobs stopped: [Dataflow Console](https://console.cloud.google.com/dataflow)\n",
    "- Check BigQuery tables: [BigQuery Console](https://console.cloud.google.com/bigquery)\n",
    "- Verify Pub/Sub: [Pub/Sub Console](https://console.cloud.google.com/cloudpubsub)\n",
    "\n",
    "**To rebuild infrastructure:**\n",
    "- Run `dataflow-setup.ipynb` to recreate tables and Pub/Sub resources\n",
    "\n",
    "### Resources\n",
    "\n",
    "- [Dataflow Job Management](https://cloud.google.com/dataflow/docs/guides/deploying-a-pipeline)\n",
    "- [BigQuery Table Management](https://cloud.google.com/bigquery/docs/managing-tables)\n",
    "- [Pub/Sub Resource Management](https://cloud.google.com/pubsub/docs/admin)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (frameworks-pytorch)",
   "language": "python",
   "name": "frameworks-pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}