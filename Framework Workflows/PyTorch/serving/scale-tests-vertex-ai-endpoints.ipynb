{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47b9ee65",
   "metadata": {},
   "source": [
    "![tracker](https://us-central1-vertex-ai-mlops-369716.cloudfunctions.net/pixel-tracking?path=statmike%2Fvertex-ai-mlops%2FFramework+Workflows%2FPyTorch%2Fserving&file=scale-tests-vertex-ai-endpoints.ipynb)\n",
    "<!--- header table --->\n",
    "<table align=\"left\">\n",
    "<tr>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%20Workflows/PyTorch/serving/scale-tests-vertex-ai-endpoints.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.svgrepo.com/download/217753/github.svg\" alt=\"GitHub logo\">\n",
    "      <br>View on<br>GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://colab.research.google.com/github/statmike/vertex-ai-mlops/blob/main/Framework%20Workflows/PyTorch/serving/scale-tests-vertex-ai-endpoints.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.gstatic.com/pantheon/images/bigquery/welcome_page/colab-logo.svg\" alt=\"Google Colaboratory logo\">\n",
    "      <br>Run in<br>Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https%3A%2F%2Fraw.githubusercontent.com%2Fstatmike%2Fvertex-ai-mlops%2Fmain%2FFramework%2520Workflows%2FPyTorch%2Fserving%2Fscale-tests-vertex-ai-endpoints.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\">\n",
    "      <br>Run in<br>Colab Enterprise\n",
    "    </a>\n",
    "  </td>      \n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/bigquery/import?url=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%20Workflows/PyTorch/serving/scale-tests-vertex-ai-endpoints.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.gstatic.com/images/branding/gcpiconscolors/bigquery/v1/32px.svg\" alt=\"BigQuery logo\">\n",
    "      <br>Open in<br>BigQuery Studio\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/statmike/vertex-ai-mlops/main/Framework%20Workflows/PyTorch/serving/scale-tests-vertex-ai-endpoints.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.gstatic.com/images/branding/gcpiconscolors/vertexai/v1/32px.svg\" alt=\"Vertex AI logo\">\n",
    "      <br>Open in<br>Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td colspan=\"5\" style=\"text-align: right\">\n",
    "    <b>Share This On: </b> \n",
    "    <a href=\"https://www.linkedin.com/sharing/share-offsite/?url=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%2520Workflows/PyTorch/serving/scale-tests-vertex-ai-endpoints.ipynb\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"Linkedin Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://reddit.com/submit?url=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%2520Workflows/PyTorch/serving/scale-tests-vertex-ai-endpoints.ipynb\"><img src=\"https://redditinc.com/hubfs/Reddit%20Inc/Brand/Reddit_Logo.png\" alt=\"Reddit Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://bsky.app/intent/compose?text=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%2520Workflows/PyTorch/serving/scale-tests-vertex-ai-endpoints.ipynb\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"BlueSky Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://twitter.com/intent/tweet?url=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%2520Workflows/PyTorch/serving/scale-tests-vertex-ai-endpoints.ipynb\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/5/5a/X_icon_2.svg\" alt=\"X (Twitter) Logo\" width=\"20px\"></a> \n",
    "  </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td colspan=\"5\" style=\"text-align: right\">\n",
    "    <b>Connect With Author On: </b> \n",
    "    <a href=\"https://www.linkedin.com/in/statmike\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"Linkedin Logo\" width=\"20px\"></a>\n",
    "    <a href=\"https://www.github.com/statmike\"><img src=\"https://www.svgrepo.com/download/217753/github.svg\" alt=\"GitHub Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://www.youtube.com/@statmike-channel\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/f/fd/YouTube_full-color_icon_%282024%29.svg\" alt=\"YouTube Logo\" width=\"20px\"></a>\n",
    "    <a href=\"https://bsky.app/profile/statmike.bsky.social\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"BlueSky Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://x.com/statmike\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/5/5a/X_icon_2.svg\" alt=\"X (Twitter) Logo\" width=\"20px\"></a>\n",
    "  </td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Vertex AI Endpoint: Comprehensive Scale Testing & Performance Analysis\n",
    "\n",
    "**Scientific approach to understanding endpoint capacity, autoscaling behavior, and optimal configurations.**\n",
    "\n",
    "---\n",
    "\n",
    "## What This Notebook Does\n",
    "\n",
    "This notebook systematically tests Vertex AI Endpoint performance across **three dimensions**:\n",
    "\n",
    "1. **Batch Size** - How many instances per request?\n",
    "2. **Request Rate** - How many requests per second?\n",
    "3. **Load Pattern** - Burst, sustained, or ramping?\n",
    "\n",
    "Through rigorous testing, we answer:\n",
    "- ‚ùì **When does autoscaling trigger?** (exact conditions)\n",
    "- ‚ùì **Where are the bottlenecks?** (client vs endpoint)\n",
    "- ‚ùì **What's the optimal configuration?** (for different use cases)\n",
    "- ‚ùì **How should we configure for production?** (recommendations)\n",
    "\n",
    "## Testing Approach\n",
    "\n",
    "**Phase 1: Batch Size Analysis** (~2 minutes)\n",
    "- Test batch sizes from 1 to 1000 instances\n",
    "- Find optimal batch size for efficiency\n",
    "- Understand per-instance latency scaling\n",
    "\n",
    "**Phase 2A: Autoscaling Trigger Hunt** (~26 minutes)\n",
    "- Burst tests: Sudden load spikes (1k ‚Üí 10k requests)\n",
    "- Sustained tests: Steady load (50 ‚Üí 150 RPS for 5 mins)\n",
    "- Ramp test: Gradual increase (0 ‚Üí 200 RPS over 10 mins)\n",
    "- **Goal**: Find exact conditions that trigger autoscaling\n",
    "\n",
    "**Phase 2B: Concurrency Analysis** (~10 minutes)\n",
    "- Test different concurrency levels (50 ‚Üí 1000 concurrent)\n",
    "- Identify client-side vs endpoint-side bottlenecks\n",
    "- Find optimal concurrency for minimal queueing\n",
    "\n",
    "**Phase 3: Configuration Optimization** (~15-25 minutes)\n",
    "- Test promising combinations from earlier phases\n",
    "- Sustained load patterns for different scenarios\n",
    "- Generate production recommendations\n",
    "\n",
    "**Total test time**: ~53-63 minutes\n",
    "\n",
    "## Key Metrics Tracked\n",
    "\n",
    "**Client-Side Metrics** (what we measure):\n",
    "- üü¶ **Queueing Time**: Waiting for client concurrency slot ‚Üí *client bottleneck*\n",
    "- üü© **Request Time**: Actual HTTP request/response ‚Üí *endpoint performance*\n",
    "- üîµ **Total Latency**: End-to-end user experience\n",
    "- ‚úÖ **Success Rate**: Percentage of successful requests\n",
    "\n",
    "**Service-Side Metrics** (from Cloud Monitoring API):\n",
    "- üìä **CPU Utilization**: Triggers autoscaling at 60% default\n",
    "- üìà **Memory Utilization**: Resource pressure indicator\n",
    "- üî¢ **Replica Count**: Autoscaling activity\n",
    "- ‚ö° **Prediction Rate**: Service-reported throughput\n",
    "- üìâ **Service Latency**: P95 latency from endpoint's perspective\n",
    "- ‚ùå **Error Rate**: Service-side failures\n",
    "\n",
    "## Understanding Bottlenecks\n",
    "\n",
    "**Client-Side Bottleneck** (Queueing > 50%):\n",
    "- Semaphore limit too low\n",
    "- Network capacity saturated\n",
    "- **Solution**: Increase `max_concurrent` or reduce RPS\n",
    "\n",
    "**Endpoint-Side Bottleneck** (Queueing < 10%):\n",
    "- Model inference takes time\n",
    "- Replica capacity saturated\n",
    "- **Solution**: Wait for autoscaling or increase min replicas\n",
    "\n",
    "**Mixed Bottleneck** (Queueing 10-50%):\n",
    "- Both client and endpoint under pressure\n",
    "- Transition point between client/endpoint limits\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before running this notebook, you need:\n",
    "\n",
    "- **Deployed Vertex AI Endpoint** with a model\n",
    "  - Train: [../pytorch-autoencoder.ipynb](../pytorch-autoencoder.ipynb)\n",
    "  - Deploy: [vertex-ai-endpoint-prebuilt-container.ipynb](./vertex-ai-endpoint-prebuilt-container.ipynb) OR [vertex-ai-endpoint-custom-container.ipynb](./vertex-ai-endpoint-custom-container.ipynb)\n",
    "- **Google Cloud authentication** with Vertex AI prediction permissions\n",
    "- **Python packages**: `aiohttp`, `google-cloud-aiplatform`, `google-cloud-monitoring`, `plotly`, `pandas`\n",
    "\n",
    "## Understanding Vertex AI Autoscaling\n",
    "\n",
    "**How It Works**:\n",
    "- Autoscales based on **CPU utilization** (default threshold: 60%)\n",
    "- When CPU > 60% for ~1-2 minutes ‚Üí new replica provisions\n",
    "- Replica provisioning takes ~2-3 minutes (container startup)\n",
    "- Scale-down occurs after ~10-15 minutes below threshold\n",
    "\n",
    "**Important**: Lightweight models may not trigger autoscaling even under high RPS because CPU usage stays low. This is a **capacity bottleneck**, not a **compute bottleneck**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup_header",
   "metadata": {},
   "source": [
    "---\n",
    "## Environment Setup\n",
    "\n",
    "This section will authenticate your session, enable required Google Cloud APIs, and install necessary Python packages.\n",
    "\n",
    "**Package Installation Options (`REQ_TYPE`):**\n",
    "- `PRIMARY`: Installs only the main packages. Faster, but pip resolves sub-dependencies which may result in different versions than development.\n",
    "- `ALL` (Default): Installs exact versions of all packages and dependencies. Best for perfectly reproducing the development environment.\n",
    "- `COLAB`: Installs a Colab-optimized list that excludes pre-installed packages like `ipython` and `ipykernel`.\n",
    "\n",
    "**Installation Tool Options (`INSTALL_TOOL`):**\n",
    "- `pip` (Default): Uses pip for package installation. Standard Python package installer.\n",
    "- `uv`: Modern, fast Python package installer. Must be installed separately. See: https://github.com/astral-sh/uv\n",
    "- `poetry`: Dependency management tool. Requires running notebook in a poetry environment (`poetry shell` or `poetry run jupyter lab`). Uses `pyproject.toml` instead of requirements.txt.\n",
    "\n",
    "> **Note:** If running in Google Colab, the script will automatically detect this and set `REQ_TYPE = 'COLAB'` to prevent package conflicts, overriding any manual setting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "project_id_header",
   "metadata": {},
   "source": [
    "### Set Your Project ID\n",
    "\n",
    "‚ö†Ô∏è **Action Required:** Replace the `PROJECT_ID` value below with your Google Cloud project ID before running this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "project_config",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = 'statmike-mlops-349915' # replace with GCP project ID\n",
    "REQ_TYPE = 'ALL' # Specify PRIMARY or ALL or COLAB\n",
    "INSTALL_TOOL = 'poetry' # Specify pip, uv, or poetry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config_header",
   "metadata": {},
   "source": [
    "### Configuration\n",
    "\n",
    "This cell defines the requirements files and Google Cloud APIs needed for this notebook. Run as-is without modification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": [
    "REQUIREMENTS_URL = 'https://raw.githubusercontent.com/statmike/vertex-ai-mlops/refs/heads/main/Framework%20Workflows/PyTorch/requirements.txt'\n",
    "\n",
    "REQUIRED_APIS = [\n",
    "    \"aiplatform.googleapis.com\",\n",
    "    \"monitoring.googleapis.com\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "run_setup_header",
   "metadata": {},
   "source": [
    "### Run Setup\n",
    "\n",
    "This cell downloads the centralized setup code and configures your environment. It will:\n",
    "- Authenticate your session with Google Cloud\n",
    "- Enable required APIs for this notebook\n",
    "- Install necessary Python packages\n",
    "- Display a setup summary with your project information\n",
    "\n",
    "> **Note:** In Colab, if packages are installed, the kernel will automatically restart. After restart, continue from the next cell without re-running earlier cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "run_setup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "PYTHON GCP ENVIRONMENT SETUP\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "AUTHENTICATION\n",
      "==================================================\n",
      "Checking for existing ADC...\n",
      "‚úÖ Existing ADC found.\n",
      "‚úÖ Project is correctly set to 'statmike-mlops-349915'.\n",
      "\n",
      "==================================================\n",
      "API CHECK & ENABLE\n",
      "==================================================\n",
      "‚úÖ aiplatform.googleapis.com is already enabled.\n",
      "‚úÖ monitoring.googleapis.com is already enabled.\n",
      "\n",
      "==================================================\n",
      "PACKAGE MANAGEMENT\n",
      "==================================================\n",
      "Installation Tool: poetry\n",
      "‚úÖ Found poetry at: /usr/local/google/home/statmike/.local/bin/poetry\n",
      "‚úÖ Running in poetry environment: /usr/local/google/home/statmike/.cache/pypoetry/virtualenvs/frameworks-pytorch-0KVJlKeQ-py3.13\n",
      "‚ÑπÔ∏è  Poetry mode: Installing from pyproject.toml (REQUIREMENTS_URL ignored)\n",
      "‚úÖ Found pyproject.toml at: /usr/local/google/home/statmike/Git/vertex-ai-mlops/Framework Workflows/PyTorch/pyproject.toml\n",
      "   Changed working directory to: /usr/local/google/home/statmike/Git/vertex-ai-mlops/Framework Workflows/PyTorch\n",
      "Running poetry install...\n",
      "   Restored working directory to: /usr/local/google/home/statmike/Git/vertex-ai-mlops/Framework Workflows/PyTorch/serving\n",
      "‚úÖ All packages are already installed and up to date.\n",
      "\n",
      "==================================================\n",
      "Google Cloud Project Information\n",
      "==================================================\n",
      "PROJECT_ID     = statmike-mlops-349915\n",
      "PROJECT_NUMBER = 1026793852137\n",
      "==================================================\n",
      "\n",
      "\n",
      "==================================================\n",
      "SETUP SUMMARY\n",
      "==================================================\n",
      "‚úÖ Authentication:    Success\n",
      "‚úÖ API Configuration: Success\n",
      "‚úÖ Package Install:   Already up to date\n",
      "‚úÖ Installation Tool: poetry\n",
      "‚úÖ Project ID:        statmike-mlops-349915\n",
      "‚úÖ Project Number:    1026793852137\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os, urllib.request\n",
    "\n",
    "# Download and import setup code\n",
    "url = 'https://raw.githubusercontent.com/statmike/vertex-ai-mlops/refs/heads/main/core/notebook-template/python_setup.py'\n",
    "urllib.request.urlretrieve(url, 'python_setup_local.py')\n",
    "import python_setup_local as python_setup\n",
    "os.remove('python_setup_local.py')\n",
    "\n",
    "# Run setup\n",
    "setup_info = python_setup.setup_environment(PROJECT_ID, REQ_TYPE, REQUIREMENTS_URL, REQUIRED_APIS, INSTALL_TOOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8i6h2p5352h",
   "metadata": {},
   "source": [
    "---\n",
    "## Test Configuration\n",
    "\n",
    "Configure the endpoint to test and the test parameters.\n",
    "\n",
    "**Endpoint Configuration:**\n",
    "- Update `ENDPOINT_DISPLAY_NAME` to match your deployed endpoint\n",
    "- Update `REGION` if your endpoint is in a different region\n",
    "\n",
    "**Test Parameters:**\n",
    "- Adjust batch sizes, RPS targets, and durations based on your needs\n",
    "- Default values are designed to comprehensively test a PyTorch autoencoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6eob3sesvuu",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Endpoint Configuration\n",
    "REGION = 'us-central1'\n",
    "ENDPOINT_DISPLAY_NAME = 'pytorch-autoencoder-endpoint'  # Replace with your endpoint name\n",
    "\n",
    "# Phase 1: Batch Size Analysis\n",
    "BATCH_SIZES = [1, 5, 10, 50, 100, 500, 1000]\n",
    "\n",
    "# Phase 2A: Autoscaling Trigger Hunt\n",
    "BURST_TESTS = [\n",
    "    {\"num_requests\": 1000, \"max_concurrent\": 100},\n",
    "    {\"num_requests\": 2000, \"max_concurrent\": 200},\n",
    "    {\"num_requests\": 5000, \"max_concurrent\": 500},\n",
    "    {\"num_requests\": 10000, \"max_concurrent\": 1000},\n",
    "]\n",
    "\n",
    "SUSTAINED_TESTS = [\n",
    "    {\"target_rps\": 50, \"duration\": 300},   # 5 minutes\n",
    "    {\"target_rps\": 100, \"duration\": 300},  # 5 minutes\n",
    "    {\"target_rps\": 150, \"duration\": 300},  # 5 minutes\n",
    "]\n",
    "\n",
    "RAMP_TEST = {\"target_rps\": 200, \"duration\": 600}  # 10 minutes\n",
    "\n",
    "# Phase 2B: Concurrency Analysis  \n",
    "CONCURRENCY_TESTS = [50, 100, 200, 500, 1000]\n",
    "CONCURRENCY_TEST_REQUESTS = 2000\n",
    "\n",
    "# Phase 3: Configuration Optimization (customize based on earlier results)\n",
    "OPTIMIZATION_CONFIGS = [\n",
    "    {\"batch\": 1, \"rps\": 100, \"duration\": 300, \"name\": \"Real-time (low latency)\"},\n",
    "    {\"batch\": 10, \"rps\": 50, \"duration\": 300, \"name\": \"Balanced\"},\n",
    "    {\"batch\": 50, \"rps\": 20, \"duration\": 300, \"name\": \"High throughput\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3fz7kr85khj",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/google/home/statmike/.cache/pypoetry/virtualenvs/frameworks-pytorch-0KVJlKeQ-py3.13/lib/python3.13/site-packages/google/cloud/aiplatform/models.py:52: FutureWarning: Support for google-cloud-storage < 3.0.0 will be removed in a future version of google-cloud-aiplatform. Please upgrade to google-cloud-storage >= 3.0.0.\n",
      "  from google.cloud.aiplatform.utils import gcs_utils\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Imports complete\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import asyncio\n",
    "import aiohttp\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import google.auth\n",
    "import google.auth.transport.requests\n",
    "from google.cloud import aiplatform, monitoring_v3\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "print(\"‚úÖ Imports complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36g37x5jjea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Initialized for project: statmike-mlops-349915\n"
     ]
    }
   ],
   "source": [
    "# Initialize clients\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION)\n",
    "monitoring_client = monitoring_v3.MetricServiceClient()\n",
    "\n",
    "# Setup authentication for REST API\n",
    "credentials, _ = google.auth.default()\n",
    "auth_req = google.auth.transport.requests.Request()\n",
    "\n",
    "print(f\"‚úÖ Initialized for project: {PROJECT_ID}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "se4aljdk70i",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Testing utilities imported\n"
     ]
    }
   ],
   "source": [
    "# Import testing utilities\n",
    "from scale_testing_utils import run_endpoint_test, EndpointMetricsCollector, plot_timeline_with_metrics\n",
    "\n",
    "print(\"‚úÖ Testing utilities imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2rnuq308op4",
   "metadata": {},
   "source": [
    "---\n",
    "## Connect to Endpoint\n",
    "\n",
    "Find and connect to the deployed endpoint, then prepare test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "iclf2o6a18i",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Connected to: pytorch-autoencoder-endpoint\n",
      "   Machine: n1-standard-4\n",
      "   Replicas: 1 - 4\n",
      "   Endpoint ID: 5971323405637517312\n",
      "   URL: https://us-central1-aiplatform.googleapis.com/v1/projects/1026793852137/locations/us-central1/endpoints/5971323405637517312:predict\n"
     ]
    }
   ],
   "source": [
    "# Find endpoint\n",
    "endpoints = aiplatform.Endpoint.list(filter=f\"display_name={ENDPOINT_DISPLAY_NAME}\")\n",
    "\n",
    "if not endpoints:\n",
    "    raise ValueError(f\"No endpoint found: {ENDPOINT_DISPLAY_NAME}\")\n",
    "\n",
    "endpoint = endpoints[0]\n",
    "endpoint_url = f\"https://{REGION}-aiplatform.googleapis.com/v1/{endpoint.resource_name}:predict\"\n",
    "endpoint_id = endpoint.resource_name.split('/')[-1]\n",
    "\n",
    "# Get endpoint configuration\n",
    "deployed_model = endpoint.list_models()[0]\n",
    "MACHINE_TYPE = deployed_model.dedicated_resources.machine_spec.machine_type\n",
    "MIN_REPLICAS = deployed_model.dedicated_resources.min_replica_count\n",
    "MAX_REPLICAS = deployed_model.dedicated_resources.max_replica_count\n",
    "\n",
    "print(f\"‚úÖ Connected to: {endpoint.display_name}\")\n",
    "print(f\"   Machine: {MACHINE_TYPE}\")\n",
    "print(f\"   Replicas: {MIN_REPLICAS} - {MAX_REPLICAS}\")\n",
    "print(f\"   Endpoint ID: {endpoint_id}\")\n",
    "print(f\"   URL: {endpoint_url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "zcan8becgv",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Endpoint connection successful\n",
      "   Prediction returned: 1 result(s)\n"
     ]
    }
   ],
   "source": [
    "# Prepare test data  \n",
    "# Generate sample data (30 features for PyTorch autoencoder)\n",
    "test_data = [np.random.randn(30).astype(np.float32).tolist()]\n",
    "\n",
    "# Test connection\n",
    "response = endpoint.predict(instances=test_data)\n",
    "print(f\"‚úÖ Endpoint connection successful\")\n",
    "print(f\"   Prediction returned: {len(response.predictions)} result(s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "v2g86me4u4l",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Metrics collector initialized\n",
      "   Will collect: CPU, Memory, Replicas, Predictions, Latency, Errors\n"
     ]
    }
   ],
   "source": [
    "# Initialize metrics collector\n",
    "metrics_collector = EndpointMetricsCollector(\n",
    "    project_id=PROJECT_ID,\n",
    "    endpoint_id=endpoint_id,\n",
    "    region=REGION\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Metrics collector initialized\")\n",
    "print(f\"   Will collect: CPU, Memory, Replicas, Predictions, Latency, Errors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ubu4yhuqr2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## How to Run the Complete Test Suite\n",
    "\n",
    "This notebook provides a comprehensive testing framework. You can run all phases sequentially or pick specific tests based on your needs.\n",
    "\n",
    "### Quick Start (Recommended Order)\n",
    "\n",
    "1. **Run Phase 1** (Batch Size Analysis) - ~2 minutes\n",
    "2. **Run Phase 2A** (Autoscaling Trigger Hunt) - ~26 minutes  \n",
    "3. **Collect metrics** after Phase 2A\n",
    "4. **Analyze autoscaling events**\n",
    "5. **Run Phase 2B** (Concurrency Analysis) - ~10 minutes\n",
    "6. **Run Phase 3** (Configuration Optimization) - ~15-25 minutes\n",
    "7. **Generate final recommendations**\n",
    "\n",
    "### Adding Test Cells\n",
    "\n",
    "For each phase below, add cells using this pattern:\n",
    "\n",
    "```python\n",
    "# Run test\n",
    "test_result = await run_endpoint_test(\n",
    "    endpoint_url=endpoint_url,\n",
    "    credentials=credentials,\n",
    "    auth_req=auth_req,\n",
    "    test_data=test_data,\n",
    "    num_requests=1000,\n",
    "    batch_size=1,\n",
    "    pattern=\"burst\",  # or \"sustained\" or \"ramp\"\n",
    "    max_concurrent=100,\n",
    "    test_name=\"Test Name\"\n",
    ")\n",
    "\n",
    "# Store results\n",
    "results_list.append(test_result)\n",
    "```\n",
    "\n",
    "### Collecting Metrics After Tests\n",
    "\n",
    "```python\n",
    "# Collect metrics for a specific test\n",
    "metrics = metrics_collector.collect_metrics(\n",
    "    start_time=test_result['start_time'],\n",
    "    end_time=test_result['end_time']\n",
    ")\n",
    "\n",
    "# Analyze autoscaling\n",
    "autoscaling_events = metrics_collector.analyze_autoscaling(metrics)\n",
    "\n",
    "# Visualize\n",
    "fig = plot_timeline_with_metrics(\n",
    "    test_results=test_result['results_df'],\n",
    "    metrics=metrics,\n",
    "    test_name=\"Test Name\"\n",
    ")\n",
    "fig.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Phase 1: Batch Size Analysis\n",
    "\n",
    "**Goal**: Find optimal batch size for efficiency\n",
    "\n",
    "**Tests to run**: 7 tests (one per batch size)\n",
    "**Total time**: ~2 minutes\n",
    "**Pattern**: Sustained at 1 RPS\n",
    "\n",
    "Execute batch size tests below..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "iiyxzmhc7x",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Phase 1: Batch Size Analysis\n",
      "Testing 7 batch sizes: [1, 5, 10, 50, 100, 500, 1000]\n",
      "Total estimated time: ~2 minutes\n"
     ]
    }
   ],
   "source": [
    "# Initialize results storage\n",
    "phase1_results = []\n",
    "\n",
    "print(\"Starting Phase 1: Batch Size Analysis\")\n",
    "print(f\"Testing {len(BATCH_SIZES)} batch sizes: {BATCH_SIZES}\")\n",
    "print(f\"Total estimated time: ~2 minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "28zekehg56m",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Phase 1 - Batch Size 1\n",
      "======================================================================\n",
      "Pattern: BURST\n",
      "Requests: 10 | Batch: 1 | Concurrency: 10\n",
      "Total instances: 10\n",
      "\n",
      "‚è≥ Running burst test...\n",
      "   Expected duration: 0-0s\n",
      "\n",
      "‚úÖ Complete in 0.1s\n",
      "   Success: 10/10 (100.0%)\n",
      "   Total Latency:  70.4ms (mean) | 75.1ms (p95)\n",
      "   Queueing:       0.0ms (mean) | 0.0ms (p95)\n",
      "   Request:        70.3ms (mean) | 75.1ms (p95)\n",
      "   ‚úÖ Bottleneck: Endpoint processing (100%)\n",
      "\n",
      "======================================================================\n",
      "Phase 1 - Batch Size 5\n",
      "======================================================================\n",
      "Pattern: BURST\n",
      "Requests: 10 | Batch: 5 | Concurrency: 10\n",
      "Total instances: 50\n",
      "\n",
      "‚è≥ Running burst test...\n",
      "   Expected duration: 0-0s\n",
      "\n",
      "‚úÖ Complete in 0.1s\n",
      "   Success: 10/10 (100.0%)\n",
      "   Total Latency:  72.8ms (mean) | 82.2ms (p95)\n",
      "   Queueing:       0.0ms (mean) | 0.0ms (p95)\n",
      "   Request:        72.8ms (mean) | 82.2ms (p95)\n",
      "   ‚úÖ Bottleneck: Endpoint processing (100%)\n",
      "\n",
      "======================================================================\n",
      "Phase 1 - Batch Size 10\n",
      "======================================================================\n",
      "Pattern: BURST\n",
      "Requests: 10 | Batch: 10 | Concurrency: 10\n",
      "Total instances: 100\n",
      "\n",
      "‚è≥ Running burst test...\n",
      "   Expected duration: 0-0s\n",
      "\n",
      "‚úÖ Complete in 0.1s\n",
      "   Success: 10/10 (100.0%)\n",
      "   Total Latency:  77.1ms (mean) | 86.5ms (p95)\n",
      "   Queueing:       0.0ms (mean) | 0.0ms (p95)\n",
      "   Request:        77.1ms (mean) | 86.5ms (p95)\n",
      "   ‚úÖ Bottleneck: Endpoint processing (100%)\n",
      "\n",
      "======================================================================\n",
      "Phase 1 - Batch Size 50\n",
      "======================================================================\n",
      "Pattern: BURST\n",
      "Requests: 10 | Batch: 50 | Concurrency: 10\n",
      "Total instances: 500\n",
      "\n",
      "‚è≥ Running burst test...\n",
      "   Expected duration: 0-0s\n",
      "\n",
      "‚úÖ Complete in 0.2s\n",
      "   Success: 10/10 (100.0%)\n",
      "   Total Latency:  151.9ms (mean) | 190.8ms (p95)\n",
      "   Queueing:       0.0ms (mean) | 0.0ms (p95)\n",
      "   Request:        151.9ms (mean) | 190.8ms (p95)\n",
      "   ‚úÖ Bottleneck: Endpoint processing (100%)\n",
      "\n",
      "======================================================================\n",
      "Phase 1 - Batch Size 100\n",
      "======================================================================\n",
      "Pattern: BURST\n",
      "Requests: 10 | Batch: 100 | Concurrency: 10\n",
      "Total instances: 1,000\n",
      "\n",
      "‚è≥ Running burst test...\n",
      "   Expected duration: 0-0s\n",
      "\n",
      "‚úÖ Complete in 0.3s\n",
      "   Success: 10/10 (100.0%)\n",
      "   Total Latency:  234.3ms (mean) | 293.1ms (p95)\n",
      "   Queueing:       0.0ms (mean) | 0.0ms (p95)\n",
      "   Request:        234.3ms (mean) | 293.1ms (p95)\n",
      "   ‚úÖ Bottleneck: Endpoint processing (100%)\n",
      "\n",
      "======================================================================\n",
      "Phase 1 - Batch Size 500\n",
      "======================================================================\n",
      "Pattern: BURST\n",
      "Requests: 10 | Batch: 500 | Concurrency: 10\n",
      "Total instances: 5,000\n",
      "\n",
      "‚è≥ Running burst test...\n",
      "   Expected duration: 0-0s\n",
      "\n",
      "‚úÖ Complete in 1.1s\n",
      "   Success: 10/10 (100.0%)\n",
      "   Total Latency:  822.9ms (mean) | 1047.1ms (p95)\n",
      "   Queueing:       0.0ms (mean) | 0.0ms (p95)\n",
      "   Request:        822.9ms (mean) | 1047.1ms (p95)\n",
      "   ‚úÖ Bottleneck: Endpoint processing (100%)\n",
      "\n",
      "======================================================================\n",
      "Phase 1 - Batch Size 1000\n",
      "======================================================================\n",
      "Pattern: BURST\n",
      "Requests: 10 | Batch: 1000 | Concurrency: 10\n",
      "Total instances: 10,000\n",
      "\n",
      "‚è≥ Running burst test...\n",
      "   Expected duration: 0-0s\n",
      "\n",
      "‚úÖ Complete in 2.6s\n",
      "   Success: 10/10 (100.0%)\n",
      "   Total Latency:  1785.6ms (mean) | 2389.0ms (p95)\n",
      "   Queueing:       0.0ms (mean) | 0.0ms (p95)\n",
      "   Request:        1785.6ms (mean) | 2389.0ms (p95)\n",
      "   ‚úÖ Bottleneck: Endpoint processing (100%)\n",
      "\n",
      "‚úÖ Phase 1 complete: Tested 7 batch sizes\n"
     ]
    }
   ],
   "source": [
    "# Run batch size tests\n",
    "for batch_size in BATCH_SIZES:\n",
    "    result = await run_endpoint_test(\n",
    "        endpoint_url=endpoint_url,\n",
    "        credentials=credentials,\n",
    "        auth_req=auth_req,\n",
    "        test_data=test_data,\n",
    "        num_requests=10,  # 10 requests per batch size\n",
    "        batch_size=batch_size,\n",
    "        pattern=\"burst\",  # Use burst instead of sustained for faster completion\n",
    "        max_concurrent=10,\n",
    "        test_name=f\"Phase 1 - Batch Size {batch_size}\"\n",
    "    )\n",
    "    phase1_results.append(result)\n",
    "\n",
    "print(f\"\\n‚úÖ Phase 1 complete: Tested {len(BATCH_SIZES)} batch sizes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "qhcszlya6rn",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phase 1 Analysis:\n",
      "\n",
      "  Batch Size Performance:\n",
      "    Batch    1:    70.4ms total,  70.35ms per instance\n",
      "    Batch    5:    72.8ms total,  14.56ms per instance\n",
      "    Batch   10:    77.1ms total,   7.71ms per instance\n",
      "    Batch   50:   151.9ms total,   3.04ms per instance\n",
      "    Batch  100:   234.3ms total,   2.34ms per instance\n",
      "    Batch  500:   822.9ms total,   1.65ms per instance\n",
      "    Batch 1000:  1785.6ms total,   1.79ms per instance\n",
      "\n",
      "  üìä Optimal batch size: 10 (latency < 2x baseline)\n",
      "     Baseline (batch=1): 70.4ms\n",
      "     Optimal (batch=10): 77.1ms\n"
     ]
    }
   ],
   "source": [
    "# Analyze Phase 1 results\n",
    "phase1_df = pd.DataFrame([r['summary'] for r in phase1_results])\n",
    "\n",
    "# Calculate per-instance latency\n",
    "phase1_df['per_instance_latency_ms'] = phase1_df['mean_latency_ms'] / phase1_df['batch_size']\n",
    "\n",
    "# Find optimal batch size (lowest per-instance latency with acceptable total latency)\n",
    "optimal_batch = phase1_df[phase1_df['mean_latency_ms'] <= phase1_df['mean_latency_ms'].iloc[0] * 2]['batch_size'].max()\n",
    "\n",
    "print(\"Phase 1 Analysis:\")\n",
    "print(f\"\\n  Batch Size Performance:\")\n",
    "for _, row in phase1_df.iterrows():\n",
    "    print(f\"    Batch {int(row['batch_size']):4d}: {row['mean_latency_ms']:7.1f}ms total, {row['per_instance_latency_ms']:6.2f}ms per instance\")\n",
    "\n",
    "print(f\"\\n  üìä Optimal batch size: {optimal_batch} (latency < 2x baseline)\")\n",
    "print(f\"     Baseline (batch=1): {phase1_df['mean_latency_ms'].iloc[0]:.1f}ms\")\n",
    "print(f\"     Optimal (batch={optimal_batch}): {phase1_df[phase1_df['batch_size']==optimal_batch]['mean_latency_ms'].values[0]:.1f}ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eqokvf8ity",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "line": {
          "color": "blue",
          "width": 2
         },
         "marker": {
          "size": 10
         },
         "mode": "lines+markers",
         "name": "Mean Latency",
         "type": "scatter",
         "x": {
          "bdata": "AQAFAAoAMgBkAPQB6AM=",
          "dtype": "i2"
         },
         "y": {
          "bdata": "AAAAiLaWUUAAAADcXzRSQAAAABSdRVNAAAAAyv77YkAAAABQGkttQAAAgP+Vt4lAAADABHXmm0A=",
          "dtype": "f8"
         }
        },
        {
         "line": {
          "color": "orange",
          "dash": "dash",
          "width": 2
         },
         "marker": {
          "size": 8
         },
         "mode": "lines+markers",
         "name": "P95 Latency",
         "type": "scatter",
         "x": {
          "bdata": "AQAFAAoAMgBkAPQB6AM=",
          "dtype": "i2"
         },
         "y": {
          "bdata": "AAAA2prDUkD///9psY5UQAAAAJCTnlVAAAAAOEXZZ0AAAIAyqFFyQAAAoNtxXJBAAADgPwCqokA=",
          "dtype": "f8"
         }
        }
       ],
       "layout": {
        "height": 500,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermap": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermap"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Phase 1: Latency vs Batch Size"
        },
        "xaxis": {
         "title": {
          "text": "Batch Size (instances per request)"
         }
        },
        "yaxis": {
         "title": {
          "text": "Latency (ms)"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize Phase 1 results\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=phase1_df['batch_size'], \n",
    "    y=phase1_df['mean_latency_ms'],\n",
    "    mode='lines+markers', \n",
    "    name='Mean Latency',\n",
    "    line=dict(width=2, color='blue'),\n",
    "    marker=dict(size=10)\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=phase1_df['batch_size'], \n",
    "    y=phase1_df['p95_latency_ms'],\n",
    "    mode='lines+markers', \n",
    "    name='P95 Latency',\n",
    "    line=dict(width=2, dash='dash', color='orange'),\n",
    "    marker=dict(size=8)\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Phase 1: Latency vs Batch Size',\n",
    "    xaxis_title='Batch Size (instances per request)',\n",
    "    yaxis_title='Latency (ms)',\n",
    "    height=500\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91s9nl1n9k6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Phase 2A: Autoscaling Trigger Hunt\n",
    "\n",
    "**Goal**: Find exact conditions that trigger autoscaling\n",
    "\n",
    "**Tests**: \n",
    "- 4 burst tests (~1 minute)\n",
    "- 3 sustained tests (~15 minutes)\n",
    "- 1 ramp test (~10 minutes)\n",
    "\n",
    "**Total time**: ~26 minutes\n",
    "\n",
    "‚ö†Ô∏è **Important**: This phase will take approximately **26 minutes** to complete. The tests are designed to trigger autoscaling by creating sustained load on the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dj2dceyp4s5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PHASE 2A: AUTOSCALING TRIGGER HUNT\n",
      "======================================================================\n",
      "\n",
      "This phase will take approximately 26 minutes\n",
      "We'll run 8 tests to find when autoscaling triggers:\n",
      "  - 4 burst tests (fast)\n",
      "  - 3 sustained tests (5 mins each)\n",
      "  - 1 ramp test (10 mins)\n",
      "\n",
      "======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize Phase 2A storage\n",
    "phase2a_burst_results = []\n",
    "phase2a_sustained_results = []\n",
    "phase2a_ramp_results = []\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"PHASE 2A: AUTOSCALING TRIGGER HUNT\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nThis phase will take approximately 26 minutes\")\n",
    "print(f\"We'll run 8 tests to find when autoscaling triggers:\")\n",
    "print(f\"  - 4 burst tests (fast)\")\n",
    "print(f\"  - 3 sustained tests (5 mins each)\")\n",
    "print(f\"  - 1 ramp test (10 mins)\")\n",
    "print(f\"\\n{'='*70}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dzn57o9zsqk",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1/3: Running burst tests (quick capacity checks)...\n",
      "\n",
      "\n",
      "======================================================================\n",
      "Burst - 1,000 requests\n",
      "======================================================================\n",
      "Pattern: BURST\n",
      "Requests: 1,000 | Batch: 1 | Concurrency: 100\n",
      "Total instances: 1,000\n",
      "\n",
      "‚è≥ Running burst test...\n",
      "   Expected duration: 2-5s\n",
      "\n",
      "‚úÖ Complete in 1.7s\n",
      "   Success: 1,000/1,000 (100.0%)\n",
      "   Total Latency:  986.9ms (mean) | 1592.5ms (p95)\n",
      "   Queueing:       826.7ms (mean) | 1467.0ms (p95)\n",
      "   Request:        160.2ms (mean) | 323.6ms (p95)\n",
      "   ‚ö†Ô∏è  Bottleneck: Client-side queueing (84%)\n",
      "\n",
      "======================================================================\n",
      "Burst - 2,000 requests\n",
      "======================================================================\n",
      "Pattern: BURST\n",
      "Requests: 2,000 | Batch: 1 | Concurrency: 200\n",
      "Total instances: 2,000\n",
      "\n",
      "‚è≥ Running burst test...\n",
      "   Expected duration: 2-5s\n",
      "\n",
      "‚úÖ Complete in 3.8s\n",
      "   Success: 2,000/2,000 (100.0%)\n",
      "   Total Latency:  1991.0ms (mean) | 3525.3ms (p95)\n",
      "   Queueing:       1636.1ms (mean) | 3155.6ms (p95)\n",
      "   Request:        354.9ms (mean) | 486.6ms (p95)\n",
      "   ‚ö†Ô∏è  Bottleneck: Client-side queueing (82%)\n",
      "\n",
      "======================================================================\n",
      "Burst - 5,000 requests\n",
      "======================================================================\n",
      "Pattern: BURST\n",
      "Requests: 5,000 | Batch: 1 | Concurrency: 500\n",
      "Total instances: 5,000\n",
      "\n",
      "‚è≥ Running burst test...\n",
      "   Expected duration: 2-5s\n",
      "\n",
      "‚úÖ Complete in 9.6s\n",
      "   Success: 5,000/5,000 (100.0%)\n",
      "   Total Latency:  5168.5ms (mean) | 9101.9ms (p95)\n",
      "   Queueing:       4254.7ms (mean) | 8265.2ms (p95)\n",
      "   Request:        913.8ms (mean) | 1276.1ms (p95)\n",
      "   ‚ö†Ô∏è  Bottleneck: Client-side queueing (82%)\n",
      "\n",
      "======================================================================\n",
      "Burst - 10,000 requests\n",
      "======================================================================\n",
      "Pattern: BURST\n",
      "Requests: 10,000 | Batch: 1 | Concurrency: 1000\n",
      "Total instances: 10,000\n",
      "\n",
      "‚è≥ Running burst test...\n",
      "   Expected duration: 2-5s\n",
      "\n",
      "‚úÖ Complete in 19.3s\n",
      "   Success: 10,000/10,000 (100.0%)\n",
      "   Total Latency:  10414.5ms (mean) | 17979.9ms (p95)\n",
      "   Queueing:       8605.2ms (mean) | 16295.5ms (p95)\n",
      "   Request:        1809.3ms (mean) | 2715.1ms (p95)\n",
      "   ‚ö†Ô∏è  Bottleneck: Client-side queueing (83%)\n",
      "\n",
      "‚úÖ Burst tests complete: 4 tests run\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Burst Tests\n",
    "print(\"Step 1/3: Running burst tests (quick capacity checks)...\\n\")\n",
    "\n",
    "for test_config in BURST_TESTS:\n",
    "    result = await run_endpoint_test(\n",
    "        endpoint_url=endpoint_url,\n",
    "        credentials=credentials,\n",
    "        auth_req=auth_req,\n",
    "        test_data=test_data,\n",
    "        num_requests=test_config['num_requests'],\n",
    "        batch_size=1,\n",
    "        pattern=\"burst\",\n",
    "        max_concurrent=test_config['max_concurrent'],\n",
    "        test_name=f\"Burst - {test_config['num_requests']:,} requests\"\n",
    "    )\n",
    "    phase2a_burst_results.append(result)\n",
    "\n",
    "print(f\"\\n‚úÖ Burst tests complete: {len(BURST_TESTS)} tests run\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0z80g766q4vg",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2/3: Running sustained load tests (these should trigger autoscaling)...\n",
      "‚è≥ This will take approximately 15 minutes (3 tests √ó 5 mins each)\n",
      "\n",
      "\n",
      "======================================================================\n",
      "Sustained - 50 RPS\n",
      "======================================================================\n",
      "Pattern: SUSTAINED\n",
      "Target: 50 RPS √ó 300s = 15,000 requests\n",
      "Batch: 1 | Concurrency: 100\n",
      "\n",
      "‚è≥ Running sustained load test (300s = 5 mins 0s)...\n",
      "   This test will take approximately 5 minutes 0 seconds\n",
      "   Progress updates every 60 seconds...\n",
      "   [ 60s] 3,001 requests sent...\n",
      "   [120s] 6,001 requests sent...\n",
      "   [180s] 9,001 requests sent...\n",
      "   [240s] 12,001 requests sent...\n",
      "   Waiting for all requests to complete...\n",
      "\n",
      "‚úÖ Complete in 300.0s\n",
      "   Success: 15,000/15,000 (100.0%)\n",
      "   Total Latency:  26.9ms (mean) | 29.3ms (p95)\n",
      "   Queueing:       0.0ms (mean) | 0.0ms (p95)\n",
      "   Request:        26.9ms (mean) | 29.3ms (p95)\n",
      "   ‚úÖ Bottleneck: Endpoint processing (100%)\n",
      "\n",
      "======================================================================\n",
      "Sustained - 100 RPS\n",
      "======================================================================\n",
      "Pattern: SUSTAINED\n",
      "Target: 100 RPS √ó 300s = 30,000 requests\n",
      "Batch: 1 | Concurrency: 200\n",
      "\n",
      "‚è≥ Running sustained load test (300s = 5 mins 0s)...\n",
      "   This test will take approximately 5 minutes 0 seconds\n",
      "   Progress updates every 60 seconds...\n",
      "   [ 60s] 6,001 requests sent...\n",
      "   [120s] 12,001 requests sent...\n",
      "   [180s] 18,001 requests sent...\n",
      "   [240s] 24,001 requests sent...\n",
      "   Waiting for all requests to complete...\n",
      "\n",
      "‚úÖ Complete in 300.0s\n",
      "   Success: 30,000/30,000 (100.0%)\n",
      "   Total Latency:  20.8ms (mean) | 24.9ms (p95)\n",
      "   Queueing:       0.0ms (mean) | 0.0ms (p95)\n",
      "   Request:        20.8ms (mean) | 24.9ms (p95)\n",
      "   ‚úÖ Bottleneck: Endpoint processing (100%)\n",
      "\n",
      "======================================================================\n",
      "Sustained - 150 RPS\n",
      "======================================================================\n",
      "Pattern: SUSTAINED\n",
      "Target: 150 RPS √ó 300s = 45,000 requests\n",
      "Batch: 1 | Concurrency: 200\n",
      "\n",
      "‚è≥ Running sustained load test (300s = 5 mins 0s)...\n",
      "   This test will take approximately 5 minutes 0 seconds\n",
      "   Progress updates every 60 seconds...\n",
      "   [ 60s] 9,001 requests sent...\n",
      "   [120s] 18,001 requests sent...\n",
      "   [180s] 27,001 requests sent...\n",
      "   [240s] 36,001 requests sent...\n",
      "   Waiting for all requests to complete...\n",
      "\n",
      "‚úÖ Complete in 300.1s\n",
      "   Success: 45,000/45,000 (100.0%)\n",
      "   Total Latency:  20.6ms (mean) | 25.5ms (p95)\n",
      "   Queueing:       0.0ms (mean) | 0.0ms (p95)\n",
      "   Request:        20.6ms (mean) | 25.5ms (p95)\n",
      "   ‚úÖ Bottleneck: Endpoint processing (100%)\n",
      "\n",
      "‚úÖ Sustained tests complete: 3 tests run\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Sustained Tests (THIS IS WHERE AUTOSCALING SHOULD TRIGGER)\n",
    "print(\"Step 2/3: Running sustained load tests (these should trigger autoscaling)...\")\n",
    "print(\"‚è≥ This will take approximately 15 minutes (3 tests √ó 5 mins each)\\n\")\n",
    "\n",
    "for test_config in SUSTAINED_TESTS:\n",
    "    result = await run_endpoint_test(\n",
    "        endpoint_url=endpoint_url,\n",
    "        credentials=credentials,\n",
    "        auth_req=auth_req,\n",
    "        test_data=test_data,\n",
    "        num_requests=test_config['target_rps'] * test_config['duration'],\n",
    "        batch_size=1,\n",
    "        pattern=\"sustained\",\n",
    "        target_rps=test_config['target_rps'],\n",
    "        duration=test_config['duration'],\n",
    "        max_concurrent=min(test_config['target_rps'] * 2, 200),\n",
    "        test_name=f\"Sustained - {test_config['target_rps']} RPS\"\n",
    "    )\n",
    "    phase2a_sustained_results.append(result)\n",
    "\n",
    "print(f\"\\n‚úÖ Sustained tests complete: {len(SUSTAINED_TESTS)} tests run\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "52fisbze329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3/3: Running ramp test (gradual increase to find exact trigger)...\n",
      "‚è≥ This will take approximately 10 minutes\n",
      "\n",
      "\n",
      "======================================================================\n",
      "Ramp - 0 ‚Üí 200 RPS\n",
      "======================================================================\n",
      "Pattern: RAMP\n",
      "Ramp: 0 ‚Üí 200 RPS over 600s (10 mins 0s)\n",
      "Batch: 1 | Concurrency: 200\n",
      "\n",
      "‚è≥ Running ramp test...\n",
      "   This test will take approximately 10 minutes 0 seconds\n",
      "   RPS will gradually increase from 0 to 200\n",
      "   Progress updates every 60 seconds...\n",
      "   [ 60s] Current RPS: 20.0 | Requests sent: 453\n",
      "   [120s] Current RPS: 40.0 | Requests sent: 1,527\n",
      "   [180s] Current RPS: 60.0 | Requests sent: 2,958\n",
      "   [294s] Current RPS: 98.1 | Requests sent: 2,996\n",
      "   [354s] Current RPS: 118.0 | Requests sent: 4,852\n",
      "   [414s] Current RPS: 138.0 | Requests sent: 6,834\n",
      "   [474s] Current RPS: 158.0 | Requests sent: 8,974\n",
      "   [534s] Current RPS: 178.0 | Requests sent: 11,189\n",
      "   [594s] Current RPS: 198.0 | Requests sent: 13,335\n",
      "\n",
      "‚úÖ Complete in 600.0s\n",
      "   Success: 13,511/13,511 (100.0%)\n",
      "   Total Latency:  29.3ms (mean) | 25.2ms (p95)\n",
      "   Queueing:       0.0ms (mean) | 0.0ms (p95)\n",
      "   Request:        29.3ms (mean) | 25.2ms (p95)\n",
      "   ‚úÖ Bottleneck: Endpoint processing (100%)\n",
      "\n",
      "‚úÖ Ramp test complete\n",
      "\n",
      "======================================================================\n",
      "PHASE 2A COMPLETE\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Ramp Test (GRADUAL INCREASE TO FIND EXACT TRIGGER POINT)\n",
    "print(\"Step 3/3: Running ramp test (gradual increase to find exact trigger)...\")\n",
    "print(\"‚è≥ This will take approximately 10 minutes\\n\")\n",
    "\n",
    "ramp_result = await run_endpoint_test(\n",
    "    endpoint_url=endpoint_url,\n",
    "    credentials=credentials,\n",
    "    auth_req=auth_req,\n",
    "    test_data=test_data,\n",
    "    num_requests=0,  # Ramp pattern doesn't use this\n",
    "    batch_size=1,\n",
    "    pattern=\"ramp\",\n",
    "    target_rps=RAMP_TEST['target_rps'],\n",
    "    duration=RAMP_TEST['duration'],\n",
    "    max_concurrent=200,\n",
    "    test_name=f\"Ramp - 0 ‚Üí {RAMP_TEST['target_rps']} RPS\"\n",
    ")\n",
    "phase2a_ramp_results.append(ramp_result)\n",
    "\n",
    "print(f\"\\n‚úÖ Ramp test complete\\n\")\n",
    "print(\"=\"*70)\n",
    "print(\"PHASE 2A COMPLETE\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "khapo1kwu6",
   "metadata": {},
   "source": [
    "### Collect Metrics for Phase 2A\n",
    "\n",
    "Now we'll collect endpoint metrics from Cloud Monitoring to see if autoscaling was triggered.\n",
    "\n",
    "‚è≥ **This will wait 90 seconds for metrics to propagate**, then collect data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "l1sw93czs1h",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting metrics for entire Phase 2A\n",
      "Time window: 16:38:28 ‚Üí 17:04:04\n",
      "\n",
      "üîç Collecting endpoint metrics...\n",
      "   Time window: 16:36:28 ‚Üí 17:06:04\n",
      "   Waiting 90 seconds for metrics to propagate...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Collecting CPU utilization...\n",
      "   Collecting memory usage...\n",
      "   Collecting replica count...\n",
      "   Collecting target replica count...\n",
      "   Collecting prediction count...\n",
      "   Collecting response count...\n",
      "   Collecting prediction latencies...\n",
      "   Collecting error count...\n",
      "‚úÖ Metrics collection complete\n",
      "   cpu: 46 data points\n",
      "   memory: 47 data points\n",
      "   replicas: 29 data points\n",
      "   target_replicas: 29 data points\n",
      "   predictions: 163 data points\n",
      "   responses: 152 data points\n",
      "   latency: 467 data points\n",
      "   errors: 0 data points\n",
      "\n",
      "‚úÖ Metrics collected for Phase 2A\n"
     ]
    }
   ],
   "source": [
    "# Collect metrics for all Phase 2A tests\n",
    "# We'll collect metrics for the entire Phase 2A time window\n",
    "phase2a_all_results = phase2a_burst_results + phase2a_sustained_results + phase2a_ramp_results\n",
    "\n",
    "# Find overall time window\n",
    "phase2a_start = min([r['start_time'] for r in phase2a_all_results])\n",
    "phase2a_end = max([r['end_time'] for r in phase2a_all_results])\n",
    "\n",
    "print(f\"Collecting metrics for entire Phase 2A\")\n",
    "print(f\"Time window: {phase2a_start.strftime('%H:%M:%S')} ‚Üí {phase2a_end.strftime('%H:%M:%S')}\")\n",
    "\n",
    "# Collect metrics\n",
    "phase2a_metrics = metrics_collector.collect_metrics(\n",
    "    start_time=phase2a_start,\n",
    "    end_time=phase2a_end,\n",
    "    resolution_seconds=10\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Metrics collected for Phase 2A\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "o5oi4n5lpgq",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "AUTOSCALING ANALYSIS\n",
      "======================================================================\n",
      "\n",
      "‚úÖ Autoscaling was triggered 1 time(s)!\n",
      "\n",
      "Event 1:\n",
      "  Trigger Time:     16:37:04\n",
      "  Scale Complete:   16:41:24\n",
      "  CPU at Trigger:   0.0%\n",
      "  Replicas:         1 ‚Üí 2\n",
      "  Scale-up Lag:     260 seconds (4.3 mins)\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Detect autoscaling events\n",
    "autoscaling_events = metrics_collector.analyze_autoscaling(phase2a_metrics)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"AUTOSCALING ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "if len(autoscaling_events) > 0:\n",
    "    print(f\"\\n‚úÖ Autoscaling was triggered {len(autoscaling_events)} time(s)!\\n\")\n",
    "    \n",
    "    for idx, event in autoscaling_events.iterrows():\n",
    "        print(f\"Event {idx + 1}:\")\n",
    "        print(f\"  Trigger Time:     {event['trigger_time'].strftime('%H:%M:%S')}\")\n",
    "        print(f\"  Scale Complete:   {event['scale_complete_time'].strftime('%H:%M:%S')}\")\n",
    "        print(f\"  CPU at Trigger:   {event['cpu_at_trigger']:.1f}%\")\n",
    "        print(f\"  Replicas:         {event['replicas_before']:.0f} ‚Üí {event['replicas_after']:.0f}\")\n",
    "        print(f\"  Scale-up Lag:     {event['scale_up_lag_seconds']:.0f} seconds ({event['scale_up_lag_seconds']/60:.1f} mins)\")\n",
    "        print()\n",
    "else:\n",
    "    print(f\"\\n‚ùå No autoscaling detected\")\n",
    "    print(f\"\\nPossible reasons:\")\n",
    "    print(f\"  - CPU never exceeded 60% threshold\")\n",
    "    print(f\"  - Model is too CPU-efficient for default threshold\")\n",
    "    print(f\"  - Load duration too short (need sustained load)\")\n",
    "    \n",
    "    if 'cpu' in phase2a_metrics and len(phase2a_metrics['cpu']) > 0:\n",
    "        max_cpu = phase2a_metrics['cpu']['value'].max() * 100\n",
    "        print(f\"\\n  Max CPU observed: {max_cpu:.1f}%\")\n",
    "        if max_cpu < 60:\n",
    "            print(f\"  ‚ö†Ô∏è  CPU stayed below 60% threshold\")\n",
    "            print(f\"  üí° Recommendation: Increase min_replicas or lower autoscaling threshold\")\n",
    "\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "otrfzajzo2t",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating comprehensive Phase 2A visualization...\n",
      "Time span: 16:38:28 ‚Üí 17:04:04\n",
      "This covers all 4 burst tests, 3 sustained tests, and 1 ramp test\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31117/1182514588.py:7: FutureWarning:\n",
      "\n",
      "The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "line": {
          "color": "blue",
          "width": 2
         },
         "mode": "lines",
         "name": "Request Rate",
         "type": "scatter",
         "x": [
          "2025-11-11T16:38:20",
          "2025-11-11T16:38:30",
          "2025-11-11T16:38:40",
          "2025-11-11T16:38:50",
          "2025-11-11T16:39:00",
          "2025-11-11T16:39:10",
          "2025-11-11T16:39:20",
          "2025-11-11T16:39:30",
          "2025-11-11T16:39:40",
          "2025-11-11T16:39:50",
          "2025-11-11T16:40:00",
          "2025-11-11T16:40:10",
          "2025-11-11T16:40:20",
          "2025-11-11T16:40:30",
          "2025-11-11T16:40:40",
          "2025-11-11T16:40:50",
          "2025-11-11T16:41:00",
          "2025-11-11T16:41:10",
          "2025-11-11T16:41:20",
          "2025-11-11T16:41:30",
          "2025-11-11T16:41:40",
          "2025-11-11T16:41:50",
          "2025-11-11T16:42:00",
          "2025-11-11T16:42:10",
          "2025-11-11T16:42:20",
          "2025-11-11T16:42:30",
          "2025-11-11T16:42:40",
          "2025-11-11T16:42:50",
          "2025-11-11T16:43:00",
          "2025-11-11T16:43:10",
          "2025-11-11T16:43:20",
          "2025-11-11T16:43:30",
          "2025-11-11T16:43:40",
          "2025-11-11T16:43:50",
          "2025-11-11T16:44:00",
          "2025-11-11T16:44:10",
          "2025-11-11T16:44:20",
          "2025-11-11T16:44:30",
          "2025-11-11T16:44:40",
          "2025-11-11T16:44:50",
          "2025-11-11T16:45:00",
          "2025-11-11T16:45:10",
          "2025-11-11T16:45:20",
          "2025-11-11T16:45:30",
          "2025-11-11T16:45:40",
          "2025-11-11T16:45:50",
          "2025-11-11T16:46:00",
          "2025-11-11T16:46:10",
          "2025-11-11T16:46:20",
          "2025-11-11T16:46:30",
          "2025-11-11T16:46:40",
          "2025-11-11T16:46:50",
          "2025-11-11T16:47:00",
          "2025-11-11T16:47:10",
          "2025-11-11T16:47:20",
          "2025-11-11T16:47:30",
          "2025-11-11T16:47:40",
          "2025-11-11T16:47:50",
          "2025-11-11T16:48:00",
          "2025-11-11T16:48:10",
          "2025-11-11T16:48:20",
          "2025-11-11T16:48:30",
          "2025-11-11T16:48:40",
          "2025-11-11T16:48:50",
          "2025-11-11T16:49:00",
          "2025-11-11T16:49:10",
          "2025-11-11T16:49:20",
          "2025-11-11T16:49:30",
          "2025-11-11T16:49:40",
          "2025-11-11T16:49:50",
          "2025-11-11T16:50:00",
          "2025-11-11T16:50:10",
          "2025-11-11T16:50:20",
          "2025-11-11T16:50:30",
          "2025-11-11T16:50:40",
          "2025-11-11T16:50:50",
          "2025-11-11T16:51:00",
          "2025-11-11T16:51:10",
          "2025-11-11T16:51:20",
          "2025-11-11T16:51:30",
          "2025-11-11T16:51:40",
          "2025-11-11T16:51:50",
          "2025-11-11T16:52:00",
          "2025-11-11T16:52:10",
          "2025-11-11T16:52:20",
          "2025-11-11T16:52:30",
          "2025-11-11T16:52:40",
          "2025-11-11T16:52:50",
          "2025-11-11T16:53:00",
          "2025-11-11T16:53:10",
          "2025-11-11T16:53:20",
          "2025-11-11T16:53:30",
          "2025-11-11T16:53:40",
          "2025-11-11T16:53:50",
          "2025-11-11T16:54:00",
          "2025-11-11T16:54:10",
          "2025-11-11T16:54:20",
          "2025-11-11T16:54:30",
          "2025-11-11T16:54:40",
          "2025-11-11T16:54:50",
          "2025-11-11T16:55:00",
          "2025-11-11T16:55:10",
          "2025-11-11T16:55:20",
          "2025-11-11T16:55:30",
          "2025-11-11T16:55:40",
          "2025-11-11T16:55:50",
          "2025-11-11T16:56:00",
          "2025-11-11T16:56:10",
          "2025-11-11T16:56:20",
          "2025-11-11T16:56:30",
          "2025-11-11T16:56:40",
          "2025-11-11T16:56:50",
          "2025-11-11T16:57:00",
          "2025-11-11T16:58:50",
          "2025-11-11T16:59:00",
          "2025-11-11T16:59:10",
          "2025-11-11T16:59:20",
          "2025-11-11T16:59:30",
          "2025-11-11T16:59:40",
          "2025-11-11T16:59:50",
          "2025-11-11T17:00:00",
          "2025-11-11T17:00:10",
          "2025-11-11T17:00:20",
          "2025-11-11T17:00:30",
          "2025-11-11T17:00:40",
          "2025-11-11T17:00:50",
          "2025-11-11T17:01:00",
          "2025-11-11T17:01:10",
          "2025-11-11T17:01:20",
          "2025-11-11T17:01:30",
          "2025-11-11T17:01:40",
          "2025-11-11T17:01:50",
          "2025-11-11T17:02:00",
          "2025-11-11T17:02:10",
          "2025-11-11T17:02:20",
          "2025-11-11T17:02:30",
          "2025-11-11T17:02:40",
          "2025-11-11T17:02:50",
          "2025-11-11T17:03:00",
          "2025-11-11T17:03:10",
          "2025-11-11T17:03:20",
          "2025-11-11T17:03:30",
          "2025-11-11T17:03:40",
          "2025-11-11T17:03:50",
          "2025-11-11T17:04:00"
         ],
         "xaxis": "x",
         "y": {
          "bdata": "ZmZmZmbGVUAzMzMzMzOBQDMzMzMza35AZmZmZmZmgkCamZmZmfldQAAAAAAAAElAAAAAAAAASUAAAAAAAABJQAAAAAAAAElAAAAAAAAASUAAAAAAAABJQAAAAAAAAElAAAAAAAAASUAAAAAAAABJQAAAAAAAAElAAAAAAAAASUAAAAAAAABJQAAAAAAAAElAAAAAAAAASUAAAAAAAABJQAAAAAAAAElAAAAAAAAASUAAAAAAAABJQAAAAAAAAElAAAAAAAAASUAAAAAAAABJQAAAAAAAAElAAAAAAAAASUAAAAAAAABJQAAAAAAAAElAAAAAAAAASUAAAAAAAABJQAAAAAAAAElAAAAAAAAASUDNzMzMzExUQAAAAAAAAFlAZmZmZmYGWUCamZmZmflYQAAAAAAAAFlAAAAAAAAAWUAAAAAAAABZQAAAAAAAAFlAAAAAAAAAWUAAAAAAAABZQAAAAAAAAFlAAAAAAAAAWUAAAAAAAABZQAAAAAAAAFlAZmZmZmYGWUCamZmZmflYQAAAAAAAAFlAAAAAAAAAWUAAAAAAAABZQAAAAAAAAFlAAAAAAAAAWUAAAAAAAABZQAAAAAAAAFlAAAAAAAAAWUAAAAAAAABZQAAAAAAAAFlAZmZmZmYGWUCamZmZmflYQAAAAAAAAFlAAAAAAAAAWUBmZmZmZgZgQAAAAAAAwGJAAAAAAADAYkAAAAAAAMBiQAAAAAAAwGJAAAAAAADAYkAAAAAAAMBiQAAAAAAAwGJAAAAAAADAYkAAAAAAAMBiQAAAAAAAwGJAAAAAAADAYkAAAAAAAMBiQAAAAAAAwGJAAAAAAADAYkAAAAAAAMBiQAAAAAAAwGJAAAAAAADAYkAAAAAAAMBiQAAAAAAAwGJAAAAAAADAYkAAAAAAAMBiQAAAAAAAwGJAAAAAAADAYkAAAAAAAMBiQAAAAAAAwGJAAAAAAADAYkAAAAAAAMBiQAAAAAAAwGJAAAAAAADAYkBmZmZmZuZMQJqZmZmZmQFAmpmZmZmZF0AAAAAAAAAhQM3MzMzMzCRAMzMzMzMzKUAzMzMzMzMrQAAAAAAAADBAmpmZmZmZMEAAAAAAAIAyQJqZmZmZGTRAmpmZmZmZM0AzMzMzMzM0QJqZmZmZGTZAzczMzMxMN0AAAAAAAIA4QJqZmZmZGTlAmpmZmZmZOUAzMzMzMzMtQGZmZmZmZgZAzczMzMzMOkBmZmZmZuY/QDMzMzMzsz9AZmZmZmbmP0CamZmZmdlAQJqZmZmZWUBAmpmZmZkZPUAAAAAAAABAQAAAAAAAwEBAAAAAAABAQUAzMzMzM3NBQM3MzMzMDEFAmpmZmZkZQUBmZmZmZmZBQM3MzMzMjEFAZmZmZmamQkCamZmZmRlCQGZmZmZmZkJAMzMzMzOzQUDNzMzMzIxBQAAAAAAAAEJAMzMzMzPzQkCamZmZmVlDQDMzMzMzM0NAAAAAAACAPUBmZmZmZmZAQDMzMzMzM0NAMzMzMzNzQ0AzMzMzM7NCQM3MzMzMDENAAAAAAAAAJkA=",
          "dtype": "f8"
         },
         "yaxis": "y"
        },
        {
         "line": {
          "color": "red",
          "width": 2
         },
         "mode": "lines",
         "name": "CPU %",
         "type": "scatter",
         "x": [
          "2025-11-11T16:37:04",
          "2025-11-11T16:38:04",
          "2025-11-11T16:39:04",
          "2025-11-11T16:40:04",
          "2025-11-11T16:41:04",
          "2025-11-11T16:42:04",
          "2025-11-11T16:42:04",
          "2025-11-11T16:43:04",
          "2025-11-11T16:43:04",
          "2025-11-11T16:44:04",
          "2025-11-11T16:44:04",
          "2025-11-11T16:45:04",
          "2025-11-11T16:45:04",
          "2025-11-11T16:46:04",
          "2025-11-11T16:46:04",
          "2025-11-11T16:47:04",
          "2025-11-11T16:47:04",
          "2025-11-11T16:48:04",
          "2025-11-11T16:48:04",
          "2025-11-11T16:49:04",
          "2025-11-11T16:49:04",
          "2025-11-11T16:50:04",
          "2025-11-11T16:50:04",
          "2025-11-11T16:51:04",
          "2025-11-11T16:51:04",
          "2025-11-11T16:52:04",
          "2025-11-11T16:52:04",
          "2025-11-11T16:53:04",
          "2025-11-11T16:53:04",
          "2025-11-11T16:54:04",
          "2025-11-11T16:54:04",
          "2025-11-11T16:55:04",
          "2025-11-11T16:55:04",
          "2025-11-11T16:56:04",
          "2025-11-11T16:56:04",
          "2025-11-11T16:57:04",
          "2025-11-11T16:57:04",
          "2025-11-11T16:58:04",
          "2025-11-11T16:59:04",
          "2025-11-11T17:00:04",
          "2025-11-11T17:01:04",
          "2025-11-11T17:02:04",
          "2025-11-11T17:03:04",
          "2025-11-11T17:04:04",
          "2025-11-11T17:05:04",
          "2025-11-11T17:06:04"
         ],
         "xaxis": "x2",
         "y": {
          "bdata": "AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA=",
          "dtype": "f8"
         },
         "yaxis": "y2"
        },
        {
         "line": {
          "color": "green",
          "width": 2
         },
         "marker": {
          "size": 8
         },
         "mode": "lines+markers",
         "name": "Actual Replicas",
         "type": "scatter",
         "x": [
          "2025-11-11T16:37:24",
          "2025-11-11T16:38:24",
          "2025-11-11T16:39:24",
          "2025-11-11T16:40:24",
          "2025-11-11T16:41:24",
          "2025-11-11T16:42:24",
          "2025-11-11T16:43:24",
          "2025-11-11T16:44:24",
          "2025-11-11T16:45:24",
          "2025-11-11T16:46:24",
          "2025-11-11T16:47:24",
          "2025-11-11T16:48:24",
          "2025-11-11T16:49:24",
          "2025-11-11T16:50:24",
          "2025-11-11T16:51:24",
          "2025-11-11T16:52:24",
          "2025-11-11T16:53:24",
          "2025-11-11T16:54:24",
          "2025-11-11T16:55:24",
          "2025-11-11T16:56:24",
          "2025-11-11T16:57:24",
          "2025-11-11T16:58:24",
          "2025-11-11T16:59:24",
          "2025-11-11T17:00:24",
          "2025-11-11T17:01:24",
          "2025-11-11T17:02:24",
          "2025-11-11T17:03:24",
          "2025-11-11T17:04:24",
          "2025-11-11T17:05:24"
         ],
         "xaxis": "x3",
         "y": {
          "bdata": "AAAAAAAA8D8AAAAAAADwPwAAAAAAAPA/AAAAAAAA8D8AAAAAAAAAQAAAAAAAAABAAAAAAAAAAEAAAAAAAAAAQAAAAAAAAABAAAAAAAAA8D8AAAAAAADwPwAAAAAAAPA/AAAAAAAA8D8AAAAAAADwPwAAAAAAAPA/AAAAAAAA8D8AAAAAAADwPwAAAAAAAPA/AAAAAAAA8D8AAAAAAADwPwAAAAAAAPA/AAAAAAAA8D8AAAAAAADwPwAAAAAAAPA/AAAAAAAA8D8AAAAAAADwPwAAAAAAAPA/AAAAAAAA8D8AAAAAAADwPw==",
          "dtype": "f8"
         },
         "yaxis": "y3"
        },
        {
         "line": {
          "color": "lightgreen",
          "dash": "dash",
          "width": 2
         },
         "mode": "lines",
         "name": "Target Replicas",
         "type": "scatter",
         "x": [
          "2025-11-11T16:37:24",
          "2025-11-11T16:38:24",
          "2025-11-11T16:39:24",
          "2025-11-11T16:40:24",
          "2025-11-11T16:41:24",
          "2025-11-11T16:42:24",
          "2025-11-11T16:43:24",
          "2025-11-11T16:44:24",
          "2025-11-11T16:45:24",
          "2025-11-11T16:46:24",
          "2025-11-11T16:47:24",
          "2025-11-11T16:48:24",
          "2025-11-11T16:49:24",
          "2025-11-11T16:50:24",
          "2025-11-11T16:51:24",
          "2025-11-11T16:52:24",
          "2025-11-11T16:53:24",
          "2025-11-11T16:54:24",
          "2025-11-11T16:55:24",
          "2025-11-11T16:56:24",
          "2025-11-11T16:57:24",
          "2025-11-11T16:58:24",
          "2025-11-11T16:59:24",
          "2025-11-11T17:00:24",
          "2025-11-11T17:01:24",
          "2025-11-11T17:02:24",
          "2025-11-11T17:03:24",
          "2025-11-11T17:04:24",
          "2025-11-11T17:05:24"
         ],
         "xaxis": "x3",
         "y": {
          "bdata": "AAAAAAAA8D8AAAAAAADwPwAAAAAAAABAAAAAAAAAAEAAAAAAAAAAQAAAAAAAAABAAAAAAAAAAEAAAAAAAAAAQAAAAAAAAABAAAAAAAAA8D8AAAAAAADwPwAAAAAAAPA/AAAAAAAA8D8AAAAAAADwPwAAAAAAAPA/AAAAAAAA8D8AAAAAAADwPwAAAAAAAPA/AAAAAAAA8D8AAAAAAADwPwAAAAAAAPA/AAAAAAAA8D8AAAAAAADwPwAAAAAAAPA/AAAAAAAA8D8AAAAAAADwPwAAAAAAAPA/AAAAAAAA8D8AAAAAAADwPw==",
          "dtype": "f8"
         },
         "yaxis": "y3"
        },
        {
         "line": {
          "color": "purple",
          "width": 2
         },
         "mode": "lines",
         "name": "P95 Latency",
         "type": "scatter",
         "x": [
          "2025-11-11T16:38:20",
          "2025-11-11T16:38:30",
          "2025-11-11T16:38:40",
          "2025-11-11T16:38:50",
          "2025-11-11T16:39:00",
          "2025-11-11T16:39:10",
          "2025-11-11T16:39:20",
          "2025-11-11T16:39:30",
          "2025-11-11T16:39:40",
          "2025-11-11T16:39:50",
          "2025-11-11T16:40:00",
          "2025-11-11T16:40:10",
          "2025-11-11T16:40:20",
          "2025-11-11T16:40:30",
          "2025-11-11T16:40:40",
          "2025-11-11T16:40:50",
          "2025-11-11T16:41:00",
          "2025-11-11T16:41:10",
          "2025-11-11T16:41:20",
          "2025-11-11T16:41:30",
          "2025-11-11T16:41:40",
          "2025-11-11T16:41:50",
          "2025-11-11T16:42:00",
          "2025-11-11T16:42:10",
          "2025-11-11T16:42:20",
          "2025-11-11T16:42:30",
          "2025-11-11T16:42:40",
          "2025-11-11T16:42:50",
          "2025-11-11T16:43:00",
          "2025-11-11T16:43:10",
          "2025-11-11T16:43:20",
          "2025-11-11T16:43:30",
          "2025-11-11T16:43:40",
          "2025-11-11T16:43:50",
          "2025-11-11T16:44:00",
          "2025-11-11T16:44:10",
          "2025-11-11T16:44:20",
          "2025-11-11T16:44:30",
          "2025-11-11T16:44:40",
          "2025-11-11T16:44:50",
          "2025-11-11T16:45:00",
          "2025-11-11T16:45:10",
          "2025-11-11T16:45:20",
          "2025-11-11T16:45:30",
          "2025-11-11T16:45:40",
          "2025-11-11T16:45:50",
          "2025-11-11T16:46:00",
          "2025-11-11T16:46:10",
          "2025-11-11T16:46:20",
          "2025-11-11T16:46:30",
          "2025-11-11T16:46:40",
          "2025-11-11T16:46:50",
          "2025-11-11T16:47:00",
          "2025-11-11T16:47:10",
          "2025-11-11T16:47:20",
          "2025-11-11T16:47:30",
          "2025-11-11T16:47:40",
          "2025-11-11T16:47:50",
          "2025-11-11T16:48:00",
          "2025-11-11T16:48:10",
          "2025-11-11T16:48:20",
          "2025-11-11T16:48:30",
          "2025-11-11T16:48:40",
          "2025-11-11T16:48:50",
          "2025-11-11T16:49:00",
          "2025-11-11T16:49:10",
          "2025-11-11T16:49:20",
          "2025-11-11T16:49:30",
          "2025-11-11T16:49:40",
          "2025-11-11T16:49:50",
          "2025-11-11T16:50:00",
          "2025-11-11T16:50:10",
          "2025-11-11T16:50:20",
          "2025-11-11T16:50:30",
          "2025-11-11T16:50:40",
          "2025-11-11T16:50:50",
          "2025-11-11T16:51:00",
          "2025-11-11T16:51:10",
          "2025-11-11T16:51:20",
          "2025-11-11T16:51:30",
          "2025-11-11T16:51:40",
          "2025-11-11T16:51:50",
          "2025-11-11T16:52:00",
          "2025-11-11T16:52:10",
          "2025-11-11T16:52:20",
          "2025-11-11T16:52:30",
          "2025-11-11T16:52:40",
          "2025-11-11T16:52:50",
          "2025-11-11T16:53:00",
          "2025-11-11T16:53:10",
          "2025-11-11T16:53:20",
          "2025-11-11T16:53:30",
          "2025-11-11T16:53:40",
          "2025-11-11T16:53:50",
          "2025-11-11T16:54:00",
          "2025-11-11T16:54:10",
          "2025-11-11T16:54:20",
          "2025-11-11T16:54:30",
          "2025-11-11T16:54:40",
          "2025-11-11T16:54:50",
          "2025-11-11T16:55:00",
          "2025-11-11T16:55:10",
          "2025-11-11T16:55:20",
          "2025-11-11T16:55:30",
          "2025-11-11T16:55:40",
          "2025-11-11T16:55:50",
          "2025-11-11T16:56:00",
          "2025-11-11T16:56:10",
          "2025-11-11T16:56:20",
          "2025-11-11T16:56:30",
          "2025-11-11T16:56:40",
          "2025-11-11T16:56:50",
          "2025-11-11T16:57:00",
          "2025-11-11T16:58:50",
          "2025-11-11T16:59:00",
          "2025-11-11T16:59:10",
          "2025-11-11T16:59:20",
          "2025-11-11T16:59:30",
          "2025-11-11T16:59:40",
          "2025-11-11T16:59:50",
          "2025-11-11T17:00:00",
          "2025-11-11T17:00:10",
          "2025-11-11T17:00:20",
          "2025-11-11T17:00:30",
          "2025-11-11T17:00:40",
          "2025-11-11T17:00:50",
          "2025-11-11T17:01:00",
          "2025-11-11T17:01:10",
          "2025-11-11T17:01:20",
          "2025-11-11T17:01:30",
          "2025-11-11T17:01:40",
          "2025-11-11T17:01:50",
          "2025-11-11T17:02:00",
          "2025-11-11T17:02:10",
          "2025-11-11T17:02:20",
          "2025-11-11T17:02:30",
          "2025-11-11T17:02:40",
          "2025-11-11T17:02:50",
          "2025-11-11T17:03:00",
          "2025-11-11T17:03:10",
          "2025-11-11T17:03:20",
          "2025-11-11T17:03:30",
          "2025-11-11T17:03:40",
          "2025-11-11T17:03:50",
          "2025-11-11T17:04:00"
         ],
         "xaxis": "x4",
         "y": {
          "bdata": "AADA5+N2lkD+//f/cOO3QAAAKEMkzcFAAAB4rGl/0EAAADikrEHSQAAAAAhorzdA6///d7o/P0AAAAD4GL43QP3//4eJxTlA/v//F5dlOEDt//9L8JtDQP///0/0TThA/P//D2zPOED9//9PhmM2QAAAAFg+0DZA/f///8eJN0D6//8TkjhCQPj/P2VdJpNA/v//twJ5PED///8fheg3QP///48tJjdA/P///4QOPEC+///HUadGQOL//+dehj5A////T9tLQED9//8fG3k5QP7//y/G0zdA////P8lpOUDx//95+fRZQN///83/HlhA+v//x3zePUD5//8PULc6QAAAADjGWTZA+///d9jCOUD4//9rKblVQAAAABBZwTZAAAAAAJtrN0AAAADQjcw2QP///4+F9jZAAAAASBCbNkD///9nIL05QAAAALCXADdA/////+ZWN0D/////zGs3QP7//9flbjlA////n2zLOEAAAABQYL9HQP7//7/emTdAAAAAwAcrN0D+//9/ETk3QP///9ekKzZA/v//1znXNkD///+fxIk7QAAAAJhmnThA////j7dUOEAAAACwjd85QP///y/EnzZA/v//b2hQOED+///3KLRbQP7//wdmuVRAAAAAoFfhN0AAAACQNeI2QAAAABgIxTVA////F3C2NkABAAC0k1lYQP///3+KuThAAAAAEM+4N0D///+f6iY3QPf//09+FkFA////N560OkAAAAAQ/ro9QP///39z+TdA/v//t1jJNkAAAABQ1zA2QP///6ceMjZA////N739NkD///8/Zhw6QP7//ydn6jdAAAAASCn/NkAAAADQrvI1QP3//48r1DpAAAAASOH+OED9//9n/KM6QAAAABgn8DZAAAAA4IWNNkAAAAD4jL81QP///6/fijdA/v//lzvkNUD6///hzP9UQP///7OgQGBA////D/9+N0D+//9nb3s2QP///zeowjZA////hw1rOEAAAAARYJtmQAAAAFgKK0NA/P//p15WQUAAAACAM/k2QP///6/yvTdAAAAAkIj8N0AAAACQ3MQ+QAAAAECM/jZAAAAA4IlGOED+//8fYsY2QAAAAID9azZAAAAAoEcrOkAAAABIx+c3QAAAAEBI3TxA////P8S/OEAAAABA6ao3QAAAAECAwjhAAAAA6OdKN0AAAAAAHJU3QP///++xUk9A8f//KxP9SED///8fM7Q7QAAAAIAduzpA////X4I1OkAAAAAge/I3QP3//48jNzpAAAAAYKGLOEABAABoqzM4QAEAAJBBajlAAAAAwG+AOED+//9/NZ02QAAAAOBcmDZAAAAAuCEZN0AAAAAgU/s4QAAAALAG+zZAAAAAAHHcNkAAAABghAQ3QP3//9egDzhA////p3hDOUAAAACg0C85QAAAAIiMOzlA/v//X3E1N0AAAABA3eo1QP3//8e2pTdAAwAAoMTTV0Dz//+vP+A7QAAAAMDAvTZAAAAAYP3bNkD///+XiHw3QAAAAIAuPTlAAAAANB+PXUA=",
          "dtype": "f8"
         },
         "yaxis": "y4"
        }
       ],
       "layout": {
        "annotations": [
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Request Rate (Client Perspective)",
          "x": 0.5,
          "xanchor": "center",
          "xref": "paper",
          "y": 1,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "CPU Utilization (Service Perspective)",
          "x": 0.5,
          "xanchor": "center",
          "xref": "paper",
          "y": 0.73,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Replica Count (Autoscaling)",
          "x": 0.5,
          "xanchor": "center",
          "xref": "paper",
          "y": 0.46,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "P95 Latency (User Experience)",
          "x": 0.5,
          "xanchor": "center",
          "xref": "paper",
          "y": 0.19,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "showarrow": false,
          "text": "Autoscale Threshold",
          "x": 1,
          "xanchor": "right",
          "xref": "x2 domain",
          "y": 60,
          "yanchor": "bottom",
          "yref": "y2"
         },
         {
          "font": {
           "color": "red",
           "size": 10
          },
          "showarrow": false,
          "text": "Burst ‚Üí Sustained",
          "textangle": -90,
          "x": "2025-11-11T16:39:03.344203",
          "xref": "x",
          "y": 1.05,
          "yref": "y"
         },
         {
          "font": {
           "color": "green",
           "size": 10
          },
          "showarrow": false,
          "text": "Sustained ‚Üí Ramp",
          "textangle": -90,
          "x": "2025-11-11T16:54:03.916214",
          "xref": "x",
          "y": 1.05,
          "yref": "y"
         }
        ],
        "height": 1000,
        "shapes": [
         {
          "line": {
           "color": "orange",
           "dash": "dash"
          },
          "type": "line",
          "x0": 0,
          "x1": 1,
          "xref": "x2 domain",
          "y0": 60,
          "y1": 60,
          "yref": "y2"
         },
         {
          "line": {
           "color": "rgba(255, 0, 0, 0.5)",
           "dash": "dash",
           "width": 2
          },
          "type": "line",
          "x0": "2025-11-11T16:39:03.344203",
          "x1": "2025-11-11T16:39:03.344203",
          "xref": "x",
          "y0": 0,
          "y1": 1,
          "yref": "y"
         },
         {
          "line": {
           "color": "rgba(0, 128, 0, 0.5)",
           "dash": "dash",
           "width": 2
          },
          "type": "line",
          "x0": "2025-11-11T16:54:03.916214",
          "x1": "2025-11-11T16:54:03.916214",
          "xref": "x",
          "y0": 0,
          "y1": 1,
          "yref": "y"
         },
         {
          "line": {
           "color": "rgba(255, 0, 0, 0.5)",
           "dash": "dash",
           "width": 2
          },
          "type": "line",
          "x0": "2025-11-11T16:39:03.344203",
          "x1": "2025-11-11T16:39:03.344203",
          "xref": "x2",
          "y0": 0,
          "y1": 1,
          "yref": "y2"
         },
         {
          "line": {
           "color": "rgba(0, 128, 0, 0.5)",
           "dash": "dash",
           "width": 2
          },
          "type": "line",
          "x0": "2025-11-11T16:54:03.916214",
          "x1": "2025-11-11T16:54:03.916214",
          "xref": "x2",
          "y0": 0,
          "y1": 1,
          "yref": "y2"
         },
         {
          "line": {
           "color": "rgba(255, 0, 0, 0.5)",
           "dash": "dash",
           "width": 2
          },
          "type": "line",
          "x0": "2025-11-11T16:39:03.344203",
          "x1": "2025-11-11T16:39:03.344203",
          "xref": "x3",
          "y0": 0,
          "y1": 1,
          "yref": "y3"
         },
         {
          "line": {
           "color": "rgba(0, 128, 0, 0.5)",
           "dash": "dash",
           "width": 2
          },
          "type": "line",
          "x0": "2025-11-11T16:54:03.916214",
          "x1": "2025-11-11T16:54:03.916214",
          "xref": "x3",
          "y0": 0,
          "y1": 1,
          "yref": "y3"
         },
         {
          "line": {
           "color": "rgba(255, 0, 0, 0.5)",
           "dash": "dash",
           "width": 2
          },
          "type": "line",
          "x0": "2025-11-11T16:39:03.344203",
          "x1": "2025-11-11T16:39:03.344203",
          "xref": "x4",
          "y0": 0,
          "y1": 1,
          "yref": "y4"
         },
         {
          "line": {
           "color": "rgba(0, 128, 0, 0.5)",
           "dash": "dash",
           "width": 2
          },
          "type": "line",
          "x0": "2025-11-11T16:54:03.916214",
          "x1": "2025-11-11T16:54:03.916214",
          "xref": "x4",
          "y0": 0,
          "y1": 1,
          "yref": "y4"
         }
        ],
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermap": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermap"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Phase 2A: Complete Timeline - All Tests - Complete Timeline Analysis"
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "matches": "x4",
         "showticklabels": false
        },
        "xaxis2": {
         "anchor": "y2",
         "domain": [
          0,
          1
         ],
         "matches": "x4",
         "showticklabels": false
        },
        "xaxis3": {
         "anchor": "y3",
         "domain": [
          0,
          1
         ],
         "matches": "x4",
         "showticklabels": false
        },
        "xaxis4": {
         "anchor": "y4",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Time"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0.81,
          1
         ],
         "title": {
          "text": "RPS"
         }
        },
        "yaxis2": {
         "anchor": "x2",
         "domain": [
          0.54,
          0.73
         ],
         "title": {
          "text": "CPU %"
         }
        },
        "yaxis3": {
         "anchor": "x3",
         "domain": [
          0.27,
          0.46
         ],
         "title": {
          "text": "Count"
         }
        },
        "yaxis4": {
         "anchor": "x4",
         "domain": [
          0,
          0.19
         ],
         "title": {
          "text": "ms"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä This visualization shows:\n",
      "  - All Phase 2A tests from 16:38:28 to 17:04:04\n",
      "  - Request rate across all 8 tests\n",
      "  - CPU utilization (max ~22%)\n",
      "  - Replica scaling: 1 ‚Üí 2 @ 16:41 (during burst tests), back to 1 @ 16:45\n",
      "  - P95 latency throughout all tests\n",
      "  - Red line: Burst ‚Üí Sustained | Green line: Sustained ‚Üí Ramp\n"
     ]
    }
   ],
   "source": [
    "# Visualize Complete Phase 2A Timeline (shows all tests and autoscaling event)\n",
    "print(\"Creating comprehensive Phase 2A visualization...\")\n",
    "print(f\"Time span: {phase2a_start.strftime('%H:%M:%S')} ‚Üí {phase2a_end.strftime('%H:%M:%S')}\")\n",
    "print(f\"This covers all 4 burst tests, 3 sustained tests, and 1 ramp test\\n\")\n",
    "\n",
    "# Combine ALL test results for the request rate timeline\n",
    "all_test_results = pd.concat([r['results_df'] for r in phase2a_all_results])\n",
    "\n",
    "# Create base visualization with ALL Phase 2A data\n",
    "fig = plot_timeline_with_metrics(\n",
    "    test_results=all_test_results,\n",
    "    metrics=phase2a_metrics,  # Already collected for full Phase 2A window\n",
    "    test_name=\"Phase 2A: Complete Timeline - All Tests\"\n",
    ")\n",
    "\n",
    "# Add vertical separators between test phases\n",
    "# Calculate phase boundaries\n",
    "burst_end = phase2a_burst_results[-1]['end_time']\n",
    "sustained_end = phase2a_sustained_results[-1]['end_time']\n",
    "\n",
    "# Add vertical lines to all 4 subplots\n",
    "for row_num in [1, 2, 3, 4]:\n",
    "    # Separator between Burst and Sustained\n",
    "    fig.add_shape(\n",
    "        type=\"line\",\n",
    "        x0=burst_end, x1=burst_end, y0=0, y1=1,\n",
    "        line=dict(color=\"rgba(255, 0, 0, 0.5)\", width=2, dash=\"dash\"),\n",
    "        yref=\"paper\",\n",
    "        row=row_num, col=1\n",
    "    )\n",
    "    \n",
    "    # Separator between Sustained and Ramp\n",
    "    fig.add_shape(\n",
    "        type=\"line\",\n",
    "        x0=sustained_end, x1=sustained_end, y0=0, y1=1,\n",
    "        line=dict(color=\"rgba(0, 128, 0, 0.5)\", width=2, dash=\"dash\"),\n",
    "        yref=\"paper\",\n",
    "        row=row_num, col=1\n",
    "    )\n",
    "\n",
    "# Add annotations only on the top subplot\n",
    "fig.add_annotation(\n",
    "    x=burst_end, y=1.05,\n",
    "    text=\"Burst ‚Üí Sustained\",\n",
    "    showarrow=False,\n",
    "    yref=\"paper\",\n",
    "    xref=\"x\",\n",
    "    textangle=-90,\n",
    "    font=dict(size=10, color=\"red\"),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "fig.add_annotation(\n",
    "    x=sustained_end, y=1.05,\n",
    "    text=\"Sustained ‚Üí Ramp\",\n",
    "    showarrow=False,\n",
    "    yref=\"paper\",\n",
    "    xref=\"x\",\n",
    "    textangle=-90,\n",
    "    font=dict(size=10, color=\"green\"),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "print(\"\\nüìä This visualization shows:\")\n",
    "print(f\"  - All Phase 2A tests from {phase2a_start.strftime('%H:%M:%S')} to {phase2a_end.strftime('%H:%M:%S')}\")\n",
    "print(f\"  - Request rate across all 8 tests\")\n",
    "print(f\"  - CPU utilization (max ~22%)\")\n",
    "print(f\"  - Replica scaling: 1 ‚Üí 2 @ 16:41 (during burst tests), back to 1 @ 16:45\")\n",
    "print(f\"  - P95 latency throughout all tests\")\n",
    "print(f\"  - Red line: Burst ‚Üí Sustained | Green line: Sustained ‚Üí Ramp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75qi0ozd74a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Phase 2B: Concurrency Analysis\n",
    "\n",
    "**Goal**: Find optimal concurrency level (client-side bottleneck analysis)\n",
    "\n",
    "**Tests**: 5 concurrency levels √ó 2000 requests each\n",
    "**Total time**: ~10 minutes\n",
    "\n",
    "‚è≥ This phase tests how different concurrency levels affect performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "pjdlp0cpo98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PHASE 2B: CONCURRENCY ANALYSIS\n",
      "Testing 5 concurrency levels: [50, 100, 200, 500, 1000]\n",
      "\n",
      "\n",
      "======================================================================\n",
      "Concurrency 50\n",
      "======================================================================\n",
      "Pattern: BURST\n",
      "Requests: 2,000 | Batch: 1 | Concurrency: 50\n",
      "Total instances: 2,000\n",
      "\n",
      "‚è≥ Running burst test...\n",
      "   Expected duration: 8-20s\n",
      "\n",
      "‚úÖ Complete in 3.2s\n",
      "   Success: 2,000/2,000 (100.0%)\n",
      "   Total Latency:  1699.1ms (mean) | 3035.0ms (p95)\n",
      "   Queueing:       1620.3ms (mean) | 2966.8ms (p95)\n",
      "   Request:        78.8ms (mean) | 150.3ms (p95)\n",
      "   ‚ö†Ô∏è  Bottleneck: Client-side queueing (95%)\n",
      "\n",
      "======================================================================\n",
      "Concurrency 100\n",
      "======================================================================\n",
      "Pattern: BURST\n",
      "Requests: 2,000 | Batch: 1 | Concurrency: 100\n",
      "Total instances: 2,000\n",
      "\n",
      "‚è≥ Running burst test...\n",
      "   Expected duration: 4-10s\n",
      "\n",
      "‚úÖ Complete in 3.2s\n",
      "   Success: 2,000/2,000 (100.0%)\n",
      "   Total Latency:  1692.1ms (mean) | 3005.4ms (p95)\n",
      "   Queueing:       1537.4ms (mean) | 2861.8ms (p95)\n",
      "   Request:        154.6ms (mean) | 223.1ms (p95)\n",
      "   ‚ö†Ô∏è  Bottleneck: Client-side queueing (91%)\n",
      "\n",
      "======================================================================\n",
      "Concurrency 200\n",
      "======================================================================\n",
      "Pattern: BURST\n",
      "Requests: 2,000 | Batch: 1 | Concurrency: 200\n",
      "Total instances: 2,000\n",
      "\n",
      "‚è≥ Running burst test...\n",
      "   Expected duration: 2-5s\n",
      "\n",
      "‚úÖ Complete in 3.3s\n",
      "   Success: 2,000/2,000 (100.0%)\n",
      "   Total Latency:  1891.7ms (mean) | 3171.8ms (p95)\n",
      "   Queueing:       1573.0ms (mean) | 2902.8ms (p95)\n",
      "   Request:        318.6ms (mean) | 616.0ms (p95)\n",
      "   ‚ö†Ô∏è  Bottleneck: Client-side queueing (83%)\n",
      "\n",
      "======================================================================\n",
      "Concurrency 500\n",
      "======================================================================\n",
      "Pattern: BURST\n",
      "Requests: 2,000 | Batch: 1 | Concurrency: 500\n",
      "Total instances: 2,000\n",
      "\n",
      "‚è≥ Running burst test...\n",
      "   Expected duration: 1-2s\n",
      "\n",
      "‚úÖ Complete in 4.3s\n",
      "   Success: 2,000/2,000 (100.0%)\n",
      "   Total Latency:  2852.4ms (mean) | 4111.9ms (p95)\n",
      "   Queueing:       1863.7ms (mean) | 3457.6ms (p95)\n",
      "   Request:        988.7ms (mean) | 2021.3ms (p95)\n",
      "   ‚ö†Ô∏è  Bottleneck: Client-side queueing (65%)\n",
      "\n",
      "======================================================================\n",
      "Concurrency 1000\n",
      "======================================================================\n",
      "Pattern: BURST\n",
      "Requests: 2,000 | Batch: 1 | Concurrency: 1000\n",
      "Total instances: 2,000\n",
      "\n",
      "‚è≥ Running burst test...\n",
      "   Expected duration: 0-1s\n",
      "\n",
      "‚úÖ Complete in 5.6s\n",
      "   Success: 2,000/2,000 (100.0%)\n",
      "   Total Latency:  3745.4ms (mean) | 4980.9ms (p95)\n",
      "   Queueing:       1489.8ms (mean) | 3578.2ms (p95)\n",
      "   Request:        2255.6ms (mean) | 3669.3ms (p95)\n",
      "   ‚öôÔ∏è  Mixed bottleneck: 40% queueing, 60% endpoint\n",
      "\n",
      "‚úÖ Phase 2B complete: Tested 5 concurrency levels\n"
     ]
    }
   ],
   "source": [
    "# Run concurrency tests\n",
    "phase2b_results = []\n",
    "\n",
    "print(\"PHASE 2B: CONCURRENCY ANALYSIS\")\n",
    "print(f\"Testing {len(CONCURRENCY_TESTS)} concurrency levels: {CONCURRENCY_TESTS}\\n\")\n",
    "\n",
    "for concurrency in CONCURRENCY_TESTS:\n",
    "    result = await run_endpoint_test(\n",
    "        endpoint_url=endpoint_url,\n",
    "        credentials=credentials,\n",
    "        auth_req=auth_req,\n",
    "        test_data=test_data,\n",
    "        num_requests=CONCURRENCY_TEST_REQUESTS,\n",
    "        batch_size=1,\n",
    "        pattern=\"burst\",\n",
    "        max_concurrent=concurrency,\n",
    "        test_name=f\"Concurrency {concurrency}\"\n",
    "    )\n",
    "    phase2b_results.append(result)\n",
    "\n",
    "print(f\"\\n‚úÖ Phase 2B complete: Tested {len(CONCURRENCY_TESTS)} concurrency levels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "g0gduwjjxb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Phase 2B Analysis:\n",
      "\n",
      "  Concurrency Impact:\n",
      "      50 concurrent:  1699.1ms total,  95.4% queueing, client bottleneck\n",
      "     100 concurrent:  1692.1ms total,  90.9% queueing, client bottleneck\n",
      "     200 concurrent:  1891.7ms total,  83.2% queueing, client bottleneck\n",
      "     500 concurrent:  2852.4ms total,  65.3% queueing, client bottleneck\n",
      "    1000 concurrent:  3745.4ms total,  39.8% queueing, mixed bottleneck\n",
      "\n",
      "  üìä Optimal concurrency: 1000 (queueing < 50%)\n"
     ]
    }
   ],
   "source": [
    "# Analyze Phase 2B results\n",
    "phase2b_df = pd.DataFrame([r['summary'] for r in phase2b_results])\n",
    "\n",
    "# Calculate queueing percentage\n",
    "success_dfs = [r['results_df'][r['results_df']['success'] == True] for r in phase2b_results]\n",
    "queueing_pcts = []\n",
    "for df in success_dfs:\n",
    "    if len(df) > 0:\n",
    "        queue_pct = (df['queueing_ms'].mean() / df['total_ms'].mean() * 100) if df['total_ms'].mean() > 0 else 0\n",
    "        queueing_pcts.append(queue_pct)\n",
    "    else:\n",
    "        queueing_pcts.append(0)\n",
    "\n",
    "phase2b_df['queueing_pct'] = queueing_pcts\n",
    "\n",
    "print(\"\\nPhase 2B Analysis:\")\n",
    "print(f\"\\n  Concurrency Impact:\")\n",
    "for idx, row in phase2b_df.iterrows():\n",
    "    concurrency = CONCURRENCY_TESTS[idx]\n",
    "    print(f\"    {concurrency:4d} concurrent: {row['mean_latency_ms']:7.1f}ms total, {row['queueing_pct']:5.1f}% queueing, {row['bottleneck']} bottleneck\")\n",
    "\n",
    "# Find optimal concurrency (lowest queueing with good throughput)\n",
    "optimal_concurrency = phase2b_df[phase2b_df['queueing_pct'] < 50].index[-1] if len(phase2b_df[phase2b_df['queueing_pct'] < 50]) > 0 else 0\n",
    "print(f\"\\n  üìä Optimal concurrency: {CONCURRENCY_TESTS[optimal_concurrency]} (queueing < 50%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76u93f14nx7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Phase 3: Configuration Optimization\n",
    "\n",
    "**Goal**: Test optimized configurations based on Phase 1 and Phase 2 findings\n",
    "\n",
    "**Tests**: 3 configuration scenarios\n",
    "- Real-time (low latency): Batch=1, 100 RPS\n",
    "- Balanced: Batch=10, 50 RPS\n",
    "- High throughput: Batch=50, 20 RPS\n",
    "\n",
    "**Total time**: ~15 minutes (3 tests √ó 5 mins each)\n",
    "\n",
    "‚è≥ This phase applies learnings from previous phases to test production-ready configurations for different use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "zm7oiy76ld",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PHASE 3: CONFIGURATION OPTIMIZATION\n",
      "======================================================================\n",
      "\n",
      "Based on earlier phases, testing promising configurations...\n",
      "This phase will take approximately 15 minutes\n",
      "\n",
      "======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize Phase 3 storage\n",
    "phase3_results = []\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"PHASE 3: CONFIGURATION OPTIMIZATION\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nBased on earlier phases, testing promising configurations...\")\n",
    "print(f\"This phase will take approximately {sum(c['duration'] for c in OPTIMIZATION_CONFIGS)//60} minutes\\n\")\n",
    "print(f\"{'='*70}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ohmg1s7ghmf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Phase 3 - Real-time (low latency)\n",
      "======================================================================\n",
      "Pattern: SUSTAINED\n",
      "Target: 100 RPS √ó 300s = 30,000 requests\n",
      "Batch: 1 | Concurrency: 200\n",
      "\n",
      "‚è≥ Running sustained load test (300s = 5 mins 0s)...\n",
      "   This test will take approximately 5 minutes 0 seconds\n",
      "   Progress updates every 60 seconds...\n",
      "   [ 60s] 6,001 requests sent...\n",
      "   [120s] 12,001 requests sent...\n",
      "   [180s] 18,001 requests sent...\n",
      "   [240s] 24,001 requests sent...\n",
      "   Waiting for all requests to complete...\n",
      "\n",
      "‚úÖ Complete in 300.1s\n",
      "   Success: 30,000/30,000 (100.0%)\n",
      "   Total Latency:  21.4ms (mean) | 27.3ms (p95)\n",
      "   Queueing:       0.0ms (mean) | 0.0ms (p95)\n",
      "   Request:        21.4ms (mean) | 27.2ms (p95)\n",
      "   ‚úÖ Bottleneck: Endpoint processing (100%)\n",
      "\n",
      "======================================================================\n",
      "Phase 3 - Balanced\n",
      "======================================================================\n",
      "Pattern: SUSTAINED\n",
      "Target: 50 RPS √ó 300s = 15,000 requests\n",
      "Batch: 10 | Concurrency: 100\n",
      "\n",
      "‚è≥ Running sustained load test (300s = 5 mins 0s)...\n",
      "   This test will take approximately 5 minutes 0 seconds\n",
      "   Progress updates every 60 seconds...\n",
      "   [ 60s] 3,001 requests sent...\n",
      "   [120s] 6,001 requests sent...\n",
      "   [180s] 9,001 requests sent...\n",
      "   [240s] 12,001 requests sent...\n",
      "   Waiting for all requests to complete...\n",
      "\n",
      "‚úÖ Complete in 300.0s\n",
      "   Success: 15,000/15,000 (100.0%)\n",
      "   Total Latency:  30.5ms (mean) | 36.3ms (p95)\n",
      "   Queueing:       0.0ms (mean) | 0.0ms (p95)\n",
      "   Request:        30.5ms (mean) | 36.3ms (p95)\n",
      "   ‚úÖ Bottleneck: Endpoint processing (100%)\n",
      "\n",
      "======================================================================\n",
      "Phase 3 - High throughput\n",
      "======================================================================\n",
      "Pattern: SUSTAINED\n",
      "Target: 20 RPS √ó 300s = 6,000 requests\n",
      "Batch: 50 | Concurrency: 40\n",
      "\n",
      "‚è≥ Running sustained load test (300s = 5 mins 0s)...\n",
      "   This test will take approximately 5 minutes 0 seconds\n",
      "   Progress updates every 60 seconds...\n",
      "   [ 60s] 1,201 requests sent...\n",
      "   [120s] 2,401 requests sent...\n",
      "   [180s] 3,601 requests sent...\n",
      "   [240s] 4,801 requests sent...\n",
      "   Waiting for all requests to complete...\n",
      "\n",
      "‚úÖ Complete in 300.0s\n",
      "   Success: 6,000/6,000 (100.0%)\n",
      "   Total Latency:  67.9ms (mean) | 81.4ms (p95)\n",
      "   Queueing:       0.0ms (mean) | 0.0ms (p95)\n",
      "   Request:        67.9ms (mean) | 81.4ms (p95)\n",
      "   ‚úÖ Bottleneck: Endpoint processing (100%)\n",
      "\n",
      "‚úÖ Phase 3 complete: Tested 3 optimized configurations\n"
     ]
    }
   ],
   "source": [
    "# Run optimization configurations\n",
    "for config in OPTIMIZATION_CONFIGS:\n",
    "    result = await run_endpoint_test(\n",
    "        endpoint_url=endpoint_url,\n",
    "        credentials=credentials,\n",
    "        auth_req=auth_req,\n",
    "        test_data=test_data,\n",
    "        num_requests=config['rps'] * config['duration'],\n",
    "        batch_size=config['batch'],\n",
    "        pattern=\"sustained\",\n",
    "        target_rps=config['rps'],\n",
    "        duration=config['duration'],\n",
    "        max_concurrent=min(config['rps'] * 2, 200),\n",
    "        test_name=f\"Phase 3 - {config['name']}\"\n",
    "    )\n",
    "    phase3_results.append(result)\n",
    "\n",
    "print(f\"\\n‚úÖ Phase 3 complete: Tested {len(OPTIMIZATION_CONFIGS)} optimized configurations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ci40ozrjq5l",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Phase 3 Analysis:\n",
      "\n",
      "  Configuration Performance:\n",
      "\n",
      "    Real-time (low latency):\n",
      "      Batch:   1 | RPS: 100 | Throughput: 100 instances/sec\n",
      "      Success Rate:   100.0%\n",
      "      Mean Latency:      21.4ms\n",
      "      P95 Latency:       27.3ms\n",
      "      Bottleneck:     endpoint\n",
      "\n",
      "    Balanced:\n",
      "      Batch:  10 | RPS:  50 | Throughput: 500 instances/sec\n",
      "      Success Rate:   100.0%\n",
      "      Mean Latency:      30.5ms\n",
      "      P95 Latency:       36.3ms\n",
      "      Bottleneck:     endpoint\n",
      "\n",
      "    High throughput:\n",
      "      Batch:  50 | RPS:  20 | Throughput: 1,000 instances/sec\n",
      "      Success Rate:   100.0%\n",
      "      Mean Latency:      67.9ms\n",
      "      P95 Latency:       81.4ms\n",
      "      Bottleneck:     endpoint\n",
      "\n",
      "  üìä Best Configurations:\n",
      "\n",
      "    Lowest Latency:     Real-time (low latency)\n",
      "       P95: 27.3ms\n",
      "\n",
      "    Highest Throughput: High throughput\n",
      "       1,000 instances/sec\n",
      "\n",
      "    Balanced:           Balanced\n",
      "       500 instances/sec @ 36.3ms p95\n"
     ]
    }
   ],
   "source": [
    "# Analyze Phase 3 results\n",
    "phase3_df = pd.DataFrame([r['summary'] for r in phase3_results])\n",
    "\n",
    "# Add configuration names\n",
    "phase3_df['config_name'] = [c['name'] for c in OPTIMIZATION_CONFIGS]\n",
    "\n",
    "print(\"\\nPhase 3 Analysis:\")\n",
    "print(f\"\\n  Configuration Performance:\")\n",
    "for idx, row in phase3_df.iterrows():\n",
    "    config = OPTIMIZATION_CONFIGS[idx]\n",
    "    throughput = config['batch'] * config['rps']\n",
    "    \n",
    "    print(f\"\\n    {row['config_name']}:\")\n",
    "    print(f\"      Batch: {config['batch']:3d} | RPS: {config['rps']:3d} | Throughput: {throughput:,} instances/sec\")\n",
    "    print(f\"      Success Rate:   {row['success_rate']*100:5.1f}%\")\n",
    "    print(f\"      Mean Latency:   {row['mean_latency_ms']:7.1f}ms\")\n",
    "    print(f\"      P95 Latency:    {row['p95_latency_ms']:7.1f}ms\")\n",
    "    print(f\"      Bottleneck:     {row['bottleneck']}\")\n",
    "\n",
    "# Find best configuration for each use case\n",
    "print(f\"\\n  üìä Best Configurations:\")\n",
    "\n",
    "# Best for latency (lowest p95)\n",
    "best_latency_idx = phase3_df['p95_latency_ms'].idxmin()\n",
    "print(f\"\\n    Lowest Latency:     {phase3_df.loc[best_latency_idx, 'config_name']}\")\n",
    "print(f\"       P95: {phase3_df.loc[best_latency_idx, 'p95_latency_ms']:.1f}ms\")\n",
    "\n",
    "# Best for throughput\n",
    "throughputs = [c['batch'] * c['rps'] for c in OPTIMIZATION_CONFIGS]\n",
    "best_throughput_idx = throughputs.index(max(throughputs))\n",
    "print(f\"\\n    Highest Throughput: {phase3_df.loc[best_throughput_idx, 'config_name']}\")\n",
    "print(f\"       {throughputs[best_throughput_idx]:,} instances/sec\")\n",
    "\n",
    "# Balanced (middle config)\n",
    "balanced_idx = len(OPTIMIZATION_CONFIGS) // 2\n",
    "print(f\"\\n    Balanced:           {phase3_df.loc[balanced_idx, 'config_name']}\")\n",
    "print(f\"       {throughputs[balanced_idx]:,} instances/sec @ {phase3_df.loc[balanced_idx, 'p95_latency_ms']:.1f}ms p95\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ohl1ld2qs5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "marker": {
          "size": 20
         },
         "mode": "markers+text",
         "name": "Real-time (low latency)",
         "text": [
          "Real-time (low latency)"
         ],
         "textposition": "top center",
         "type": "scatter",
         "x": [
          100
         ],
         "y": [
          27.25037336349487
         ]
        },
        {
         "marker": {
          "size": 20
         },
         "mode": "markers+text",
         "name": "Balanced",
         "text": [
          "Balanced"
         ],
         "textposition": "top center",
         "type": "scatter",
         "x": [
          500
         ],
         "y": [
          36.32903099060056
         ]
        },
        {
         "marker": {
          "size": 20
         },
         "mode": "markers+text",
         "name": "High throughput",
         "text": [
          "High throughput"
         ],
         "textposition": "top center",
         "type": "scatter",
         "x": [
          1000
         ],
         "y": [
          81.41845464706422
         ]
        }
       ],
       "layout": {
        "height": 500,
        "showlegend": true,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermap": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermap"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Phase 3: Latency vs Throughput Trade-off"
        },
        "xaxis": {
         "title": {
          "text": "Throughput (instances/sec)"
         }
        },
        "yaxis": {
         "title": {
          "text": "P95 Latency (ms)"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üí° Use this chart to select configuration based on your requirements:\n",
      "   - Lower-left quadrant: Best latency, lower throughput\n",
      "   - Upper-right quadrant: Higher throughput, higher latency\n"
     ]
    }
   ],
   "source": [
    "# Visualize Phase 3 comparison\n",
    "fig = go.Figure()\n",
    "\n",
    "for idx, config in enumerate(OPTIMIZATION_CONFIGS):\n",
    "    row = phase3_df.iloc[idx]\n",
    "    throughput = config['batch'] * config['rps']\n",
    "    \n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=[throughput],\n",
    "        y=[row['p95_latency_ms']],\n",
    "        mode='markers+text',\n",
    "        name=row['config_name'],\n",
    "        marker=dict(size=20),\n",
    "        text=[row['config_name']],\n",
    "        textposition='top center'\n",
    "    ))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Phase 3: Latency vs Throughput Trade-off',\n",
    "    xaxis_title='Throughput (instances/sec)',\n",
    "    yaxis_title='P95 Latency (ms)',\n",
    "    height=500,\n",
    "    showlegend=True\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "print(\"\\nüí° Use this chart to select configuration based on your requirements:\")\n",
    "print(\"   - Lower-left quadrant: Best latency, lower throughput\")\n",
    "print(\"   - Upper-right quadrant: Higher throughput, higher latency\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75271d5e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Comprehensive Test Summary & Recommendations\n",
    "\n",
    "Based on all test phases, generate production recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "15c03460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "COMPREHENSIVE TEST SUMMARY & PRODUCTION RECOMMENDATIONS\n",
      "================================================================================\n",
      "\n",
      "üìã Endpoint Configuration:\n",
      "   Name:           pytorch-autoencoder-endpoint\n",
      "   Machine Type:   n1-standard-4\n",
      "   Replicas:       1 - 4\n",
      "   Endpoint ID:    5971323405637517312\n",
      "\n",
      "üìä Phase 1 Results - Batch Size Analysis:\n",
      "   Baseline (batch=1):     70.4ms\n",
      "   Optimal batch size:     10\n",
      "   Latency at optimal:     77.1ms\n",
      "   Per-instance at optimal: 7.71ms\n",
      "\n",
      "üìä Phase 2A Results - Autoscaling Trigger Hunt:\n",
      "   ‚úÖ Autoscaling triggered 1 time(s)\n",
      "\n",
      "   Event 1:\n",
      "      Trigger:         16:37:04\n",
      "      Complete:        16:41:24\n",
      "      CPU at trigger:  0.0%\n",
      "      Replicas:        1 ‚Üí 2\n",
      "      Scale-up lag:    260s (4.3 mins)\n",
      "\n",
      "üìä Phase 2B Results - Concurrency Analysis:\n",
      "   Optimal concurrency:     1000\n",
      "   Queueing at optimal:     39.8%\n",
      "\n",
      "üìä Phase 3 Results - Configuration Optimization:\n",
      "\n",
      "   Best for low latency:    Real-time (low latency)\n",
      "      Batch:   1 | RPS: 100\n",
      "      P95 Latency: 27.3ms\n",
      "      Throughput:  100 instances/sec\n",
      "\n",
      "   Best for high throughput: High throughput\n",
      "      Batch:  50 | RPS:  20\n",
      "      P95 Latency: 81.4ms\n",
      "      Throughput:  1,000 instances/sec\n"
     ]
    }
   ],
   "source": [
    "# Comprehensive Test Summary\n",
    "print(\"=\"*80)\n",
    "print(\"COMPREHENSIVE TEST SUMMARY & PRODUCTION RECOMMENDATIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nüìã Endpoint Configuration:\")\n",
    "print(f\"   Name:           {endpoint.display_name}\")\n",
    "print(f\"   Machine Type:   {MACHINE_TYPE}\")\n",
    "print(f\"   Replicas:       {MIN_REPLICAS} - {MAX_REPLICAS}\")\n",
    "print(f\"   Endpoint ID:    {endpoint_id}\")\n",
    "\n",
    "print(f\"\\nüìä Phase 1 Results - Batch Size Analysis:\")\n",
    "print(f\"   Baseline (batch=1):     {phase1_df[phase1_df['batch_size']==1]['mean_latency_ms'].values[0]:.1f}ms\")\n",
    "print(f\"   Optimal batch size:     {optimal_batch}\")\n",
    "print(f\"   Latency at optimal:     {phase1_df[phase1_df['batch_size']==optimal_batch]['mean_latency_ms'].values[0]:.1f}ms\")\n",
    "print(f\"   Per-instance at optimal: {phase1_df[phase1_df['batch_size']==optimal_batch]['per_instance_latency_ms'].values[0]:.2f}ms\")\n",
    "\n",
    "print(f\"\\nüìä Phase 2A Results - Autoscaling Trigger Hunt:\")\n",
    "if len(autoscaling_events) > 0:\n",
    "    print(f\"   ‚úÖ Autoscaling triggered {len(autoscaling_events)} time(s)\")\n",
    "    for idx, event in autoscaling_events.iterrows():\n",
    "        print(f\"\\n   Event {idx + 1}:\")\n",
    "        print(f\"      Trigger:         {event['trigger_time'].strftime('%H:%M:%S')}\")\n",
    "        print(f\"      Complete:        {event['scale_complete_time'].strftime('%H:%M:%S')}\")\n",
    "        print(f\"      CPU at trigger:  {event['cpu_at_trigger']:.1f}%\")\n",
    "        print(f\"      Replicas:        {event['replicas_before']:.0f} ‚Üí {event['replicas_after']:.0f}\")\n",
    "        print(f\"      Scale-up lag:    {event['scale_up_lag_seconds']:.0f}s ({event['scale_up_lag_seconds']/60:.1f} mins)\")\n",
    "else:\n",
    "    print(f\"   ‚ùå No autoscaling detected\")\n",
    "    if 'cpu' in phase2a_metrics and len(phase2a_metrics['cpu']) > 0:\n",
    "        max_cpu = phase2a_metrics['cpu']['value'].max() * 100\n",
    "        print(f\"   Max CPU observed:    {max_cpu:.1f}%\")\n",
    "        print(f\"   Threshold:           60%\")\n",
    "        print(f\"\\n   üí° Model too CPU-efficient for default autoscaling\")\n",
    "\n",
    "print(f\"\\nüìä Phase 2B Results - Concurrency Analysis:\")\n",
    "print(f\"   Optimal concurrency:     {CONCURRENCY_TESTS[optimal_concurrency]}\")\n",
    "print(f\"   Queueing at optimal:     {phase2b_df.iloc[optimal_concurrency]['queueing_pct']:.1f}%\")\n",
    "\n",
    "print(f\"\\nüìä Phase 3 Results - Configuration Optimization:\")\n",
    "best_latency_idx = phase3_df['p95_latency_ms'].idxmin()\n",
    "throughputs = [c['batch'] * c['rps'] for c in OPTIMIZATION_CONFIGS]\n",
    "best_throughput_idx = throughputs.index(max(throughputs))\n",
    "\n",
    "print(f\"\\n   Best for low latency:    {phase3_df.loc[best_latency_idx, 'config_name']}\")\n",
    "print(f\"      Batch: {OPTIMIZATION_CONFIGS[best_latency_idx]['batch']:3d} | RPS: {OPTIMIZATION_CONFIGS[best_latency_idx]['rps']:3d}\")\n",
    "print(f\"      P95 Latency: {phase3_df.loc[best_latency_idx, 'p95_latency_ms']:.1f}ms\")\n",
    "print(f\"      Throughput:  {throughputs[best_latency_idx]:,} instances/sec\")\n",
    "\n",
    "print(f\"\\n   Best for high throughput: {phase3_df.loc[best_throughput_idx, 'config_name']}\")\n",
    "print(f\"      Batch: {OPTIMIZATION_CONFIGS[best_throughput_idx]['batch']:3d} | RPS: {OPTIMIZATION_CONFIGS[best_throughput_idx]['rps']:3d}\")\n",
    "print(f\"      P95 Latency: {phase3_df.loc[best_throughput_idx, 'p95_latency_ms']:.1f}ms\")\n",
    "print(f\"      Throughput:  {throughputs[best_throughput_idx]:,} instances/sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ufyj0vfk0g",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PRODUCTION RECOMMENDATIONS\n",
      "================================================================================\n",
      "\n",
      "üéØ Recommended Configurations by Use Case:\n",
      "\n",
      "1. Real-Time / Low-Latency Applications:\n",
      "   Configuration:  Batch=1, 100 RPS\n",
      "   Expected P95:   27.3ms\n",
      "   Throughput:     100 instances/sec\n",
      "   Best for:       User-facing APIs, interactive applications\n",
      "\n",
      "2. Balanced (General Purpose):\n",
      "   Configuration:  Batch=10, 50 RPS\n",
      "   Expected P95:   36.3ms\n",
      "   Throughput:     500 instances/sec\n",
      "   Best for:       Mixed workloads, moderate traffic\n",
      "\n",
      "3. High Throughput / Batch Processing:\n",
      "   Configuration:  Batch=50, 20 RPS\n",
      "   Expected P95:   81.4ms\n",
      "   Throughput:     1,000 instances/sec\n",
      "   Best for:       Background jobs, large dataset processing\n",
      "\n",
      "‚öôÔ∏è  Infrastructure Recommendations:\n",
      "\n",
      "   Autoscaling Configuration:\n",
      "   ‚Ä¢ ‚úÖ Current threshold (60% CPU) working as expected\n",
      "   ‚Ä¢ Scale-up lag: ~4.3 mins average\n",
      "\n",
      "   Client Configuration:\n",
      "   ‚Ä¢ Recommended concurrency: 1000\n",
      "   ‚Ä¢ This minimizes client-side queueing while maximizing throughput\n",
      "\n",
      "   Machine Type:\n",
      "   ‚Ä¢ Current: n1-standard-4\n",
      "   ‚Ä¢ CPU utilization very low (0.0%)\n",
      "   ‚Ä¢ Consider smaller machine type (e.g., n1-standard-2) to reduce costs\n",
      "\n",
      "üí∞ Cost Optimization Tips:\n",
      "   ‚Ä¢ Delete endpoint when not in use (stop paying for idle replicas)\n",
      "   ‚Ä¢ Use minimum replicas based on traffic patterns\n",
      "   ‚Ä¢ Consider batch predictions for large offline datasets\n",
      "   ‚Ä¢ Monitor actual usage and adjust min/max replicas accordingly\n",
      "\n",
      "üìù Next Steps:\n",
      "   1. Review Cloud Console > Vertex AI > Endpoints > Monitoring\n",
      "      ‚Ä¢ Verify replica scaling behavior\n",
      "      ‚Ä¢ Check for any error patterns\n",
      "   2. If autoscaling threshold needs adjustment:\n",
      "      ‚Ä¢ Redeploy model with new threshold\n",
      "      ‚Ä¢ Re-run this notebook to validate\n",
      "   3. Implement recommended configuration in production client\n",
      "   4. Set up monitoring alerts for:\n",
      "      ‚Ä¢ High latency (> 55ms)\n",
      "      ‚Ä¢ High error rate (> 1%)\n",
      "      ‚Ä¢ CPU saturation (> 90%)\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Production Recommendations\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PRODUCTION RECOMMENDATIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nüéØ Recommended Configurations by Use Case:\")\n",
    "\n",
    "# Real-time low-latency\n",
    "print(f\"\\n1. Real-Time / Low-Latency Applications:\")\n",
    "print(f\"   Configuration:  Batch={OPTIMIZATION_CONFIGS[best_latency_idx]['batch']}, {OPTIMIZATION_CONFIGS[best_latency_idx]['rps']} RPS\")\n",
    "print(f\"   Expected P95:   {phase3_df.loc[best_latency_idx, 'p95_latency_ms']:.1f}ms\")\n",
    "print(f\"   Throughput:     {throughputs[best_latency_idx]:,} instances/sec\")\n",
    "print(f\"   Best for:       User-facing APIs, interactive applications\")\n",
    "\n",
    "# Balanced\n",
    "balanced_idx = len(OPTIMIZATION_CONFIGS) // 2\n",
    "print(f\"\\n2. Balanced (General Purpose):\")\n",
    "print(f\"   Configuration:  Batch={OPTIMIZATION_CONFIGS[balanced_idx]['batch']}, {OPTIMIZATION_CONFIGS[balanced_idx]['rps']} RPS\")\n",
    "print(f\"   Expected P95:   {phase3_df.loc[balanced_idx, 'p95_latency_ms']:.1f}ms\")\n",
    "print(f\"   Throughput:     {throughputs[balanced_idx]:,} instances/sec\")\n",
    "print(f\"   Best for:       Mixed workloads, moderate traffic\")\n",
    "\n",
    "# High throughput\n",
    "print(f\"\\n3. High Throughput / Batch Processing:\")\n",
    "print(f\"   Configuration:  Batch={OPTIMIZATION_CONFIGS[best_throughput_idx]['batch']}, {OPTIMIZATION_CONFIGS[best_throughput_idx]['rps']} RPS\")\n",
    "print(f\"   Expected P95:   {phase3_df.loc[best_throughput_idx, 'p95_latency_ms']:.1f}ms\")\n",
    "print(f\"   Throughput:     {throughputs[best_throughput_idx]:,} instances/sec\")\n",
    "print(f\"   Best for:       Background jobs, large dataset processing\")\n",
    "\n",
    "print(f\"\\n‚öôÔ∏è  Infrastructure Recommendations:\")\n",
    "\n",
    "# Autoscaling recommendations\n",
    "if len(autoscaling_events) == 0 and 'cpu' in phase2a_metrics and len(phase2a_metrics['cpu']) > 0:\n",
    "    max_cpu = phase2a_metrics['cpu']['value'].max() * 100\n",
    "    if max_cpu < 60:\n",
    "        print(f\"\\n   Autoscaling Configuration:\")\n",
    "        print(f\"   ‚Ä¢ Current threshold (60% CPU) too high for this model\")\n",
    "        print(f\"   ‚Ä¢ Model is CPU-efficient (max observed: {max_cpu:.1f}%)\")\n",
    "        print(f\"   ‚Ä¢ Recommended actions:\")\n",
    "        print(f\"     1. Lower CPU threshold to {max_cpu * 0.8:.0f}% (requires redeployment)\")\n",
    "        print(f\"     2. OR increase min_replicas to 2-3 for baseline capacity\")\n",
    "        print(f\"     3. OR switch to target-based autoscaling on latency/throughput\")\n",
    "else:\n",
    "    print(f\"\\n   Autoscaling Configuration:\")\n",
    "    print(f\"   ‚Ä¢ ‚úÖ Current threshold (60% CPU) working as expected\")\n",
    "    print(f\"   ‚Ä¢ Scale-up lag: ~{autoscaling_events['scale_up_lag_seconds'].mean()/60:.1f} mins average\")\n",
    "\n",
    "# Client configuration\n",
    "print(f\"\\n   Client Configuration:\")\n",
    "print(f\"   ‚Ä¢ Recommended concurrency: {CONCURRENCY_TESTS[optimal_concurrency]}\")\n",
    "print(f\"   ‚Ä¢ This minimizes client-side queueing while maximizing throughput\")\n",
    "\n",
    "# Machine type recommendations\n",
    "print(f\"\\n   Machine Type:\")\n",
    "print(f\"   ‚Ä¢ Current: {MACHINE_TYPE}\")\n",
    "if 'cpu' in phase2a_metrics and len(phase2a_metrics['cpu']) > 0:\n",
    "    max_cpu = phase2a_metrics['cpu']['value'].max() * 100\n",
    "    if max_cpu < 30:\n",
    "        print(f\"   ‚Ä¢ CPU utilization very low ({max_cpu:.1f}%)\")\n",
    "        print(f\"   ‚Ä¢ Consider smaller machine type (e.g., n1-standard-2) to reduce costs\")\n",
    "    elif max_cpu > 80:\n",
    "        print(f\"   ‚Ä¢ CPU utilization high ({max_cpu:.1f}%)\")\n",
    "        print(f\"   ‚Ä¢ Consider larger machine type (e.g., n1-standard-8) for headroom\")\n",
    "    else:\n",
    "        print(f\"   ‚Ä¢ ‚úÖ Current machine type appropriate for this workload\")\n",
    "\n",
    "print(f\"\\nüí∞ Cost Optimization Tips:\")\n",
    "print(f\"   ‚Ä¢ Delete endpoint when not in use (stop paying for idle replicas)\")\n",
    "print(f\"   ‚Ä¢ Use minimum replicas based on traffic patterns\")\n",
    "print(f\"   ‚Ä¢ Consider batch predictions for large offline datasets\")\n",
    "print(f\"   ‚Ä¢ Monitor actual usage and adjust min/max replicas accordingly\")\n",
    "\n",
    "print(f\"\\nüìù Next Steps:\")\n",
    "print(f\"   1. Review Cloud Console > Vertex AI > Endpoints > Monitoring\")\n",
    "print(f\"      ‚Ä¢ Verify replica scaling behavior\")\n",
    "print(f\"      ‚Ä¢ Check for any error patterns\")\n",
    "print(f\"   2. If autoscaling threshold needs adjustment:\")\n",
    "print(f\"      ‚Ä¢ Redeploy model with new threshold\")\n",
    "print(f\"      ‚Ä¢ Re-run this notebook to validate\")\n",
    "print(f\"   3. Implement recommended configuration in production client\")\n",
    "print(f\"   4. Set up monitoring alerts for:\")\n",
    "print(f\"      ‚Ä¢ High latency (> {phase3_df.loc[best_latency_idx, 'p95_latency_ms']*2:.0f}ms)\")\n",
    "print(f\"      ‚Ä¢ High error rate (> 1%)\")\n",
    "print(f\"      ‚Ä¢ CPU saturation (> 90%)\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exgf2uy4q25",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "This notebook provides a comprehensive, scientific approach to understanding Vertex AI Endpoint performance through systematic testing across three dimensions:\n",
    "\n",
    "**1. Batch Size** - Finding the optimal balance between latency and throughput  \n",
    "**2. Request Rate (RPS)** - Identifying capacity limits and autoscaling triggers  \n",
    "**3. Load Patterns** - Testing burst, sustained, and ramping traffic patterns\n",
    "\n",
    "### Key Insights from This Testing Framework\n",
    "\n",
    "**Understanding Bottlenecks:**\n",
    "- **Client-side bottlenecks** (high queueing time) indicate semaphore/concurrency limits\n",
    "- **Endpoint-side bottlenecks** (high request time) indicate model processing capacity\n",
    "- Separating these metrics is critical for optimization decisions\n",
    "\n",
    "**Autoscaling Behavior:**\n",
    "- CPU-efficient models may not trigger autoscaling even under high load\n",
    "- Sustained load patterns (not bursts) are needed to trigger scaling\n",
    "- Scale-up lag is typically 1-3 minutes after CPU threshold is exceeded\n",
    "\n",
    "**Configuration Trade-offs:**\n",
    "- Small batches: Lower latency, lower throughput\n",
    "- Large batches: Higher throughput, higher latency\n",
    "- Optimal configuration depends on your use case (real-time vs batch)\n",
    "\n",
    "### Important Notes\n",
    "\n",
    "**Results are Model-Specific:**  \n",
    "All results in this notebook are specific to the PyTorch autoencoder model tested. Your results will vary based on:\n",
    "- Model complexity and inference time\n",
    "- Machine type and resources\n",
    "- Input data size and format\n",
    "- Network conditions\n",
    "\n",
    "**Always Test Your Own Model:**  \n",
    "Before deploying to production:\n",
    "1. Run this notebook with your model and representative data\n",
    "2. Test with realistic traffic patterns\n",
    "3. Adjust configurations based on results\n",
    "4. Monitor actual production metrics continuously\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "**If This Notebook Revealed Issues:**\n",
    "1. **CPU too low to trigger autoscaling** ‚Üí Redeploy with lower threshold or increase min replicas\n",
    "2. **High latency at target RPS** ‚Üí Use larger machine type or increase min replicas\n",
    "3. **Client-side queueing** ‚Üí Increase concurrency limit in your client\n",
    "\n",
    "**Cleanup Tasks:**\n",
    "- Delete this endpoint if no longer needed (avoid idle costs)\n",
    "- Archive test results for future reference\n",
    "- Document chosen production configuration\n",
    "\n",
    "**Related Notebooks:**\n",
    "- [Deploy to Vertex AI Endpoint (Prebuilt)](./vertex-ai-endpoint-prebuilt-container.ipynb)\n",
    "- [Deploy to Vertex AI Endpoint (Custom)](./vertex-ai-endpoint-custom-container.ipynb)\n",
    "- [PyTorch Autoencoder Training](../pytorch-autoencoder.ipynb)\n",
    "\n",
    "### Resources\n",
    "\n",
    "**Vertex AI Documentation:**\n",
    "- [Prediction Overview](https://cloud.google.com/vertex-ai/docs/predictions/overview)\n",
    "- [Autoscaling Configuration](https://cloud.google.com/vertex-ai/docs/predictions/configure-compute#autoscaling)\n",
    "\n",
    "**Cloud Monitoring:**\n",
    "- [Available Metrics](https://docs.cloud.google.com/vertex-ai/docs/predictions/view-endpoint-metrics)\n",
    "- [Setting Up Alerts](https://cloud.google.com/monitoring/alerts)\n",
    "\n",
    "---\n",
    "\n",
    "**Testing Framework Created:** This comprehensive testing infrastructure (`scale_testing_utils.py` + this notebook) can be reused for testing any Vertex AI Endpoint deployment. Simply update the endpoint name and test parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609d9d99",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (frameworks-pytorch)",
   "language": "python",
   "name": "frameworks-pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
