{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47b9ee65",
   "metadata": {},
   "source": [
    "![tracker](https://us-central1-vertex-ai-mlops-369716.cloudfunctions.net/pixel-tracking?path=statmike%2Fvertex-ai-mlops%2FFramework+Workflows%2FPyTorch%2Fserving&file=scale-tests-vertex-ai-endpoints.ipynb)\n",
    "<!--- header table --->\n",
    "<table align=\"left\">\n",
    "<tr>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%20Workflows/PyTorch/serving/scale-tests-vertex-ai-endpoints.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.svgrepo.com/download/217753/github.svg\" alt=\"GitHub logo\">\n",
    "      <br>View on<br>GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://colab.research.google.com/github/statmike/vertex-ai-mlops/blob/main/Framework%20Workflows/PyTorch/serving/scale-tests-vertex-ai-endpoints.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.gstatic.com/pantheon/images/bigquery/welcome_page/colab-logo.svg\" alt=\"Google Colaboratory logo\">\n",
    "      <br>Run in<br>Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https%3A%2F%2Fraw.githubusercontent.com%2Fstatmike%2Fvertex-ai-mlops%2Fmain%2FFramework%2520Workflows%2FPyTorch%2Fserving%2Fscale-tests-vertex-ai-endpoints.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\">\n",
    "      <br>Run in<br>Colab Enterprise\n",
    "    </a>\n",
    "  </td>      \n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/bigquery/import?url=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%20Workflows/PyTorch/serving/scale-tests-vertex-ai-endpoints.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.gstatic.com/images/branding/gcpiconscolors/bigquery/v1/32px.svg\" alt=\"BigQuery logo\">\n",
    "      <br>Open in<br>BigQuery Studio\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/statmike/vertex-ai-mlops/main/Framework%20Workflows/PyTorch/serving/scale-tests-vertex-ai-endpoints.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.gstatic.com/images/branding/gcpiconscolors/vertexai/v1/32px.svg\" alt=\"Vertex AI logo\">\n",
    "      <br>Open in<br>Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td colspan=\"5\" style=\"text-align: right\">\n",
    "    <b>Share This On: </b> \n",
    "    <a href=\"https://www.linkedin.com/sharing/share-offsite/?url=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%2520Workflows/PyTorch/serving/scale-tests-vertex-ai-endpoints.ipynb\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"Linkedin Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://reddit.com/submit?url=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%2520Workflows/PyTorch/serving/scale-tests-vertex-ai-endpoints.ipynb\"><img src=\"https://redditinc.com/hubfs/Reddit%20Inc/Brand/Reddit_Logo.png\" alt=\"Reddit Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://bsky.app/intent/compose?text=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%2520Workflows/PyTorch/serving/scale-tests-vertex-ai-endpoints.ipynb\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"BlueSky Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://twitter.com/intent/tweet?url=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%2520Workflows/PyTorch/serving/scale-tests-vertex-ai-endpoints.ipynb\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/5/5a/X_icon_2.svg\" alt=\"X (Twitter) Logo\" width=\"20px\"></a> \n",
    "  </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td colspan=\"5\" style=\"text-align: right\">\n",
    "    <b>Connect With Author On: </b> \n",
    "    <a href=\"https://www.linkedin.com/in/statmike\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"Linkedin Logo\" width=\"20px\"></a>\n",
    "    <a href=\"https://www.github.com/statmike\"><img src=\"https://www.svgrepo.com/download/217753/github.svg\" alt=\"GitHub Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://www.youtube.com/@statmike-channel\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/f/fd/YouTube_full-color_icon_%282024%29.svg\" alt=\"YouTube Logo\" width=\"20px\"></a>\n",
    "    <a href=\"https://bsky.app/profile/statmike.bsky.social\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"BlueSky Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://x.com/statmike\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/5/5a/X_icon_2.svg\" alt=\"X (Twitter) Logo\" width=\"20px\"></a>\n",
    "  </td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Vertex AI Endpoint: Comprehensive Scale Testing & Performance Analysis\n",
    "\n",
    "**Scientific approach to understanding endpoint capacity, autoscaling behavior, and optimal configurations.**\n",
    "\n",
    "---\n",
    "\n",
    "## What This Notebook Does\n",
    "\n",
    "This notebook systematically tests Vertex AI Endpoint performance across **three dimensions**:\n",
    "\n",
    "1. **Batch Size** - How many instances per request?\n",
    "2. **Request Rate** - How many requests per second?\n",
    "3. **Load Pattern** - Burst, sustained, or ramping?\n",
    "\n",
    "Through rigorous testing, we answer:\n",
    "- â“ **When does autoscaling trigger?** (exact conditions)\n",
    "- â“ **Where are the bottlenecks?** (client vs endpoint)\n",
    "- â“ **What's the optimal configuration?** (for different use cases)\n",
    "- â“ **How should we configure for production?** (recommendations)\n",
    "\n",
    "## Testing Approach\n",
    "\n",
    "**Phase 1: Batch Size Analysis** (~2 minutes)\n",
    "- Test batch sizes from 1 to 1000 instances\n",
    "- Find optimal batch size for efficiency\n",
    "- Understand per-instance latency scaling\n",
    "\n",
    "**Phase 2A: Autoscaling Trigger Hunt** (~26 minutes)\n",
    "- Burst tests: Sudden load spikes (1k â†’ 10k requests)\n",
    "- Sustained tests: Steady load (50 â†’ 150 RPS for 5 mins)\n",
    "- Ramp test: Gradual increase (0 â†’ 200 RPS over 10 mins)\n",
    "- **Goal**: Find exact conditions that trigger autoscaling\n",
    "\n",
    "**Phase 2B: Concurrency Analysis** (~10 minutes)\n",
    "- Test different concurrency levels (50 â†’ 1000 concurrent)\n",
    "- Identify client-side vs endpoint-side bottlenecks\n",
    "- Find optimal concurrency for minimal queueing\n",
    "\n",
    "**Phase 3: Configuration Optimization** (~15-25 minutes)\n",
    "- Test promising combinations from earlier phases\n",
    "- Sustained load patterns for different scenarios\n",
    "- Generate production recommendations\n",
    "\n",
    "**Total test time**: ~53-63 minutes\n",
    "\n",
    "## Key Metrics Tracked\n",
    "\n",
    "**Client-Side Metrics** (what we measure):\n",
    "- ðŸŸ¦ **Queueing Time**: Waiting for client concurrency slot â†’ *client bottleneck*\n",
    "- ðŸŸ© **Request Time**: Actual HTTP request/response â†’ *endpoint performance*\n",
    "- ðŸ”µ **Total Latency**: End-to-end user experience\n",
    "- âœ… **Success Rate**: Percentage of successful requests\n",
    "\n",
    "**Service-Side Metrics** (from Cloud Monitoring API):\n",
    "- ðŸ“Š **CPU Utilization**: Triggers autoscaling at 60% default\n",
    "- ðŸ“ˆ **Memory Utilization**: Resource pressure indicator\n",
    "- ðŸ”¢ **Replica Count**: Autoscaling activity\n",
    "- âš¡ **Prediction Rate**: Service-reported throughput\n",
    "- ðŸ“‰ **Service Latency**: P95 latency from endpoint's perspective\n",
    "- âŒ **Error Rate**: Service-side failures\n",
    "\n",
    "## Understanding Bottlenecks\n",
    "\n",
    "**Client-Side Bottleneck** (Queueing > 50%):\n",
    "- Semaphore limit too low\n",
    "- Network capacity saturated\n",
    "- **Solution**: Increase `max_concurrent` or reduce RPS\n",
    "\n",
    "**Endpoint-Side Bottleneck** (Queueing < 10%):\n",
    "- Model inference takes time\n",
    "- Replica capacity saturated\n",
    "- **Solution**: Wait for autoscaling or increase min replicas\n",
    "\n",
    "**Mixed Bottleneck** (Queueing 10-50%):\n",
    "- Both client and endpoint under pressure\n",
    "- Transition point between client/endpoint limits\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before running this notebook, you need:\n",
    "\n",
    "- **Deployed Vertex AI Endpoint** with a model\n",
    "  - Train: [../pytorch-autoencoder.ipynb](../pytorch-autoencoder.ipynb)\n",
    "  - Deploy: [vertex-ai-endpoint-prebuilt-container.ipynb](./vertex-ai-endpoint-prebuilt-container.ipynb) OR [vertex-ai-endpoint-custom-container.ipynb](./vertex-ai-endpoint-custom-container.ipynb)\n",
    "- **Google Cloud authentication** with Vertex AI prediction permissions\n",
    "- **Python packages**: `aiohttp`, `google-cloud-aiplatform`, `google-cloud-monitoring`, `plotly`, `pandas`\n",
    "\n",
    "## Understanding Vertex AI Autoscaling\n",
    "\n",
    "**How It Works**:\n",
    "- Autoscales based on **CPU utilization** (default threshold: 60%)\n",
    "- When CPU > 60% for ~1-2 minutes â†’ new replica provisions\n",
    "- Replica provisioning takes ~2-3 minutes (container startup)\n",
    "- Scale-down occurs after ~10-15 minutes below threshold\n",
    "\n",
    "**Important**: Lightweight models may not trigger autoscaling even under high RPS because CPU usage stays low. This is a **capacity bottleneck**, not a **compute bottleneck**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup_header",
   "metadata": {},
   "source": [
    "---\n",
    "## Environment Setup\n",
    "\n",
    "This section will authenticate your session, enable required Google Cloud APIs, and install necessary Python packages.\n",
    "\n",
    "**Package Installation Options (`REQ_TYPE`):**\n",
    "- `PRIMARY`: Installs only the main packages. Faster, but pip resolves sub-dependencies which may result in different versions than development.\n",
    "- `ALL` (Default): Installs exact versions of all packages and dependencies. Best for perfectly reproducing the development environment.\n",
    "- `COLAB`: Installs a Colab-optimized list that excludes pre-installed packages like `ipython` and `ipykernel`.\n",
    "\n",
    "**Installation Tool Options (`INSTALL_TOOL`):**\n",
    "- `pip` (Default): Uses pip for package installation. Standard Python package installer.\n",
    "- `uv`: Modern, fast Python package installer. Must be installed separately. See: https://github.com/astral-sh/uv\n",
    "- `poetry`: Dependency management tool. Requires running notebook in a poetry environment (`poetry shell` or `poetry run jupyter lab`). Uses `pyproject.toml` instead of requirements.txt.\n",
    "\n",
    "> **Note:** If running in Google Colab, the script will automatically detect this and set `REQ_TYPE = 'COLAB'` to prevent package conflicts, overriding any manual setting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "project_id_header",
   "metadata": {},
   "source": [
    "### Set Your Project ID\n",
    "\n",
    "âš ï¸ **Action Required:** Replace the `PROJECT_ID` value below with your Google Cloud project ID before running this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "project_config",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = 'statmike-mlops-349915' # replace with GCP project ID\n",
    "REQ_TYPE = 'ALL' # Specify PRIMARY or ALL or COLAB\n",
    "INSTALL_TOOL = 'poetry' # Specify pip, uv, or poetry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config_header",
   "metadata": {},
   "source": [
    "### Configuration\n",
    "\n",
    "This cell defines the requirements files and Google Cloud APIs needed for this notebook. Run as-is without modification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": [
    "REQUIREMENTS_URL = 'https://raw.githubusercontent.com/statmike/vertex-ai-mlops/refs/heads/main/Framework%20Workflows/PyTorch/requirements.txt'\n",
    "\n",
    "REQUIRED_APIS = [\n",
    "    \"aiplatform.googleapis.com\",\n",
    "    \"monitoring.googleapis.com\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "run_setup_header",
   "metadata": {},
   "source": [
    "### Run Setup\n",
    "\n",
    "This cell downloads the centralized setup code and configures your environment. It will:\n",
    "- Authenticate your session with Google Cloud\n",
    "- Enable required APIs for this notebook\n",
    "- Install necessary Python packages\n",
    "- Display a setup summary with your project information\n",
    "\n",
    "> **Note:** In Colab, if packages are installed, the kernel will automatically restart. After restart, continue from the next cell without re-running earlier cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run_setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, urllib.request\n",
    "\n",
    "# Download and import setup code\n",
    "url = 'https://raw.githubusercontent.com/statmike/vertex-ai-mlops/refs/heads/main/core/notebook-template/python_setup.py'\n",
    "urllib.request.urlretrieve(url, 'python_setup_local.py')\n",
    "import python_setup_local as python_setup\n",
    "os.remove('python_setup_local.py')\n",
    "\n",
    "# Run setup\n",
    "setup_info = python_setup.setup_environment(PROJECT_ID, REQ_TYPE, REQUIREMENTS_URL, REQUIRED_APIS, INSTALL_TOOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8i6h2p5352h",
   "metadata": {},
   "source": [
    "---\n",
    "## Test Configuration\n",
    "\n",
    "Configure the endpoint to test and the test parameters.\n",
    "\n",
    "**Endpoint Configuration:**\n",
    "- Update `ENDPOINT_DISPLAY_NAME` to match your deployed endpoint\n",
    "- Update `REGION` if your endpoint is in a different region\n",
    "\n",
    "**Test Parameters:**\n",
    "- Adjust batch sizes, RPS targets, and durations based on your needs\n",
    "- Default values are designed to comprehensively test a PyTorch autoencoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eob3sesvuu",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Endpoint Configuration\n",
    "REGION = 'us-central1'\n",
    "ENDPOINT_DISPLAY_NAME = 'pytorch-autoencoder-endpoint'  # Replace with your endpoint name\n",
    "\n",
    "# Phase 1: Batch Size Analysis\n",
    "BATCH_SIZES = [1, 5, 10, 50, 100, 500, 1000]\n",
    "\n",
    "# Phase 2A: Autoscaling Trigger Hunt\n",
    "BURST_TESTS = [\n",
    "    {\"num_requests\": 1000, \"max_concurrent\": 100},\n",
    "    {\"num_requests\": 2000, \"max_concurrent\": 200},\n",
    "    {\"num_requests\": 5000, \"max_concurrent\": 500},\n",
    "    {\"num_requests\": 10000, \"max_concurrent\": 1000},\n",
    "]\n",
    "\n",
    "SUSTAINED_TESTS = [\n",
    "    {\"target_rps\": 50, \"duration\": 300},   # 5 minutes\n",
    "    {\"target_rps\": 100, \"duration\": 300},  # 5 minutes\n",
    "    {\"target_rps\": 150, \"duration\": 300},  # 5 minutes\n",
    "]\n",
    "\n",
    "RAMP_TEST = {\"target_rps\": 200, \"duration\": 600}  # 10 minutes\n",
    "\n",
    "# Phase 2B: Concurrency Analysis  \n",
    "CONCURRENCY_TESTS = [50, 100, 200, 500, 1000]\n",
    "CONCURRENCY_TEST_REQUESTS = 2000\n",
    "\n",
    "# Phase 3: Configuration Optimization (customize based on earlier results)\n",
    "OPTIMIZATION_CONFIGS = [\n",
    "    {\"batch\": 1, \"rps\": 100, \"duration\": 300, \"name\": \"Real-time (low latency)\"},\n",
    "    {\"batch\": 10, \"rps\": 50, \"duration\": 300, \"name\": \"Balanced\"},\n",
    "    {\"batch\": 50, \"rps\": 20, \"duration\": 300, \"name\": \"High throughput\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fz7kr85khj",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import asyncio\n",
    "import aiohttp\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import google.auth\n",
    "import google.auth.transport.requests\n",
    "from google.cloud import aiplatform, monitoring_v3\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "print(\"âœ… Imports complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36g37x5jjea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize clients\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION)\n",
    "monitoring_client = monitoring_v3.MetricServiceClient()\n",
    "\n",
    "# Setup authentication for REST API\n",
    "credentials, _ = google.auth.default()\n",
    "auth_req = google.auth.transport.requests.Request()\n",
    "\n",
    "print(f\"âœ… Initialized for project: {PROJECT_ID}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "se4aljdk70i",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import testing utilities\n",
    "from scale_testing_utils import run_endpoint_test, EndpointMetricsCollector, plot_timeline_with_metrics\n",
    "\n",
    "print(\"âœ… Testing utilities imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2rnuq308op4",
   "metadata": {},
   "source": [
    "---\n",
    "## Connect to Endpoint\n",
    "\n",
    "Find and connect to the deployed endpoint, then prepare test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iclf2o6a18i",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find endpoint\n",
    "endpoints = aiplatform.Endpoint.list(filter=f\"display_name={ENDPOINT_DISPLAY_NAME}\")\n",
    "\n",
    "if not endpoints:\n",
    "    raise ValueError(f\"No endpoint found: {ENDPOINT_DISPLAY_NAME}\")\n",
    "\n",
    "endpoint = endpoints[0]\n",
    "endpoint_url = f\"https://{REGION}-aiplatform.googleapis.com/v1/{endpoint.resource_name}:predict\"\n",
    "endpoint_id = endpoint.resource_name.split('/')[-1]\n",
    "\n",
    "# Get endpoint configuration\n",
    "deployed_model = endpoint.list_models()[0]\n",
    "MACHINE_TYPE = deployed_model.dedicated_resources.machine_spec.machine_type\n",
    "MIN_REPLICAS = deployed_model.dedicated_resources.min_replica_count\n",
    "MAX_REPLICAS = deployed_model.dedicated_resources.max_replica_count\n",
    "\n",
    "print(f\"âœ… Connected to: {endpoint.display_name}\")\n",
    "print(f\"   Machine: {MACHINE_TYPE}\")\n",
    "print(f\"   Replicas: {MIN_REPLICAS} - {MAX_REPLICAS}\")\n",
    "print(f\"   Endpoint ID: {endpoint_id}\")\n",
    "print(f\"   URL: {endpoint_url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zcan8becgv",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare test data  \n",
    "# Generate sample data (30 features for PyTorch autoencoder)\n",
    "test_data = [np.random.randn(30).astype(np.float32).tolist()]\n",
    "\n",
    "# Test connection\n",
    "response = endpoint.predict(instances=test_data)\n",
    "print(f\"âœ… Endpoint connection successful\")\n",
    "print(f\"   Prediction returned: {len(response.predictions)} result(s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "v2g86me4u4l",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize metrics collector\n",
    "metrics_collector = EndpointMetricsCollector(\n",
    "    project_id=PROJECT_ID,\n",
    "    endpoint_id=endpoint_id,\n",
    "    region=REGION\n",
    ")\n",
    "\n",
    "print(f\"âœ… Metrics collector initialized\")\n",
    "print(f\"   Will collect: CPU, Memory, Replicas, Predictions, Latency, Errors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ubu4yhuqr2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## How to Run the Complete Test Suite\n",
    "\n",
    "This notebook provides a comprehensive testing framework. You can run all phases sequentially or pick specific tests based on your needs.\n",
    "\n",
    "### Quick Start (Recommended Order)\n",
    "\n",
    "1. **Run Phase 1** (Batch Size Analysis) - ~2 minutes\n",
    "2. **Run Phase 2A** (Autoscaling Trigger Hunt) - ~26 minutes  \n",
    "3. **Collect metrics** after Phase 2A\n",
    "4. **Analyze autoscaling events**\n",
    "5. **Run Phase 2B** (Concurrency Analysis) - ~10 minutes\n",
    "6. **Run Phase 3** (Configuration Optimization) - ~15-25 minutes\n",
    "7. **Generate final recommendations**\n",
    "\n",
    "### Adding Test Cells\n",
    "\n",
    "For each phase below, add cells using this pattern:\n",
    "\n",
    "```python\n",
    "# Run test\n",
    "test_result = await run_endpoint_test(\n",
    "    endpoint_url=endpoint_url,\n",
    "    credentials=credentials,\n",
    "    auth_req=auth_req,\n",
    "    test_data=test_data,\n",
    "    num_requests=1000,\n",
    "    batch_size=1,\n",
    "    pattern=\"burst\",  # or \"sustained\" or \"ramp\"\n",
    "    max_concurrent=100,\n",
    "    test_name=\"Test Name\"\n",
    ")\n",
    "\n",
    "# Store results\n",
    "results_list.append(test_result)\n",
    "```\n",
    "\n",
    "### Collecting Metrics After Tests\n",
    "\n",
    "```python\n",
    "# Collect metrics for a specific test\n",
    "metrics = metrics_collector.collect_metrics(\n",
    "    start_time=test_result['start_time'],\n",
    "    end_time=test_result['end_time']\n",
    ")\n",
    "\n",
    "# Analyze autoscaling\n",
    "autoscaling_events = metrics_collector.analyze_autoscaling(metrics)\n",
    "\n",
    "# Visualize\n",
    "fig = plot_timeline_with_metrics(\n",
    "    test_results=test_result['results_df'],\n",
    "    metrics=metrics,\n",
    "    test_name=\"Test Name\"\n",
    ")\n",
    "fig.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Phase 1: Batch Size Analysis\n",
    "\n",
    "**Goal**: Find optimal batch size for efficiency\n",
    "\n",
    "**Tests to run**: 7 tests (one per batch size)\n",
    "**Total time**: ~2 minutes\n",
    "**Pattern**: Sustained at 1 RPS\n",
    "\n",
    "Execute batch size tests below..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iiyxzmhc7x",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize results storage\n",
    "phase1_results = []\n",
    "\n",
    "print(\"Starting Phase 1: Batch Size Analysis\")\n",
    "print(f\"Testing {len(BATCH_SIZES)} batch sizes: {BATCH_SIZES}\")\n",
    "print(f\"Total estimated time: ~2 minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28zekehg56m",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run batch size tests\n",
    "for batch_size in BATCH_SIZES:\n",
    "    result = await run_endpoint_test(\n",
    "        endpoint_url=endpoint_url,\n",
    "        credentials=credentials,\n",
    "        auth_req=auth_req,\n",
    "        test_data=test_data,\n",
    "        num_requests=10,  # 10 requests per batch size\n",
    "        batch_size=batch_size,\n",
    "        pattern=\"burst\",  # Use burst instead of sustained for faster completion\n",
    "        max_concurrent=10,\n",
    "        test_name=f\"Phase 1 - Batch Size {batch_size}\"\n",
    "    )\n",
    "    phase1_results.append(result)\n",
    "\n",
    "print(f\"\\nâœ… Phase 1 complete: Tested {len(BATCH_SIZES)} batch sizes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qhcszlya6rn",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze Phase 1 results\n",
    "phase1_df = pd.DataFrame([r['summary'] for r in phase1_results])\n",
    "\n",
    "# Calculate per-instance latency\n",
    "phase1_df['per_instance_latency_ms'] = phase1_df['mean_latency_ms'] / phase1_df['batch_size']\n",
    "\n",
    "# Find optimal batch size (lowest per-instance latency with acceptable total latency)\n",
    "optimal_batch = phase1_df[phase1_df['mean_latency_ms'] <= phase1_df['mean_latency_ms'].iloc[0] * 2]['batch_size'].max()\n",
    "\n",
    "print(\"Phase 1 Analysis:\")\n",
    "print(f\"\\n  Batch Size Performance:\")\n",
    "for _, row in phase1_df.iterrows():\n",
    "    print(f\"    Batch {int(row['batch_size']):4d}: {row['mean_latency_ms']:7.1f}ms total, {row['per_instance_latency_ms']:6.2f}ms per instance\")\n",
    "\n",
    "print(f\"\\n  ðŸ“Š Optimal batch size: {optimal_batch} (latency < 2x baseline)\")\n",
    "print(f\"     Baseline (batch=1): {phase1_df['mean_latency_ms'].iloc[0]:.1f}ms\")\n",
    "print(f\"     Optimal (batch={optimal_batch}): {phase1_df[phase1_df['batch_size']==optimal_batch]['mean_latency_ms'].values[0]:.1f}ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eqokvf8ity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Phase 1 results\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=phase1_df['batch_size'], \n",
    "    y=phase1_df['mean_latency_ms'],\n",
    "    mode='lines+markers', \n",
    "    name='Mean Latency',\n",
    "    line=dict(width=2, color='blue'),\n",
    "    marker=dict(size=10)\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=phase1_df['batch_size'], \n",
    "    y=phase1_df['p95_latency_ms'],\n",
    "    mode='lines+markers', \n",
    "    name='P95 Latency',\n",
    "    line=dict(width=2, dash='dash', color='orange'),\n",
    "    marker=dict(size=8)\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Phase 1: Latency vs Batch Size',\n",
    "    xaxis_title='Batch Size (instances per request)',\n",
    "    yaxis_title='Latency (ms)',\n",
    "    height=500\n",
    ")\n",
    "\n",
    "fig.show(renderer=\"png\", width=1400, height=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91s9nl1n9k6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Phase 2A: Autoscaling Trigger Hunt\n",
    "\n",
    "**Goal**: Find exact conditions that trigger autoscaling\n",
    "\n",
    "**Tests**: \n",
    "- 4 burst tests (~1 minute)\n",
    "- 3 sustained tests (~15 minutes)\n",
    "- 1 ramp test (~10 minutes)\n",
    "\n",
    "**Total time**: ~26 minutes\n",
    "\n",
    "âš ï¸ **Important**: This phase will take approximately **26 minutes** to complete. The tests are designed to trigger autoscaling by creating sustained load on the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dj2dceyp4s5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Phase 2A storage\n",
    "phase2a_burst_results = []\n",
    "phase2a_sustained_results = []\n",
    "phase2a_ramp_results = []\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"PHASE 2A: AUTOSCALING TRIGGER HUNT\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nThis phase will take approximately 26 minutes\")\n",
    "print(f\"We'll run 8 tests to find when autoscaling triggers:\")\n",
    "print(f\"  - 4 burst tests (fast)\")\n",
    "print(f\"  - 3 sustained tests (5 mins each)\")\n",
    "print(f\"  - 1 ramp test (10 mins)\")\n",
    "print(f\"\\n{'='*70}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dzn57o9zsqk",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Burst Tests\n",
    "print(\"Step 1/3: Running burst tests (quick capacity checks)...\\n\")\n",
    "\n",
    "for test_config in BURST_TESTS:\n",
    "    result = await run_endpoint_test(\n",
    "        endpoint_url=endpoint_url,\n",
    "        credentials=credentials,\n",
    "        auth_req=auth_req,\n",
    "        test_data=test_data,\n",
    "        num_requests=test_config['num_requests'],\n",
    "        batch_size=1,\n",
    "        pattern=\"burst\",\n",
    "        max_concurrent=test_config['max_concurrent'],\n",
    "        test_name=f\"Burst - {test_config['num_requests']:,} requests\"\n",
    "    )\n",
    "    phase2a_burst_results.append(result)\n",
    "\n",
    "print(f\"\\nâœ… Burst tests complete: {len(BURST_TESTS)} tests run\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0z80g766q4vg",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Sustained Tests (THIS IS WHERE AUTOSCALING SHOULD TRIGGER)\n",
    "print(\"Step 2/3: Running sustained load tests (these should trigger autoscaling)...\")\n",
    "print(\"â³ This will take approximately 15 minutes (3 tests Ã— 5 mins each)\\n\")\n",
    "\n",
    "for test_config in SUSTAINED_TESTS:\n",
    "    result = await run_endpoint_test(\n",
    "        endpoint_url=endpoint_url,\n",
    "        credentials=credentials,\n",
    "        auth_req=auth_req,\n",
    "        test_data=test_data,\n",
    "        num_requests=test_config['target_rps'] * test_config['duration'],\n",
    "        batch_size=1,\n",
    "        pattern=\"sustained\",\n",
    "        target_rps=test_config['target_rps'],\n",
    "        duration=test_config['duration'],\n",
    "        max_concurrent=min(test_config['target_rps'] * 2, 200),\n",
    "        test_name=f\"Sustained - {test_config['target_rps']} RPS\"\n",
    "    )\n",
    "    phase2a_sustained_results.append(result)\n",
    "\n",
    "print(f\"\\nâœ… Sustained tests complete: {len(SUSTAINED_TESTS)} tests run\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52fisbze329",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Ramp Test (GRADUAL INCREASE TO FIND EXACT TRIGGER POINT)\n",
    "print(\"Step 3/3: Running ramp test (gradual increase to find exact trigger)...\")\n",
    "print(\"â³ This will take approximately 10 minutes\\n\")\n",
    "\n",
    "ramp_result = await run_endpoint_test(\n",
    "    endpoint_url=endpoint_url,\n",
    "    credentials=credentials,\n",
    "    auth_req=auth_req,\n",
    "    test_data=test_data,\n",
    "    num_requests=0,  # Ramp pattern doesn't use this\n",
    "    batch_size=1,\n",
    "    pattern=\"ramp\",\n",
    "    target_rps=RAMP_TEST['target_rps'],\n",
    "    duration=RAMP_TEST['duration'],\n",
    "    max_concurrent=200,\n",
    "    test_name=f\"Ramp - 0 â†’ {RAMP_TEST['target_rps']} RPS\"\n",
    ")\n",
    "phase2a_ramp_results.append(ramp_result)\n",
    "\n",
    "print(f\"\\nâœ… Ramp test complete\\n\")\n",
    "print(\"=\"*70)\n",
    "print(\"PHASE 2A COMPLETE\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "khapo1kwu6",
   "metadata": {},
   "source": [
    "### Collect Metrics for Phase 2A\n",
    "\n",
    "Now we'll collect endpoint metrics from Cloud Monitoring to see if autoscaling was triggered.\n",
    "\n",
    "â³ **This will wait 90 seconds for metrics to propagate**, then collect data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "l1sw93czs1h",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect metrics for all Phase 2A tests\n",
    "# We'll collect metrics for the entire Phase 2A time window\n",
    "phase2a_all_results = phase2a_burst_results + phase2a_sustained_results + phase2a_ramp_results\n",
    "\n",
    "# Find overall time window\n",
    "phase2a_start = min([r['start_time'] for r in phase2a_all_results])\n",
    "phase2a_end = max([r['end_time'] for r in phase2a_all_results])\n",
    "\n",
    "print(f\"Collecting metrics for entire Phase 2A\")\n",
    "print(f\"Time window: {phase2a_start.strftime('%H:%M:%S')} â†’ {phase2a_end.strftime('%H:%M:%S')}\")\n",
    "\n",
    "# Collect metrics\n",
    "phase2a_metrics = metrics_collector.collect_metrics(\n",
    "    start_time=phase2a_start,\n",
    "    end_time=phase2a_end,\n",
    "    resolution_seconds=10\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ… Metrics collected for Phase 2A\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "o5oi4n5lpgq",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect autoscaling events\n",
    "autoscaling_events = metrics_collector.analyze_autoscaling(phase2a_metrics)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"AUTOSCALING ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "if len(autoscaling_events) > 0:\n",
    "    print(f\"\\nâœ… Autoscaling was triggered {len(autoscaling_events)} time(s)!\\n\")\n",
    "    \n",
    "    for idx, event in autoscaling_events.iterrows():\n",
    "        print(f\"Event {idx + 1}:\")\n",
    "        print(f\"  Trigger Time:     {event['trigger_time'].strftime('%H:%M:%S')}\")\n",
    "        print(f\"  Scale Complete:   {event['scale_complete_time'].strftime('%H:%M:%S')}\")\n",
    "        print(f\"  CPU at Trigger:   {event['cpu_at_trigger']:.1f}%\")\n",
    "        print(f\"  Replicas:         {event['replicas_before']:.0f} â†’ {event['replicas_after']:.0f}\")\n",
    "        print(f\"  Scale-up Lag:     {event['scale_up_lag_seconds']:.0f} seconds ({event['scale_up_lag_seconds']/60:.1f} mins)\")\n",
    "        print()\n",
    "else:\n",
    "    print(f\"\\nâŒ No autoscaling detected\")\n",
    "    print(f\"\\nPossible reasons:\")\n",
    "    print(f\"  - CPU never exceeded 60% threshold\")\n",
    "    print(f\"  - Model is too CPU-efficient for default threshold\")\n",
    "    print(f\"  - Load duration too short (need sustained load)\")\n",
    "    \n",
    "    if 'cpu' in phase2a_metrics and len(phase2a_metrics['cpu']) > 0:\n",
    "        max_cpu = phase2a_metrics['cpu']['value'].max() * 100\n",
    "        print(f\"\\n  Max CPU observed: {max_cpu:.1f}%\")\n",
    "        if max_cpu < 60:\n",
    "            print(f\"  âš ï¸  CPU stayed below 60% threshold\")\n",
    "            print(f\"  ðŸ’¡ Recommendation: Increase min_replicas or lower autoscaling threshold\")\n",
    "\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "otrfzajzo2t",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Complete Phase 2A Timeline (shows all tests and autoscaling event)\n",
    "print(\"Creating comprehensive Phase 2A visualization...\")\n",
    "print(f\"Time span: {phase2a_start.strftime('%H:%M:%S')} â†’ {phase2a_end.strftime('%H:%M:%S')}\")\n",
    "print(f\"This covers all 4 burst tests, 3 sustained tests, and 1 ramp test\\n\")\n",
    "\n",
    "# Combine ALL test results for the request rate timeline\n",
    "all_test_results = pd.concat([r['results_df'] for r in phase2a_all_results])\n",
    "\n",
    "# Create base visualization with ALL Phase 2A data\n",
    "fig = plot_timeline_with_metrics(\n",
    "    test_results=all_test_results,\n",
    "    metrics=phase2a_metrics,  # Already collected for full Phase 2A window\n",
    "    test_name=\"Phase 2A: Complete Timeline - All Tests\"\n",
    ")\n",
    "\n",
    "# Add vertical separators between test phases\n",
    "# Calculate phase boundaries\n",
    "burst_end = phase2a_burst_results[-1]['end_time']\n",
    "sustained_end = phase2a_sustained_results[-1]['end_time']\n",
    "\n",
    "# Add vertical lines to all 4 subplots\n",
    "for row_num in [1, 2, 3, 4]:\n",
    "    # Separator between Burst and Sustained\n",
    "    fig.add_shape(\n",
    "        type=\"line\",\n",
    "        x0=burst_end, x1=burst_end, y0=0, y1=1,\n",
    "        line=dict(color=\"rgba(255, 0, 0, 0.5)\", width=2, dash=\"dash\"),\n",
    "        yref=\"paper\",\n",
    "        row=row_num, col=1\n",
    "    )\n",
    "    \n",
    "    # Separator between Sustained and Ramp\n",
    "    fig.add_shape(\n",
    "        type=\"line\",\n",
    "        x0=sustained_end, x1=sustained_end, y0=0, y1=1,\n",
    "        line=dict(color=\"rgba(0, 128, 0, 0.5)\", width=2, dash=\"dash\"),\n",
    "        yref=\"paper\",\n",
    "        row=row_num, col=1\n",
    "    )\n",
    "\n",
    "# Add annotations only on the top subplot\n",
    "fig.add_annotation(\n",
    "    x=burst_end, y=1.05,\n",
    "    text=\"Burst â†’ Sustained\",\n",
    "    showarrow=False,\n",
    "    yref=\"paper\",\n",
    "    xref=\"x\",\n",
    "    textangle=-90,\n",
    "    font=dict(size=10, color=\"red\"),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "fig.add_annotation(\n",
    "    x=sustained_end, y=1.05,\n",
    "    text=\"Sustained â†’ Ramp\",\n",
    "    showarrow=False,\n",
    "    yref=\"paper\",\n",
    "    xref=\"x\",\n",
    "    textangle=-90,\n",
    "    font=dict(size=10, color=\"green\"),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "fig.show(renderer=\"png\", width=1400, height=1000)\n",
    "\n",
    "print(\"\\nðŸ“Š This visualization shows:\")\n",
    "print(f\"  - All Phase 2A tests from {phase2a_start.strftime('%H:%M:%S')} to {phase2a_end.strftime('%H:%M:%S')}\")\n",
    "print(f\"  - Request rate across all 8 tests\")\n",
    "print(f\"  - CPU utilization (max ~22%)\")\n",
    "print(f\"  - Replica scaling: 1 â†’ 2 @ 16:41 (during burst tests), back to 1 @ 16:45\")\n",
    "print(f\"  - P95 latency throughout all tests\")\n",
    "print(f\"  - Red line: Burst â†’ Sustained | Green line: Sustained â†’ Ramp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75qi0ozd74a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Phase 2B: Concurrency Analysis\n",
    "\n",
    "**Goal**: Find optimal concurrency level (client-side bottleneck analysis)\n",
    "\n",
    "**Tests**: 5 concurrency levels Ã— 2000 requests each\n",
    "**Total time**: ~10 minutes\n",
    "\n",
    "â³ This phase tests how different concurrency levels affect performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pjdlp0cpo98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run concurrency tests\n",
    "phase2b_results = []\n",
    "\n",
    "print(\"PHASE 2B: CONCURRENCY ANALYSIS\")\n",
    "print(f\"Testing {len(CONCURRENCY_TESTS)} concurrency levels: {CONCURRENCY_TESTS}\\n\")\n",
    "\n",
    "for concurrency in CONCURRENCY_TESTS:\n",
    "    result = await run_endpoint_test(\n",
    "        endpoint_url=endpoint_url,\n",
    "        credentials=credentials,\n",
    "        auth_req=auth_req,\n",
    "        test_data=test_data,\n",
    "        num_requests=CONCURRENCY_TEST_REQUESTS,\n",
    "        batch_size=1,\n",
    "        pattern=\"burst\",\n",
    "        max_concurrent=concurrency,\n",
    "        test_name=f\"Concurrency {concurrency}\"\n",
    "    )\n",
    "    phase2b_results.append(result)\n",
    "\n",
    "print(f\"\\nâœ… Phase 2B complete: Tested {len(CONCURRENCY_TESTS)} concurrency levels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "g0gduwjjxb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze Phase 2B results\n",
    "phase2b_df = pd.DataFrame([r['summary'] for r in phase2b_results])\n",
    "\n",
    "# Calculate queueing percentage\n",
    "success_dfs = [r['results_df'][r['results_df']['success'] == True] for r in phase2b_results]\n",
    "queueing_pcts = []\n",
    "for df in success_dfs:\n",
    "    if len(df) > 0:\n",
    "        queue_pct = (df['queueing_ms'].mean() / df['total_ms'].mean() * 100) if df['total_ms'].mean() > 0 else 0\n",
    "        queueing_pcts.append(queue_pct)\n",
    "    else:\n",
    "        queueing_pcts.append(0)\n",
    "\n",
    "phase2b_df['queueing_pct'] = queueing_pcts\n",
    "\n",
    "print(\"\\nPhase 2B Analysis:\")\n",
    "print(f\"\\n  Concurrency Impact:\")\n",
    "for idx, row in phase2b_df.iterrows():\n",
    "    concurrency = CONCURRENCY_TESTS[idx]\n",
    "    print(f\"    {concurrency:4d} concurrent: {row['mean_latency_ms']:7.1f}ms total, {row['queueing_pct']:5.1f}% queueing, {row['bottleneck']} bottleneck\")\n",
    "\n",
    "# Find optimal concurrency (lowest queueing with good throughput)\n",
    "optimal_concurrency = phase2b_df[phase2b_df['queueing_pct'] < 50].index[-1] if len(phase2b_df[phase2b_df['queueing_pct'] < 50]) > 0 else 0\n",
    "print(f\"\\n  ðŸ“Š Optimal concurrency: {CONCURRENCY_TESTS[optimal_concurrency]} (queueing < 50%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76u93f14nx7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Phase 3: Configuration Optimization\n",
    "\n",
    "**Goal**: Test optimized configurations based on Phase 1 and Phase 2 findings\n",
    "\n",
    "**Tests**: 3 configuration scenarios\n",
    "- Real-time (low latency): Batch=1, 100 RPS\n",
    "- Balanced: Batch=10, 50 RPS\n",
    "- High throughput: Batch=50, 20 RPS\n",
    "\n",
    "**Total time**: ~15 minutes (3 tests Ã— 5 mins each)\n",
    "\n",
    "â³ This phase applies learnings from previous phases to test production-ready configurations for different use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zm7oiy76ld",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Phase 3 storage\n",
    "phase3_results = []\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"PHASE 3: CONFIGURATION OPTIMIZATION\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nBased on earlier phases, testing promising configurations...\")\n",
    "print(f\"This phase will take approximately {sum(c['duration'] for c in OPTIMIZATION_CONFIGS)//60} minutes\\n\")\n",
    "print(f\"{'='*70}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ohmg1s7ghmf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run optimization configurations\n",
    "for config in OPTIMIZATION_CONFIGS:\n",
    "    result = await run_endpoint_test(\n",
    "        endpoint_url=endpoint_url,\n",
    "        credentials=credentials,\n",
    "        auth_req=auth_req,\n",
    "        test_data=test_data,\n",
    "        num_requests=config['rps'] * config['duration'],\n",
    "        batch_size=config['batch'],\n",
    "        pattern=\"sustained\",\n",
    "        target_rps=config['rps'],\n",
    "        duration=config['duration'],\n",
    "        max_concurrent=min(config['rps'] * 2, 200),\n",
    "        test_name=f\"Phase 3 - {config['name']}\"\n",
    "    )\n",
    "    phase3_results.append(result)\n",
    "\n",
    "print(f\"\\nâœ… Phase 3 complete: Tested {len(OPTIMIZATION_CONFIGS)} optimized configurations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ci40ozrjq5l",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze Phase 3 results\n",
    "phase3_df = pd.DataFrame([r['summary'] for r in phase3_results])\n",
    "\n",
    "# Add configuration names\n",
    "phase3_df['config_name'] = [c['name'] for c in OPTIMIZATION_CONFIGS]\n",
    "\n",
    "print(\"\\nPhase 3 Analysis:\")\n",
    "print(f\"\\n  Configuration Performance:\")\n",
    "for idx, row in phase3_df.iterrows():\n",
    "    config = OPTIMIZATION_CONFIGS[idx]\n",
    "    throughput = config['batch'] * config['rps']\n",
    "    \n",
    "    print(f\"\\n    {row['config_name']}:\")\n",
    "    print(f\"      Batch: {config['batch']:3d} | RPS: {config['rps']:3d} | Throughput: {throughput:,} instances/sec\")\n",
    "    print(f\"      Success Rate:   {row['success_rate']*100:5.1f}%\")\n",
    "    print(f\"      Mean Latency:   {row['mean_latency_ms']:7.1f}ms\")\n",
    "    print(f\"      P95 Latency:    {row['p95_latency_ms']:7.1f}ms\")\n",
    "    print(f\"      Bottleneck:     {row['bottleneck']}\")\n",
    "\n",
    "# Find best configuration for each use case\n",
    "print(f\"\\n  ðŸ“Š Best Configurations:\")\n",
    "\n",
    "# Best for latency (lowest p95)\n",
    "best_latency_idx = phase3_df['p95_latency_ms'].idxmin()\n",
    "print(f\"\\n    Lowest Latency:     {phase3_df.loc[best_latency_idx, 'config_name']}\")\n",
    "print(f\"       P95: {phase3_df.loc[best_latency_idx, 'p95_latency_ms']:.1f}ms\")\n",
    "\n",
    "# Best for throughput\n",
    "throughputs = [c['batch'] * c['rps'] for c in OPTIMIZATION_CONFIGS]\n",
    "best_throughput_idx = throughputs.index(max(throughputs))\n",
    "print(f\"\\n    Highest Throughput: {phase3_df.loc[best_throughput_idx, 'config_name']}\")\n",
    "print(f\"       {throughputs[best_throughput_idx]:,} instances/sec\")\n",
    "\n",
    "# Balanced (middle config)\n",
    "balanced_idx = len(OPTIMIZATION_CONFIGS) // 2\n",
    "print(f\"\\n    Balanced:           {phase3_df.loc[balanced_idx, 'config_name']}\")\n",
    "print(f\"       {throughputs[balanced_idx]:,} instances/sec @ {phase3_df.loc[balanced_idx, 'p95_latency_ms']:.1f}ms p95\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ohl1ld2qs5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Phase 3 comparison\n",
    "fig = go.Figure()\n",
    "\n",
    "for idx, config in enumerate(OPTIMIZATION_CONFIGS):\n",
    "    row = phase3_df.iloc[idx]\n",
    "    throughput = config['batch'] * config['rps']\n",
    "    \n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=[throughput],\n",
    "        y=[row['p95_latency_ms']],\n",
    "        mode='markers+text',\n",
    "        name=row['config_name'],\n",
    "        marker=dict(size=20),\n",
    "        text=[row['config_name']],\n",
    "        textposition='top center'\n",
    "    ))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Phase 3: Latency vs Throughput Trade-off',\n",
    "    xaxis_title='Throughput (instances/sec)',\n",
    "    yaxis_title='P95 Latency (ms)',\n",
    "    height=500,\n",
    "    showlegend=True\n",
    ")\n",
    "fig.show(renderer=\"png\", width=1400, height=1000)\n",
    "\n",
    "print(\"\\nðŸ’¡ Use this chart to select configuration based on your requirements:\")\n",
    "print(\"   - Lower-left quadrant: Best latency, lower throughput\")\n",
    "print(\"   - Upper-right quadrant: Higher throughput, higher latency\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75271d5e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Comprehensive Test Summary & Recommendations\n",
    "\n",
    "Based on all test phases, generate production recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c03460",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive Test Summary\n",
    "print(\"=\"*80)\n",
    "print(\"COMPREHENSIVE TEST SUMMARY & PRODUCTION RECOMMENDATIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nðŸ“‹ Endpoint Configuration:\")\n",
    "print(f\"   Name:           {endpoint.display_name}\")\n",
    "print(f\"   Machine Type:   {MACHINE_TYPE}\")\n",
    "print(f\"   Replicas:       {MIN_REPLICAS} - {MAX_REPLICAS}\")\n",
    "print(f\"   Endpoint ID:    {endpoint_id}\")\n",
    "\n",
    "print(f\"\\nðŸ“Š Phase 1 Results - Batch Size Analysis:\")\n",
    "print(f\"   Baseline (batch=1):     {phase1_df[phase1_df['batch_size']==1]['mean_latency_ms'].values[0]:.1f}ms\")\n",
    "print(f\"   Optimal batch size:     {optimal_batch}\")\n",
    "print(f\"   Latency at optimal:     {phase1_df[phase1_df['batch_size']==optimal_batch]['mean_latency_ms'].values[0]:.1f}ms\")\n",
    "print(f\"   Per-instance at optimal: {phase1_df[phase1_df['batch_size']==optimal_batch]['per_instance_latency_ms'].values[0]:.2f}ms\")\n",
    "\n",
    "print(f\"\\nðŸ“Š Phase 2A Results - Autoscaling Trigger Hunt:\")\n",
    "if len(autoscaling_events) > 0:\n",
    "    print(f\"   âœ… Autoscaling triggered {len(autoscaling_events)} time(s)\")\n",
    "    for idx, event in autoscaling_events.iterrows():\n",
    "        print(f\"\\n   Event {idx + 1}:\")\n",
    "        print(f\"      Trigger:         {event['trigger_time'].strftime('%H:%M:%S')}\")\n",
    "        print(f\"      Complete:        {event['scale_complete_time'].strftime('%H:%M:%S')}\")\n",
    "        print(f\"      CPU at trigger:  {event['cpu_at_trigger']:.1f}%\")\n",
    "        print(f\"      Replicas:        {event['replicas_before']:.0f} â†’ {event['replicas_after']:.0f}\")\n",
    "        print(f\"      Scale-up lag:    {event['scale_up_lag_seconds']:.0f}s ({event['scale_up_lag_seconds']/60:.1f} mins)\")\n",
    "else:\n",
    "    print(f\"   âŒ No autoscaling detected\")\n",
    "    if 'cpu' in phase2a_metrics and len(phase2a_metrics['cpu']) > 0:\n",
    "        max_cpu = phase2a_metrics['cpu']['value'].max() * 100\n",
    "        print(f\"   Max CPU observed:    {max_cpu:.1f}%\")\n",
    "        print(f\"   Threshold:           60%\")\n",
    "        print(f\"\\n   ðŸ’¡ Model too CPU-efficient for default autoscaling\")\n",
    "\n",
    "print(f\"\\nðŸ“Š Phase 2B Results - Concurrency Analysis:\")\n",
    "print(f\"   Optimal concurrency:     {CONCURRENCY_TESTS[optimal_concurrency]}\")\n",
    "print(f\"   Queueing at optimal:     {phase2b_df.iloc[optimal_concurrency]['queueing_pct']:.1f}%\")\n",
    "\n",
    "print(f\"\\nðŸ“Š Phase 3 Results - Configuration Optimization:\")\n",
    "best_latency_idx = phase3_df['p95_latency_ms'].idxmin()\n",
    "throughputs = [c['batch'] * c['rps'] for c in OPTIMIZATION_CONFIGS]\n",
    "best_throughput_idx = throughputs.index(max(throughputs))\n",
    "\n",
    "print(f\"\\n   Best for low latency:    {phase3_df.loc[best_latency_idx, 'config_name']}\")\n",
    "print(f\"      Batch: {OPTIMIZATION_CONFIGS[best_latency_idx]['batch']:3d} | RPS: {OPTIMIZATION_CONFIGS[best_latency_idx]['rps']:3d}\")\n",
    "print(f\"      P95 Latency: {phase3_df.loc[best_latency_idx, 'p95_latency_ms']:.1f}ms\")\n",
    "print(f\"      Throughput:  {throughputs[best_latency_idx]:,} instances/sec\")\n",
    "\n",
    "print(f\"\\n   Best for high throughput: {phase3_df.loc[best_throughput_idx, 'config_name']}\")\n",
    "print(f\"      Batch: {OPTIMIZATION_CONFIGS[best_throughput_idx]['batch']:3d} | RPS: {OPTIMIZATION_CONFIGS[best_throughput_idx]['rps']:3d}\")\n",
    "print(f\"      P95 Latency: {phase3_df.loc[best_throughput_idx, 'p95_latency_ms']:.1f}ms\")\n",
    "print(f\"      Throughput:  {throughputs[best_throughput_idx]:,} instances/sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ufyj0vfk0g",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Production Recommendations\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PRODUCTION RECOMMENDATIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nðŸŽ¯ Recommended Configurations by Use Case:\")\n",
    "\n",
    "# Real-time low-latency\n",
    "print(f\"\\n1. Real-Time / Low-Latency Applications:\")\n",
    "print(f\"   Configuration:  Batch={OPTIMIZATION_CONFIGS[best_latency_idx]['batch']}, {OPTIMIZATION_CONFIGS[best_latency_idx]['rps']} RPS\")\n",
    "print(f\"   Expected P95:   {phase3_df.loc[best_latency_idx, 'p95_latency_ms']:.1f}ms\")\n",
    "print(f\"   Throughput:     {throughputs[best_latency_idx]:,} instances/sec\")\n",
    "print(f\"   Best for:       User-facing APIs, interactive applications\")\n",
    "\n",
    "# Balanced\n",
    "balanced_idx = len(OPTIMIZATION_CONFIGS) // 2\n",
    "print(f\"\\n2. Balanced (General Purpose):\")\n",
    "print(f\"   Configuration:  Batch={OPTIMIZATION_CONFIGS[balanced_idx]['batch']}, {OPTIMIZATION_CONFIGS[balanced_idx]['rps']} RPS\")\n",
    "print(f\"   Expected P95:   {phase3_df.loc[balanced_idx, 'p95_latency_ms']:.1f}ms\")\n",
    "print(f\"   Throughput:     {throughputs[balanced_idx]:,} instances/sec\")\n",
    "print(f\"   Best for:       Mixed workloads, moderate traffic\")\n",
    "\n",
    "# High throughput\n",
    "print(f\"\\n3. High Throughput / Batch Processing:\")\n",
    "print(f\"   Configuration:  Batch={OPTIMIZATION_CONFIGS[best_throughput_idx]['batch']}, {OPTIMIZATION_CONFIGS[best_throughput_idx]['rps']} RPS\")\n",
    "print(f\"   Expected P95:   {phase3_df.loc[best_throughput_idx, 'p95_latency_ms']:.1f}ms\")\n",
    "print(f\"   Throughput:     {throughputs[best_throughput_idx]:,} instances/sec\")\n",
    "print(f\"   Best for:       Background jobs, large dataset processing\")\n",
    "\n",
    "print(f\"\\nâš™ï¸  Infrastructure Recommendations:\")\n",
    "\n",
    "# Autoscaling recommendations\n",
    "if len(autoscaling_events) == 0 and 'cpu' in phase2a_metrics and len(phase2a_metrics['cpu']) > 0:\n",
    "    max_cpu = phase2a_metrics['cpu']['value'].max() * 100\n",
    "    if max_cpu < 60:\n",
    "        print(f\"\\n   Autoscaling Configuration:\")\n",
    "        print(f\"   â€¢ Current threshold (60% CPU) too high for this model\")\n",
    "        print(f\"   â€¢ Model is CPU-efficient (max observed: {max_cpu:.1f}%)\")\n",
    "        print(f\"   â€¢ Recommended actions:\")\n",
    "        print(f\"     1. Lower CPU threshold to {max_cpu * 0.8:.0f}% (requires redeployment)\")\n",
    "        print(f\"     2. OR increase min_replicas to 2-3 for baseline capacity\")\n",
    "        print(f\"     3. OR switch to target-based autoscaling on latency/throughput\")\n",
    "else:\n",
    "    print(f\"\\n   Autoscaling Configuration:\")\n",
    "    print(f\"   â€¢ âœ… Current threshold (60% CPU) working as expected\")\n",
    "    print(f\"   â€¢ Scale-up lag: ~{autoscaling_events['scale_up_lag_seconds'].mean()/60:.1f} mins average\")\n",
    "\n",
    "# Client configuration\n",
    "print(f\"\\n   Client Configuration:\")\n",
    "print(f\"   â€¢ Recommended concurrency: {CONCURRENCY_TESTS[optimal_concurrency]}\")\n",
    "print(f\"   â€¢ This minimizes client-side queueing while maximizing throughput\")\n",
    "\n",
    "# Machine type recommendations\n",
    "print(f\"\\n   Machine Type:\")\n",
    "print(f\"   â€¢ Current: {MACHINE_TYPE}\")\n",
    "if 'cpu' in phase2a_metrics and len(phase2a_metrics['cpu']) > 0:\n",
    "    max_cpu = phase2a_metrics['cpu']['value'].max() * 100\n",
    "    if max_cpu < 30:\n",
    "        print(f\"   â€¢ CPU utilization very low ({max_cpu:.1f}%)\")\n",
    "        print(f\"   â€¢ Consider smaller machine type (e.g., n1-standard-2) to reduce costs\")\n",
    "    elif max_cpu > 80:\n",
    "        print(f\"   â€¢ CPU utilization high ({max_cpu:.1f}%)\")\n",
    "        print(f\"   â€¢ Consider larger machine type (e.g., n1-standard-8) for headroom\")\n",
    "    else:\n",
    "        print(f\"   â€¢ âœ… Current machine type appropriate for this workload\")\n",
    "\n",
    "print(f\"\\nðŸ’° Cost Optimization Tips:\")\n",
    "print(f\"   â€¢ Delete endpoint when not in use (stop paying for idle replicas)\")\n",
    "print(f\"   â€¢ Use minimum replicas based on traffic patterns\")\n",
    "print(f\"   â€¢ Consider batch predictions for large offline datasets\")\n",
    "print(f\"   â€¢ Monitor actual usage and adjust min/max replicas accordingly\")\n",
    "\n",
    "print(f\"\\nðŸ“ Next Steps:\")\n",
    "print(f\"   1. Review Cloud Console > Vertex AI > Endpoints > Monitoring\")\n",
    "print(f\"      â€¢ Verify replica scaling behavior\")\n",
    "print(f\"      â€¢ Check for any error patterns\")\n",
    "print(f\"   2. If autoscaling threshold needs adjustment:\")\n",
    "print(f\"      â€¢ Redeploy model with new threshold\")\n",
    "print(f\"      â€¢ Re-run this notebook to validate\")\n",
    "print(f\"   3. Implement recommended configuration in production client\")\n",
    "print(f\"   4. Set up monitoring alerts for:\")\n",
    "print(f\"      â€¢ High latency (> {phase3_df.loc[best_latency_idx, 'p95_latency_ms']*2:.0f}ms)\")\n",
    "print(f\"      â€¢ High error rate (> 1%)\")\n",
    "print(f\"      â€¢ CPU saturation (> 90%)\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exgf2uy4q25",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "This notebook provides a comprehensive, scientific approach to understanding Vertex AI Endpoint performance through systematic testing across three dimensions:\n",
    "\n",
    "**1. Batch Size** - Finding the optimal balance between latency and throughput  \n",
    "**2. Request Rate (RPS)** - Identifying capacity limits and autoscaling triggers  \n",
    "**3. Load Patterns** - Testing burst, sustained, and ramping traffic patterns\n",
    "\n",
    "### Key Insights from This Testing Framework\n",
    "\n",
    "**Understanding Bottlenecks:**\n",
    "- **Client-side bottlenecks** (high queueing time) indicate semaphore/concurrency limits\n",
    "- **Endpoint-side bottlenecks** (high request time) indicate model processing capacity\n",
    "- Separating these metrics is critical for optimization decisions\n",
    "\n",
    "**Autoscaling Behavior:**\n",
    "- CPU-efficient models may not trigger autoscaling even under high load\n",
    "- Sustained load patterns (not bursts) are needed to trigger scaling\n",
    "- Scale-up lag is typically 1-3 minutes after CPU threshold is exceeded\n",
    "\n",
    "**Configuration Trade-offs:**\n",
    "- Small batches: Lower latency, lower throughput\n",
    "- Large batches: Higher throughput, higher latency\n",
    "- Optimal configuration depends on your use case (real-time vs batch)\n",
    "\n",
    "### Important Notes\n",
    "\n",
    "**Results are Model-Specific:**  \n",
    "All results in this notebook are specific to the PyTorch autoencoder model tested. Your results will vary based on:\n",
    "- Model complexity and inference time\n",
    "- Machine type and resources\n",
    "- Input data size and format\n",
    "- Network conditions\n",
    "\n",
    "**Always Test Your Own Model:**  \n",
    "Before deploying to production:\n",
    "1. Run this notebook with your model and representative data\n",
    "2. Test with realistic traffic patterns\n",
    "3. Adjust configurations based on results\n",
    "4. Monitor actual production metrics continuously\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "**If This Notebook Revealed Issues:**\n",
    "1. **CPU too low to trigger autoscaling** â†’ Redeploy with lower threshold or increase min replicas\n",
    "2. **High latency at target RPS** â†’ Use larger machine type or increase min replicas\n",
    "3. **Client-side queueing** â†’ Increase concurrency limit in your client\n",
    "\n",
    "**Cleanup Tasks:**\n",
    "- Delete this endpoint if no longer needed (avoid idle costs)\n",
    "- Archive test results for future reference\n",
    "- Document chosen production configuration\n",
    "\n",
    "**Related Notebooks:**\n",
    "- [Deploy to Vertex AI Endpoint (Prebuilt)](./vertex-ai-endpoint-prebuilt-container.ipynb)\n",
    "- [Deploy to Vertex AI Endpoint (Custom)](./vertex-ai-endpoint-custom-container.ipynb)\n",
    "- [PyTorch Autoencoder Training](../pytorch-autoencoder.ipynb)\n",
    "\n",
    "### Resources\n",
    "\n",
    "**Vertex AI Documentation:**\n",
    "- [Prediction Overview](https://cloud.google.com/vertex-ai/docs/predictions/overview)\n",
    "- [Autoscaling Configuration](https://cloud.google.com/vertex-ai/docs/predictions/configure-compute#autoscaling)\n",
    "\n",
    "**Cloud Monitoring:**\n",
    "- [Available Metrics](https://docs.cloud.google.com/vertex-ai/docs/predictions/view-endpoint-metrics)\n",
    "- [Setting Up Alerts](https://cloud.google.com/monitoring/alerts)\n",
    "\n",
    "---\n",
    "\n",
    "**Testing Framework Created:** This comprehensive testing infrastructure (`scale_testing_utils.py` + this notebook) can be reused for testing any Vertex AI Endpoint deployment. Simply update the endpoint name and test parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609d9d99",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (frameworks-pytorch)",
   "language": "python",
   "name": "frameworks-pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
