{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "daababd3",
   "metadata": {},
   "source": [
    "![tracker](https://us-central1-vertex-ai-mlops-369716.cloudfunctions.net/pixel-tracking?path=statmike%2Fvertex-ai-mlops%2FFramework+Workflows%2FPyTorch%2Fserving&file=scale-tests-dataflow-streaming-runinference.ipynb)\n",
    "<!--- header table --->\n",
    "<table align=\"left\">\n",
    "<tr>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%20Workflows/PyTorch/serving/scale-tests-dataflow-streaming-runinference.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.svgrepo.com/download/217753/github.svg\" alt=\"GitHub logo\">\n",
    "      <br>View on<br>GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://colab.research.google.com/github/statmike/vertex-ai-mlops/blob/main/Framework%20Workflows/PyTorch/serving/scale-tests-dataflow-streaming-runinference.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.gstatic.com/pantheon/images/bigquery/welcome_page/colab-logo.svg\" alt=\"Google Colaboratory logo\">\n",
    "      <br>Run in<br>Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https%3A%2F%2Fraw.githubusercontent.com%2Fstatmike%2Fvertex-ai-mlops%2Fmain%2FFramework%2520Workflows%2FPyTorch%2Fserving%2Fscale-tests-dataflow-streaming-runinference.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\">\n",
    "      <br>Run in<br>Colab Enterprise\n",
    "    </a>\n",
    "  </td>      \n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/bigquery/import?url=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%20Workflows/PyTorch/serving/scale-tests-dataflow-streaming-runinference.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.gstatic.com/images/branding/gcpiconscolors/bigquery/v1/32px.svg\" alt=\"BigQuery logo\">\n",
    "      <br>Open in<br>BigQuery Studio\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/statmike/vertex-ai-mlops/main/Framework%20Workflows/PyTorch/serving/scale-tests-dataflow-streaming-runinference.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.gstatic.com/images/branding/gcpiconscolors/vertexai/v1/32px.svg\" alt=\"Vertex AI logo\">\n",
    "      <br>Open in<br>Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td colspan=\"5\" style=\"text-align: right\">\n",
    "    <b>Share This On: </b> \n",
    "    <a href=\"https://www.linkedin.com/sharing/share-offsite/?url=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%2520Workflows/PyTorch/serving/scale-tests-dataflow-streaming-runinference.ipynb\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"Linkedin Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://reddit.com/submit?url=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%2520Workflows/PyTorch/serving/scale-tests-dataflow-streaming-runinference.ipynb\"><img src=\"https://redditinc.com/hubfs/Reddit%20Inc/Brand/Reddit_Logo.png\" alt=\"Reddit Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://bsky.app/intent/compose?text=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%2520Workflows/PyTorch/serving/scale-tests-dataflow-streaming-runinference.ipynb\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"BlueSky Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://twitter.com/intent/tweet?url=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%2520Workflows/PyTorch/serving/scale-tests-dataflow-streaming-runinference.ipynb\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/5/5a/X_icon_2.svg\" alt=\"X (Twitter) Logo\" width=\"20px\"></a> \n",
    "  </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td colspan=\"5\" style=\"text-align: right\">\n",
    "    <b>Connect With Author On: </b> \n",
    "    <a href=\"https://www.linkedin.com/in/statmike\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"Linkedin Logo\" width=\"20px\"></a>\n",
    "    <a href=\"https://www.github.com/statmike\"><img src=\"https://www.svgrepo.com/download/217753/github.svg\" alt=\"GitHub Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://www.youtube.com/@statmike-channel\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/f/fd/YouTube_full-color_icon_%282024%29.svg\" alt=\"YouTube Logo\" width=\"20px\"></a>\n",
    "    <a href=\"https://bsky.app/profile/statmike.bsky.social\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"BlueSky Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://x.com/statmike\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/5/5a/X_icon_2.svg\" alt=\"X (Twitter) Logo\" width=\"20px\"></a>\n",
    "  </td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Dataflow Streaming: Comprehensive Scale Testing & Performance Analysis\n",
    "\n",
    "**Scientific approach to understanding pipeline capacity, worker autoscaling, and optimal configurations.**\n",
    "\n",
    "---\n",
    "\n",
    "## What This Notebook Does\n",
    "\n",
    "This notebook systematically tests Dataflow Streaming performance across **three dimensions**:\n",
    "\n",
    "1. **Message Rate** - How many messages per second can the pipeline handle?\n",
    "2. **Load Pattern** - Burst, sustained, or ramping traffic?\n",
    "3. **Latency Characteristics** - Where do delays occur in the pipeline?\n",
    "\n",
    "Through rigorous testing, we answer:\n",
    "- ‚ùì **When does worker autoscaling trigger?** (exact conditions)\n",
    "- ‚ùì **Where are the bottlenecks?** (windowing vs processing vs pub/sub)\n",
    "- ‚ùì **What's the throughput threshold?** (messages/second before backlog builds)\n",
    "- ‚ùì **How should we configure for production?** (recommendations)\n",
    "\n",
    "## Testing Approach\n",
    "\n",
    "**Phase 1: Baseline Performance** (~5 minutes)\n",
    "- Test 10 msg/sec for 2 mins (sustained)\n",
    "- Test 25 msg/sec for 2 mins (sustained)\n",
    "- **Goal**: Establish baseline latency and confirm pipeline health\n",
    "\n",
    "**Phase 2A: Throughput Threshold Hunt** (~20 minutes)\n",
    "- Sustained tests: 50, 100, 200 msg/sec for 5 mins each\n",
    "- **Goal**: Find exact threshold where backlog builds and autoscaling triggers\n",
    "\n",
    "**Phase 2B: Burst Capacity** (~10 minutes)\n",
    "- Burst tests: 1000, 5000, 10000 messages as fast as possible\n",
    "- **Goal**: Test backlog recovery time and worker scaling response\n",
    "\n",
    "**Phase 3: Ramp Test** (~15 minutes)\n",
    "- Ramp 0 ‚Üí 500 msg/sec over 15 mins\n",
    "- **Goal**: Find exact autoscaling trigger point\n",
    "\n",
    "**Total test time**: ~50-70 minutes\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before running this notebook, you need:\n",
    "\n",
    "- **Running Dataflow Streaming Job** from [dataflow-streaming-runinference.ipynb](./dataflow-streaming-runinference.ipynb)\n",
    "  - ‚ö†Ô∏è This notebook does NOT deploy a new job - it tests an existing one\n",
    "  - Job must be in \"Running\" state with workers active\n",
    "- **Testing utilities**: `scale_testing_dataflow_utils.py` must be in the same directory\n",
    "- **Total test time**: ~70 minutes\n",
    "\n",
    "## Understanding Dataflow Autoscaling\n",
    "\n",
    "**How It Works**:\n",
    "- Dataflow autoscales based on **Pub/Sub backlog** and **system lag**\n",
    "- When backlog builds or lag increases ‚Üí new workers provision\n",
    "- Worker provisioning takes ~2-4 minutes (container startup)\n",
    "- Scale-down occurs after ~10-15 minutes of low load\n",
    "\n",
    "**Key Metrics**:\n",
    "- **System Lag**: How far behind real-time is the pipeline? (seconds)\n",
    "- **Backlog**: Number of undelivered messages in Pub/Sub subscription\n",
    "- **Worker Count**: Number of active Dataflow workers\n",
    "- **Element Count**: Throughput (elements/second processed)\n",
    "\n",
    "**Latency Breakdown** (measured from Pub/Sub output):\n",
    "- **Window Wait**: Publish time ‚Üí Window close time (up to 60s for 60s windows)\n",
    "- **Processing**: Window close ‚Üí Pipeline output (model inference + transforms)\n",
    "- **Pub/Sub Delivery**: Pipeline output ‚Üí Consumer receive (usually <100ms)\n",
    "- **Total**: End-to-end latency from publish to receive\n",
    "\n",
    "## Machine Type and GPU Considerations\n",
    "\n",
    "**Current Configuration**: n1-standard-4 (4 vCPUs, 15 GB memory)\n",
    "\n",
    "**When to Consider Other Machine Types**:\n",
    "\n",
    "1. **CPU-Intensive Models** (high CPU during inference):\n",
    "   - Use `c2-standard-4` (compute-optimized)\n",
    "   - More CPU power per worker\n",
    "   - Better for models with complex calculations\n",
    "\n",
    "2. **Memory-Intensive Models** (large model size or batch processing):\n",
    "   - Use `n1-highmem-4` (4 vCPUs, 26 GB memory)\n",
    "   - Use `n1-highmem-8` (8 vCPUs, 52 GB memory)\n",
    "   - Prevents OOM errors during model loading\n",
    "\n",
    "3. **GPU Acceleration** (large models with GPU support):\n",
    "   - Add `--worker_gpu_type=nvidia-tesla-t4` (cost-effective)\n",
    "   - Add `--worker_gpu_type=nvidia-tesla-v100` (high performance)\n",
    "   - Add `--worker_gpu_count=1` (GPUs per worker)\n",
    "   - Update model handler to use `device=\"cuda\"`\n",
    "   - **When to use**: Models with >100M parameters, batch inference benefits from GPU\n",
    "   - **Cost**: ~10x more expensive than CPU workers\n",
    "   - **PyTorch requirement**: Must use GPU-enabled PyTorch build\n",
    "\n",
    "**For This Notebook**: We test only n1-standard-4 (our current configuration). If results show CPU saturation or memory pressure, recommendations will suggest alternatives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup_header",
   "metadata": {},
   "source": [
    "---\n",
    "## Environment Setup\n",
    "\n",
    "This section will authenticate your session, enable required Google Cloud APIs, and install necessary Python packages.\n",
    "\n",
    "**Package Installation Options (`REQ_TYPE`)**:\n",
    "- `PRIMARY`: Installs only the main packages. Faster, but pip resolves sub-dependencies which may result in different versions than development.\n",
    "- `ALL` (Default): Installs exact versions of all packages and dependencies. Best for perfectly reproducing the development environment.\n",
    "- `COLAB`: Installs a Colab-optimized list that excludes pre-installed packages like `ipython` and `ipykernel`.\n",
    "\n",
    "**Installation Tool Options (`INSTALL_TOOL`)**:\n",
    "- `pip` (Default): Uses pip for package installation. Standard Python package installer.\n",
    "- `uv`: Modern, fast Python package installer. Must be installed separately. See: https://github.com/astral-sh/uv\n",
    "- `poetry`: Dependency management tool. Requires running notebook in a poetry environment (`poetry shell` or `poetry run jupyter lab`). Uses `pyproject.toml` instead of requirements.txt.\n",
    "\n",
    "> **Note:** If running in Google Colab, the script will automatically detect this and set `REQ_TYPE = 'COLAB'` to prevent package conflicts, overriding any manual setting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "project_id_header",
   "metadata": {},
   "source": [
    "### Set Your Project ID\n",
    "\n",
    "‚ö†Ô∏è **Action Required:** Replace the `PROJECT_ID` value below with your Google Cloud project ID before running this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "project_config",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = 'statmike-mlops-349915' # replace with GCP project ID\n",
    "REQ_TYPE = 'ALL' # Specify PRIMARY or ALL or COLAB\n",
    "INSTALL_TOOL = 'poetry' # Specify pip, uv, or poetry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config_header",
   "metadata": {},
   "source": [
    "### Configuration\n",
    "\n",
    "This cell defines the requirements files and Google Cloud APIs needed for this notebook. Run as-is without modification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": [
    "REQUIREMENTS_URL = 'https://raw.githubusercontent.com/statmike/vertex-ai-mlops/refs/heads/main/Framework%20Workflows/PyTorch/requirements.txt'\n",
    "\n",
    "REQUIRED_APIS = [\n",
    "    \"dataflow.googleapis.com\",\n",
    "    \"pubsub.googleapis.com\",\n",
    "    \"monitoring.googleapis.com\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "run_setup_header",
   "metadata": {},
   "source": [
    "### Run Setup\n",
    "\n",
    "This cell downloads the centralized setup code and configures your environment. It will:\n",
    "- Authenticate your session with Google Cloud\n",
    "- Enable required APIs for this notebook\n",
    "- Install necessary Python packages\n",
    "- Display a setup summary with your project information\n",
    "\n",
    "> **Note:** In Colab, if packages are installed, the kernel will automatically restart. After restart, continue from the next cell without re-running earlier cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "run_setup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "PYTHON GCP ENVIRONMENT SETUP\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "AUTHENTICATION\n",
      "==================================================\n",
      "Checking for existing ADC...\n",
      "‚úÖ Existing ADC found.\n",
      "‚úÖ Project is correctly set to 'statmike-mlops-349915'.\n",
      "\n",
      "==================================================\n",
      "API CHECK & ENABLE\n",
      "==================================================\n",
      "‚úÖ dataflow.googleapis.com is already enabled.\n",
      "‚úÖ pubsub.googleapis.com is already enabled.\n",
      "‚úÖ monitoring.googleapis.com is already enabled.\n",
      "\n",
      "==================================================\n",
      "PACKAGE MANAGEMENT\n",
      "==================================================\n",
      "Installation Tool: poetry\n",
      "‚úÖ Found poetry at: /usr/local/google/home/statmike/.local/bin/poetry\n",
      "‚úÖ Running in poetry environment: /usr/local/google/home/statmike/.cache/pypoetry/virtualenvs/frameworks-pytorch-0KVJlKeQ-py3.13\n",
      "‚ÑπÔ∏è  Poetry mode: Installing from pyproject.toml (REQUIREMENTS_URL ignored)\n",
      "‚úÖ Found pyproject.toml at: /usr/local/google/home/statmike/Git/vertex-ai-mlops/Framework Workflows/PyTorch/pyproject.toml\n",
      "   Changed working directory to: /usr/local/google/home/statmike/Git/vertex-ai-mlops/Framework Workflows/PyTorch\n",
      "Running poetry install...\n",
      "   Restored working directory to: /usr/local/google/home/statmike/Git/vertex-ai-mlops/Framework Workflows/PyTorch/serving\n",
      "‚úÖ All packages are already installed and up to date.\n",
      "\n",
      "==================================================\n",
      "Google Cloud Project Information\n",
      "==================================================\n",
      "PROJECT_ID     = statmike-mlops-349915\n",
      "PROJECT_NUMBER = 1026793852137\n",
      "==================================================\n",
      "\n",
      "\n",
      "==================================================\n",
      "SETUP SUMMARY\n",
      "==================================================\n",
      "‚úÖ Authentication:    Success\n",
      "‚úÖ API Configuration: Success\n",
      "‚úÖ Package Install:   Already up to date\n",
      "‚úÖ Installation Tool: poetry\n",
      "‚úÖ Project ID:        statmike-mlops-349915\n",
      "‚úÖ Project Number:    1026793852137\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os, urllib.request\n",
    "\n",
    "# Download and import setup code\n",
    "url = 'https://raw.githubusercontent.com/statmike/vertex-ai-mlops/refs/heads/main/core/notebook-template/python_setup.py'\n",
    "urllib.request.urlretrieve(url, 'python_setup_local.py')\n",
    "import python_setup_local as python_setup\n",
    "os.remove('python_setup_local.py')\n",
    "\n",
    "# Run setup\n",
    "setup_info = python_setup.setup_environment(PROJECT_ID, REQ_TYPE, REQUIREMENTS_URL, REQUIRED_APIS, INSTALL_TOOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "test_config_header",
   "metadata": {},
   "source": [
    "---\n",
    "## Test Configuration\n",
    "\n",
    "Configure the Dataflow job to test and the test parameters.\n",
    "\n",
    "**Job Configuration:**\n",
    "- Update `JOB_ID` with your running Dataflow streaming job ID\n",
    "- Update `REGION` if your job is in a different region\n",
    "- Job must be in \"Running\" state (check [Dataflow Console](https://console.cloud.google.com/dataflow))\n",
    "\n",
    "**Test Parameters:**\n",
    "- Adjust message rates and durations based on your needs\n",
    "- Default values are designed to comprehensively test a PyTorch streaming pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "job_config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataflow Job Configuration\n",
    "REGION = 'us-central1'\n",
    "JOB_NAME_PREFIX = 'pytorch-streaming-'  # Job name prefix from dataflow-streaming-runinference.ipynb\n",
    "\n",
    "# Pub/Sub Configuration (LOCAL model pipeline - isolated from Vertex endpoint pipeline)\n",
    "EXPERIMENT = 'pytorch-autoencoder'\n",
    "INPUT_TOPIC = f'projects/{PROJECT_ID}/topics/{EXPERIMENT}-input-local'\n",
    "OUTPUT_SUBSCRIPTION = f'projects/{PROJECT_ID}/subscriptions/{EXPERIMENT}-output-sub-local'\n",
    "\n",
    "# Phase 1: Baseline Performance\n",
    "BASELINE_TESTS = [\n",
    "    {\"target_rate\": 10, \"duration\": 120, \"name\": \"Baseline - 10 msg/sec\"},\n",
    "    {\"target_rate\": 25, \"duration\": 120, \"name\": \"Baseline - 25 msg/sec\"},\n",
    "]\n",
    "\n",
    "# Phase 2A: Throughput Threshold Hunt\n",
    "SUSTAINED_TESTS = [\n",
    "    {\"target_rate\": 50, \"duration\": 300, \"name\": \"Sustained - 50 msg/sec\"},\n",
    "    {\"target_rate\": 100, \"duration\": 300, \"name\": \"Sustained - 100 msg/sec\"},\n",
    "    {\"target_rate\": 200, \"duration\": 300, \"name\": \"Sustained - 200 msg/sec\"},\n",
    "]\n",
    "\n",
    "# Phase 2B: Burst Capacity\n",
    "BURST_TESTS = [\n",
    "    {\"num_messages\": 1000, \"name\": \"Burst - 1,000 messages\"},\n",
    "    {\"num_messages\": 5000, \"name\": \"Burst - 5,000 messages\"},\n",
    "    {\"num_messages\": 10000, \"name\": \"Burst - 10,000 messages\"},\n",
    "]\n",
    "\n",
    "# Phase 3: Ramp Test\n",
    "RAMP_TEST = {\"target_rate\": 500, \"duration\": 900, \"name\": \"Ramp - 0‚Üí500 msg/sec\"}  # 15 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "imports",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Imports complete\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import asyncio\n",
    "import time\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from scale_testing_dataflow_utils import (\n",
    "    PubSubLoadGenerator,\n",
    "    DataflowMetricsCollector,\n",
    "    plot_dataflow_timeline\n",
    ")\n",
    "from google.cloud import dataflow_v1beta3\n",
    "\n",
    "print(\"‚úÖ Imports complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verify_job_header",
   "metadata": {},
   "source": [
    "### Verify Job is Running\n",
    "\n",
    "Before starting tests, verify the Dataflow job is in \"Running\" state with active workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0fd6222",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for running Dataflow jobs...\n",
      "\n",
      "======================================================================\n",
      "DATAFLOW JOB STATUS\n",
      "======================================================================\n",
      "Name: pytorch-streaming-20251112-222612\n",
      "ID: 2025-11-12_15_18_16-2368771781355830795\n",
      "State: JOB_STATE_RUNNING\n",
      "Created: 2025-11-12 23:18:17.444123+00:00\n",
      "Type: JOB_TYPE_STREAMING\n",
      "\n",
      "‚úÖ Job is running and ready for testing\n",
      "======================================================================\n",
      "\n",
      "Monitor job: https://console.cloud.google.com/dataflow/jobs/us-central1/2025-11-12_15_18_16-2368771781355830795?project=statmike-mlops-349915\n"
     ]
    }
   ],
   "source": [
    "# Find and verify Dataflow job\n",
    "client = dataflow_v1beta3.JobsV1Beta3Client()\n",
    "\n",
    "# Step 1: Auto-discover running job with expected prefix\n",
    "print(\"Searching for running Dataflow jobs...\")\n",
    "request = dataflow_v1beta3.ListJobsRequest(\n",
    "    project_id=PROJECT_ID,\n",
    "    location=REGION\n",
    ")\n",
    "\n",
    "JOB_ID = None\n",
    "JOB_NAME = None\n",
    "\n",
    "try:\n",
    "    response = client.list_jobs(request=request)\n",
    "    \n",
    "    for job in response:\n",
    "        # Only match LOCAL model jobs (exclude -vertex endpoint jobs)\n",
    "        if (job.current_state.name == \"JOB_STATE_RUNNING\" and\n",
    "            job.name.startswith(JOB_NAME_PREFIX) and\n",
    "            \"-vertex\" not in job.name):\n",
    "            JOB_ID = job.id\n",
    "            JOB_NAME = job.name\n",
    "            break\n",
    "    \n",
    "    if not JOB_ID:\n",
    "        print(f\"\\n‚ö†Ô∏è  No running LOCAL model jobs found with prefix: {JOB_NAME_PREFIX}\")\n",
    "        print(\"   (Vertex endpoint jobs excluded - this tests LOCAL model only)\")\n",
    "        print(\"   Please start a LOCAL model job using dataflow-streaming-runinference.ipynb first\")\n",
    "        raise ValueError(\"No running LOCAL model Dataflow job found\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ö†Ô∏è  Error finding Dataflow job: {e}\")\n",
    "    raise\n",
    "\n",
    "# Step 2: Get full job details and verify status\n",
    "job = client.get_job(\n",
    "    request={\n",
    "        \"project_id\": PROJECT_ID,\n",
    "        \"location\": REGION,\n",
    "        \"job_id\": JOB_ID\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"DATAFLOW JOB STATUS\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Name: {job.name}\")\n",
    "print(f\"ID: {job.id}\")\n",
    "print(f\"State: {job.current_state.name}\")\n",
    "print(f\"Created: {job.create_time}\")\n",
    "print(f\"Type: {job.type_.name}\")\n",
    "\n",
    "if job.current_state.name == \"JOB_STATE_RUNNING\":\n",
    "    print(\"\\n‚úÖ Job is running and ready for testing\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è  Job is not running (state: {job.current_state.name})\")\n",
    "    print(\"   Please start the job before running tests\")\n",
    "    raise ValueError(f\"Job is not in RUNNING state: {job.current_state.name}\")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nMonitor job: https://console.cloud.google.com/dataflow/jobs/{REGION}/{JOB_ID}?project={PROJECT_ID}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lym26b1dizo",
   "metadata": {},
   "source": [
    "### Initialize Testing Infrastructure\n",
    "\n",
    "Now that we've verified the Dataflow job is running, initialize the testing components:\n",
    "\n",
    "- **PubSubLoadGenerator**: Publishes test messages to the input topic with different load patterns (burst, sustained, ramp)\n",
    "- **DataflowMetricsCollector**: Collects metrics from Cloud Monitoring (worker count, system lag, backlog) and end-to-end latency from Pub/Sub output\n",
    "\n",
    "These components will be used throughout all test phases to generate load and measure performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff374321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Testing infrastructure initialized\n",
      "   Project: statmike-mlops-349915\n",
      "   Job ID: 2025-11-12_15_18_16-2368771781355830795\n",
      "   Region: us-central1\n",
      "   Input topic: projects/statmike-mlops-349915/topics/pytorch-autoencoder-input-local\n",
      "   Output subscription: projects/statmike-mlops-349915/subscriptions/pytorch-autoencoder-output-sub-local\n"
     ]
    }
   ],
   "source": [
    "# Initialize testing infrastructure\n",
    "load_generator = PubSubLoadGenerator(\n",
    "    project_id=PROJECT_ID,\n",
    "    topic_name=f\"{EXPERIMENT}-input-local\"  # LOCAL model topic (isolated from Vertex endpoint)\n",
    ")\n",
    "\n",
    "metrics_collector = DataflowMetricsCollector(\n",
    "    project_id=PROJECT_ID,\n",
    "    job_id=JOB_ID,\n",
    "    region=REGION,\n",
    "    output_subscription=OUTPUT_SUBSCRIPTION  # LOCAL model output subscription\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Testing infrastructure initialized\")\n",
    "print(f\"   Project: {PROJECT_ID}\")\n",
    "print(f\"   Job ID: {JOB_ID}\")\n",
    "print(f\"   Region: {REGION}\")\n",
    "print(f\"   Input topic: {INPUT_TOPIC}\")\n",
    "print(f\"   Output subscription: {OUTPUT_SUBSCRIPTION}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phase1_header",
   "metadata": {},
   "source": [
    "---\n",
    "## Phase 1: Baseline Performance\n",
    "\n",
    "**Goal**: Establish baseline latency and confirm pipeline health\n",
    "\n",
    "**Tests**: 2 tests (~5 minutes total)\n",
    "- 10 msg/sec for 2 minutes\n",
    "- 25 msg/sec for 2 minutes\n",
    "\n",
    "**What We're Looking For**:\n",
    "- Baseline end-to-end latency at low load\n",
    "- Latency breakdown (window wait vs processing vs pub/sub)\n",
    "- Confirm no backlog builds at low rates\n",
    "- Verify pipeline is healthy and processing correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "phase1_init",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PHASE 1: BASELINE PERFORMANCE\n",
      "======================================================================\n",
      "Testing 2 baseline rates\n",
      "Total estimated time: ~5 minutes\n",
      "======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize Phase 1 storage\n",
    "phase1_results = []\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"PHASE 1: BASELINE PERFORMANCE\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Testing {len(BASELINE_TESTS)} baseline rates\")\n",
    "print(f\"Total estimated time: ~5 minutes\")\n",
    "print(\"=\"*70 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "phase1_run",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Baseline - 10 msg/sec\n",
      "======================================================================\n",
      "Pattern: SUSTAINED\n",
      "Target: 10 messages/sec √ó 120s = 1,200 messages\n",
      "\n",
      "‚è≥ Running sustained load test (120s = 2 mins 0s)...\n",
      "   Progress updates every 60 seconds...\n",
      "   [ 60s] 601 messages sent...\n",
      "\n",
      "‚úÖ Complete in 119.9s\n",
      "   Sent: 1,200 messages\n",
      "   Rate: 10.0 messages/sec\n",
      "\n",
      "   Waiting 30 seconds before next test...\n",
      "\n",
      "\n",
      "======================================================================\n",
      "Baseline - 25 msg/sec\n",
      "======================================================================\n",
      "Pattern: SUSTAINED\n",
      "Target: 25 messages/sec √ó 120s = 3,000 messages\n",
      "\n",
      "‚è≥ Running sustained load test (120s = 2 mins 0s)...\n",
      "   Progress updates every 60 seconds...\n",
      "   [ 60s] 1,501 messages sent...\n",
      "\n",
      "‚úÖ Complete in 120.0s\n",
      "   Sent: 3,000 messages\n",
      "   Rate: 25.0 messages/sec\n",
      "\n",
      "   Waiting 30 seconds before next test...\n",
      "\n",
      "\n",
      "‚úÖ Phase 1 complete: 2 baseline tests run\n"
     ]
    }
   ],
   "source": [
    "# Run baseline tests\n",
    "for test_config in BASELINE_TESTS:\n",
    "    result = await load_generator.run_load_test(\n",
    "        pattern=\"sustained\",\n",
    "        target_rate=test_config[\"target_rate\"],\n",
    "        duration=test_config[\"duration\"],\n",
    "        test_name=test_config[\"name\"]\n",
    "    )\n",
    "    phase1_results.append(result)\n",
    "    \n",
    "    # Small delay between tests\n",
    "    print(\"\\n   Waiting 30 seconds before next test...\\n\")\n",
    "    await asyncio.sleep(30)\n",
    "\n",
    "print(f\"\\n‚úÖ Phase 1 complete: {len(BASELINE_TESTS)} baseline tests run\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phase1_metrics_header",
   "metadata": {},
   "source": [
    "### Collect Metrics for Phase 1\n",
    "\n",
    "Collect Dataflow metrics (workers, system lag, backlog) and end-to-end latency from Pub/Sub output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "phase1_metrics",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting metrics for Phase 1\n",
      "Time window: 00:00:18 ‚Üí 00:04:47\n",
      "\n",
      "üîç Collecting Dataflow metrics...\n",
      "   Time window: 00:00:18 ‚Üí 00:04:47\n",
      "   Waiting 90 seconds for metrics to propagate...\n",
      "   Collecting worker count...\n",
      "   Collecting system lag...\n",
      "   Collecting element count...\n",
      "   Collecting Pub/Sub backlog...\n",
      "‚úÖ Metrics collection complete\n",
      "   workers: 6 data points\n",
      "   system_lag: 6 data points\n",
      "   element_count: 5 data points\n",
      "   backlog: 7 data points\n",
      "\n",
      "‚úÖ Metrics collected for Phase 1\n"
     ]
    }
   ],
   "source": [
    "# Collect metrics for entire Phase 1\n",
    "phase1_start = min([r['start_time'] for r in phase1_results])\n",
    "phase1_end = max([r['end_time'] for r in phase1_results])\n",
    "\n",
    "print(f\"Collecting metrics for Phase 1\")\n",
    "print(f\"Time window: {phase1_start.strftime('%H:%M:%S')} ‚Üí {phase1_end.strftime('%H:%M:%S')}\")\n",
    "\n",
    "phase1_metrics = metrics_collector.collect_metrics(\n",
    "    start_time=phase1_start,\n",
    "    end_time=phase1_end,\n",
    "    resolution_seconds=10\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Metrics collected for Phase 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "phase1_latency",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Collecting end-to-end latency from Pub/Sub output...\n",
      "   Subscription: projects/statmike-mlops-349915/subscriptions/pytorch-autoencoder-output-sub-local\n",
      "   Test ID: sustained-20251113-000018\n",
      "   Expected messages: 1,200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Collected 100/1,200 messages... (2s elapsed)\n",
      "   Collected 200/1,200 messages... (3s elapsed)\n",
      "   Collected 300/1,200 messages... (4s elapsed)\n",
      "   Collected 400/1,200 messages... (5s elapsed)\n",
      "   Collected 500/1,200 messages... (6s elapsed)\n",
      "   Collected 600/1,200 messages... (7s elapsed)\n",
      "   Collected 700/1,200 messages... (9s elapsed)\n",
      "   Collected 800/1,200 messages... (10s elapsed)\n",
      "   Collected 900/1,200 messages... (11s elapsed)\n",
      "   Collected 1,000/1,200 messages... (12s elapsed)\n",
      "   Collected 1,100/1,200 messages... (14s elapsed)\n",
      "   Collected 1,200/1,200 messages... (15s elapsed)\n",
      "‚úÖ Latency collection complete: 1,200 messages\n",
      "\n",
      "üìä Collecting end-to-end latency from Pub/Sub output...\n",
      "   Subscription: projects/statmike-mlops-349915/subscriptions/pytorch-autoencoder-output-sub-local\n",
      "   Test ID: sustained-20251113-000247\n",
      "   Expected messages: 3,000\n",
      "   Collected 100/3,000 messages... (1s elapsed)\n",
      "   Collected 200/3,000 messages... (2s elapsed)\n",
      "   Collected 3,000/3,000 messages... (31s elapsed)\n",
      "‚úÖ Latency collection complete: 3,000 messages\n",
      "\n",
      "‚úÖ Latency data collected for 2 tests\n"
     ]
    }
   ],
   "source": [
    "# Collect end-to-end latency for each test\n",
    "phase1_latencies = []\n",
    "\n",
    "for result in phase1_results:\n",
    "    latency_df = metrics_collector.collect_end_to_end_latency(\n",
    "        test_id=result['test_id'],\n",
    "        expected_messages=result['num_messages'],\n",
    "        timeout=180\n",
    "    )\n",
    "    phase1_latencies.append(latency_df)\n",
    "\n",
    "print(f\"\\n‚úÖ Latency data collected for {len(phase1_latencies)} tests\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phase1_analysis_header",
   "metadata": {},
   "source": [
    "### Analyze Phase 1 Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "phase1_analysis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PHASE 1 ANALYSIS\n",
      "======================================================================\n",
      "\n",
      "Baseline - 10 msg/sec:\n",
      "  Target: 10 msg/sec for 120s\n",
      "  Sent: 1,200 messages\n",
      "  Actual rate: 10.0 msg/sec\n",
      "\n",
      "  Latency Breakdown:\n",
      "    Window Wait:       532.7ms (mean) |   982.7ms (p95)\n",
      "    Processing:       -161.3ms (mean) |   715.4ms (p95)\n",
      "    Pub/Sub Delivery:338121.1ms (mean) | 386651.5ms (p95)\n",
      "    Total E2E:         371.4ms (mean) |   881.7ms (p95)\n",
      "\n",
      "  Latency Composition:\n",
      "    Window Wait: 143.4%\n",
      "    Processing: -43.4%\n",
      "    Pub/Sub Delivery: -0.0%\n",
      "\n",
      "Baseline - 25 msg/sec:\n",
      "  Target: 25 msg/sec for 120s\n",
      "  Sent: 3,000 messages\n",
      "  Actual rate: 25.0 msg/sec\n",
      "\n",
      "  Latency Breakdown:\n",
      "    Window Wait:       503.4ms (mean) |   943.0ms (p95)\n",
      "    Processing:        -92.8ms (mean) |   475.8ms (p95)\n",
      "    Pub/Sub Delivery:212744.4ms (mean) | 251223.3ms (p95)\n",
      "    Total E2E:         410.5ms (mean) |   752.7ms (p95)\n",
      "\n",
      "  Latency Composition:\n",
      "    Window Wait: 122.6%\n",
      "    Processing: -22.6%\n",
      "    Pub/Sub Delivery: -0.0%\n",
      "\n",
      "======================================================================\n",
      "AUTOSCALING ANALYSIS\n",
      "======================================================================\n",
      "‚ÑπÔ∏è  No autoscaling detected at baseline rates (expected)\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Analyze Phase 1 results\n",
    "print(\"=\"*70)\n",
    "print(\"PHASE 1 ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for i, (result, latency_df) in enumerate(zip(phase1_results, phase1_latencies)):\n",
    "    test_config = BASELINE_TESTS[i]\n",
    "    \n",
    "    print(f\"\\n{test_config['name']}:\")\n",
    "    print(f\"  Target: {test_config['target_rate']} msg/sec for {test_config['duration']}s\")\n",
    "    print(f\"  Sent: {result['num_messages']:,} messages\")\n",
    "    print(f\"  Actual rate: {result['actual_rate']:.1f} msg/sec\")\n",
    "    \n",
    "    if len(latency_df) > 0:\n",
    "        print(f\"\\n  Latency Breakdown:\")\n",
    "        print(f\"    Window Wait:     {latency_df['window_wait_ms'].mean():7.1f}ms (mean) | {latency_df['window_wait_ms'].quantile(0.95):7.1f}ms (p95)\")\n",
    "        print(f\"    Processing:      {latency_df['processing_ms'].mean():7.1f}ms (mean) | {latency_df['processing_ms'].quantile(0.95):7.1f}ms (p95)\")\n",
    "        print(f\"    Pub/Sub Delivery:{latency_df['queue_wait_ms'].mean():7.1f}ms (mean) | {latency_df['queue_wait_ms'].quantile(0.95):7.1f}ms (p95)\")\n",
    "        print(f\"    Total E2E:       {latency_df['pipeline_latency_ms'].mean():7.1f}ms (mean) | {latency_df['pipeline_latency_ms'].quantile(0.95):7.1f}ms (p95)\")\n",
    "        \n",
    "        # Identify bottleneck\n",
    "        window_pct = latency_df['window_wait_ms'].mean() / latency_df['pipeline_latency_ms'].mean() * 100\n",
    "        processing_pct = latency_df['processing_ms'].mean() / latency_df['pipeline_latency_ms'].mean() * 100\n",
    "        \n",
    "        print(f\"\\n  Latency Composition:\")\n",
    "        print(f\"    Window Wait: {window_pct:.1f}%\")\n",
    "        print(f\"    Processing: {processing_pct:.1f}%\")\n",
    "        print(f\"    Pub/Sub Delivery: {100 - window_pct - processing_pct:.1f}%\")\n",
    "    else:\n",
    "        print(f\"\\n  ‚ö†Ô∏è  No latency data collected (messages may not have been processed yet)\")\n",
    "\n",
    "# Check for autoscaling\n",
    "autoscaling_events = metrics_collector.analyze_autoscaling(phase1_metrics)\n",
    "\n",
    "print(f\"\\n\" + \"=\"*70)\n",
    "print(\"AUTOSCALING ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "if len(autoscaling_events) > 0:\n",
    "    print(f\"‚úÖ Autoscaling triggered {len(autoscaling_events)} time(s)\")\n",
    "    for idx, event in autoscaling_events.iterrows():\n",
    "        print(f\"\\nEvent {idx + 1}:\")\n",
    "        print(f\"  Trigger: {event['trigger_time'].strftime('%H:%M:%S')}\")\n",
    "        print(f\"  Complete: {event['scale_complete_time'].strftime('%H:%M:%S')}\")\n",
    "        print(f\"  Workers: {event['workers_before']:.0f} ‚Üí {event['workers_after']:.0f}\")\n",
    "        print(f\"  Lag: {event['scale_up_lag_seconds']:.0f}s ({event['scale_up_lag_seconds']/60:.1f} mins)\")\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è  No autoscaling detected at baseline rates (expected)\")\n",
    "\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "phase1_viz",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Collecting Dataflow metrics...\n",
      "   Time window: 00:00:18 ‚Üí 00:02:17\n",
      "   Waiting 90 seconds for metrics to propagate...\n",
      "   Collecting worker count...\n",
      "   Collecting system lag...\n",
      "   Collecting element count...\n",
      "   Collecting Pub/Sub backlog...\n",
      "‚úÖ Metrics collection complete\n",
      "   workers: 6 data points\n",
      "   system_lag: 6 data points\n",
      "   element_count: 6 data points\n",
      "   backlog: 6 data points\n"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "line": {
          "color": "blue",
          "width": 2
         },
         "mode": "lines",
         "name": "Publish Rate",
         "type": "scatter",
         "x": [
          "2025-11-13T00:00:10",
          "2025-11-13T00:00:20",
          "2025-11-13T00:00:30",
          "2025-11-13T00:00:40",
          "2025-11-13T00:00:50",
          "2025-11-13T00:01:00",
          "2025-11-13T00:01:10",
          "2025-11-13T00:01:20",
          "2025-11-13T00:01:30",
          "2025-11-13T00:01:40",
          "2025-11-13T00:01:50",
          "2025-11-13T00:02:00",
          "2025-11-13T00:02:10"
         ],
         "xaxis": "x",
         "y": {
          "bdata": "AAAAAAAAAEAAAAAAAAAkQAAAAAAAACRAAAAAAAAAJEAAAAAAAAAkQAAAAAAAACRAAAAAAAAAJEAAAAAAAAAkQAAAAAAAACRAAAAAAAAAJEAAAAAAAAAkQAAAAAAAACRAAAAAAAAAIEA=",
          "dtype": "f8"
         },
         "yaxis": "y"
        },
        {
         "line": {
          "color": "green",
          "width": 2
         },
         "marker": {
          "size": 6
         },
         "mode": "lines+markers",
         "name": "Workers",
         "type": "scatter",
         "x": [
          "2025-11-12T23:59:07",
          "2025-11-13T00:00:07",
          "2025-11-13T00:01:07",
          "2025-11-13T00:02:07",
          "2025-11-13T00:03:07",
          "2025-11-13T00:04:07"
         ],
         "xaxis": "x2",
         "y": {
          "bdata": "AAAAAAAAEEAAAAAAAAAQQAAAAAAAABBAAAAAAAAAEEAAAAAAAAAQQAAAAAAAABBA",
          "dtype": "f8"
         },
         "yaxis": "y2"
        },
        {
         "line": {
          "color": "orange",
          "width": 2
         },
         "mode": "lines",
         "name": "System Lag",
         "type": "scatter",
         "x": [
          "2025-11-12T23:59:07",
          "2025-11-13T00:00:07",
          "2025-11-13T00:01:07",
          "2025-11-13T00:02:07",
          "2025-11-13T00:03:07",
          "2025-11-13T00:04:07"
         ],
         "xaxis": "x3",
         "y": {
          "bdata": "AAAAAAAAAAD8qfHSTWJQP/yp8dJNYlA//Knx0k1iUD8AAAAAAAAAAPyp8dJNYlA/",
          "dtype": "f8"
         },
         "yaxis": "y3"
        },
        {
         "line": {
          "color": "red",
          "width": 2
         },
         "mode": "lines",
         "name": "Backlog",
         "type": "scatter",
         "x": [
          "2025-11-12T23:59:07",
          "2025-11-13T00:00:07",
          "2025-11-13T00:01:07",
          "2025-11-13T00:02:07",
          "2025-11-13T00:03:07",
          "2025-11-13T00:04:07"
         ],
         "xaxis": "x4",
         "y": {
          "bdata": "AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAPiBQAAAAAAAuJFA",
          "dtype": "f8"
         },
         "yaxis": "y4"
        },
        {
         "line": {
          "color": "purple",
          "width": 2
         },
         "mode": "lines",
         "name": "P95 Pipeline Latency",
         "type": "scatter",
         "x": [
          "2025-11-13T00:06:50",
          "2025-11-13T00:07:00"
         ],
         "xaxis": "x5",
         "y": {
          "bdata": "AABALA5QjEABAEAq/p2CQA==",
          "dtype": "f8"
         },
         "yaxis": "y5"
        }
       ],
       "layout": {
        "annotations": [
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Message Rate",
          "x": 0.5,
          "xanchor": "center",
          "xref": "paper",
          "y": 1,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Worker Count (Autoscaling)",
          "x": 0.5,
          "xanchor": "center",
          "xref": "paper",
          "y": 0.7879999999999999,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "System Lag (Processing Delay)",
          "x": 0.5,
          "xanchor": "center",
          "xref": "paper",
          "y": 0.576,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Backlog Size (Unprocessed Messages)",
          "x": 0.5,
          "xanchor": "center",
          "xref": "paper",
          "y": 0.364,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "P95 End-to-End Latency",
          "x": 0.5,
          "xanchor": "center",
          "xref": "paper",
          "y": 0.152,
          "yanchor": "bottom",
          "yref": "paper"
         }
        ],
        "height": 1200,
        "hovermode": "x unified",
        "showlegend": true,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermap": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermap"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Baseline - 10 msg/sec"
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "matches": "x5",
         "showticklabels": false
        },
        "xaxis2": {
         "anchor": "y2",
         "domain": [
          0,
          1
         ],
         "matches": "x5",
         "showticklabels": false
        },
        "xaxis3": {
         "anchor": "y3",
         "domain": [
          0,
          1
         ],
         "matches": "x5",
         "showticklabels": false
        },
        "xaxis4": {
         "anchor": "y4",
         "domain": [
          0,
          1
         ],
         "matches": "x5",
         "showticklabels": false
        },
        "xaxis5": {
         "anchor": "y5",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Time"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0.848,
          1
         ],
         "title": {
          "text": "Messages/sec"
         }
        },
        "yaxis2": {
         "anchor": "x2",
         "domain": [
          0.6359999999999999,
          0.7879999999999999
         ],
         "title": {
          "text": "Workers"
         }
        },
        "yaxis3": {
         "anchor": "x3",
         "domain": [
          0.424,
          0.576
         ],
         "title": {
          "text": "Lag (ms)"
         }
        },
        "yaxis4": {
         "anchor": "x4",
         "domain": [
          0.212,
          0.364
         ],
         "title": {
          "text": "Messages"
         }
        },
        "yaxis5": {
         "anchor": "x5",
         "domain": [
          0,
          0.152
         ],
         "title": {
          "text": "Latency (ms)"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Collecting Dataflow metrics...\n",
      "   Time window: 00:02:47 ‚Üí 00:04:47\n",
      "   Waiting 90 seconds for metrics to propagate...\n",
      "   Collecting worker count...\n",
      "   Collecting system lag...\n",
      "   Collecting element count...\n",
      "   Collecting Pub/Sub backlog...\n",
      "‚úÖ Metrics collection complete\n",
      "   workers: 6 data points\n",
      "   system_lag: 6 data points\n",
      "   element_count: 6 data points\n",
      "   backlog: 6 data points\n"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "line": {
          "color": "blue",
          "width": 2
         },
         "mode": "lines",
         "name": "Publish Rate",
         "type": "scatter",
         "x": [
          "2025-11-13T00:02:40",
          "2025-11-13T00:02:50",
          "2025-11-13T00:03:00",
          "2025-11-13T00:03:10",
          "2025-11-13T00:03:20",
          "2025-11-13T00:03:30",
          "2025-11-13T00:03:40",
          "2025-11-13T00:03:50",
          "2025-11-13T00:04:00",
          "2025-11-13T00:04:10",
          "2025-11-13T00:04:20",
          "2025-11-13T00:04:30",
          "2025-11-13T00:04:40"
         ],
         "xaxis": "x",
         "y": {
          "bdata": "zczMzMzMFEAAAAAAAAA5QAAAAAAAADlAAAAAAAAAOUAAAAAAAAA5QAAAAAAAADlAAAAAAAAAOUAAAAAAAAA5QAAAAAAAADlAAAAAAAAAOUAAAAAAAAA5QAAAAAAAADlAzczMzMzMM0A=",
          "dtype": "f8"
         },
         "yaxis": "y"
        },
        {
         "line": {
          "color": "green",
          "width": 2
         },
         "marker": {
          "size": 6
         },
         "mode": "lines+markers",
         "name": "Workers",
         "type": "scatter",
         "x": [
          "2025-11-13T00:01:07",
          "2025-11-13T00:02:07",
          "2025-11-13T00:03:07",
          "2025-11-13T00:04:07",
          "2025-11-13T00:05:07",
          "2025-11-13T00:06:07"
         ],
         "xaxis": "x2",
         "y": {
          "bdata": "AAAAAAAAEEAAAAAAAAAQQAAAAAAAABBAAAAAAAAAEEAAAAAAAAAQQAAAAAAAABBA",
          "dtype": "f8"
         },
         "yaxis": "y2"
        },
        {
         "line": {
          "color": "orange",
          "width": 2
         },
         "mode": "lines",
         "name": "System Lag",
         "type": "scatter",
         "x": [
          "2025-11-13T00:01:07",
          "2025-11-13T00:02:07",
          "2025-11-13T00:03:07",
          "2025-11-13T00:04:07",
          "2025-11-13T00:05:07",
          "2025-11-13T00:06:07"
         ],
         "xaxis": "x3",
         "y": {
          "bdata": "/Knx0k1iUD/8qfHSTWJQPwAAAAAAAAAA/Knx0k1iUD8AAAAAAAAAAAAAAAAAAAAA",
          "dtype": "f8"
         },
         "yaxis": "y3"
        },
        {
         "line": {
          "color": "red",
          "width": 2
         },
         "mode": "lines",
         "name": "Backlog",
         "type": "scatter",
         "x": [
          "2025-11-13T00:01:07",
          "2025-11-13T00:02:07",
          "2025-11-13T00:03:07",
          "2025-11-13T00:04:07",
          "2025-11-13T00:05:07",
          "2025-11-13T00:06:07"
         ],
         "xaxis": "x4",
         "y": {
          "bdata": "AAAAAAAAAAAAAAAAAAAAAAAAAAAA+IFAAAAAAAC4kUAAAAAAADKjQAAAAAAA3q5A",
          "dtype": "f8"
         },
         "yaxis": "y4"
        },
        {
         "line": {
          "color": "purple",
          "width": 2
         },
         "mode": "lines",
         "name": "P95 Pipeline Latency",
         "type": "scatter",
         "x": [
          "2025-11-13T00:07:00",
          "2025-11-13T00:07:10",
          "2025-11-13T00:07:20",
          "2025-11-13T00:07:30"
         ],
         "xaxis": "x5",
         "y": {
          "bdata": "/v//J8kghkAAAMCq0HaGQP3/v+wHf4dAAADATIMuiUA=",
          "dtype": "f8"
         },
         "yaxis": "y5"
        }
       ],
       "layout": {
        "annotations": [
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Message Rate",
          "x": 0.5,
          "xanchor": "center",
          "xref": "paper",
          "y": 1,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Worker Count (Autoscaling)",
          "x": 0.5,
          "xanchor": "center",
          "xref": "paper",
          "y": 0.7879999999999999,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "System Lag (Processing Delay)",
          "x": 0.5,
          "xanchor": "center",
          "xref": "paper",
          "y": 0.576,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Backlog Size (Unprocessed Messages)",
          "x": 0.5,
          "xanchor": "center",
          "xref": "paper",
          "y": 0.364,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "P95 End-to-End Latency",
          "x": 0.5,
          "xanchor": "center",
          "xref": "paper",
          "y": 0.152,
          "yanchor": "bottom",
          "yref": "paper"
         }
        ],
        "height": 1200,
        "hovermode": "x unified",
        "showlegend": true,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermap": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermap"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Baseline - 25 msg/sec"
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "matches": "x5",
         "showticklabels": false
        },
        "xaxis2": {
         "anchor": "y2",
         "domain": [
          0,
          1
         ],
         "matches": "x5",
         "showticklabels": false
        },
        "xaxis3": {
         "anchor": "y3",
         "domain": [
          0,
          1
         ],
         "matches": "x5",
         "showticklabels": false
        },
        "xaxis4": {
         "anchor": "y4",
         "domain": [
          0,
          1
         ],
         "matches": "x5",
         "showticklabels": false
        },
        "xaxis5": {
         "anchor": "y5",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Time"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0.848,
          1
         ],
         "title": {
          "text": "Messages/sec"
         }
        },
        "yaxis2": {
         "anchor": "x2",
         "domain": [
          0.6359999999999999,
          0.7879999999999999
         ],
         "title": {
          "text": "Workers"
         }
        },
        "yaxis3": {
         "anchor": "x3",
         "domain": [
          0.424,
          0.576
         ],
         "title": {
          "text": "Lag (ms)"
         }
        },
        "yaxis4": {
         "anchor": "x4",
         "domain": [
          0.212,
          0.364
         ],
         "title": {
          "text": "Messages"
         }
        },
        "yaxis5": {
         "anchor": "x5",
         "domain": [
          0,
          0.152
         ],
         "title": {
          "text": "Latency (ms)"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize Phase 1 - Individual test timelines\n",
    "for i, (result, latency_df) in enumerate(zip(phase1_results, phase1_latencies)):\n",
    "    test_config = BASELINE_TESTS[i]\n",
    "    \n",
    "    # Collect metrics for this specific test\n",
    "    test_metrics = metrics_collector.collect_metrics(\n",
    "        start_time=result['start_time'],\n",
    "        end_time=result['end_time'],\n",
    "        resolution_seconds=10\n",
    "    )\n",
    "    \n",
    "    fig = plot_dataflow_timeline(\n",
    "        test_results=result,\n",
    "        metrics=test_metrics,\n",
    "        latency_df=latency_df,\n",
    "        test_name=test_config['name']\n",
    "    )\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phase2a_header",
   "metadata": {},
   "source": [
    "---\n",
    "## Phase 2A: Throughput Threshold Hunt\n",
    "\n",
    "**Goal**: Find exact threshold where backlog builds and autoscaling triggers\n",
    "\n",
    "**Tests**: 3 sustained load tests (~20 minutes total)\n",
    "- 50 msg/sec for 5 minutes\n",
    "- 100 msg/sec for 5 minutes\n",
    "- 200 msg/sec for 5 minutes\n",
    "\n",
    "**What We're Looking For**:\n",
    "- At what message rate does backlog start building?\n",
    "- At what point does system lag increase?\n",
    "- When do workers autoscale up?\n",
    "- How long does it take for new workers to become active?\n",
    "\n",
    "‚è≥ **This phase will take approximately 20 minutes** (3 tests √ó 5 mins each + metrics collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "phase2a_init",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Phase 2A storage\n",
    "phase2a_results = []\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"PHASE 2A: THROUGHPUT THRESHOLD HUNT\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Testing {len(SUSTAINED_TESTS)} sustained load patterns\")\n",
    "print(f\"Total estimated time: ~{sum(t['duration'] for t in SUSTAINED_TESTS) // 60} minutes\")\n",
    "print(\"\\nThis phase tests sustained high load to trigger autoscaling.\")\n",
    "print(\"=\"*70 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "phase2a_run",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run sustained load tests\n",
    "for test_config in SUSTAINED_TESTS:\n",
    "    result = await load_generator.run_load_test(\n",
    "        pattern=\"sustained\",\n",
    "        target_rate=test_config[\"target_rate\"],\n",
    "        duration=test_config[\"duration\"],\n",
    "        test_name=test_config[\"name\"]\n",
    "    )\n",
    "    phase2a_results.append(result)\n",
    "    \n",
    "    # Small delay between tests to let pipeline stabilize\n",
    "    print(\"\\n   Waiting 60 seconds before next test...\\n\")\n",
    "    await asyncio.sleep(60)\n",
    "\n",
    "print(f\"\\n‚úÖ Phase 2A complete: {len(SUSTAINED_TESTS)} sustained tests run\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phase2a_metrics_header",
   "metadata": {},
   "source": [
    "### Collect Metrics for Phase 2A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "phase2a_metrics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect metrics for entire Phase 2A\n",
    "phase2a_start = min([r['start_time'] for r in phase2a_results])\n",
    "phase2a_end = max([r['end_time'] for r in phase2a_results])\n",
    "\n",
    "print(f\"Collecting metrics for Phase 2A\")\n",
    "print(f\"Time window: {phase2a_start.strftime('%H:%M:%S')} ‚Üí {phase2a_end.strftime('%H:%M:%S')}\")\n",
    "\n",
    "phase2a_metrics = metrics_collector.collect_metrics(\n",
    "    start_time=phase2a_start,\n",
    "    end_time=phase2a_end,\n",
    "    resolution_seconds=10\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Metrics collected for Phase 2A\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "phase2a_latency",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect end-to-end latency for each test\n",
    "phase2a_latencies = []\n",
    "\n",
    "for result in phase2a_results:\n",
    "    latency_df = metrics_collector.collect_end_to_end_latency(\n",
    "        test_id=result['test_id'],\n",
    "        expected_messages=result['num_messages'],\n",
    "        timeout=300  # Longer timeout for high-volume tests\n",
    "    )\n",
    "    phase2a_latencies.append(latency_df)\n",
    "\n",
    "print(f\"\\n‚úÖ Latency data collected for {len(phase2a_latencies)} tests\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phase2a_analysis_header",
   "metadata": {},
   "source": [
    "### Analyze Phase 2A Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "phase2a_analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed analysis of Phase 2A\n",
    "print(\"=\"*70)\n",
    "print(\"PHASE 2A ANALYSIS: THROUGHPUT THRESHOLDS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for i, (result, latency_df) in enumerate(zip(phase2a_results, phase2a_latencies)):\n",
    "    test_config = SUSTAINED_TESTS[i]\n",
    "    \n",
    "    print(f\"\\n{test_config['name']}:\")\n",
    "    print(f\"  Configuration: {test_config['target_rate']} msg/sec for {test_config['duration']}s\")\n",
    "    print(f\"  Messages sent: {result['num_messages']:,}\")\n",
    "    print(f\"  Actual rate: {result['actual_rate']:.1f} msg/sec\")\n",
    "    \n",
    "    if len(latency_df) > 0:\n",
    "        print(f\"\\n  Latency Performance:\")\n",
    "        print(f\"    Mean E2E: {latency_df['pipeline_latency_ms'].mean():7.1f}ms\")\n",
    "        print(f\"    P95 E2E:  {latency_df['pipeline_latency_ms'].quantile(0.95):7.1f}ms\")\n",
    "        print(f\"    P99 E2E:  {latency_df['pipeline_latency_ms'].quantile(0.99):7.1f}ms\")\n",
    "        \n",
    "        # Latency degradation check\n",
    "        if i > 0 and len(phase2a_latencies[0]) > 0:\n",
    "            baseline_p95 = phase2a_latencies[0]['pipeline_latency_ms'].quantile(0.95)\n",
    "            current_p95 = latency_df['pipeline_latency_ms'].quantile(0.95)\n",
    "            degradation_pct = ((current_p95 - baseline_p95) / baseline_p95) * 100\n",
    "            \n",
    "            print(f\"\\n  Latency vs Baseline:\")\n",
    "            print(f\"    Baseline P95: {baseline_p95:.1f}ms\")\n",
    "            print(f\"    Current P95:  {current_p95:.1f}ms\")\n",
    "            print(f\"    Change: {degradation_pct:+.1f}%\")\n",
    "            \n",
    "            if degradation_pct > 50:\n",
    "                print(f\"    ‚ö†Ô∏è  Significant latency increase - may indicate capacity limits\")\n",
    "    else:\n",
    "        print(f\"\\n  ‚ö†Ô∏è  No latency data (processing may be delayed)\")\n",
    "\n",
    "# Analyze autoscaling behavior\n",
    "print(f\"\\n\" + \"=\"*70)\n",
    "print(\"AUTOSCALING EVENTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "autoscaling_events = metrics_collector.analyze_autoscaling(phase2a_metrics)\n",
    "\n",
    "if len(autoscaling_events) > 0:\n",
    "    print(f\"\\n‚úÖ Autoscaling triggered {len(autoscaling_events)} time(s) during Phase 2A\\n\")\n",
    "    \n",
    "    for idx, event in autoscaling_events.iterrows():\n",
    "        print(f\"Event {idx + 1}:\")\n",
    "        print(f\"  Trigger time:     {event['trigger_time'].strftime('%H:%M:%S')}\")\n",
    "        print(f\"  Complete time:    {event['scale_complete_time'].strftime('%H:%M:%S')}\")\n",
    "        print(f\"  Workers:          {event['workers_before']:.0f} ‚Üí {event['workers_after']:.0f}\")\n",
    "        print(f\"  Provisioning lag: {event['scale_up_lag_seconds']:.0f}s ({event['scale_up_lag_seconds']/60:.1f} mins)\")\n",
    "        print()\n",
    "    \n",
    "    # Calculate average provisioning time\n",
    "    avg_lag = autoscaling_events['scale_up_lag_seconds'].mean()\n",
    "    print(f\"Average worker provisioning time: {avg_lag:.0f}s ({avg_lag/60:.1f} mins)\")\n",
    "else:\n",
    "    print(\"\\n‚ÑπÔ∏è  No autoscaling detected\")\n",
    "    print(\"   Possible reasons:\")\n",
    "    print(\"   - Message rate not high enough to trigger autoscaling\")\n",
    "    print(\"   - Current worker capacity sufficient for tested load\")\n",
    "    print(\"   - Processing is very efficient\")\n",
    "\n",
    "# Analyze backlog behavior\n",
    "if 'backlog' in phase2a_metrics and len(phase2a_metrics['backlog']) > 0:\n",
    "    print(f\"\\n\" + \"=\"*70)\n",
    "    print(\"BACKLOG ANALYSIS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    max_backlog = phase2a_metrics['backlog']['value'].max()\n",
    "    mean_backlog = phase2a_metrics['backlog']['value'].mean()\n",
    "    \n",
    "    print(f\"\\nBacklog Statistics:\")\n",
    "    print(f\"  Max backlog: {max_backlog:,.0f} messages\")\n",
    "    print(f\"  Mean backlog: {mean_backlog:,.0f} messages\")\n",
    "    \n",
    "    if max_backlog > 1000:\n",
    "        print(f\"\\n  ‚ö†Ô∏è  High backlog detected - pipeline may be at capacity\")\n",
    "    elif max_backlog > 100:\n",
    "        print(f\"\\n  ‚ÑπÔ∏è  Moderate backlog - pipeline handling load but close to limits\")\n",
    "    else:\n",
    "        print(f\"\\n  ‚úÖ Backlog under control - pipeline has headroom\")\n",
    "\n",
    "# Analyze system lag\n",
    "if 'system_lag' in phase2a_metrics and len(phase2a_metrics['system_lag']) > 0:\n",
    "    print(f\"\\n\" + \"=\"*70)\n",
    "    print(\"SYSTEM LAG ANALYSIS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    max_lag_ms = phase2a_metrics['system_lag']['value'].max() / 1000  # Convert to ms\n",
    "    mean_lag_ms = phase2a_metrics['system_lag']['value'].mean() / 1000\n",
    "    \n",
    "    print(f\"\\nSystem Lag Statistics:\")\n",
    "    print(f\"  Max lag: {max_lag_ms:,.1f}ms\")\n",
    "    print(f\"  Mean lag: {mean_lag_ms:,.1f}ms\")\n",
    "    \n",
    "    if max_lag_ms > 60000:  # 1 minute\n",
    "        print(f\"\\n  ‚ö†Ô∏è  High system lag - pipeline falling behind real-time\")\n",
    "    elif max_lag_ms > 10000:  # 10 seconds\n",
    "        print(f\"\\n  ‚ÑπÔ∏è  Moderate system lag - processing slightly delayed\")\n",
    "    else:\n",
    "        print(f\"\\n  ‚úÖ System lag low - processing near real-time\")\n",
    "\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "phase2a_viz",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize complete Phase 2A timeline\n",
    "print(\"Creating comprehensive Phase 2A visualization...\")\n",
    "print(f\"This shows all {len(SUSTAINED_TESTS)} sustained load tests in sequence\\n\")\n",
    "\n",
    "# Combine all test results and latencies\n",
    "combined_result = {\n",
    "    'test_id': 'phase2a-combined',\n",
    "    'test_name': 'Phase 2A - All Sustained Tests',\n",
    "    'start_time': phase2a_start,\n",
    "    'end_time': phase2a_end,\n",
    "    'message_data': pd.concat([r['message_data'] for r in phase2a_results], ignore_index=True)\n",
    "}\n",
    "\n",
    "combined_latency = pd.concat(phase2a_latencies, ignore_index=True)\n",
    "\n",
    "fig = plot_dataflow_timeline(\n",
    "    test_results=combined_result,\n",
    "    metrics=phase2a_metrics,\n",
    "    latency_df=combined_latency,\n",
    "    test_name=\"Phase 2A: Throughput Threshold Hunt - Complete Timeline\"\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "print(\"\\nüí° This visualization shows:\")\n",
    "print(f\"   - Message rate progression: {SUSTAINED_TESTS[0]['target_rate']} ‚Üí {SUSTAINED_TESTS[-1]['target_rate']} msg/sec\")\n",
    "print(f\"   - Worker scaling behavior over {len(SUSTAINED_TESTS)} tests\")\n",
    "print(f\"   - System lag and backlog buildup\")\n",
    "print(f\"   - P95 latency trends as load increases\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phase2b_header",
   "metadata": {},
   "source": [
    "---\n",
    "## Phase 2B: Burst Capacity\n",
    "\n",
    "**Goal**: Test backlog recovery time and worker scaling response to sudden spikes\n",
    "\n",
    "**Tests**: 3 burst tests (~10 minutes total)\n",
    "- 1,000 messages sent as fast as possible\n",
    "- 5,000 messages sent as fast as possible\n",
    "- 10,000 messages sent as fast as possible\n",
    "\n",
    "**What We're Looking For**:\n",
    "- How quickly does backlog clear after burst?\n",
    "- Does autoscaling respond to sudden traffic spikes?\n",
    "- What's the maximum burst size before sustained backlog?\n",
    "- Recovery time from burst to normal operation\n",
    "\n",
    "‚è≥ **This phase will take approximately 10 minutes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "phase2b_init",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Phase 2B storage\n",
    "phase2b_results = []\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"PHASE 2B: BURST CAPACITY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Testing {len(BURST_TESTS)} burst patterns\")\n",
    "print(f\"Total estimated time: ~10 minutes\")\n",
    "print(\"\\nThis phase tests sudden traffic spikes and recovery behavior.\")\n",
    "print(\"=\"*70 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "phase2b_run",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run burst tests\n",
    "for test_config in BURST_TESTS:\n",
    "    result = await load_generator.run_load_test(\n",
    "        pattern=\"burst\",\n",
    "        num_messages=test_config[\"num_messages\"],\n",
    "        test_name=test_config[\"name\"]\n",
    "    )\n",
    "    phase2b_results.append(result)\n",
    "    \n",
    "    # Wait for backlog to clear before next burst\n",
    "    print(\"\\n   Waiting 120 seconds for backlog to clear...\\n\")\n",
    "    await asyncio.sleep(120)\n",
    "\n",
    "print(f\"\\n‚úÖ Phase 2B complete: {len(BURST_TESTS)} burst tests run\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phase2b_metrics_header",
   "metadata": {},
   "source": [
    "### Collect Metrics for Phase 2B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "phase2b_metrics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect metrics for entire Phase 2B\n",
    "phase2b_start = min([r['start_time'] for r in phase2b_results])\n",
    "phase2b_end = max([r['end_time'] for r in phase2b_results])\n",
    "\n",
    "print(f\"Collecting metrics for Phase 2B\")\n",
    "print(f\"Time window: {phase2b_start.strftime('%H:%M:%S')} ‚Üí {phase2b_end.strftime('%H:%M:%S')}\")\n",
    "\n",
    "phase2b_metrics = metrics_collector.collect_metrics(\n",
    "    start_time=phase2b_start,\n",
    "    end_time=phase2b_end,\n",
    "    resolution_seconds=10\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Metrics collected for Phase 2B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "phase2b_latency",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect end-to-end latency for burst tests\n",
    "phase2b_latencies = []\n",
    "\n",
    "for result in phase2b_results:\n",
    "    latency_df = metrics_collector.collect_end_to_end_latency(\n",
    "        test_id=result['test_id'],\n",
    "        expected_messages=result['num_messages'],\n",
    "        timeout=300\n",
    "    )\n",
    "    phase2b_latencies.append(latency_df)\n",
    "\n",
    "print(f\"\\n‚úÖ Latency data collected for {len(phase2b_latencies)} burst tests\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phase2b_analysis_header",
   "metadata": {},
   "source": [
    "### Analyze Phase 2B Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "phase2b_analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze burst behavior and recovery\n",
    "print(\"=\"*70)\n",
    "print(\"PHASE 2B ANALYSIS: BURST CAPACITY & RECOVERY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for i, (result, latency_df) in enumerate(zip(phase2b_results, phase2b_latencies)):\n",
    "    test_config = BURST_TESTS[i]\n",
    "    \n",
    "    print(f\"\\n{test_config['name']}:\")\n",
    "    print(f\"  Messages: {test_config['num_messages']:,} sent as fast as possible\")\n",
    "    print(f\"  Publish time: {result['elapsed_seconds']:.1f}s\")\n",
    "    print(f\"  Publish rate: {result['actual_rate']:.1f} msg/sec\")\n",
    "    \n",
    "    if len(latency_df) > 0:\n",
    "        # Calculate recovery metrics\n",
    "        first_msg_latency = latency_df.iloc[0]['total_e2e_ms']\n",
    "        last_msg_latency = latency_df.iloc[-1]['total_e2e_ms']\n",
    "        max_latency = latency_df['pipeline_latency_ms'].max()\n",
    "        \n",
    "        print(f\"\\n  Latency During Burst:\")\n",
    "        print(f\"    First message: {first_msg_latency:,.1f}ms\")\n",
    "        print(f\"    Last message:  {last_msg_latency:,.1f}ms\")\n",
    "        print(f\"    Peak latency:  {max_latency:,.1f}ms\")\n",
    "        print(f\"    Mean latency:  {latency_df['pipeline_latency_ms'].mean():,.1f}ms\")\n",
    "        print(f\"    P95 latency:   {latency_df['pipeline_latency_ms'].quantile(0.95):,.1f}ms\")\n",
    "        \n",
    "        # Analyze latency distribution (early vs late messages)\n",
    "        first_third = latency_df.iloc[:len(latency_df)//3]\n",
    "        last_third = latency_df.iloc[-len(latency_df)//3:]\n",
    "        \n",
    "        print(f\"\\n  Latency Evolution:\")\n",
    "        print(f\"    First 1/3 mean:  {first_third['pipeline_latency_ms'].mean():,.1f}ms\")\n",
    "        print(f\"    Last 1/3 mean:   {last_third['pipeline_latency_ms'].mean():,.1f}ms\")\n",
    "        \n",
    "        if last_third['pipeline_latency_ms'].mean() < first_third['pipeline_latency_ms'].mean():\n",
    "            improvement = ((first_third['pipeline_latency_ms'].mean() - last_third['pipeline_latency_ms'].mean()) / \n",
    "                          first_third['pipeline_latency_ms'].mean() * 100)\n",
    "            print(f\"    ‚úÖ Latency improved by {improvement:.1f}% (backlog clearing)\")\n",
    "        else:\n",
    "            degradation = ((last_third['pipeline_latency_ms'].mean() - first_third['pipeline_latency_ms'].mean()) / \n",
    "                          first_third['pipeline_latency_ms'].mean() * 100)\n",
    "            print(f\"    ‚ö†Ô∏è  Latency degraded by {degradation:.1f}% (backlog building)\")\n",
    "    else:\n",
    "        print(f\"\\n  ‚ö†Ô∏è  No latency data collected\")\n",
    "\n",
    "# Analyze burst autoscaling\n",
    "print(f\"\\n\" + \"=\"*70)\n",
    "print(\"AUTOSCALING RESPONSE TO BURSTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "autoscaling_events = metrics_collector.analyze_autoscaling(phase2b_metrics)\n",
    "\n",
    "if len(autoscaling_events) > 0:\n",
    "    print(f\"\\n‚úÖ Autoscaling responded {len(autoscaling_events)} time(s) to burst traffic\\n\")\n",
    "    \n",
    "    for idx, event in autoscaling_events.iterrows():\n",
    "        print(f\"Event {idx + 1}:\")\n",
    "        print(f\"  Trigger: {event['trigger_time'].strftime('%H:%M:%S')}\")\n",
    "        print(f\"  Complete: {event['scale_complete_time'].strftime('%H:%M:%S')}\")\n",
    "        print(f\"  Workers: {event['workers_before']:.0f} ‚Üí {event['workers_after']:.0f}\")\n",
    "        print(f\"  Response time: {event['scale_up_lag_seconds']:.0f}s\")\n",
    "        print()\n",
    "else:\n",
    "    print(\"\\n‚ÑπÔ∏è  No autoscaling triggered by bursts\")\n",
    "    print(\"   Bursts may be too short-lived to trigger autoscaling\")\n",
    "    print(\"   Current workers may have sufficient capacity\")\n",
    "\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "phase2b_viz",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize burst tests\n",
    "for i, (result, latency_df) in enumerate(zip(phase2b_results, phase2b_latencies)):\n",
    "    test_config = BURST_TESTS[i]\n",
    "    \n",
    "    # Get metrics for this specific burst\n",
    "    test_metrics = metrics_collector.collect_metrics(\n",
    "        start_time=result['start_time'],\n",
    "        end_time=result['end_time'],\n",
    "        resolution_seconds=10\n",
    "    )\n",
    "    \n",
    "    fig = plot_dataflow_timeline(\n",
    "        test_results=result,\n",
    "        metrics=test_metrics,\n",
    "        latency_df=latency_df,\n",
    "        test_name=test_config['name']\n",
    "    )\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phase3_header",
   "metadata": {},
   "source": [
    "---\n",
    "## Phase 3: Ramp Test\n",
    "\n",
    "**Goal**: Find exact autoscaling trigger point with gradual load increase\n",
    "\n",
    "**Test**: Single ramp test (~15 minutes)\n",
    "- Gradually increase from 0 ‚Üí 500 msg/sec over 15 minutes\n",
    "- Linear ramp to simulate traffic growth\n",
    "\n",
    "**What We're Looking For**:\n",
    "- At what exact message rate does autoscaling trigger?\n",
    "- How does the pipeline respond to gradual load increase?\n",
    "- Is there a \"sweet spot\" for sustained throughput?\n",
    "- When does latency start degrading?\n",
    "\n",
    "‚è≥ **This phase will take approximately 15 minutes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "phase3_init",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Phase 3 storage\n",
    "phase3_results = []\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"PHASE 3: RAMP TEST\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Ramping from 0 ‚Üí {RAMP_TEST['target_rate']} msg/sec over {RAMP_TEST['duration']//60} minutes\")\n",
    "print(f\"Total estimated time: ~{RAMP_TEST['duration']//60} minutes\")\n",
    "print(\"\\nThis test gradually increases load to find autoscaling threshold.\")\n",
    "print(\"=\"*70 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "phase3_run",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run ramp test\n",
    "ramp_result = await load_generator.run_load_test(\n",
    "    pattern=\"ramp\",\n",
    "    target_rate=RAMP_TEST[\"target_rate\"],\n",
    "    duration=RAMP_TEST[\"duration\"],\n",
    "    test_name=RAMP_TEST[\"name\"]\n",
    ")\n",
    "phase3_results.append(ramp_result)\n",
    "\n",
    "print(f\"\\n‚úÖ Phase 3 complete: Ramp test finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phase3_metrics_header",
   "metadata": {},
   "source": [
    "### Collect Metrics for Phase 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "phase3_metrics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect metrics for ramp test\n",
    "phase3_start = ramp_result['start_time']\n",
    "phase3_end = ramp_result['end_time']\n",
    "\n",
    "print(f\"Collecting metrics for Phase 3\")\n",
    "print(f\"Time window: {phase3_start.strftime('%H:%M:%S')} ‚Üí {phase3_end.strftime('%H:%M:%S')}\")\n",
    "\n",
    "phase3_metrics = metrics_collector.collect_metrics(\n",
    "    start_time=phase3_start,\n",
    "    end_time=phase3_end,\n",
    "    resolution_seconds=10\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Metrics collected for Phase 3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "phase3_latency",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect end-to-end latency for ramp test\n",
    "phase3_latency = metrics_collector.collect_end_to_end_latency(\n",
    "    test_id=ramp_result['test_id'],\n",
    "    expected_messages=ramp_result['num_messages'],\n",
    "    timeout=300\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Latency data collected: {len(phase3_latency):,} messages\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phase3_analysis_header",
   "metadata": {},
   "source": [
    "### Analyze Phase 3 Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "phase3_analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze ramp test to find autoscaling trigger point\n",
    "print(\"=\"*70)\n",
    "print(\"PHASE 3 ANALYSIS: AUTOSCALING TRIGGER POINT\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\n{RAMP_TEST['name']}:\")\n",
    "print(f\"  Duration: {ramp_result['elapsed_seconds']:.0f}s ({ramp_result['elapsed_seconds']//60:.0f} mins)\")\n",
    "print(f\"  Messages sent: {ramp_result['num_messages']:,}\")\n",
    "print(f\"  Average rate: {ramp_result['actual_rate']:.1f} msg/sec\")\n",
    "print(f\"  Peak rate: {RAMP_TEST['target_rate']} msg/sec\")\n",
    "\n",
    "if len(phase3_latency) > 0:\n",
    "    print(f\"\\n  Latency Statistics:\")\n",
    "    print(f\"    Mean E2E: {phase3_latency['pipeline_latency_ms'].mean():,.1f}ms\")\n",
    "    print(f\"    P50 E2E:  {phase3_latency['pipeline_latency_ms'].quantile(0.50):,.1f}ms\")\n",
    "    print(f\"    P95 E2E:  {phase3_latency['pipeline_latency_ms'].quantile(0.95):,.1f}ms\")\n",
    "    print(f\"    P99 E2E:  {phase3_latency['pipeline_latency_ms'].quantile(0.99):,.1f}ms\")\n",
    "    \n",
    "    # Analyze latency trend over time (early vs late)\n",
    "    phase3_latency_sorted = phase3_latency.sort_values('publish_time')\n",
    "    \n",
    "    # Split into quartiles to see latency progression\n",
    "    quartile_size = len(phase3_latency_sorted) // 4\n",
    "    q1 = phase3_latency_sorted.iloc[:quartile_size]\n",
    "    q2 = phase3_latency_sorted.iloc[quartile_size:2*quartile_size]\n",
    "    q3 = phase3_latency_sorted.iloc[2*quartile_size:3*quartile_size]\n",
    "    q4 = phase3_latency_sorted.iloc[3*quartile_size:]\n",
    "    \n",
    "    print(f\"\\n  Latency Progression (as rate increases):\")\n",
    "    print(f\"    Q1 (0-25%):   {q1['pipeline_latency_ms'].mean():,.1f}ms mean\")\n",
    "    print(f\"    Q2 (25-50%):  {q2['pipeline_latency_ms'].mean():,.1f}ms mean\")\n",
    "    print(f\"    Q3 (50-75%):  {q3['pipeline_latency_ms'].mean():,.1f}ms mean\")\n",
    "    print(f\"    Q4 (75-100%): {q4['pipeline_latency_ms'].mean():,.1f}ms mean\")\n",
    "    \n",
    "    # Identify when latency starts degrading significantly\n",
    "    q1_mean = q1['pipeline_latency_ms'].mean()\n",
    "    if q4['pipeline_latency_ms'].mean() > q1_mean * 1.5:\n",
    "        print(f\"\\n    ‚ö†Ô∏è  Latency degraded significantly in later quartiles\")\n",
    "        print(f\"       Suggests capacity limits reached at higher rates\")\n",
    "    else:\n",
    "        print(f\"\\n    ‚úÖ Latency remained stable throughout ramp\")\n",
    "\n",
    "# Analyze autoscaling during ramp\n",
    "print(f\"\\n\" + \"=\"*70)\n",
    "print(\"AUTOSCALING BEHAVIOR DURING RAMP\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "autoscaling_events = metrics_collector.analyze_autoscaling(phase3_metrics)\n",
    "\n",
    "if len(autoscaling_events) > 0:\n",
    "    print(f\"\\n‚úÖ Autoscaling triggered {len(autoscaling_events)} time(s) during ramp\\n\")\n",
    "    \n",
    "    for idx, event in autoscaling_events.iterrows():\n",
    "        # Calculate approximate message rate at trigger time\n",
    "        elapsed_at_trigger = (event['trigger_time'] - phase3_start).total_seconds()\n",
    "        rate_at_trigger = (elapsed_at_trigger / RAMP_TEST['duration']) * RAMP_TEST['target_rate']\n",
    "        \n",
    "        print(f\"Event {idx + 1}:\")\n",
    "        print(f\"  Trigger time:       {event['trigger_time'].strftime('%H:%M:%S')}\")\n",
    "        print(f\"  Elapsed:            {elapsed_at_trigger:.0f}s into ramp\")\n",
    "        print(f\"  Approx rate:        {rate_at_trigger:.0f} msg/sec\")\n",
    "        print(f\"  Workers:            {event['workers_before']:.0f} ‚Üí {event['workers_after']:.0f}\")\n",
    "        print(f\"  Provisioning time:  {event['scale_up_lag_seconds']:.0f}s\")\n",
    "        print()\n",
    "    \n",
    "    # Identify threshold\n",
    "    first_trigger = autoscaling_events.iloc[0]\n",
    "    first_trigger_elapsed = (first_trigger['trigger_time'] - phase3_start).total_seconds()\n",
    "    first_trigger_rate = (first_trigger_elapsed / RAMP_TEST['duration']) * RAMP_TEST['target_rate']\n",
    "    \n",
    "    print(f\"üìä Key Finding:\")\n",
    "    print(f\"   Autoscaling triggered at approximately {first_trigger_rate:.0f} msg/sec\")\n",
    "    print(f\"   This is the throughput threshold for current configuration\")\n",
    "else:\n",
    "    print(\"\\n‚ÑπÔ∏è  No autoscaling triggered during ramp\")\n",
    "    print(f\"   Current workers handled up to {RAMP_TEST['target_rate']} msg/sec\")\n",
    "    print(\"   Pipeline has significant headroom at this configuration\")\n",
    "\n",
    "# Analyze backlog behavior during ramp\n",
    "if 'backlog' in phase3_metrics and len(phase3_metrics['backlog']) > 0:\n",
    "    print(f\"\\n\" + \"=\"*70)\n",
    "    print(\"BACKLOG BEHAVIOR DURING RAMP\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    backlog_df = phase3_metrics['backlog'].sort_values('timestamp')\n",
    "    max_backlog = backlog_df['value'].max()\n",
    "    \n",
    "    print(f\"\\nBacklog Statistics:\")\n",
    "    print(f\"  Max backlog: {max_backlog:,.0f} messages\")\n",
    "    print(f\"  Mean backlog: {backlog_df['value'].mean():,.0f} messages\")\n",
    "    \n",
    "    # Find when backlog started building\n",
    "    backlog_threshold = 100  # Consider backlog \"building\" at 100+ messages\n",
    "    backlog_building = backlog_df[backlog_df['value'] > backlog_threshold]\n",
    "    \n",
    "    if len(backlog_building) > 0:\n",
    "        first_backlog_time = backlog_building.iloc[0]['timestamp']\n",
    "        elapsed_at_backlog = (first_backlog_time - phase3_start).total_seconds()\n",
    "        rate_at_backlog = (elapsed_at_backlog / RAMP_TEST['duration']) * RAMP_TEST['target_rate']\n",
    "        \n",
    "        print(f\"\\n  Backlog started building:\")\n",
    "        print(f\"    Time: {first_backlog_time.strftime('%H:%M:%S')}\")\n",
    "        print(f\"    Elapsed: {elapsed_at_backlog:.0f}s into ramp\")\n",
    "        print(f\"    Approx rate: {rate_at_backlog:.0f} msg/sec\")\n",
    "        print(f\"\\n    üí° This indicates pipeline capacity threshold\")\n",
    "    else:\n",
    "        print(f\"\\n  ‚úÖ Backlog remained under control throughout ramp\")\n",
    "\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "phase3_viz",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize ramp test\n",
    "fig = plot_dataflow_timeline(\n",
    "    test_results=ramp_result,\n",
    "    metrics=phase3_metrics,\n",
    "    latency_df=phase3_latency,\n",
    "    test_name=\"Phase 3: Ramp Test - Finding Autoscaling Threshold\"\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "print(\"\\nüí° This visualization shows:\")\n",
    "print(f\"   - Gradual message rate increase: 0 ‚Üí {RAMP_TEST['target_rate']} msg/sec\")\n",
    "print(f\"   - Exact moment when workers scaled up\")\n",
    "print(f\"   - System lag progression as load increased\")\n",
    "print(f\"   - Backlog buildup and recovery\")\n",
    "print(f\"   - Latency trends throughout the ramp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary_header",
   "metadata": {},
   "source": [
    "---\n",
    "## Comprehensive Test Summary & Production Recommendations\n",
    "\n",
    "Based on all test phases, generate production recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comprehensive_summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive summary\n",
    "print(\"=\"*80)\n",
    "print(\"COMPREHENSIVE TEST SUMMARY & PRODUCTION RECOMMENDATIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Job configuration\n",
    "print(f\"\\nüìã Dataflow Job Configuration:\")\n",
    "print(f\"   Job ID:         {JOB_ID}\")\n",
    "print(f\"   Region:         {REGION}\")\n",
    "print(f\"   Machine Type:   n1-standard-4 (4 vCPUs, 15 GB memory)\")\n",
    "print(f\"   Window Size:    60 seconds\")\n",
    "\n",
    "# Phase 1 summary\n",
    "print(f\"\\nüìä Phase 1: Baseline Performance\")\n",
    "if len(phase1_latencies) > 0 and len(phase1_latencies[0]) > 0:\n",
    "    baseline_latency = phase1_latencies[0]['pipeline_latency_ms'].quantile(0.95)\n",
    "    print(f\"   Baseline P95 latency: {baseline_latency:,.1f}ms at {BASELINE_TESTS[0]['target_rate']} msg/sec\")\n",
    "else:\n",
    "    print(f\"   Tests completed but latency data unavailable\")\n",
    "\n",
    "# Phase 2A summary\n",
    "print(f\"\\nüìä Phase 2A: Throughput Threshold Hunt\")\n",
    "phase2a_autoscaling = metrics_collector.analyze_autoscaling(phase2a_metrics)\n",
    "if len(phase2a_autoscaling) > 0:\n",
    "    print(f\"   ‚úÖ Autoscaling triggered {len(phase2a_autoscaling)} time(s)\")\n",
    "    print(f\"   Average provisioning time: {phase2a_autoscaling['scale_up_lag_seconds'].mean():.0f}s\")\n",
    "else:\n",
    "    print(f\"   No autoscaling at tested rates (50-200 msg/sec)\")\n",
    "\n",
    "if len(phase2a_latencies) > 0:\n",
    "    # Compare latency degradation across sustained tests\n",
    "    latencies_50 = phase2a_latencies[0]['pipeline_latency_ms'].quantile(0.95) if len(phase2a_latencies[0]) > 0 else None\n",
    "    latencies_200 = phase2a_latencies[-1]['pipeline_latency_ms'].quantile(0.95) if len(phase2a_latencies[-1]) > 0 else None\n",
    "    \n",
    "    if latencies_50 and latencies_200:\n",
    "        degradation = ((latencies_200 - latencies_50) / latencies_50) * 100\n",
    "        print(f\"   Latency at 50 msg/sec:  {latencies_50:,.1f}ms (P95)\")\n",
    "        print(f\"   Latency at 200 msg/sec: {latencies_200:,.1f}ms (P95)\")\n",
    "        print(f\"   Degradation: {degradation:+.1f}%\")\n",
    "\n",
    "# Phase 2B summary\n",
    "print(f\"\\nüìä Phase 2B: Burst Capacity\")\n",
    "if len(phase2b_results) > 0:\n",
    "    largest_burst = BURST_TESTS[-1]\n",
    "    print(f\"   Largest burst: {largest_burst['num_messages']:,} messages\")\n",
    "    if len(phase2b_latencies[-1]) > 0:\n",
    "        burst_p95 = phase2b_latencies[-1]['pipeline_latency_ms'].quantile(0.95)\n",
    "        print(f\"   P95 latency during burst: {burst_p95:,.1f}ms\")\n",
    "\n",
    "# Phase 3 summary\n",
    "print(f\"\\nüìä Phase 3: Ramp Test\")\n",
    "phase3_autoscaling = metrics_collector.analyze_autoscaling(phase3_metrics)\n",
    "if len(phase3_autoscaling) > 0:\n",
    "    first_trigger = phase3_autoscaling.iloc[0]\n",
    "    elapsed = (first_trigger['trigger_time'] - phase3_start).total_seconds()\n",
    "    trigger_rate = (elapsed / RAMP_TEST['duration']) * RAMP_TEST['target_rate']\n",
    "    print(f\"   ‚úÖ Autoscaling triggered at ~{trigger_rate:.0f} msg/sec\")\n",
    "    print(f\"   This is the capacity threshold\")\n",
    "else:\n",
    "    print(f\"   No autoscaling triggered up to {RAMP_TEST['target_rate']} msg/sec\")\n",
    "    print(f\"   Pipeline has significant headroom\")\n",
    "\n",
    "if len(phase3_latency) > 0:\n",
    "    ramp_p95 = phase3_latency['pipeline_latency_ms'].quantile(0.95)\n",
    "    print(f\"   Overall P95 latency: {ramp_p95:,.1f}ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "production_recommendations",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Production recommendations\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PRODUCTION RECOMMENDATIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Determine recommended throughput\n",
    "print(\"\\nüéØ Recommended Throughput Configurations:\")\n",
    "\n",
    "# Try to determine capacity from autoscaling events\n",
    "all_autoscaling = pd.concat([\n",
    "    metrics_collector.analyze_autoscaling(phase1_metrics),\n",
    "    metrics_collector.analyze_autoscaling(phase2a_metrics),\n",
    "    metrics_collector.analyze_autoscaling(phase2b_metrics),\n",
    "    metrics_collector.analyze_autoscaling(phase3_metrics)\n",
    "], ignore_index=True)\n",
    "\n",
    "if len(all_autoscaling) > 0:\n",
    "    # Use first autoscaling event to estimate capacity\n",
    "    print(\"\\n   Based on autoscaling behavior:\")\n",
    "    print(\"   ‚Ä¢ Conservative (70% capacity): Suitable for production with headroom\")\n",
    "    print(\"   ‚Ä¢ Balanced (85% capacity): Good for predictable workloads\")\n",
    "    print(\"   ‚Ä¢ Aggressive (95% capacity): Maximum throughput, relies on autoscaling\")\n",
    "else:\n",
    "    print(\"\\n   Based on test results (no autoscaling triggered):\")\n",
    "    print(f\"   ‚Ä¢ Current configuration handled {SUSTAINED_TESTS[-1]['target_rate']} msg/sec comfortably\")\n",
    "    print(\"   ‚Ä¢ Significant headroom available\")\n",
    "    print(\"   ‚Ä¢ Consider testing higher rates to find true limits\")\n",
    "\n",
    "# Worker configuration\n",
    "print(\"\\n‚öôÔ∏è  Worker Configuration Recommendations:\")\n",
    "\n",
    "if len(all_autoscaling) > 0:\n",
    "    avg_provisioning = all_autoscaling['scale_up_lag_seconds'].mean()\n",
    "    print(f\"\\n   Autoscaling Performance:\")\n",
    "    print(f\"   ‚Ä¢ Average worker provisioning: {avg_provisioning:.0f}s ({avg_provisioning/60:.1f} mins)\")\n",
    "    print(f\"   ‚Ä¢ Triggered {len(all_autoscaling)} time(s) across all tests\")\n",
    "    \n",
    "    if avg_provisioning > 300:  # 5 minutes\n",
    "        print(f\"\\n   ‚ö†Ô∏è  Slow provisioning time detected\")\n",
    "        print(f\"   ‚Ä¢ Consider increasing MIN_WORKERS for faster response\")\n",
    "        print(f\"   ‚Ä¢ Pre-warm capacity reduces latency spikes\")\n",
    "    else:\n",
    "        print(f\"\\n   ‚úÖ Autoscaling performance acceptable\")\n",
    "else:\n",
    "    print(f\"\\n   Current MIN_WORKERS=2, MAX_WORKERS=20\")\n",
    "    print(f\"   ‚Ä¢ No autoscaling needed at tested rates\")\n",
    "    print(f\"   ‚Ä¢ Current configuration appropriate\")\n",
    "\n",
    "# Machine type recommendations\n",
    "print(\"\\nüíª Machine Type Recommendations:\")\n",
    "print(f\"\\n   Current: n1-standard-4 (4 vCPUs, 15 GB memory)\")\n",
    "\n",
    "# Check if we have system lag data to determine if CPU-bound\n",
    "all_system_lag = []\n",
    "for metrics in [phase1_metrics, phase2a_metrics, phase2b_metrics, phase3_metrics]:\n",
    "    if 'system_lag' in metrics and len(metrics['system_lag']) > 0:\n",
    "        all_system_lag.extend(metrics['system_lag']['value'].tolist())\n",
    "\n",
    "if all_system_lag:\n",
    "    max_lag_ms = max(all_system_lag) / 1000  # Convert to ms\n",
    "    \n",
    "    if max_lag_ms > 60000:  # 1 minute\n",
    "        print(f\"\\n   ‚ö†Ô∏è  High system lag detected ({max_lag_ms/1000:.1f}s max)\")\n",
    "        print(f\"   ‚Ä¢ Consider c2-standard-4 for more CPU power\")\n",
    "        print(f\"   ‚Ä¢ Or increase worker count instead\")\n",
    "    else:\n",
    "        print(f\"\\n   ‚úÖ Current machine type performing well\")\n",
    "        print(f\"   ‚Ä¢ System lag remained under control (max: {max_lag_ms/1000:.1f}s)\")\n",
    "\n",
    "print(f\"\\n   When to consider alternatives:\")\n",
    "print(f\"   ‚Ä¢ CPU-intensive model ‚Üí c2-standard-4 (compute-optimized)\")\n",
    "print(f\"   ‚Ä¢ Large model size ‚Üí n1-highmem-4 (more memory)\")\n",
    "print(f\"   ‚Ä¢ GPU-compatible model ‚Üí Add --worker_gpu_type=nvidia-tesla-t4\")\n",
    "\n",
    "# Latency optimization\n",
    "print(\"\\n‚è±Ô∏è  Latency Optimization:\")\n",
    "\n",
    "if len(phase1_latencies) > 0 and len(phase1_latencies[0]) > 0:\n",
    "    baseline = phase1_latencies[0]\n",
    "    window_pct = baseline['window_wait_ms'].mean() / baseline['pipeline_latency_ms'].mean() * 100\n",
    "    processing_pct = baseline['processing_ms'].mean() / baseline['pipeline_latency_ms'].mean() * 100\n",
    "    \n",
    "    print(f\"\\n   Latency Breakdown (baseline):\")\n",
    "    print(f\"   ‚Ä¢ Window wait: {window_pct:.1f}% of total latency\")\n",
    "    print(f\"   ‚Ä¢ Processing: {processing_pct:.1f}% of total latency\")\n",
    "    print(f\"   ‚Ä¢ Pub/Sub delivery: {100 - window_pct - processing_pct:.1f}% of total latency\")\n",
    "    \n",
    "    if window_pct > 50:\n",
    "        print(f\"\\n   üí° Window wait dominates latency\")\n",
    "        print(f\"   ‚Ä¢ Current window: 60 seconds (fixed)\")\n",
    "        print(f\"   ‚Ä¢ To reduce latency: Use smaller windows (30s, 15s)\")\n",
    "        print(f\"   ‚Ä¢ Trade-off: Smaller windows = less batching efficiency\")\n",
    "    \n",
    "    if processing_pct > 30:\n",
    "        print(f\"\\n   üí° Processing time significant\")\n",
    "        print(f\"   ‚Ä¢ Consider model optimization\")\n",
    "        print(f\"   ‚Ä¢ Or use more powerful machine type\")\n",
    "        print(f\"   ‚Ä¢ Or GPU acceleration for large models\")\n",
    "\n",
    "# Cost optimization\n",
    "print(\"\\nüí∞ Cost Optimization Tips:\")\n",
    "print(f\"   ‚Ä¢ Monitor actual traffic patterns and adjust MIN/MAX workers\")\n",
    "print(f\"   ‚Ä¢ Use batch processing for historical analysis (cheaper)\")\n",
    "print(f\"   ‚Ä¢ Set up alerts for unexpected worker scaling\")\n",
    "print(f\"   ‚Ä¢ Consider spot instances for non-critical workloads (when available)\")\n",
    "print(f\"   ‚Ä¢ Drain and stop job when not actively processing (restart when needed)\")\n",
    "\n",
    "# Monitoring\n",
    "print(\"\\nüìä Monitoring & Alerts:\")\n",
    "print(f\"   Set up Cloud Monitoring alerts for:\")\n",
    "print(f\"   ‚Ä¢ System lag > 60 seconds (pipeline falling behind)\")\n",
    "print(f\"   ‚Ä¢ Backlog > 10,000 messages (capacity issues)\")\n",
    "print(f\"   ‚Ä¢ Worker count at MAX_WORKERS (may need to increase limit)\")\n",
    "print(f\"   ‚Ä¢ Element count drops to 0 (pipeline stalled)\")\n",
    "\n",
    "# Next steps\n",
    "print(\"\\nüìù Next Steps:\")\n",
    "print(f\"   1. Review Dataflow job metrics in Cloud Console\")\n",
    "print(f\"      https://console.cloud.google.com/dataflow/jobs/{REGION}/{JOB_ID}?project={PROJECT_ID}\")\n",
    "print(f\"   2. If autoscaling threshold needs adjustment:\")\n",
    "print(f\"      ‚Ä¢ Modify MIN_WORKERS or MAX_WORKERS in deployment\")\n",
    "print(f\"      ‚Ä¢ Re-run this notebook to validate changes\")\n",
    "print(f\"   3. Implement recommended monitoring alerts\")\n",
    "print(f\"   4. Set up dashboard for real-time visibility\")\n",
    "print(f\"   5. Document findings for future capacity planning\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion_header",
   "metadata": {},
   "source": [
    "---\n",
    "## Conclusion\n",
    "\n",
    "This notebook provides a comprehensive, scientific approach to understanding Dataflow Streaming performance through systematic testing across three dimensions:\n",
    "\n",
    "**1. Message Rate** - Baseline, sustained, and peak throughput testing  \n",
    "**2. Load Patterns** - Burst, sustained, and ramping traffic patterns  \n",
    "**3. Latency Characteristics** - End-to-end breakdown and bottleneck identification\n",
    "\n",
    "### Key Insights from This Testing Framework\n",
    "\n",
    "**Understanding Dataflow Autoscaling:**\n",
    "- Dataflow scales based on **Pub/Sub backlog** and **system lag** (not just CPU like Vertex AI)\n",
    "- Worker provisioning takes **2-4 minutes** (plan for this delay)\n",
    "- Sustained load (not bursts) typically triggers autoscaling\n",
    "- Ramp tests reveal exact throughput thresholds\n",
    "\n",
    "**Latency Composition:**\n",
    "- **Window Wait**: Time from publish to window close (up to window size)\n",
    "- **Processing**: Model inference + transforms (controllable via machine type)\n",
    "- **Pub/Sub Delivery**: Usually <100ms (network overhead)\n",
    "- Smaller windows = lower latency but less batching efficiency\n",
    "\n",
    "**Bottleneck Identification:**\n",
    "- High **system lag** ‚Üí Processing capacity issues (need more workers or better machine type)\n",
    "- High **backlog** ‚Üí Ingestion rate exceeds processing capacity (autoscaling needed)\n",
    "- High **window wait %** ‚Üí Latency dominated by batching (consider smaller windows)\n",
    "- High **processing %** ‚Üí Model inference slow (consider GPU or model optimization)\n",
    "\n",
    "### Important Notes\n",
    "\n",
    "**Results are Pipeline-Specific:**  \n",
    "All results in this notebook are specific to the tested pipeline configuration:\n",
    "- PyTorch autoencoder model with 30 features\n",
    "- n1-standard-4 workers (4 vCPUs, 15 GB memory)\n",
    "- 60-second fixed windows\n",
    "- MIN=2, MAX=20 workers\n",
    "\n",
    "Your results will vary based on:\n",
    "- Model complexity and inference time\n",
    "- Machine type and resources\n",
    "- Window size configuration\n",
    "- Message size and format\n",
    "\n",
    "**Always Test Your Own Pipeline:**  \n",
    "Before production deployment:\n",
    "1. Run this notebook with your pipeline and representative data\n",
    "2. Test with realistic traffic patterns\n",
    "3. Adjust worker configuration based on results\n",
    "4. Monitor production metrics continuously\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "**If This Notebook Revealed Issues:**\n",
    "1. **Autoscaling too slow** ‚Üí Increase MIN_WORKERS for pre-warmed capacity\n",
    "2. **High system lag** ‚Üí Use c2-standard-4 or increase MAX_WORKERS\n",
    "3. **High latency** ‚Üí Reduce window size or optimize model\n",
    "4. **Backlog building** ‚Üí Increase MAX_WORKERS or improve processing efficiency\n",
    "\n",
    "**Production Deployment Tasks:**\n",
    "- Set up Cloud Monitoring alerts (system lag, backlog, worker count)\n",
    "- Create dashboard for real-time visibility\n",
    "- Document capacity planning findings\n",
    "- Implement gradual rollout with monitoring\n",
    "\n",
    "**Related Notebooks:**\n",
    "- [Dataflow Streaming RunInference](./dataflow-streaming-runinference.ipynb) - Deploy the pipeline tested here\n",
    "- [Dataflow Batch RunInference](./dataflow-batch-runinference.ipynb) - Batch processing alternative\n",
    "- [Vertex AI Endpoint Scale Tests](./scale-tests-vertex-ai-endpoints.ipynb) - Compare with endpoint serving\n",
    "\n",
    "### Resources\n",
    "\n",
    "**Dataflow Documentation:**\n",
    "- [Streaming Pipelines](https://cloud.google.com/dataflow/docs/concepts/streaming-pipelines)\n",
    "- [Autoscaling](https://cloud.google.com/dataflow/docs/guides/deploying-a-pipeline#autoscaling)\n",
    "- [Monitoring](https://cloud.google.com/dataflow/docs/guides/using-cloud-monitoring)\n",
    "\n",
    "**Apache Beam:**\n",
    "- [RunInference](https://beam.apache.org/documentation/ml/about-ml/)\n",
    "- [Windowing](https://beam.apache.org/documentation/programming-guide/#windowing)\n",
    "- [PyTorch Handler](https://beam.apache.org/documentation/ml/pytorch-inference/)\n",
    "\n",
    "**Cloud Monitoring:**\n",
    "- [Dataflow Metrics](https://cloud.google.com/dataflow/docs/guides/using-cloud-monitoring#available_dataflow_metrics)\n",
    "- [Setting Up Alerts](https://cloud.google.com/monitoring/alerts)\n",
    "\n",
    "---\n",
    "\n",
    "**Testing Framework Created:** This comprehensive testing infrastructure (`scale_testing_dataflow_utils.py` + this notebook) can be reused for testing any Dataflow Streaming pipeline. Simply update the job ID and test parameters."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (frameworks-pytorch)",
   "language": "python",
   "name": "frameworks-pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
