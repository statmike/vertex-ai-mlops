{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "daababd3",
   "metadata": {},
   "source": [
    "![tracker](https://us-central1-vertex-ai-mlops-369716.cloudfunctions.net/pixel-tracking?path=statmike%2Fvertex-ai-mlops%2FFramework+Workflows%2FPyTorch%2Fserving&file=scale-tests-dataflow-streaming-runinference.ipynb)\n",
    "<!--- header table --->\n",
    "<table align=\"left\">\n",
    "<tr>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%20Workflows/PyTorch/serving/scale-tests-dataflow-streaming-runinference.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.svgrepo.com/download/217753/github.svg\" alt=\"GitHub logo\">\n",
    "      <br>View on<br>GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://colab.research.google.com/github/statmike/vertex-ai-mlops/blob/main/Framework%20Workflows/PyTorch/serving/scale-tests-dataflow-streaming-runinference.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.gstatic.com/pantheon/images/bigquery/welcome_page/colab-logo.svg\" alt=\"Google Colaboratory logo\">\n",
    "      <br>Run in<br>Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https%3A%2F%2Fraw.githubusercontent.com%2Fstatmike%2Fvertex-ai-mlops%2Fmain%2FFramework%2520Workflows%2FPyTorch%2Fserving%2Fscale-tests-dataflow-streaming-runinference.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\">\n",
    "      <br>Run in<br>Colab Enterprise\n",
    "    </a>\n",
    "  </td>      \n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/bigquery/import?url=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%20Workflows/PyTorch/serving/scale-tests-dataflow-streaming-runinference.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.gstatic.com/images/branding/gcpiconscolors/bigquery/v1/32px.svg\" alt=\"BigQuery logo\">\n",
    "      <br>Open in<br>BigQuery Studio\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/statmike/vertex-ai-mlops/main/Framework%20Workflows/PyTorch/serving/scale-tests-dataflow-streaming-runinference.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.gstatic.com/images/branding/gcpiconscolors/vertexai/v1/32px.svg\" alt=\"Vertex AI logo\">\n",
    "      <br>Open in<br>Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td colspan=\"5\" style=\"text-align: right\">\n",
    "    <b>Share This On: </b> \n",
    "    <a href=\"https://www.linkedin.com/sharing/share-offsite/?url=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%2520Workflows/PyTorch/serving/scale-tests-dataflow-streaming-runinference.ipynb\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"Linkedin Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://reddit.com/submit?url=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%2520Workflows/PyTorch/serving/scale-tests-dataflow-streaming-runinference.ipynb\"><img src=\"https://redditinc.com/hubfs/Reddit%20Inc/Brand/Reddit_Logo.png\" alt=\"Reddit Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://bsky.app/intent/compose?text=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%2520Workflows/PyTorch/serving/scale-tests-dataflow-streaming-runinference.ipynb\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"BlueSky Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://twitter.com/intent/tweet?url=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%2520Workflows/PyTorch/serving/scale-tests-dataflow-streaming-runinference.ipynb\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/5/5a/X_icon_2.svg\" alt=\"X (Twitter) Logo\" width=\"20px\"></a> \n",
    "  </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td colspan=\"5\" style=\"text-align: right\">\n",
    "    <b>Connect With Author On: </b> \n",
    "    <a href=\"https://www.linkedin.com/in/statmike\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"Linkedin Logo\" width=\"20px\"></a>\n",
    "    <a href=\"https://www.github.com/statmike\"><img src=\"https://www.svgrepo.com/download/217753/github.svg\" alt=\"GitHub Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://www.youtube.com/@statmike-channel\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/f/fd/YouTube_full-color_icon_%282024%29.svg\" alt=\"YouTube Logo\" width=\"20px\"></a>\n",
    "    <a href=\"https://bsky.app/profile/statmike.bsky.social\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"BlueSky Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://x.com/statmike\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/5/5a/X_icon_2.svg\" alt=\"X (Twitter) Logo\" width=\"20px\"></a>\n",
    "  </td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Dataflow Streaming: Comprehensive Scale Testing & Performance Analysis\n",
    "\n",
    "**Scientific approach to understanding pipeline capacity, worker autoscaling, and optimal configurations.**\n",
    "\n",
    "---\n",
    "\n",
    "## What This Notebook Does\n",
    "\n",
    "This notebook systematically tests Dataflow Streaming performance across **three dimensions**:\n",
    "\n",
    "1. **Message Rate** - How many messages per second can the pipeline handle?\n",
    "2. **Load Pattern** - Burst, sustained, or ramping traffic?\n",
    "3. **Latency Characteristics** - Where do delays occur in the pipeline?\n",
    "\n",
    "Through rigorous testing, we answer:\n",
    "- ‚ùì **When does worker autoscaling trigger?** (exact conditions)\n",
    "- ‚ùì **Where are the bottlenecks?** (windowing vs processing vs pub/sub)\n",
    "- ‚ùì **What's the throughput threshold?** (messages/second before backlog builds)\n",
    "- ‚ùì **How should we configure for production?** (recommendations)\n",
    "\n",
    "## Testing Approach\n",
    "\n",
    "**Phase 1: Baseline Performance** (~5 minutes)\n",
    "- Test 10 msg/sec for 2 mins (sustained)\n",
    "- Test 25 msg/sec for 2 mins (sustained)\n",
    "- **Goal**: Establish baseline latency and confirm pipeline health\n",
    "\n",
    "**Phase 2A: Throughput Threshold Hunt** (~20 minutes)\n",
    "- Sustained tests: 50, 100, 200 msg/sec for 5 mins each\n",
    "- **Goal**: Find exact threshold where backlog builds and autoscaling triggers\n",
    "\n",
    "**Phase 2B: Burst Capacity** (~10 minutes)\n",
    "- Burst tests: 1000, 5000, 10000 messages as fast as possible\n",
    "- **Goal**: Test backlog recovery time and worker scaling response\n",
    "\n",
    "**Phase 3: Ramp Test** (~15 minutes)\n",
    "- Ramp 0 ‚Üí 500 msg/sec over 15 mins\n",
    "- **Goal**: Find exact autoscaling trigger point\n",
    "\n",
    "**Total test time**: ~50-70 minutes\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before running this notebook, you need:\n",
    "\n",
    "- **Running Dataflow Streaming Job** from [dataflow-streaming-runinference.ipynb](./dataflow-streaming-runinference.ipynb)\n",
    "  - ‚ö†Ô∏è This notebook does NOT deploy a new job - it tests an existing one\n",
    "  - Job must be in \"Running\" state with workers active\n",
    "- **Testing utilities**: `scale_testing_dataflow_utils.py` must be in the same directory\n",
    "- **Total test time**: ~70 minutes\n",
    "\n",
    "## Understanding Dataflow Autoscaling\n",
    "\n",
    "**How It Works**:\n",
    "- Dataflow autoscales based on **Pub/Sub backlog** and **system lag**\n",
    "- When backlog builds or lag increases ‚Üí new workers provision\n",
    "- Worker provisioning takes ~2-4 minutes (container startup)\n",
    "- Scale-down occurs after ~10-15 minutes of low load\n",
    "\n",
    "**Key Metrics**:\n",
    "- **System Lag**: How far behind real-time is the pipeline? (seconds)\n",
    "- **Backlog**: Number of undelivered messages in Pub/Sub subscription\n",
    "- **Worker Count**: Number of active Dataflow workers\n",
    "- **Element Count**: Throughput (elements/second processed)\n",
    "\n",
    "**Latency Breakdown** (measured from Pub/Sub output):\n",
    "- **Window Wait**: Publish time ‚Üí Window close time (up to 60s for 60s windows)\n",
    "- **Processing**: Window close ‚Üí Pipeline output (model inference + transforms)\n",
    "- **Pub/Sub Delivery**: Pipeline output ‚Üí Consumer receive (usually <100ms)\n",
    "- **Total**: End-to-end latency from publish to receive\n",
    "\n",
    "## Machine Type and GPU Considerations\n",
    "\n",
    "**Current Configuration**: n1-standard-4 (4 vCPUs, 15 GB memory)\n",
    "\n",
    "**When to Consider Other Machine Types**:\n",
    "\n",
    "1. **CPU-Intensive Models** (high CPU during inference):\n",
    "   - Use `c2-standard-4` (compute-optimized)\n",
    "   - More CPU power per worker\n",
    "   - Better for models with complex calculations\n",
    "\n",
    "2. **Memory-Intensive Models** (large model size or batch processing):\n",
    "   - Use `n1-highmem-4` (4 vCPUs, 26 GB memory)\n",
    "   - Use `n1-highmem-8` (8 vCPUs, 52 GB memory)\n",
    "   - Prevents OOM errors during model loading\n",
    "\n",
    "3. **GPU Acceleration** (large models with GPU support):\n",
    "   - Add `--worker_gpu_type=nvidia-tesla-t4` (cost-effective)\n",
    "   - Add `--worker_gpu_type=nvidia-tesla-v100` (high performance)\n",
    "   - Add `--worker_gpu_count=1` (GPUs per worker)\n",
    "   - Update model handler to use `device=\"cuda\"`\n",
    "   - **When to use**: Models with >100M parameters, batch inference benefits from GPU\n",
    "   - **Cost**: ~10x more expensive than CPU workers\n",
    "   - **PyTorch requirement**: Must use GPU-enabled PyTorch build\n",
    "\n",
    "**For This Notebook**: We test only n1-standard-4 (our current configuration). If results show CPU saturation or memory pressure, recommendations will suggest alternatives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup_header",
   "metadata": {},
   "source": [
    "---\n",
    "## Environment Setup\n",
    "\n",
    "This section will authenticate your session, enable required Google Cloud APIs, and install necessary Python packages.\n",
    "\n",
    "**Package Installation Options (`REQ_TYPE`)**:\n",
    "- `PRIMARY`: Installs only the main packages. Faster, but pip resolves sub-dependencies which may result in different versions than development.\n",
    "- `ALL` (Default): Installs exact versions of all packages and dependencies. Best for perfectly reproducing the development environment.\n",
    "- `COLAB`: Installs a Colab-optimized list that excludes pre-installed packages like `ipython` and `ipykernel`.\n",
    "\n",
    "**Installation Tool Options (`INSTALL_TOOL`)**:\n",
    "- `pip` (Default): Uses pip for package installation. Standard Python package installer.\n",
    "- `uv`: Modern, fast Python package installer. Must be installed separately. See: https://github.com/astral-sh/uv\n",
    "- `poetry`: Dependency management tool. Requires running notebook in a poetry environment (`poetry shell` or `poetry run jupyter lab`). Uses `pyproject.toml` instead of requirements.txt.\n",
    "\n",
    "> **Note:** If running in Google Colab, the script will automatically detect this and set `REQ_TYPE = 'COLAB'` to prevent package conflicts, overriding any manual setting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "project_id_header",
   "metadata": {},
   "source": [
    "### Set Your Project ID\n",
    "\n",
    "‚ö†Ô∏è **Action Required:** Replace the `PROJECT_ID` value below with your Google Cloud project ID before running this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "project_config",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = 'statmike-mlops-349915' # replace with GCP project ID\n",
    "REQ_TYPE = 'ALL' # Specify PRIMARY or ALL or COLAB\n",
    "INSTALL_TOOL = 'poetry' # Specify pip, uv, or poetry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config_header",
   "metadata": {},
   "source": [
    "### Configuration\n",
    "\n",
    "This cell defines the requirements files and Google Cloud APIs needed for this notebook. Run as-is without modification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": [
    "REQUIREMENTS_URL = 'https://raw.githubusercontent.com/statmike/vertex-ai-mlops/refs/heads/main/Framework%20Workflows/PyTorch/requirements.txt'\n",
    "\n",
    "REQUIRED_APIS = [\n",
    "    \"dataflow.googleapis.com\",\n",
    "    \"pubsub.googleapis.com\",\n",
    "    \"monitoring.googleapis.com\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "run_setup_header",
   "metadata": {},
   "source": [
    "### Run Setup\n",
    "\n",
    "This cell downloads the centralized setup code and configures your environment. It will:\n",
    "- Authenticate your session with Google Cloud\n",
    "- Enable required APIs for this notebook\n",
    "- Install necessary Python packages\n",
    "- Display a setup summary with your project information\n",
    "\n",
    "> **Note:** In Colab, if packages are installed, the kernel will automatically restart. After restart, continue from the next cell without re-running earlier cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run_setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, urllib.request\n",
    "\n",
    "# Download and import setup code\n",
    "url = 'https://raw.githubusercontent.com/statmike/vertex-ai-mlops/refs/heads/main/core/notebook-template/python_setup.py'\n",
    "urllib.request.urlretrieve(url, 'python_setup_local.py')\n",
    "import python_setup_local as python_setup\n",
    "os.remove('python_setup_local.py')\n",
    "\n",
    "# Run setup\n",
    "setup_info = python_setup.setup_environment(PROJECT_ID, REQ_TYPE, REQUIREMENTS_URL, REQUIRED_APIS, INSTALL_TOOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "test_config_header",
   "metadata": {},
   "source": [
    "---\n",
    "## Test Configuration\n",
    "\n",
    "Configure the Dataflow job to test and the test parameters.\n",
    "\n",
    "**Job Configuration:**\n",
    "- Update `JOB_ID` with your running Dataflow streaming job ID\n",
    "- Update `REGION` if your job is in a different region\n",
    "- Job must be in \"Running\" state (check [Dataflow Console](https://console.cloud.google.com/dataflow))\n",
    "\n",
    "**Test Parameters:**\n",
    "- Adjust message rates and durations based on your needs\n",
    "- Default values are designed to comprehensively test a PyTorch streaming pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "job_config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataflow Job Configuration\n",
    "REGION = 'us-central1'\n",
    "JOB_NAME_PREFIX = 'pytorch-streaming-'  # Job name prefix from dataflow-streaming-runinference.ipynb\n",
    "\n",
    "# Pub/Sub Configuration (LOCAL model pipeline - isolated from Vertex endpoint pipeline)\n",
    "EXPERIMENT = 'pytorch-autoencoder'\n",
    "INPUT_TOPIC = f'projects/{PROJECT_ID}/topics/{EXPERIMENT}-input-local'\n",
    "OUTPUT_SUBSCRIPTION = f'projects/{PROJECT_ID}/subscriptions/{EXPERIMENT}-output-sub-local'\n",
    "\n",
    "# Phase 1: Baseline Performance\n",
    "BASELINE_TESTS = [\n",
    "    {\"target_rate\": 10, \"duration\": 120, \"name\": \"Baseline - 10 msg/sec\"},\n",
    "    {\"target_rate\": 25, \"duration\": 120, \"name\": \"Baseline - 25 msg/sec\"},\n",
    "]\n",
    "\n",
    "# Phase 2A: Throughput Threshold Hunt\n",
    "# Based on previous run: capacity threshold ~78 msg/sec\n",
    "SUSTAINED_TESTS = [\n",
    "    {\"target_rate\": 40, \"duration\": 300, \"name\": \"Sustained - 40 msg/sec\"},\n",
    "    {\"target_rate\": 60, \"duration\": 300, \"name\": \"Sustained - 60 msg/sec\"},\n",
    "    {\"target_rate\": 80, \"duration\": 300, \"name\": \"Sustained - 80 msg/sec\"},\n",
    "]\n",
    "\n",
    "# Phase 2B: Burst Capacity\n",
    "BURST_TESTS = [\n",
    "    {\"num_messages\": 1000, \"name\": \"Burst - 1,000 messages\"},\n",
    "    {\"num_messages\": 5000, \"name\": \"Burst - 5,000 messages\"},\n",
    "    {\"num_messages\": 10000, \"name\": \"Burst - 10,000 messages\"},\n",
    "]\n",
    "\n",
    "# Phase 3: Ramp Test\n",
    "# Target just above capacity threshold to observe autoscaling\n",
    "RAMP_TEST = {\"target_rate\": 100, \"duration\": 600, \"name\": \"Ramp - 0‚Üí100 msg/sec\"}  # 10 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import asyncio\n",
    "import time\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from scale_testing_dataflow_utils import (\n",
    "    PubSubLoadGenerator,\n",
    "    DataflowMetricsCollector,\n",
    "    plot_dataflow_timeline\n",
    ")\n",
    "from google.cloud import dataflow_v1beta3\n",
    "\n",
    "print(\"‚úÖ Imports complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verify_job_header",
   "metadata": {},
   "source": [
    "### Verify Job is Running\n",
    "\n",
    "Before starting tests, verify the Dataflow job is in \"Running\" state with active workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0fd6222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find and verify Dataflow job\n",
    "client = dataflow_v1beta3.JobsV1Beta3Client()\n",
    "\n",
    "# Step 1: Auto-discover running job with expected prefix\n",
    "print(\"Searching for running Dataflow jobs...\")\n",
    "request = dataflow_v1beta3.ListJobsRequest(\n",
    "    project_id=PROJECT_ID,\n",
    "    location=REGION\n",
    ")\n",
    "\n",
    "JOB_ID = None\n",
    "JOB_NAME = None\n",
    "\n",
    "try:\n",
    "    response = client.list_jobs(request=request)\n",
    "    \n",
    "    for job in response:\n",
    "        # Only match LOCAL model jobs (exclude -vertex endpoint jobs)\n",
    "        if (job.current_state.name == \"JOB_STATE_RUNNING\" and\n",
    "            job.name.startswith(JOB_NAME_PREFIX) and\n",
    "            \"-vertex\" not in job.name):\n",
    "            JOB_ID = job.id\n",
    "            JOB_NAME = job.name\n",
    "            break\n",
    "    \n",
    "    if not JOB_ID:\n",
    "        print(f\"\\n‚ö†Ô∏è  No running LOCAL model jobs found with prefix: {JOB_NAME_PREFIX}\")\n",
    "        print(\"   (Vertex endpoint jobs excluded - this tests LOCAL model only)\")\n",
    "        print(\"   Please start a LOCAL model job using dataflow-streaming-runinference.ipynb first\")\n",
    "        raise ValueError(\"No running LOCAL model Dataflow job found\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ö†Ô∏è  Error finding Dataflow job: {e}\")\n",
    "    raise\n",
    "\n",
    "# Step 2: Get full job details and verify status\n",
    "job = client.get_job(\n",
    "    request={\n",
    "        \"project_id\": PROJECT_ID,\n",
    "        \"location\": REGION,\n",
    "        \"job_id\": JOB_ID\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"DATAFLOW JOB STATUS\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Name: {job.name}\")\n",
    "print(f\"ID: {job.id}\")\n",
    "print(f\"State: {job.current_state.name}\")\n",
    "print(f\"Created: {job.create_time}\")\n",
    "print(f\"Type: {job.type_.name}\")\n",
    "\n",
    "if job.current_state.name == \"JOB_STATE_RUNNING\":\n",
    "    print(\"\\n‚úÖ Job is running and ready for testing\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è  Job is not running (state: {job.current_state.name})\")\n",
    "    print(\"   Please start the job before running tests\")\n",
    "    raise ValueError(f\"Job is not in RUNNING state: {job.current_state.name}\")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nMonitor job: https://console.cloud.google.com/dataflow/jobs/{REGION}/{JOB_ID}?project={PROJECT_ID}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lym26b1dizo",
   "metadata": {},
   "source": [
    "### Initialize Testing Infrastructure\n",
    "\n",
    "Now that we've verified the Dataflow job is running, initialize the testing components:\n",
    "\n",
    "- **PubSubLoadGenerator**: Publishes test messages to the input topic with different load patterns (burst, sustained, ramp)\n",
    "- **DataflowMetricsCollector**: Collects metrics from Cloud Monitoring (worker count, system lag, backlog) and end-to-end latency from Pub/Sub output\n",
    "\n",
    "These components will be used throughout all test phases to generate load and measure performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff374321",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize testing infrastructure\n",
    "load_generator = PubSubLoadGenerator(\n",
    "    project_id=PROJECT_ID,\n",
    "    topic_name=f\"{EXPERIMENT}-input-local\"  # LOCAL model topic (isolated from Vertex endpoint)\n",
    ")\n",
    "\n",
    "metrics_collector = DataflowMetricsCollector(\n",
    "    project_id=PROJECT_ID,\n",
    "    job_id=JOB_ID,\n",
    "    region=REGION,\n",
    "    output_subscription=OUTPUT_SUBSCRIPTION  # LOCAL model output subscription\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Testing infrastructure initialized\")\n",
    "print(f\"   Project: {PROJECT_ID}\")\n",
    "print(f\"   Job ID: {JOB_ID}\")\n",
    "print(f\"   Region: {REGION}\")\n",
    "print(f\"   Input topic: {INPUT_TOPIC}\")\n",
    "print(f\"   Output subscription: {OUTPUT_SUBSCRIPTION}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phase1_header",
   "metadata": {},
   "source": [
    "---\n",
    "## Phase 1: Baseline Performance\n",
    "\n",
    "**Goal**: Establish baseline latency and confirm pipeline health\n",
    "\n",
    "**Tests**: 2 tests (~5 minutes total)\n",
    "- 10 msg/sec for 2 minutes\n",
    "- 25 msg/sec for 2 minutes\n",
    "\n",
    "**What We're Looking For**:\n",
    "- Baseline end-to-end latency at low load\n",
    "- Latency breakdown (window wait vs processing vs pub/sub)\n",
    "- Confirm no backlog builds at low rates\n",
    "- Verify pipeline is healthy and processing correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "phase1_init",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Phase 1 storage\n",
    "phase1_results = []\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"PHASE 1: BASELINE PERFORMANCE\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Testing {len(BASELINE_TESTS)} baseline rates\")\n",
    "print(f\"Total estimated time: ~5 minutes\")\n",
    "print(\"=\"*70 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "phase1_run",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run baseline tests\n",
    "for test_config in BASELINE_TESTS:\n",
    "    result = await load_generator.run_load_test(\n",
    "        pattern=\"sustained\",\n",
    "        target_rate=test_config[\"target_rate\"],\n",
    "        duration=test_config[\"duration\"],\n",
    "        test_name=test_config[\"name\"]\n",
    "    )\n",
    "    phase1_results.append(result)\n",
    "    \n",
    "    # Small delay between tests\n",
    "    print(\"\\n   Waiting 30 seconds before next test...\\n\")\n",
    "    await asyncio.sleep(30)\n",
    "\n",
    "print(f\"\\n‚úÖ Phase 1 complete: {len(BASELINE_TESTS)} baseline tests run\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phase1_metrics_header",
   "metadata": {},
   "source": [
    "### Collect Metrics for Phase 1\n",
    "\n",
    "Collect Dataflow metrics (workers, system lag, backlog) and end-to-end latency from Pub/Sub output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "phase1_metrics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect metrics for entire Phase 1\n",
    "phase1_start = min([r['start_time'] for r in phase1_results])\n",
    "phase1_end = max([r['end_time'] for r in phase1_results])\n",
    "\n",
    "print(f\"Collecting metrics for Phase 1\")\n",
    "print(f\"Time window: {phase1_start.strftime('%H:%M:%S')} ‚Üí {phase1_end.strftime('%H:%M:%S')}\")\n",
    "\n",
    "phase1_metrics = metrics_collector.collect_metrics(\n",
    "    start_time=phase1_start,\n",
    "    end_time=phase1_end,\n",
    "    resolution_seconds=10\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Metrics collected for Phase 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "phase1_latency",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect end-to-end latency for each test\n",
    "phase1_latencies = []\n",
    "\n",
    "for result in phase1_results:\n",
    "    latency_df = metrics_collector.collect_end_to_end_latency(\n",
    "        test_id=result['test_id'],\n",
    "        expected_messages=result['num_messages'],\n",
    "        timeout=180\n",
    "    )\n",
    "    phase1_latencies.append(latency_df)\n",
    "\n",
    "print(f\"\\n‚úÖ Latency data collected for {len(phase1_latencies)} tests\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phase1_analysis_header",
   "metadata": {},
   "source": [
    "### Analyze Phase 1 Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "phase1_analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze Phase 1 results\n",
    "print(\"=\"*70)\n",
    "print(\"PHASE 1 ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for i, (result, latency_df) in enumerate(zip(phase1_results, phase1_latencies)):\n",
    "    test_config = BASELINE_TESTS[i]\n",
    "\n",
    "    print(f\"\\n{test_config['name']}:\")\n",
    "    print(f\"  Target: {test_config['target_rate']} msg/sec for {test_config['duration']}s\")\n",
    "    print(f\"  Sent: {result['num_messages']:,} messages\")\n",
    "    print(f\"  Actual rate: {result['actual_rate']:.1f} msg/sec\")\n",
    "\n",
    "    if len(latency_df) > 0:\n",
    "        print(f\"\\n  Latency Statistics:\")\n",
    "        print(f\"    Pipeline (P50):  {latency_df['pipeline_latency_ms'].quantile(0.50):7.1f}ms\")\n",
    "        print(f\"    Pipeline (P95):  {latency_df['pipeline_latency_ms'].quantile(0.95):7.1f}ms\")\n",
    "        print(f\"    Pipeline (P99):  {latency_df['pipeline_latency_ms'].quantile(0.99):7.1f}ms\")\n",
    "        print(f\"    Pipeline (mean): {latency_df['pipeline_latency_ms'].mean():7.1f}ms\")\n",
    "        print(f\"\\n  Note: Pipeline latency includes all processing (inference + transforms + windowing)\")\n",
    "    else:\n",
    "        print(f\"\\n  ‚ö†Ô∏è  No latency data collected (messages may not have been processed yet)\")\n",
    "\n",
    "# Check for autoscaling\n",
    "autoscaling_events = metrics_collector.analyze_autoscaling(phase1_metrics)\n",
    "\n",
    "print(f\"\\n\" + \"=\"*70)\n",
    "print(\"AUTOSCALING ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "if len(autoscaling_events) > 0:\n",
    "    print(f\"‚úÖ Autoscaling triggered {len(autoscaling_events)} time(s)\")\n",
    "    for idx, event in autoscaling_events.iterrows():\n",
    "        print(f\"\\nEvent {idx + 1}:\")\n",
    "        print(f\"  Trigger: {event['trigger_time'].strftime('%H:%M:%S')}\")\n",
    "        print(f\"  Complete: {event['scale_complete_time'].strftime('%H:%M:%S')}\")\n",
    "        print(f\"  Workers: {event['workers_before']:.0f} ‚Üí {event['workers_after']:.0f}\")\n",
    "        print(f\"  Lag: {event['scale_up_lag_seconds']:.0f}s ({event['scale_up_lag_seconds']/60:.1f} mins)\")\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è  No autoscaling detected at baseline rates (expected)\")\n",
    "\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4114b23b",
   "metadata": {},
   "source": [
    "### üìä How to Read the Visualization\n",
    "\n",
    "The timeline visualization below shows 5 panels that track different aspects of pipeline performance. Here's what each panel tells you:\n",
    "\n",
    "#### **Panel 1: Incoming Message Rate**\n",
    "- **What it shows:** Messages published per second to the input Pub/Sub topic\n",
    "- **What to look for:**\n",
    "  - Flat line = steady rate (good for controlled testing)\n",
    "  - Spikes = burst traffic\n",
    "  - Gradual increase = ramp test pattern\n",
    "- **Interpretation:** This is your **input load** - what you're sending to the pipeline\n",
    "\n",
    "#### **Panel 2: Worker Count (Autoscaling)**\n",
    "- **What it shows:** Number of active Dataflow workers over time\n",
    "- **What to look for:**\n",
    "  - Flat line = no autoscaling needed\n",
    "  - Step up = autoscaling triggered (workers being added)\n",
    "  - Step down = scale-down (workers being removed after low load period)\n",
    "- **Interpretation:** Shows when pipeline capacity changes. Worker provisioning takes **2-4 minutes**, so you'll see a delay between load increase and worker scale-up.\n",
    "\n",
    "#### **Panel 3: System Lag (Processing Delay: Oldest To Current Message)**\n",
    "- **What it shows:** How far behind real-time the pipeline is processing (in milliseconds)\n",
    "- **What to look for:**\n",
    "  - Near-zero lag = pipeline keeping up with real-time\n",
    "  - Increasing lag = pipeline falling behind\n",
    "  - High lag (>60s) = serious capacity issues\n",
    "- **Interpretation:** This is your **real-time health indicator**. Rising lag means you need more workers or better machine type.\n",
    "\n",
    "#### **Panel 4: Output Subscription Backlog**\n",
    "- **What it shows:** Number of messages waiting in the output Pub/Sub subscription\n",
    "- **What to look for:**\n",
    "  - Zero or low = messages being consumed quickly\n",
    "  - Linearly increasing = messages accumulating (expected during testing)\n",
    "  - Very high (>10,000) = potential capacity issues\n",
    "- **Interpretation:** This backlog includes **both** pipeline processing queue AND messages waiting for the test framework to pull them. During tests, this will grow because we're not consuming as fast as we're producing.\n",
    "\n",
    "#### **Panel 5: P95 Pipeline Latency (publish ‚Üí output queue)**\n",
    "- **What it shows:** 95th percentile latency from message publish to arriving in output queue\n",
    "- **What to look for:**\n",
    "  - Steady line = consistent performance\n",
    "  - Gradual increase = latency degrading under load\n",
    "  - Spikes at test end = shutdown artifacts (normal)\n",
    "- **Interpretation:** This is your **actual pipeline performance metric** - the one you care about most. It excludes the time messages sit in the queue waiting for the test to pull them.\n",
    "- **Y-axis note:** The axis is auto-scaled to focus on steady-state latency using the 99th percentile. Very high spikes (like test shutdown) may be clipped but won't distort the main trend.\n",
    "- **Timeline fix:** Now uses **publish_time** instead of receive_time for better alignment with test execution.\n",
    "\n",
    "---\n",
    "\n",
    "**üí° Key Insights to Extract:**\n",
    "1. **When does autoscaling trigger?** ‚Üí Look for worker count step-up in Panel 2\n",
    "2. **What's the latency at different loads?** ‚Üí Compare Panel 5 across different message rates in Panel 1\n",
    "3. **Is the pipeline keeping up?** ‚Üí Panel 3 system lag should stay near zero\n",
    "4. **Is backlog building up?** ‚Üí Panel 4 shows message accumulation\n",
    "5. **Where's the bottleneck?** ‚Üí If latency is high but system lag is low, it's windowing. If system lag is high, it's processing capacity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "phase1_viz",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Phase 1 - Individual test timelines\n",
    "for i, (result, latency_df) in enumerate(zip(phase1_results, phase1_latencies)):\n",
    "    test_config = BASELINE_TESTS[i]\n",
    "    \n",
    "    # Collect metrics for this specific test\n",
    "    test_metrics = metrics_collector.collect_metrics(\n",
    "        start_time=result['start_time'],\n",
    "        end_time=result['end_time'],\n",
    "        resolution_seconds=10\n",
    "    )\n",
    "    \n",
    "    fig = plot_dataflow_timeline(\n",
    "        test_results=result,\n",
    "        metrics=test_metrics,\n",
    "        latency_df=latency_df,\n",
    "        test_name=test_config['name']\n",
    "    )\n",
    "    fig.show(renderer=\"png\", width=1400, height=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç Queuing Diagnostic: Where Does the Latency Come From?\n",
    "\n",
    "The visualization above shows WHAT happened. Now let's understand WHY.\n",
    "\n",
    "This diagnostic reveals:\n",
    "- **Latency Attribution**: What % is window assignment vs internal queuing\n",
    "- **Queue Growth**: How queue depth evolves over time\n",
    "- **Autoscaling Response**: Did workers/replicas scale in response to queuing?\n",
    "\n",
    "**How to Read the Diagnostic:**\n",
    "\n",
    "**Panel 1 (Pie Chart)**: Latency Attribution\n",
    "- üîµ **Blue = Window Assignment** (just metadata timestamps, 0-1 second)\n",
    "- üî¥ **Red = Internal Queuing** (ACTUAL bottleneck - messages waiting inside workers)\n",
    "- ‚úÖ Good: Red < 20% (window config is appropriate)\n",
    "- ‚ö†Ô∏è  Concern: Red > 50% (workers can't keep up)\n",
    "- ‚ùå Problem: Red > 80% (severe worker saturation)\n",
    "\n",
    "**Panel 2 (Stacked Area)**: Components Over Time\n",
    "- Shows how latency components change as load increases\n",
    "- Watch for red area growing ‚Üí queue building up\n",
    "\n",
    "**Panel 3 (Heatmap)**: Queue Depth Over Time\n",
    "- Dark red = Messages severely delayed\n",
    "- Shows WHEN queuing starts (correlate with load changes)\n",
    "\n",
    "**Panel 4 (Autoscaling)**: THE SMOKING GUN\n",
    "- üîµ Blue line = Worker count (Dataflow)\n",
    "- üü¢ Green dashed = Replica count (Vertex Endpoint)\n",
    "- üî¥ Red line = Queue depth\n",
    "- **Key insight**: If queue grows but workers stay flat ‚Üí autoscaling failed\n",
    "\n",
    "**Common Patterns:**\n",
    "\n",
    "1. **Healthy System**:\n",
    "   - Window assignment: 5-15%\n",
    "   - Internal queue: <20%\n",
    "   - Workers scale up as queue grows\n",
    "   - Queue depth stays low (<1 second)\n",
    "\n",
    "2. **Worker Autoscaling Failure** (most common):\n",
    "   - Internal queue: 80-95%\n",
    "   - Workers stay at 1 despite configuration\n",
    "   - Queue depth grows to 10+ seconds\n",
    "   - **Fix**: See recommended fixes below\n",
    "\n",
    "3. **Endpoint Bottleneck**:\n",
    "   - Internal queue: 30-50%\n",
    "   - Replicas stay at min\n",
    "   - Endpoint CPU at 100%\n",
    "   - **Fix**: Increase max replicas or machine type\n",
    "\n",
    "**If You See Workers NOT Scaling:**\n",
    "\n",
    "The autoscaling algorithm may not be triggering because:\n",
    "1. Pub/Sub backlog stays small (workers pull fast, process slow)\n",
    "2. System lag metric doesn't capture internal worker queuing\n",
    "3. Wrong autoscaling algorithm for the workload\n",
    "\n",
    "**Recommended Fixes:**\n",
    "\n",
    "```python\n",
    "# In pipeline options, add:\n",
    "\"--autoscaling_algorithm=THROUGHPUT_BASED\",  # Scale based on throughput, not backlog\n",
    "\"--target_worker_utilization=0.7\",            # Scale at 70% capacity (more aggressive)\n",
    "\"--num_workers=2\",                             # Ensure proper initial worker count\n",
    "```\n",
    "\n",
    "Additionally in the pipeline code:\n",
    "```python\n",
    "# Reduce buffering to expose backpressure faster\n",
    "| \"Batch elements\" >> beam.BatchElements(\n",
    "    min_batch_size=1,\n",
    "    max_batch_size=50,\n",
    "    max_batch_duration_secs=0.001  # 1ms instead of 10ms\n",
    ")\n",
    "```\n",
    "\n",
    "**Understanding the Metrics:**\n",
    "\n",
    "- **Window wait time**: Time from publish to window_end timestamp (metadata only, not actual waiting)\n",
    "- **Internal queue time**: Time from window_end to pipeline output (REAL delay - workers saturated)\n",
    "- **P95 queue depth**: 95th percentile worst-case delay (most sensitive metric)\n",
    "\n",
    "The key insight: Window configuration is almost NEVER the bottleneck. The real issue is always worker saturation or endpoint throttling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c19c70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create queuing diagnostic\n",
    "from scale_testing_dataflow_utils import create_queuing_diagnostic, print_queuing_summary\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"QUEUING DIAGNOSTIC\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create visualization\n",
    "fig_diagnostic = create_queuing_diagnostic(\n",
    "    latency_df=pd.concat(phase1_latencies, ignore_index=True),\n",
    "    worker_metrics=phase1_metrics['workers'],\n",
    "    test_name=\"Phase 1: Baseline Performance\"\n",
    ")\n",
    "\n",
    "fig_diagnostic.show(renderer=\"png\", width=1400, height=1200)\n",
    "\n",
    "# Print summary\n",
    "print_queuing_summary(\n",
    "    latency_df=pd.concat(phase1_latencies, ignore_index=True),\n",
    "    worker_metrics=phase1_metrics['workers']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phase2a_header",
   "metadata": {},
   "source": [
    "---\n",
    "## Phase 2A: Throughput Threshold Hunt\n",
    "\n",
    "**Goal**: Find exact threshold where backlog builds and autoscaling triggers\n",
    "\n",
    "**Tests**: 3 sustained load tests (~20 minutes total)\n",
    "- 50 msg/sec for 5 minutes\n",
    "- 100 msg/sec for 5 minutes\n",
    "- 200 msg/sec for 5 minutes\n",
    "\n",
    "**What We're Looking For**:\n",
    "- At what message rate does backlog start building?\n",
    "- At what point does system lag increase?\n",
    "- When do workers autoscale up?\n",
    "- How long does it take for new workers to become active?\n",
    "\n",
    "‚è≥ **This phase will take approximately 20 minutes** (3 tests √ó 5 mins each + metrics collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "phase2a_init",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Phase 2A storage\n",
    "phase2a_results = []\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"PHASE 2A: THROUGHPUT THRESHOLD HUNT\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Testing {len(SUSTAINED_TESTS)} sustained load patterns\")\n",
    "print(f\"Total estimated time: ~{sum(t['duration'] for t in SUSTAINED_TESTS) // 60} minutes\")\n",
    "print(\"\\nThis phase tests sustained high load to trigger autoscaling.\")\n",
    "print(\"=\"*70 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "phase2a_run",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run sustained load tests\n",
    "for test_config in SUSTAINED_TESTS:\n",
    "    result = await load_generator.run_load_test(\n",
    "        pattern=\"sustained\",\n",
    "        target_rate=test_config[\"target_rate\"],\n",
    "        duration=test_config[\"duration\"],\n",
    "        test_name=test_config[\"name\"]\n",
    "    )\n",
    "    phase2a_results.append(result)\n",
    "    \n",
    "    # Small delay between tests to let pipeline stabilize\n",
    "    print(\"\\n   Waiting 60 seconds before next test...\\n\")\n",
    "    await asyncio.sleep(60)\n",
    "\n",
    "print(f\"\\n‚úÖ Phase 2A complete: {len(SUSTAINED_TESTS)} sustained tests run\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phase2a_metrics_header",
   "metadata": {},
   "source": [
    "### Collect Metrics for Phase 2A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "phase2a_metrics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect metrics for entire Phase 2A\n",
    "phase2a_start = min([r['start_time'] for r in phase2a_results])\n",
    "phase2a_end = max([r['end_time'] for r in phase2a_results])\n",
    "\n",
    "print(f\"Collecting metrics for Phase 2A\")\n",
    "print(f\"Time window: {phase2a_start.strftime('%H:%M:%S')} ‚Üí {phase2a_end.strftime('%H:%M:%S')}\")\n",
    "\n",
    "phase2a_metrics = metrics_collector.collect_metrics(\n",
    "    start_time=phase2a_start,\n",
    "    end_time=phase2a_end,\n",
    "    resolution_seconds=10\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Metrics collected for Phase 2A\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "phase2a_latency",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect end-to-end latency for each test\n",
    "phase2a_latencies = []\n",
    "\n",
    "for result in phase2a_results:\n",
    "    latency_df = metrics_collector.collect_end_to_end_latency(\n",
    "        test_id=result['test_id'],\n",
    "        expected_messages=result['num_messages'],\n",
    "        timeout=300  # Longer timeout for high-volume tests\n",
    "    )\n",
    "    phase2a_latencies.append(latency_df)\n",
    "\n",
    "print(f\"\\n‚úÖ Latency data collected for {len(phase2a_latencies)} tests\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phase2a_analysis_header",
   "metadata": {},
   "source": [
    "### Analyze Phase 2A Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "phase2a_analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed analysis of Phase 2A\n",
    "print(\"=\"*70)\n",
    "print(\"PHASE 2A ANALYSIS: THROUGHPUT THRESHOLDS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for i, (result, latency_df) in enumerate(zip(phase2a_results, phase2a_latencies)):\n",
    "    test_config = SUSTAINED_TESTS[i]\n",
    "    \n",
    "    print(f\"\\n{test_config['name']}:\")\n",
    "    print(f\"  Configuration: {test_config['target_rate']} msg/sec for {test_config['duration']}s\")\n",
    "    print(f\"  Messages sent: {result['num_messages']:,}\")\n",
    "    print(f\"  Actual rate: {result['actual_rate']:.1f} msg/sec\")\n",
    "    \n",
    "    if len(latency_df) > 0:\n",
    "        print(f\"\\n  Latency Performance:\")\n",
    "        print(f\"    Mean E2E: {latency_df['pipeline_latency_ms'].mean():7.1f}ms\")\n",
    "        print(f\"    P95 E2E:  {latency_df['pipeline_latency_ms'].quantile(0.95):7.1f}ms\")\n",
    "        print(f\"    P99 E2E:  {latency_df['pipeline_latency_ms'].quantile(0.99):7.1f}ms\")\n",
    "        \n",
    "        # Latency degradation check\n",
    "        if i > 0 and len(phase2a_latencies[0]) > 0:\n",
    "            baseline_p95 = phase2a_latencies[0]['pipeline_latency_ms'].quantile(0.95)\n",
    "            current_p95 = latency_df['pipeline_latency_ms'].quantile(0.95)\n",
    "            degradation_pct = ((current_p95 - baseline_p95) / baseline_p95) * 100\n",
    "            \n",
    "            print(f\"\\n  Latency vs Baseline:\")\n",
    "            print(f\"    Baseline P95: {baseline_p95:.1f}ms\")\n",
    "            print(f\"    Current P95:  {current_p95:.1f}ms\")\n",
    "            print(f\"    Change: {degradation_pct:+.1f}%\")\n",
    "            \n",
    "            if degradation_pct > 50:\n",
    "                print(f\"    ‚ö†Ô∏è  Significant latency increase - may indicate capacity limits\")\n",
    "    else:\n",
    "        print(f\"\\n  ‚ö†Ô∏è  No latency data (processing may be delayed)\")\n",
    "\n",
    "# Analyze autoscaling behavior\n",
    "print(f\"\\n\" + \"=\"*70)\n",
    "print(\"AUTOSCALING EVENTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "autoscaling_events = metrics_collector.analyze_autoscaling(phase2a_metrics)\n",
    "\n",
    "if len(autoscaling_events) > 0:\n",
    "    print(f\"\\n‚úÖ Autoscaling triggered {len(autoscaling_events)} time(s) during Phase 2A\\n\")\n",
    "    \n",
    "    for idx, event in autoscaling_events.iterrows():\n",
    "        print(f\"Event {idx + 1}:\")\n",
    "        print(f\"  Trigger time:     {event['trigger_time'].strftime('%H:%M:%S')}\")\n",
    "        print(f\"  Complete time:    {event['scale_complete_time'].strftime('%H:%M:%S')}\")\n",
    "        print(f\"  Workers:          {event['workers_before']:.0f} ‚Üí {event['workers_after']:.0f}\")\n",
    "        print(f\"  Provisioning lag: {event['scale_up_lag_seconds']:.0f}s ({event['scale_up_lag_seconds']/60:.1f} mins)\")\n",
    "        print()\n",
    "    \n",
    "    # Calculate average provisioning time\n",
    "    avg_lag = autoscaling_events['scale_up_lag_seconds'].mean()\n",
    "    print(f\"Average worker provisioning time: {avg_lag:.0f}s ({avg_lag/60:.1f} mins)\")\n",
    "else:\n",
    "    print(\"\\n‚ÑπÔ∏è  No autoscaling detected\")\n",
    "    print(\"   Possible reasons:\")\n",
    "    print(\"   - Message rate not high enough to trigger autoscaling\")\n",
    "    print(\"   - Current worker capacity sufficient for tested load\")\n",
    "    print(\"   - Processing is very efficient\")\n",
    "\n",
    "# Analyze backlog behavior\n",
    "if 'backlog' in phase2a_metrics and len(phase2a_metrics['backlog']) > 0:\n",
    "    print(f\"\\n\" + \"=\"*70)\n",
    "    print(\"BACKLOG ANALYSIS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    max_backlog = phase2a_metrics['backlog']['value'].max()\n",
    "    mean_backlog = phase2a_metrics['backlog']['value'].mean()\n",
    "    \n",
    "    print(f\"\\nBacklog Statistics:\")\n",
    "    print(f\"  Max backlog: {max_backlog:,.0f} messages\")\n",
    "    print(f\"  Mean backlog: {mean_backlog:,.0f} messages\")\n",
    "    \n",
    "    if max_backlog > 1000:\n",
    "        print(f\"\\n  ‚ö†Ô∏è  High backlog detected - pipeline may be at capacity\")\n",
    "    elif max_backlog > 100:\n",
    "        print(f\"\\n  ‚ÑπÔ∏è  Moderate backlog - pipeline handling load but close to limits\")\n",
    "    else:\n",
    "        print(f\"\\n  ‚úÖ Backlog under control - pipeline has headroom\")\n",
    "\n",
    "# Analyze system lag\n",
    "if 'system_lag' in phase2a_metrics and len(phase2a_metrics['system_lag']) > 0:\n",
    "    print(f\"\\n\" + \"=\"*70)\n",
    "    print(\"SYSTEM LAG ANALYSIS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    max_lag_ms = phase2a_metrics['system_lag']['value'].max() / 1000  # Convert to ms\n",
    "    mean_lag_ms = phase2a_metrics['system_lag']['value'].mean() / 1000\n",
    "    \n",
    "    print(f\"\\nSystem Lag Statistics:\")\n",
    "    print(f\"  Max lag: {max_lag_ms:,.1f}ms\")\n",
    "    print(f\"  Mean lag: {mean_lag_ms:,.1f}ms\")\n",
    "    \n",
    "    if max_lag_ms > 60000:  # 1 minute\n",
    "        print(f\"\\n  ‚ö†Ô∏è  High system lag - pipeline falling behind real-time\")\n",
    "    elif max_lag_ms > 10000:  # 10 seconds\n",
    "        print(f\"\\n  ‚ÑπÔ∏è  Moderate system lag - processing slightly delayed\")\n",
    "    else:\n",
    "        print(f\"\\n  ‚úÖ System lag low - processing near real-time\")\n",
    "\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ac73ff",
   "metadata": {},
   "source": [
    "### üìä How to Read the Visualization\n",
    "\n",
    "The timeline visualization below shows 5 panels that track different aspects of pipeline performance. Here's what each panel tells you:\n",
    "\n",
    "#### **Panel 1: Incoming Message Rate**\n",
    "- **What it shows:** Messages published per second to the input Pub/Sub topic\n",
    "- **What to look for:**\n",
    "  - Flat line = steady rate (good for controlled testing)\n",
    "  - Spikes = burst traffic\n",
    "  - Gradual increase = ramp test pattern\n",
    "- **Interpretation:** This is your **input load** - what you're sending to the pipeline\n",
    "\n",
    "#### **Panel 2: Worker Count (Autoscaling)**\n",
    "- **What it shows:** Number of active Dataflow workers over time\n",
    "- **What to look for:**\n",
    "  - Flat line = no autoscaling needed\n",
    "  - Step up = autoscaling triggered (workers being added)\n",
    "  - Step down = scale-down (workers being removed after low load period)\n",
    "- **Interpretation:** Shows when pipeline capacity changes. Worker provisioning takes **2-4 minutes**, so you'll see a delay between load increase and worker scale-up.\n",
    "\n",
    "#### **Panel 3: System Lag (Processing Delay: Oldest To Current Message)**\n",
    "- **What it shows:** How far behind real-time the pipeline is processing (in milliseconds)\n",
    "- **What to look for:**\n",
    "  - Near-zero lag = pipeline keeping up with real-time\n",
    "  - Increasing lag = pipeline falling behind\n",
    "  - High lag (>60s) = serious capacity issues\n",
    "- **Interpretation:** This is your **real-time health indicator**. Rising lag means you need more workers or better machine type.\n",
    "\n",
    "#### **Panel 4: Output Subscription Queue**\n",
    "- **What it shows:** Number of messages waiting in the output Pub/Sub subscription\n",
    "- **What to look for:**\n",
    "  - Zero or low = messages being consumed quickly\n",
    "  - Linearly increasing = messages accumulating (expected during testing)\n",
    "  - Very high (>10,000) = potential capacity issues\n",
    "- **Interpretation:** This backlog includes **both** pipeline processing queue AND messages waiting for the test framework to pull them. During tests, this will grow because we're not consuming as fast as we're producing.\n",
    "\n",
    "#### **Panel 5: P95 Pipeline Latency (publish ‚Üí output queue, excludes queue wait)**\n",
    "- **What it shows:** 95th percentile latency from message publish to arriving in output queue\n",
    "- **What to look for:**\n",
    "  - Steady line = consistent performance\n",
    "  - Gradual increase = latency degrading under load\n",
    "  - Spikes at test end = shutdown artifacts (normal)\n",
    "- **Interpretation:** This is your **actual pipeline performance metric** - the one you care about most. It excludes the time messages sit in the queue waiting for the test to pull them.\n",
    "- **Y-axis note:** The axis is auto-scaled to focus on steady-state latency using the 99th percentile. Very high spikes (like test shutdown) may be clipped but won't distort the main trend.\n",
    "\n",
    "---\n",
    "\n",
    "**üí° Key Insights to Extract:**\n",
    "1. **When does autoscaling trigger?** ‚Üí Look for worker count step-up in Panel 2\n",
    "2. **What's the latency at different loads?** ‚Üí Compare Panel 5 across different message rates in Panel 1\n",
    "3. **Is the pipeline keeping up?** ‚Üí Panel 3 system lag should stay near zero\n",
    "4. **Where's the bottleneck?** ‚Üí If latency is high but system lag is low, it's windowing. If system lag is high, it's processing capacity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "phase2a_viz",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize complete Phase 2A timeline\n",
    "print(\"Creating comprehensive Phase 2A visualization...\")\n",
    "print(f\"This shows all {len(SUSTAINED_TESTS)} sustained load tests in sequence\\n\")\n",
    "\n",
    "# Combine all test results and latencies\n",
    "combined_result = {\n",
    "    'test_id': 'phase2a-combined',\n",
    "    'test_name': 'Phase 2A - All Sustained Tests',\n",
    "    'start_time': phase2a_start,\n",
    "    'end_time': phase2a_end,\n",
    "    'message_data': pd.concat([r['message_data'] for r in phase2a_results], ignore_index=True)\n",
    "}\n",
    "\n",
    "combined_latency = pd.concat(phase2a_latencies, ignore_index=True)\n",
    "\n",
    "fig = plot_dataflow_timeline(\n",
    "    test_results=combined_result,\n",
    "    metrics=phase2a_metrics,\n",
    "    latency_df=combined_latency,\n",
    "    test_name=\"Phase 2A: Throughput Threshold Hunt - Complete Timeline\"\n",
    ")\n",
    "\n",
    "fig.show(renderer=\"png\", width=1400, height=1000)\n",
    "\n",
    "print(\"\\nüí° This visualization shows:\")\n",
    "print(f\"   - Message rate progression: {SUSTAINED_TESTS[0]['target_rate']} ‚Üí {SUSTAINED_TESTS[-1]['target_rate']} msg/sec\")\n",
    "print(f\"   - Worker scaling behavior over {len(SUSTAINED_TESTS)} tests\")\n",
    "print(f\"   - System lag and backlog buildup\")\n",
    "print(f\"   - P95 latency trends as load increases\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab386b64",
   "metadata": {},
   "source": [
    "### üîç Queuing Diagnostic: Where Does the Latency Come From?\n",
    "\n",
    "The visualization above shows WHAT happened. Now let's understand WHY.\n",
    "\n",
    "This diagnostic reveals:\n",
    "- **Latency Attribution**: What % is window assignment vs internal queuing\n",
    "- **Queue Growth**: How queue depth evolves over time\n",
    "- **Autoscaling Response**: Did workers/replicas scale in response to queuing?\n",
    "\n",
    "**How to Read the Diagnostic:**\n",
    "\n",
    "**Panel 1 (Pie Chart)**: Latency Attribution\n",
    "- üîµ **Blue = Window Assignment** (just metadata timestamps, 0-1 second)\n",
    "- üî¥ **Red = Internal Queuing** (ACTUAL bottleneck - messages waiting inside workers)\n",
    "- ‚úÖ Good: Red < 20% (window config is appropriate)\n",
    "- ‚ö†Ô∏è  Concern: Red > 50% (workers can't keep up)\n",
    "- ‚ùå Problem: Red > 80% (severe worker saturation)\n",
    "\n",
    "**Panel 2 (Stacked Area)**: Components Over Time\n",
    "- Shows how latency components change as load increases\n",
    "- Watch for red area growing ‚Üí queue building up\n",
    "\n",
    "**Panel 3 (Heatmap)**: Queue Depth Over Time\n",
    "- Dark red = Messages severely delayed\n",
    "- Shows WHEN queuing starts (correlate with load changes)\n",
    "\n",
    "**Panel 4 (Autoscaling)**: THE SMOKING GUN\n",
    "- üîµ Blue line = Worker count (Dataflow)\n",
    "- üü¢ Green dashed = Replica count (Vertex Endpoint)\n",
    "- üî¥ Red line = Queue depth\n",
    "- **Key insight**: If queue grows but workers stay flat ‚Üí autoscaling failed\n",
    "\n",
    "**Common Patterns:**\n",
    "\n",
    "1. **Healthy System**:\n",
    "   - Window assignment: 5-15%\n",
    "   - Internal queue: <20%\n",
    "   - Workers scale up as queue grows\n",
    "   - Queue depth stays low (<1 second)\n",
    "\n",
    "2. **Worker Autoscaling Failure** (most common):\n",
    "   - Internal queue: 80-95%\n",
    "   - Workers stay at 1 despite configuration\n",
    "   - Queue depth grows to 10+ seconds\n",
    "   - **Fix**: See recommended fixes below\n",
    "\n",
    "3. **Endpoint Bottleneck**:\n",
    "   - Internal queue: 30-50%\n",
    "   - Replicas stay at min\n",
    "   - Endpoint CPU at 100%\n",
    "   - **Fix**: Increase max replicas or machine type\n",
    "\n",
    "**If You See Workers NOT Scaling:**\n",
    "\n",
    "The autoscaling algorithm may not be triggering because:\n",
    "1. Pub/Sub backlog stays small (workers pull fast, process slow)\n",
    "2. System lag metric doesn't capture internal worker queuing\n",
    "3. Wrong autoscaling algorithm for the workload\n",
    "\n",
    "**Recommended Fixes:**\n",
    "\n",
    "```python\n",
    "# In pipeline options, add:\n",
    "\"--autoscaling_algorithm=THROUGHPUT_BASED\",  # Scale based on throughput, not backlog\n",
    "\"--target_worker_utilization=0.7\",            # Scale at 70% capacity (more aggressive)\n",
    "\"--num_workers=2\",                             # Ensure proper initial worker count\n",
    "```\n",
    "\n",
    "Additionally in the pipeline code:\n",
    "```python\n",
    "# Reduce buffering to expose backpressure faster\n",
    "| \"Batch elements\" >> beam.BatchElements(\n",
    "    min_batch_size=1,\n",
    "    max_batch_size=50,\n",
    "    max_batch_duration_secs=0.001  # 1ms instead of 10ms\n",
    ")\n",
    "```\n",
    "\n",
    "**Understanding the Metrics:**\n",
    "\n",
    "- **Window wait time**: Time from publish to window_end timestamp (metadata only, not actual waiting)\n",
    "- **Internal queue time**: Time from window_end to pipeline output (REAL delay - workers saturated)\n",
    "- **P95 queue depth**: 95th percentile worst-case delay (most sensitive metric)\n",
    "\n",
    "The key insight: Window configuration is almost NEVER the bottleneck. The real issue is always worker saturation or endpoint throttling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ba77f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create queuing diagnostic\n",
    "from scale_testing_dataflow_utils import create_queuing_diagnostic, print_queuing_summary\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"QUEUING DIAGNOSTIC\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create visualization\n",
    "fig_diagnostic = create_queuing_diagnostic(\n",
    "    latency_df=combined_latency,\n",
    "    worker_metrics=phase2a_metrics['workers'],\n",
    "    test_name=\"Phase 2A: Stress Dataflow Workers\"\n",
    ")\n",
    "\n",
    "fig_diagnostic.show(renderer=\"png\", width=1400, height=1200)\n",
    "\n",
    "# Print summary\n",
    "print_queuing_summary(\n",
    "    latency_df=combined_latency,\n",
    "    worker_metrics=phase2a_metrics['workers']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phase2b_header",
   "metadata": {},
   "source": [
    "---\n",
    "## Phase 2B: Burst Capacity\n",
    "\n",
    "**Goal**: Test backlog recovery time and worker scaling response to sudden spikes\n",
    "\n",
    "**Tests**: 3 burst tests (~10 minutes total)\n",
    "- 1,000 messages sent as fast as possible\n",
    "- 5,000 messages sent as fast as possible\n",
    "- 10,000 messages sent as fast as possible\n",
    "\n",
    "**What We're Looking For**:\n",
    "- How quickly does backlog clear after burst?\n",
    "- Does autoscaling respond to sudden traffic spikes?\n",
    "- What's the maximum burst size before sustained backlog?\n",
    "- Recovery time from burst to normal operation\n",
    "\n",
    "‚è≥ **This phase will take approximately 10 minutes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "phase2b_init",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Phase 2B storage\n",
    "phase2b_results = []\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"PHASE 2B: BURST CAPACITY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Testing {len(BURST_TESTS)} burst patterns\")\n",
    "print(f\"Total estimated time: ~10 minutes\")\n",
    "print(\"\\nThis phase tests sudden traffic spikes and recovery behavior.\")\n",
    "print(\"=\"*70 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "phase2b_run",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run burst tests\n",
    "for test_config in BURST_TESTS:\n",
    "    result = await load_generator.run_load_test(\n",
    "        pattern=\"burst\",\n",
    "        num_messages=test_config[\"num_messages\"],\n",
    "        test_name=test_config[\"name\"]\n",
    "    )\n",
    "    phase2b_results.append(result)\n",
    "    \n",
    "    # Wait for backlog to clear before next burst\n",
    "    print(\"\\n   Waiting 120 seconds for backlog to clear...\\n\")\n",
    "    await asyncio.sleep(120)\n",
    "\n",
    "print(f\"\\n‚úÖ Phase 2B complete: {len(BURST_TESTS)} burst tests run\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phase2b_metrics_header",
   "metadata": {},
   "source": [
    "### Collect Metrics for Phase 2B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "phase2b_metrics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect metrics for entire Phase 2B\n",
    "phase2b_start = min([r['start_time'] for r in phase2b_results])\n",
    "phase2b_end = max([r['end_time'] for r in phase2b_results])\n",
    "\n",
    "print(f\"Collecting metrics for Phase 2B\")\n",
    "print(f\"Time window: {phase2b_start.strftime('%H:%M:%S')} ‚Üí {phase2b_end.strftime('%H:%M:%S')}\")\n",
    "\n",
    "phase2b_metrics = metrics_collector.collect_metrics(\n",
    "    start_time=phase2b_start,\n",
    "    end_time=phase2b_end,\n",
    "    resolution_seconds=10\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Metrics collected for Phase 2B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "phase2b_latency",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect end-to-end latency for burst tests\n",
    "phase2b_latencies = []\n",
    "\n",
    "for result in phase2b_results:\n",
    "    latency_df = metrics_collector.collect_end_to_end_latency(\n",
    "        test_id=result['test_id'],\n",
    "        expected_messages=result['num_messages'],\n",
    "        timeout=300\n",
    "    )\n",
    "    phase2b_latencies.append(latency_df)\n",
    "\n",
    "print(f\"\\n‚úÖ Latency data collected for {len(phase2b_latencies)} burst tests\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phase2b_analysis_header",
   "metadata": {},
   "source": [
    "### Analyze Phase 2B Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "phase2b_analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze burst behavior and recovery\n",
    "print(\"=\"*70)\n",
    "print(\"PHASE 2B ANALYSIS: BURST CAPACITY & RECOVERY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for i, (result, latency_df) in enumerate(zip(phase2b_results, phase2b_latencies)):\n",
    "    test_config = BURST_TESTS[i]\n",
    "    \n",
    "    print(f\"\\n{test_config['name']}:\")\n",
    "    print(f\"  Messages: {test_config['num_messages']:,} sent as fast as possible\")\n",
    "    print(f\"  Publish time: {result['elapsed_seconds']:.1f}s\")\n",
    "    print(f\"  Publish rate: {result['actual_rate']:.1f} msg/sec\")\n",
    "    \n",
    "    if len(latency_df) > 0:\n",
    "        # Calculate recovery metrics\n",
    "        first_msg_latency = latency_df.iloc[0]['total_e2e_ms']\n",
    "        last_msg_latency = latency_df.iloc[-1]['total_e2e_ms']\n",
    "        max_latency = latency_df['pipeline_latency_ms'].max()\n",
    "        \n",
    "        print(f\"\\n  Latency During Burst:\")\n",
    "        print(f\"    First message: {first_msg_latency:,.1f}ms\")\n",
    "        print(f\"    Last message:  {last_msg_latency:,.1f}ms\")\n",
    "        print(f\"    Peak latency:  {max_latency:,.1f}ms\")\n",
    "        print(f\"    Mean latency:  {latency_df['pipeline_latency_ms'].mean():,.1f}ms\")\n",
    "        print(f\"    P95 latency:   {latency_df['pipeline_latency_ms'].quantile(0.95):,.1f}ms\")\n",
    "        \n",
    "        # Analyze latency distribution (early vs late messages)\n",
    "        first_third = latency_df.iloc[:len(latency_df)//3]\n",
    "        last_third = latency_df.iloc[-len(latency_df)//3:]\n",
    "        \n",
    "        print(f\"\\n  Latency Evolution:\")\n",
    "        print(f\"    First 1/3 mean:  {first_third['pipeline_latency_ms'].mean():,.1f}ms\")\n",
    "        print(f\"    Last 1/3 mean:   {last_third['pipeline_latency_ms'].mean():,.1f}ms\")\n",
    "        \n",
    "        if last_third['pipeline_latency_ms'].mean() < first_third['pipeline_latency_ms'].mean():\n",
    "            improvement = ((first_third['pipeline_latency_ms'].mean() - last_third['pipeline_latency_ms'].mean()) / \n",
    "                          first_third['pipeline_latency_ms'].mean() * 100)\n",
    "            print(f\"    ‚úÖ Latency improved by {improvement:.1f}% (backlog clearing)\")\n",
    "        else:\n",
    "            degradation = ((last_third['pipeline_latency_ms'].mean() - first_third['pipeline_latency_ms'].mean()) / \n",
    "                          first_third['pipeline_latency_ms'].mean() * 100)\n",
    "            print(f\"    ‚ö†Ô∏è  Latency degraded by {degradation:.1f}% (backlog building)\")\n",
    "    else:\n",
    "        print(f\"\\n  ‚ö†Ô∏è  No latency data collected\")\n",
    "\n",
    "# Analyze burst autoscaling\n",
    "print(f\"\\n\" + \"=\"*70)\n",
    "print(\"AUTOSCALING RESPONSE TO BURSTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "autoscaling_events = metrics_collector.analyze_autoscaling(phase2b_metrics)\n",
    "\n",
    "if len(autoscaling_events) > 0:\n",
    "    print(f\"\\n‚úÖ Autoscaling responded {len(autoscaling_events)} time(s) to burst traffic\\n\")\n",
    "    \n",
    "    for idx, event in autoscaling_events.iterrows():\n",
    "        print(f\"Event {idx + 1}:\")\n",
    "        print(f\"  Trigger: {event['trigger_time'].strftime('%H:%M:%S')}\")\n",
    "        print(f\"  Complete: {event['scale_complete_time'].strftime('%H:%M:%S')}\")\n",
    "        print(f\"  Workers: {event['workers_before']:.0f} ‚Üí {event['workers_after']:.0f}\")\n",
    "        print(f\"  Response time: {event['scale_up_lag_seconds']:.0f}s\")\n",
    "        print()\n",
    "else:\n",
    "    print(\"\\n‚ÑπÔ∏è  No autoscaling triggered by bursts\")\n",
    "    print(\"   Bursts may be too short-lived to trigger autoscaling\")\n",
    "    print(\"   Current workers may have sufficient capacity\")\n",
    "\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798f210c",
   "metadata": {},
   "source": [
    "### üìä How to Read the Visualization\n",
    "\n",
    "The timeline visualization below shows 5 panels that track different aspects of pipeline performance. Here's what each panel tells you:\n",
    "\n",
    "#### **Panel 1: Incoming Message Rate**\n",
    "- **What it shows:** Messages published per second to the input Pub/Sub topic\n",
    "- **What to look for:**\n",
    "  - Flat line = steady rate (good for controlled testing)\n",
    "  - Spikes = burst traffic\n",
    "  - Gradual increase = ramp test pattern\n",
    "- **Interpretation:** This is your **input load** - what you're sending to the pipeline\n",
    "\n",
    "#### **Panel 2: Worker Count (Autoscaling)**\n",
    "- **What it shows:** Number of active Dataflow workers over time\n",
    "- **What to look for:**\n",
    "  - Flat line = no autoscaling needed\n",
    "  - Step up = autoscaling triggered (workers being added)\n",
    "  - Step down = scale-down (workers being removed after low load period)\n",
    "- **Interpretation:** Shows when pipeline capacity changes. Worker provisioning takes **2-4 minutes**, so you'll see a delay between load increase and worker scale-up.\n",
    "\n",
    "#### **Panel 3: System Lag (Processing Delay: Oldest To Current Message)**\n",
    "- **What it shows:** How far behind real-time the pipeline is processing (in milliseconds)\n",
    "- **What to look for:**\n",
    "  - Near-zero lag = pipeline keeping up with real-time\n",
    "  - Increasing lag = pipeline falling behind\n",
    "  - High lag (>60s) = serious capacity issues\n",
    "- **Interpretation:** This is your **real-time health indicator**. Rising lag means you need more workers or better machine type.\n",
    "\n",
    "#### **Panel 4: Output Subscription Queue**\n",
    "- **What it shows:** Number of messages waiting in the output Pub/Sub subscription\n",
    "- **What to look for:**\n",
    "  - Zero or low = messages being consumed quickly\n",
    "  - Linearly increasing = messages accumulating (expected during testing)\n",
    "  - Very high (>10,000) = potential capacity issues\n",
    "- **Interpretation:** This backlog includes **both** pipeline processing queue AND messages waiting for the test framework to pull them. During tests, this will grow because we're not consuming as fast as we're producing.\n",
    "\n",
    "#### **Panel 5: P95 Pipeline Latency (publish ‚Üí output queue, excludes queue wait)**\n",
    "- **What it shows:** 95th percentile latency from message publish to arriving in output queue\n",
    "- **What to look for:**\n",
    "  - Steady line = consistent performance\n",
    "  - Gradual increase = latency degrading under load\n",
    "  - Spikes at test end = shutdown artifacts (normal)\n",
    "- **Interpretation:** This is your **actual pipeline performance metric** - the one you care about most. It excludes the time messages sit in the queue waiting for the test to pull them.\n",
    "- **Y-axis note:** The axis is auto-scaled to focus on steady-state latency using the 99th percentile. Very high spikes (like test shutdown) may be clipped but won't distort the main trend.\n",
    "\n",
    "---\n",
    "\n",
    "**üí° Key Insights to Extract:**\n",
    "1. **When does autoscaling trigger?** ‚Üí Look for worker count step-up in Panel 2\n",
    "2. **What's the latency at different loads?** ‚Üí Compare Panel 5 across different message rates in Panel 1\n",
    "3. **Is the pipeline keeping up?** ‚Üí Panel 3 system lag should stay near zero\n",
    "4. **Where's the bottleneck?** ‚Üí If latency is high but system lag is low, it's windowing. If system lag is high, it's processing capacity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "phase2b_viz",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize burst tests\n",
    "for i, (result, latency_df) in enumerate(zip(phase2b_results, phase2b_latencies)):\n",
    "    test_config = BURST_TESTS[i]\n",
    "    \n",
    "    # Get metrics for this specific burst\n",
    "    test_metrics = metrics_collector.collect_metrics(\n",
    "        start_time=result['start_time'],\n",
    "        end_time=result['end_time'],\n",
    "        resolution_seconds=10\n",
    "    )\n",
    "    \n",
    "    fig = plot_dataflow_timeline(\n",
    "        test_results=result,\n",
    "        metrics=test_metrics,\n",
    "        latency_df=latency_df,\n",
    "        test_name=test_config['name']\n",
    "    )\n",
    "    fig.show(renderer=\"png\", width=1400, height=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç Queuing Diagnostic: Where Does the Latency Come From?\n",
    "\n",
    "The visualization above shows WHAT happened. Now let's understand WHY.\n",
    "\n",
    "This diagnostic reveals:\n",
    "- **Latency Attribution**: What % is window assignment vs internal queuing\n",
    "- **Queue Growth**: How queue depth evolves over time\n",
    "- **Autoscaling Response**: Did workers/replicas scale in response to queuing?\n",
    "\n",
    "**How to Read the Diagnostic:**\n",
    "\n",
    "**Panel 1 (Pie Chart)**: Latency Attribution\n",
    "- üîµ **Blue = Window Assignment** (just metadata timestamps, 0-1 second)\n",
    "- üî¥ **Red = Internal Queuing** (ACTUAL bottleneck - messages waiting inside workers)\n",
    "- ‚úÖ Good: Red < 20% (window config is appropriate)\n",
    "- ‚ö†Ô∏è  Concern: Red > 50% (workers can't keep up)\n",
    "- ‚ùå Problem: Red > 80% (severe worker saturation)\n",
    "\n",
    "**Panel 2 (Stacked Area)**: Components Over Time\n",
    "- Shows how latency components change as load increases\n",
    "- Watch for red area growing ‚Üí queue building up\n",
    "\n",
    "**Panel 3 (Heatmap)**: Queue Depth Over Time\n",
    "- Dark red = Messages severely delayed\n",
    "- Shows WHEN queuing starts (correlate with load changes)\n",
    "\n",
    "**Panel 4 (Autoscaling)**: THE SMOKING GUN\n",
    "- üîµ Blue line = Worker count (Dataflow)\n",
    "- üü¢ Green dashed = Replica count (Vertex Endpoint)\n",
    "- üî¥ Red line = Queue depth\n",
    "- **Key insight**: If queue grows but workers stay flat ‚Üí autoscaling failed\n",
    "\n",
    "**Common Patterns:**\n",
    "\n",
    "1. **Healthy System**:\n",
    "   - Window assignment: 5-15%\n",
    "   - Internal queue: <20%\n",
    "   - Workers scale up as queue grows\n",
    "   - Queue depth stays low (<1 second)\n",
    "\n",
    "2. **Worker Autoscaling Failure** (most common):\n",
    "   - Internal queue: 80-95%\n",
    "   - Workers stay at 1 despite configuration\n",
    "   - Queue depth grows to 10+ seconds\n",
    "   - **Fix**: See recommended fixes below\n",
    "\n",
    "3. **Endpoint Bottleneck**:\n",
    "   - Internal queue: 30-50%\n",
    "   - Replicas stay at min\n",
    "   - Endpoint CPU at 100%\n",
    "   - **Fix**: Increase max replicas or machine type\n",
    "\n",
    "**If You See Workers NOT Scaling:**\n",
    "\n",
    "The autoscaling algorithm may not be triggering because:\n",
    "1. Pub/Sub backlog stays small (workers pull fast, process slow)\n",
    "2. System lag metric doesn't capture internal worker queuing\n",
    "3. Wrong autoscaling algorithm for the workload\n",
    "\n",
    "**Recommended Fixes:**\n",
    "\n",
    "```python\n",
    "# In pipeline options, add:\n",
    "\"--autoscaling_algorithm=THROUGHPUT_BASED\",  # Scale based on throughput, not backlog\n",
    "\"--target_worker_utilization=0.7\",            # Scale at 70% capacity (more aggressive)\n",
    "\"--num_workers=2\",                             # Ensure proper initial worker count\n",
    "```\n",
    "\n",
    "Additionally in the pipeline code:\n",
    "```python\n",
    "# Reduce buffering to expose backpressure faster\n",
    "| \"Batch elements\" >> beam.BatchElements(\n",
    "    min_batch_size=1,\n",
    "    max_batch_size=50,\n",
    "    max_batch_duration_secs=0.001  # 1ms instead of 10ms\n",
    ")\n",
    "```\n",
    "\n",
    "**Understanding the Metrics:**\n",
    "\n",
    "- **Window wait time**: Time from publish to window_end timestamp (metadata only, not actual waiting)\n",
    "- **Internal queue time**: Time from window_end to pipeline output (REAL delay - workers saturated)\n",
    "- **P95 queue depth**: 95th percentile worst-case delay (most sensitive metric)\n",
    "\n",
    "The key insight: Window configuration is almost NEVER the bottleneck. The real issue is always worker saturation or endpoint throttling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create queuing diagnostic\n",
    "from scale_testing_dataflow_utils import create_queuing_diagnostic, print_queuing_summary\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"QUEUING DIAGNOSTIC\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create visualization\n",
    "fig_diagnostic = create_queuing_diagnostic(\n",
    "    latency_df=phase2b_latencies[0],\n",
    "    worker_metrics=phase2b_metrics['workers'],\n",
    "    test_name=\"Phase 2B: Stress Vertex Endpoint\"\n",
    ")\n",
    "\n",
    "fig_diagnostic.show(renderer=\"png\", width=1400, height=1200)\n",
    "\n",
    "# Print summary\n",
    "print_queuing_summary(\n",
    "    latency_df=phase2b_latencies[0],\n",
    "    worker_metrics=phase2b_metrics['workers']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phase3_header",
   "metadata": {},
   "source": [
    "---\n",
    "## Phase 3: Ramp Test\n",
    "\n",
    "**Goal**: Find exact autoscaling trigger point with gradual load increase\n",
    "\n",
    "**Test**: Single ramp test (~15 minutes)\n",
    "- Gradually increase from 0 ‚Üí 500 msg/sec over 15 minutes\n",
    "- Linear ramp to simulate traffic growth\n",
    "\n",
    "**What We're Looking For**:\n",
    "- At what exact message rate does autoscaling trigger?\n",
    "- How does the pipeline respond to gradual load increase?\n",
    "- Is there a \"sweet spot\" for sustained throughput?\n",
    "- When does latency start degrading?\n",
    "\n",
    "‚è≥ **This phase will take approximately 15 minutes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "phase3_init",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Phase 3 storage\n",
    "phase3_results = []\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"PHASE 3: RAMP TEST\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Ramping from 0 ‚Üí {RAMP_TEST['target_rate']} msg/sec over {RAMP_TEST['duration']//60} minutes\")\n",
    "print(f\"Total estimated time: ~{RAMP_TEST['duration']//60} minutes\")\n",
    "print(\"\\nThis test gradually increases load to find autoscaling threshold.\")\n",
    "print(\"=\"*70 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34dc188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run ramp test\n",
    "ramp_result = await load_generator.run_load_test(\n",
    "    pattern=\"ramp\",\n",
    "    target_rate=RAMP_TEST[\"target_rate\"],\n",
    "    duration=RAMP_TEST[\"duration\"],\n",
    "    test_name=RAMP_TEST[\"name\"]\n",
    ")\n",
    "phase3_results.append(ramp_result)\n",
    "\n",
    "print(f\"\\n‚úÖ Phase 3 complete: Ramp test finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phase3_metrics_header",
   "metadata": {},
   "source": [
    "### Collect Metrics for Phase 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "phase3_metrics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect metrics for ramp test\n",
    "phase3_start = ramp_result['start_time']\n",
    "phase3_end = ramp_result['end_time']\n",
    "\n",
    "print(f\"Collecting metrics for Phase 3\")\n",
    "print(f\"Time window: {phase3_start.strftime('%H:%M:%S')} ‚Üí {phase3_end.strftime('%H:%M:%S')}\")\n",
    "\n",
    "phase3_metrics = metrics_collector.collect_metrics(\n",
    "    start_time=phase3_start,\n",
    "    end_time=phase3_end,\n",
    "    resolution_seconds=10\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Metrics collected for Phase 3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "phase3_latency",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect end-to-end latency for ramp test\n",
    "phase3_latency = metrics_collector.collect_end_to_end_latency(\n",
    "    test_id=ramp_result['test_id'],\n",
    "    expected_messages=ramp_result['num_messages'],\n",
    "    timeout=300\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Latency data collected: {len(phase3_latency):,} messages\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phase3_analysis_header",
   "metadata": {},
   "source": [
    "### Analyze Phase 3 Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "phase3_analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze ramp test to find autoscaling trigger point\n",
    "print(\"=\"*70)\n",
    "print(\"PHASE 3 ANALYSIS: AUTOSCALING TRIGGER POINT\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\n{RAMP_TEST['name']}:\")\n",
    "print(f\"  Duration: {ramp_result['elapsed_seconds']:.0f}s ({ramp_result['elapsed_seconds']//60:.0f} mins)\")\n",
    "print(f\"  Messages sent: {ramp_result['num_messages']:,}\")\n",
    "print(f\"  Average rate: {ramp_result['actual_rate']:.1f} msg/sec\")\n",
    "print(f\"  Peak rate: {RAMP_TEST['target_rate']} msg/sec\")\n",
    "\n",
    "if len(phase3_latency) > 0:\n",
    "    print(f\"\\n  Latency Statistics:\")\n",
    "    print(f\"    Mean E2E: {phase3_latency['pipeline_latency_ms'].mean():,.1f}ms\")\n",
    "    print(f\"    P50 E2E:  {phase3_latency['pipeline_latency_ms'].quantile(0.50):,.1f}ms\")\n",
    "    print(f\"    P95 E2E:  {phase3_latency['pipeline_latency_ms'].quantile(0.95):,.1f}ms\")\n",
    "    print(f\"    P99 E2E:  {phase3_latency['pipeline_latency_ms'].quantile(0.99):,.1f}ms\")\n",
    "    \n",
    "    # Analyze latency trend over time (early vs late)\n",
    "    phase3_latency_sorted = phase3_latency.sort_values('publish_time')\n",
    "    \n",
    "    # Split into quartiles to see latency progression\n",
    "    quartile_size = len(phase3_latency_sorted) // 4\n",
    "    q1 = phase3_latency_sorted.iloc[:quartile_size]\n",
    "    q2 = phase3_latency_sorted.iloc[quartile_size:2*quartile_size]\n",
    "    q3 = phase3_latency_sorted.iloc[2*quartile_size:3*quartile_size]\n",
    "    q4 = phase3_latency_sorted.iloc[3*quartile_size:]\n",
    "    \n",
    "    print(f\"\\n  Latency Progression (as rate increases):\")\n",
    "    print(f\"    Q1 (0-25%):   {q1['pipeline_latency_ms'].mean():,.1f}ms mean\")\n",
    "    print(f\"    Q2 (25-50%):  {q2['pipeline_latency_ms'].mean():,.1f}ms mean\")\n",
    "    print(f\"    Q3 (50-75%):  {q3['pipeline_latency_ms'].mean():,.1f}ms mean\")\n",
    "    print(f\"    Q4 (75-100%): {q4['pipeline_latency_ms'].mean():,.1f}ms mean\")\n",
    "    \n",
    "    # Identify when latency starts degrading significantly\n",
    "    q1_mean = q1['pipeline_latency_ms'].mean()\n",
    "    if q4['pipeline_latency_ms'].mean() > q1_mean * 1.5:\n",
    "        print(f\"\\n    ‚ö†Ô∏è  Latency degraded significantly in later quartiles\")\n",
    "        print(f\"       Suggests capacity limits reached at higher rates\")\n",
    "    else:\n",
    "        print(f\"\\n    ‚úÖ Latency remained stable throughout ramp\")\n",
    "\n",
    "# Analyze autoscaling during ramp\n",
    "print(f\"\\n\" + \"=\"*70)\n",
    "print(\"AUTOSCALING BEHAVIOR DURING RAMP\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "autoscaling_events = metrics_collector.analyze_autoscaling(phase3_metrics)\n",
    "\n",
    "if len(autoscaling_events) > 0:\n",
    "    print(f\"\\n‚úÖ Autoscaling triggered {len(autoscaling_events)} time(s) during ramp\\n\")\n",
    "    \n",
    "    for idx, event in autoscaling_events.iterrows():\n",
    "        # Calculate approximate message rate at trigger time\n",
    "        trigger_time = event['trigger_time']\n",
    "        # Use .value to get nanoseconds since epoch, works regardless of timezone\n",
    "        elapsed_at_trigger_ns = trigger_time.value - pd.Timestamp(phase3_start).value\n",
    "        elapsed_at_trigger = elapsed_at_trigger_ns / 1e9  # Convert nanoseconds to seconds\n",
    "        rate_at_trigger = (elapsed_at_trigger / RAMP_TEST['duration']) * RAMP_TEST['target_rate']\n",
    "        \n",
    "        print(f\"Event {idx + 1}:\")\n",
    "        print(f\"  Trigger time:       {event['trigger_time'].strftime('%H:%M:%S')}\")\n",
    "        print(f\"  Elapsed:            {elapsed_at_trigger:.0f}s into ramp\")\n",
    "        print(f\"  Approx rate:        {rate_at_trigger:.0f} msg/sec\")\n",
    "        print(f\"  Workers:            {event['workers_before']:.0f} ‚Üí {event['workers_after']:.0f}\")\n",
    "        print(f\"  Provisioning time:  {event['scale_up_lag_seconds']:.0f}s\")\n",
    "        print()\n",
    "    \n",
    "    # Identify threshold\n",
    "    first_trigger = autoscaling_events.iloc[0]\n",
    "    first_trigger_time = first_trigger['trigger_time']\n",
    "    # Use .value to get nanoseconds since epoch, works regardless of timezone\n",
    "    first_trigger_elapsed_ns = first_trigger_time.value - pd.Timestamp(phase3_start).value\n",
    "    first_trigger_elapsed = first_trigger_elapsed_ns / 1e9  # Convert nanoseconds to seconds\n",
    "    first_trigger_rate = (first_trigger_elapsed / RAMP_TEST['duration']) * RAMP_TEST['target_rate']\n",
    "    \n",
    "    print(f\"üìä Key Finding:\")\n",
    "    print(f\"   Autoscaling triggered at approximately {first_trigger_rate:.0f} msg/sec\")\n",
    "    print(f\"   This is the throughput threshold for current configuration\")\n",
    "else:\n",
    "    print(\"\\n‚ÑπÔ∏è  No autoscaling triggered during ramp\")\n",
    "    print(f\"   Current workers handled up to {RAMP_TEST['target_rate']} msg/sec\")\n",
    "    print(\"   Pipeline has significant headroom at this configuration\")\n",
    "\n",
    "# Analyze backlog behavior during ramp\n",
    "if 'backlog' in phase3_metrics and len(phase3_metrics['backlog']) > 0:\n",
    "    print(f\"\\n\" + \"=\"*70)\n",
    "    print(\"BACKLOG BEHAVIOR DURING RAMP\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    backlog_df = phase3_metrics['backlog'].sort_values('timestamp')\n",
    "    max_backlog = backlog_df['value'].max()\n",
    "    \n",
    "    print(f\"\\nBacklog Statistics:\")\n",
    "    print(f\"  Max backlog: {max_backlog:,.0f} messages\")\n",
    "    print(f\"  Mean backlog: {backlog_df['value'].mean():,.0f} messages\")\n",
    "    \n",
    "    # Find when backlog started building\n",
    "    backlog_threshold = 100  # Consider backlog \"building\" at 100+ messages\n",
    "    backlog_building = backlog_df[backlog_df['value'] > backlog_threshold]\n",
    "    \n",
    "    if len(backlog_building) > 0:\n",
    "        first_backlog_time = backlog_building.iloc[0]['timestamp']\n",
    "        # Use .value to get nanoseconds since epoch, works regardless of timezone\n",
    "        elapsed_at_backlog_ns = first_backlog_time.value - pd.Timestamp(phase3_start).value\n",
    "        elapsed_at_backlog = elapsed_at_backlog_ns / 1e9  # Convert nanoseconds to seconds\n",
    "        rate_at_backlog = (elapsed_at_backlog / RAMP_TEST['duration']) * RAMP_TEST['target_rate']\n",
    "        \n",
    "        print(f\"\\n  Backlog started building:\")\n",
    "        print(f\"    Time: {first_backlog_time.strftime('%H:%M:%S')}\")\n",
    "        print(f\"    Elapsed: {elapsed_at_backlog:.0f}s into ramp\")\n",
    "        print(f\"    Approx rate: {rate_at_backlog:.0f} msg/sec\")\n",
    "        print(f\"\\n    üí° This indicates pipeline capacity threshold\")\n",
    "    else:\n",
    "        print(f\"\\n  ‚úÖ Backlog remained under control throughout ramp\")\n",
    "\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db57294",
   "metadata": {},
   "source": [
    "### üìä How to Read the Visualization\n",
    "\n",
    "The timeline visualization below shows 5 panels that track different aspects of pipeline performance. Here's what each panel tells you:\n",
    "\n",
    "#### **Panel 1: Incoming Message Rate**\n",
    "- **What it shows:** Messages published per second to the input Pub/Sub topic\n",
    "- **What to look for:**\n",
    "  - Flat line = steady rate (good for controlled testing)\n",
    "  - Spikes = burst traffic\n",
    "  - Gradual increase = ramp test pattern\n",
    "- **Interpretation:** This is your **input load** - what you're sending to the pipeline\n",
    "\n",
    "#### **Panel 2: Worker Count (Autoscaling)**\n",
    "- **What it shows:** Number of active Dataflow workers over time\n",
    "- **What to look for:**\n",
    "  - Flat line = no autoscaling needed\n",
    "  - Step up = autoscaling triggered (workers being added)\n",
    "  - Step down = scale-down (workers being removed after low load period)\n",
    "- **Interpretation:** Shows when pipeline capacity changes. Worker provisioning takes **2-4 minutes**, so you'll see a delay between load increase and worker scale-up.\n",
    "\n",
    "#### **Panel 3: System Lag (Processing Delay: Oldest To Current Message)**\n",
    "- **What it shows:** How far behind real-time the pipeline is processing (in milliseconds)\n",
    "- **What to look for:**\n",
    "  - Near-zero lag = pipeline keeping up with real-time\n",
    "  - Increasing lag = pipeline falling behind\n",
    "  - High lag (>60s) = serious capacity issues\n",
    "- **Interpretation:** This is your **real-time health indicator**. Rising lag means you need more workers or better machine type.\n",
    "\n",
    "#### **Panel 4: Output Subscription Queue**\n",
    "- **What it shows:** Number of messages waiting in the output Pub/Sub subscription\n",
    "- **What to look for:**\n",
    "  - Zero or low = messages being consumed quickly\n",
    "  - Linearly increasing = messages accumulating (expected during testing)\n",
    "  - Very high (>10,000) = potential capacity issues\n",
    "- **Interpretation:** This backlog includes **both** pipeline processing queue AND messages waiting for the test framework to pull them. During tests, this will grow because we're not consuming as fast as we're producing.\n",
    "\n",
    "#### **Panel 5: P95 Pipeline Latency (publish ‚Üí output queue, excludes queue wait)**\n",
    "- **What it shows:** 95th percentile latency from message publish to arriving in output queue\n",
    "- **What to look for:**\n",
    "  - Steady line = consistent performance\n",
    "  - Gradual increase = latency degrading under load\n",
    "  - Spikes at test end = shutdown artifacts (normal)\n",
    "- **Interpretation:** This is your **actual pipeline performance metric** - the one you care about most. It excludes the time messages sit in the queue waiting for the test to pull them.\n",
    "- **Y-axis note:** The axis is auto-scaled to focus on steady-state latency using the 99th percentile. Very high spikes (like test shutdown) may be clipped but won't distort the main trend.\n",
    "\n",
    "---\n",
    "\n",
    "**üí° Key Insights to Extract:**\n",
    "1. **When does autoscaling trigger?** ‚Üí Look for worker count step-up in Panel 2\n",
    "2. **What's the latency at different loads?** ‚Üí Compare Panel 5 across different message rates in Panel 1\n",
    "3. **Is the pipeline keeping up?** ‚Üí Panel 3 system lag should stay near zero\n",
    "4. **Where's the bottleneck?** ‚Üí If latency is high but system lag is low, it's windowing. If system lag is high, it's processing capacity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "phase3_viz",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize ramp test\n",
    "fig = plot_dataflow_timeline(\n",
    "    test_results=ramp_result,\n",
    "    metrics=phase3_metrics,\n",
    "    latency_df=phase3_latency,\n",
    "    test_name=\"Phase 3: Ramp Test - Finding Autoscaling Threshold\"\n",
    ")\n",
    "\n",
    "fig.show(renderer=\"png\", width=1400, height=1000)\n",
    "\n",
    "print(\"\\nüí° This visualization shows:\")\n",
    "print(f\"   - Gradual message rate increase: 0 ‚Üí {RAMP_TEST['target_rate']} msg/sec\")\n",
    "print(f\"   - Exact moment when workers scaled up\")\n",
    "print(f\"   - System lag progression as load increased\")\n",
    "print(f\"   - Backlog buildup and recovery\")\n",
    "print(f\"   - Latency trends throughout the ramp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç Queuing Diagnostic: Where Does the Latency Come From?\n",
    "\n",
    "The visualization above shows WHAT happened. Now let's understand WHY.\n",
    "\n",
    "This diagnostic reveals:\n",
    "- **Latency Attribution**: What % is window assignment vs internal queuing\n",
    "- **Queue Growth**: How queue depth evolves over time\n",
    "- **Autoscaling Response**: Did workers/replicas scale in response to queuing?\n",
    "\n",
    "**How to Read the Diagnostic:**\n",
    "\n",
    "**Panel 1 (Pie Chart)**: Latency Attribution\n",
    "- üîµ **Blue = Window Assignment** (just metadata timestamps, 0-1 second)\n",
    "- üî¥ **Red = Internal Queuing** (ACTUAL bottleneck - messages waiting inside workers)\n",
    "- ‚úÖ Good: Red < 20% (window config is appropriate)\n",
    "- ‚ö†Ô∏è  Concern: Red > 50% (workers can't keep up)\n",
    "- ‚ùå Problem: Red > 80% (severe worker saturation)\n",
    "\n",
    "**Panel 2 (Stacked Area)**: Components Over Time\n",
    "- Shows how latency components change as load increases\n",
    "- Watch for red area growing ‚Üí queue building up\n",
    "\n",
    "**Panel 3 (Heatmap)**: Queue Depth Over Time\n",
    "- Dark red = Messages severely delayed\n",
    "- Shows WHEN queuing starts (correlate with load changes)\n",
    "\n",
    "**Panel 4 (Autoscaling)**: THE SMOKING GUN\n",
    "- üîµ Blue line = Worker count (Dataflow)\n",
    "- üü¢ Green dashed = Replica count (Vertex Endpoint)\n",
    "- üî¥ Red line = Queue depth\n",
    "- **Key insight**: If queue grows but workers stay flat ‚Üí autoscaling failed\n",
    "\n",
    "**Common Patterns:**\n",
    "\n",
    "1. **Healthy System**:\n",
    "   - Window assignment: 5-15%\n",
    "   - Internal queue: <20%\n",
    "   - Workers scale up as queue grows\n",
    "   - Queue depth stays low (<1 second)\n",
    "\n",
    "2. **Worker Autoscaling Failure** (most common):\n",
    "   - Internal queue: 80-95%\n",
    "   - Workers stay at 1 despite configuration\n",
    "   - Queue depth grows to 10+ seconds\n",
    "   - **Fix**: See recommended fixes below\n",
    "\n",
    "3. **Endpoint Bottleneck**:\n",
    "   - Internal queue: 30-50%\n",
    "   - Replicas stay at min\n",
    "   - Endpoint CPU at 100%\n",
    "   - **Fix**: Increase max replicas or machine type\n",
    "\n",
    "**If You See Workers NOT Scaling:**\n",
    "\n",
    "The autoscaling algorithm may not be triggering because:\n",
    "1. Pub/Sub backlog stays small (workers pull fast, process slow)\n",
    "2. System lag metric doesn't capture internal worker queuing\n",
    "3. Wrong autoscaling algorithm for the workload\n",
    "\n",
    "**Recommended Fixes:**\n",
    "\n",
    "```python\n",
    "# In pipeline options, add:\n",
    "\"--autoscaling_algorithm=THROUGHPUT_BASED\",  # Scale based on throughput, not backlog\n",
    "\"--target_worker_utilization=0.7\",            # Scale at 70% capacity (more aggressive)\n",
    "\"--num_workers=2\",                             # Ensure proper initial worker count\n",
    "```\n",
    "\n",
    "Additionally in the pipeline code:\n",
    "```python\n",
    "# Reduce buffering to expose backpressure faster\n",
    "| \"Batch elements\" >> beam.BatchElements(\n",
    "    min_batch_size=1,\n",
    "    max_batch_size=50,\n",
    "    max_batch_duration_secs=0.001  # 1ms instead of 10ms\n",
    ")\n",
    "```\n",
    "\n",
    "**Understanding the Metrics:**\n",
    "\n",
    "- **Window wait time**: Time from publish to window_end timestamp (metadata only, not actual waiting)\n",
    "- **Internal queue time**: Time from window_end to pipeline output (REAL delay - workers saturated)\n",
    "- **P95 queue depth**: 95th percentile worst-case delay (most sensitive metric)\n",
    "\n",
    "The key insight: Window configuration is almost NEVER the bottleneck. The real issue is always worker saturation or endpoint throttling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create queuing diagnostic\n",
    "from scale_testing_dataflow_utils import create_queuing_diagnostic, print_queuing_summary\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"QUEUING DIAGNOSTIC\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create visualization\n",
    "fig_diagnostic = create_queuing_diagnostic(\n",
    "    latency_df=phase3_latency,\n",
    "    worker_metrics=phase3_metrics['workers'],\n",
    "    test_name=\"Phase 3: Ramp Test - Finding Autoscaling Threshold\"\n",
    ")\n",
    "\n",
    "fig_diagnostic.show(renderer=\"png\", width=1400, height=1200)\n",
    "\n",
    "# Print summary\n",
    "print_queuing_summary(\n",
    "    latency_df=phase3_latency,\n",
    "    worker_metrics=phase3_metrics['workers']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary_header",
   "metadata": {},
   "source": [
    "---\n",
    "## Comprehensive Test Summary & Production Recommendations\n",
    "\n",
    "Based on all test phases, generate production recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comprehensive_summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive summary\n",
    "print(\"=\"*80)\n",
    "print(\"COMPREHENSIVE TEST SUMMARY & PRODUCTION RECOMMENDATIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Job configuration\n",
    "print(f\"\\nüìã Dataflow Job Configuration:\")\n",
    "print(f\"   Job ID:         {JOB_ID}\")\n",
    "print(f\"   Region:         {REGION}\")\n",
    "print(f\"   Machine Type:   n1-standard-4 (4 vCPUs, 15 GB memory)\")\n",
    "print(f\"   Window Size:    60 seconds\")\n",
    "\n",
    "# Phase 1 summary\n",
    "print(f\"\\nüìä Phase 1: Baseline Performance\")\n",
    "if len(phase1_latencies) > 0 and len(phase1_latencies[0]) > 0:\n",
    "    baseline_latency = phase1_latencies[0]['pipeline_latency_ms'].quantile(0.95)\n",
    "    print(f\"   Baseline P95 latency: {baseline_latency:,.1f}ms at {BASELINE_TESTS[0]['target_rate']} msg/sec\")\n",
    "else:\n",
    "    print(f\"   Tests completed but latency data unavailable\")\n",
    "\n",
    "# Phase 2A summary\n",
    "print(f\"\\nüìä Phase 2A: Throughput Threshold Hunt\")\n",
    "phase2a_autoscaling = metrics_collector.analyze_autoscaling(phase2a_metrics)\n",
    "if len(phase2a_autoscaling) > 0:\n",
    "    print(f\"   ‚úÖ Autoscaling triggered {len(phase2a_autoscaling)} time(s)\")\n",
    "    print(f\"   Average provisioning time: {phase2a_autoscaling['scale_up_lag_seconds'].mean():.0f}s\")\n",
    "else:\n",
    "    print(f\"   No autoscaling at tested rates (50-200 msg/sec)\")\n",
    "\n",
    "if len(phase2a_latencies) > 0:\n",
    "    # Compare latency degradation across sustained tests\n",
    "    latencies_50 = phase2a_latencies[0]['pipeline_latency_ms'].quantile(0.95) if len(phase2a_latencies[0]) > 0 else None\n",
    "    latencies_200 = phase2a_latencies[-1]['pipeline_latency_ms'].quantile(0.95) if len(phase2a_latencies[-1]) > 0 else None\n",
    "    \n",
    "    if latencies_50 and latencies_200:\n",
    "        degradation = ((latencies_200 - latencies_50) / latencies_50) * 100\n",
    "        print(f\"   Latency at 50 msg/sec:  {latencies_50:,.1f}ms (P95)\")\n",
    "        print(f\"   Latency at 200 msg/sec: {latencies_200:,.1f}ms (P95)\")\n",
    "        print(f\"   Degradation: {degradation:+.1f}%\")\n",
    "\n",
    "# Phase 2B summary\n",
    "print(f\"\\nüìä Phase 2B: Burst Capacity\")\n",
    "if len(phase2b_results) > 0:\n",
    "    largest_burst = BURST_TESTS[-1]\n",
    "    print(f\"   Largest burst: {largest_burst['num_messages']:,} messages\")\n",
    "    if len(phase2b_latencies[-1]) > 0:\n",
    "        burst_p95 = phase2b_latencies[-1]['pipeline_latency_ms'].quantile(0.95)\n",
    "        print(f\"   P95 latency during burst: {burst_p95:,.1f}ms\")\n",
    "\n",
    "# Phase 3 summary\n",
    "print(f\"\\nüìä Phase 3: Ramp Test\")\n",
    "phase3_autoscaling = metrics_collector.analyze_autoscaling(phase3_metrics)\n",
    "if len(phase3_autoscaling) > 0:\n",
    "    first_trigger = phase3_autoscaling.iloc[0]\n",
    "    elapsed = (first_trigger['trigger_time'] - phase3_start).total_seconds()\n",
    "    trigger_rate = (elapsed / RAMP_TEST['duration']) * RAMP_TEST['target_rate']\n",
    "    print(f\"   ‚úÖ Autoscaling triggered at ~{trigger_rate:.0f} msg/sec\")\n",
    "    print(f\"   This is the capacity threshold\")\n",
    "else:\n",
    "    print(f\"   No autoscaling triggered up to {RAMP_TEST['target_rate']} msg/sec\")\n",
    "    print(f\"   Pipeline has significant headroom\")\n",
    "\n",
    "if len(phase3_latency) > 0:\n",
    "    ramp_p95 = phase3_latency['pipeline_latency_ms'].quantile(0.95)\n",
    "    print(f\"   Overall P95 latency: {ramp_p95:,.1f}ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "production_recommendations",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Production recommendations\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PRODUCTION RECOMMENDATIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Determine recommended throughput\n",
    "print(\"\\nüéØ Recommended Throughput Configurations:\")\n",
    "\n",
    "# Try to determine capacity from autoscaling events\n",
    "all_autoscaling = pd.concat([\n",
    "    metrics_collector.analyze_autoscaling(phase1_metrics),\n",
    "    metrics_collector.analyze_autoscaling(phase2a_metrics),\n",
    "    metrics_collector.analyze_autoscaling(phase2b_metrics),\n",
    "    metrics_collector.analyze_autoscaling(phase3_metrics)\n",
    "], ignore_index=True)\n",
    "\n",
    "if len(all_autoscaling) > 0:\n",
    "    # Use first autoscaling event to estimate capacity\n",
    "    print(\"\\n   Based on autoscaling behavior:\")\n",
    "    print(\"   ‚Ä¢ Conservative (70% capacity): Suitable for production with headroom\")\n",
    "    print(\"   ‚Ä¢ Balanced (85% capacity): Good for predictable workloads\")\n",
    "    print(\"   ‚Ä¢ Aggressive (95% capacity): Maximum throughput, relies on autoscaling\")\n",
    "else:\n",
    "    print(\"\\n   Based on test results (no autoscaling triggered):\")\n",
    "    print(f\"   ‚Ä¢ Current configuration handled {SUSTAINED_TESTS[-1]['target_rate']} msg/sec comfortably\")\n",
    "    print(\"   ‚Ä¢ Significant headroom available\")\n",
    "    print(\"   ‚Ä¢ Consider testing higher rates to find true limits\")\n",
    "\n",
    "# Worker configuration\n",
    "print(\"\\n‚öôÔ∏è  Worker Configuration Recommendations:\")\n",
    "\n",
    "if len(all_autoscaling) > 0:\n",
    "    avg_provisioning = all_autoscaling['scale_up_lag_seconds'].mean()\n",
    "    print(f\"\\n   Autoscaling Performance:\")\n",
    "    print(f\"   ‚Ä¢ Average worker provisioning: {avg_provisioning:.0f}s ({avg_provisioning/60:.1f} mins)\")\n",
    "    print(f\"   ‚Ä¢ Triggered {len(all_autoscaling)} time(s) across all tests\")\n",
    "    \n",
    "    if avg_provisioning > 300:  # 5 minutes\n",
    "        print(f\"\\n   ‚ö†Ô∏è  Slow provisioning time detected\")\n",
    "        print(f\"   ‚Ä¢ Consider increasing MIN_WORKERS for faster response\")\n",
    "        print(f\"   ‚Ä¢ Pre-warm capacity reduces latency spikes\")\n",
    "    else:\n",
    "        print(f\"\\n   ‚úÖ Autoscaling performance acceptable\")\n",
    "else:\n",
    "    print(f\"\\n   Current MIN_WORKERS=2, MAX_WORKERS=20\")\n",
    "    print(f\"   ‚Ä¢ No autoscaling needed at tested rates\")\n",
    "    print(f\"   ‚Ä¢ Current configuration appropriate\")\n",
    "\n",
    "# Machine type recommendations\n",
    "print(\"\\nüíª Machine Type Recommendations:\")\n",
    "print(f\"\\n   Current: n1-standard-4 (4 vCPUs, 15 GB memory)\")\n",
    "\n",
    "# Check if we have system lag data to determine if CPU-bound\n",
    "all_system_lag = []\n",
    "for metrics in [phase1_metrics, phase2a_metrics, phase2b_metrics, phase3_metrics]:\n",
    "    if 'system_lag' in metrics and len(metrics['system_lag']) > 0:\n",
    "        all_system_lag.extend(metrics['system_lag']['value'].tolist())\n",
    "\n",
    "if all_system_lag:\n",
    "    max_lag_ms = max(all_system_lag) / 1000  # Convert to ms\n",
    "    \n",
    "    if max_lag_ms > 60000:  # 1 minute\n",
    "        print(f\"\\n   ‚ö†Ô∏è  High system lag detected ({max_lag_ms/1000:.1f}s max)\")\n",
    "        print(f\"   ‚Ä¢ Consider c2-standard-4 for more CPU power\")\n",
    "        print(f\"   ‚Ä¢ Or increase worker count instead\")\n",
    "    else:\n",
    "        print(f\"\\n   ‚úÖ Current machine type performing well\")\n",
    "        print(f\"   ‚Ä¢ System lag remained under control (max: {max_lag_ms/1000:.1f}s)\")\n",
    "\n",
    "print(f\"\\n   When to consider alternatives:\")\n",
    "print(f\"   ‚Ä¢ CPU-intensive model ‚Üí c2-standard-4 (compute-optimized)\")\n",
    "print(f\"   ‚Ä¢ Large model size ‚Üí n1-highmem-4 (more memory)\")\n",
    "print(f\"   ‚Ä¢ GPU-compatible model ‚Üí Add --worker_gpu_type=nvidia-tesla-t4\")\n",
    "\n",
    "# Latency optimization\n",
    "print(\"\\n‚è±Ô∏è  Latency Optimization:\")\n",
    "\n",
    "if len(phase1_latencies) > 0 and len(phase1_latencies[0]) > 0:\n",
    "    baseline = phase1_latencies[0]\n",
    "    baseline_p50 = baseline['pipeline_latency_ms'].quantile(0.50)\n",
    "    baseline_p95 = baseline['pipeline_latency_ms'].quantile(0.95)\n",
    "    baseline_p99 = baseline['pipeline_latency_ms'].quantile(0.99)\n",
    "    \n",
    "    print(f\"\\n   Pipeline Latency (baseline):\")\n",
    "    print(f\"   ‚Ä¢ P50: {baseline_p50:.1f}ms\")\n",
    "    print(f\"   ‚Ä¢ P95: {baseline_p95:.1f}ms\")\n",
    "    print(f\"   ‚Ä¢ P99: {baseline_p99:.1f}ms\")\n",
    "    print(f\"\\n   Note: Pipeline latency includes all processing (inference + transforms + windowing)\")\n",
    "    \n",
    "    if baseline_p95 > 5000:  # >5 seconds\n",
    "        print(f\"\\n   ‚ö†Ô∏è  High latency detected\")\n",
    "        print(f\"   ‚Ä¢ Consider model optimization\")\n",
    "        print(f\"   ‚Ä¢ Or use more powerful machine type (c2-standard-4)\")\n",
    "        print(f\"   ‚Ä¢ Or GPU acceleration for large models\")\n",
    "    elif baseline_p95 > 1000:  # >1 second\n",
    "        print(f\"\\n   ‚ÑπÔ∏è  Moderate latency\")\n",
    "        print(f\"   ‚Ä¢ May benefit from model optimization or faster machine type\")\n",
    "    else:\n",
    "        print(f\"\\n   ‚úÖ Low latency - pipeline performing well\")\n",
    "\n",
    "# Cost optimization\n",
    "print(\"\\nüí∞ Cost Optimization Tips:\")\n",
    "print(f\"   ‚Ä¢ Monitor actual traffic patterns and adjust MIN/MAX workers\")\n",
    "print(f\"   ‚Ä¢ Use batch processing for historical analysis (cheaper)\")\n",
    "print(f\"   ‚Ä¢ Set up alerts for unexpected worker scaling\")\n",
    "print(f\"   ‚Ä¢ Consider spot instances for non-critical workloads (when available)\")\n",
    "print(f\"   ‚Ä¢ Drain and stop job when not actively processing (restart when needed)\")\n",
    "\n",
    "# Monitoring\n",
    "print(\"\\nüìä Monitoring & Alerts:\")\n",
    "print(f\"   Set up Cloud Monitoring alerts for:\")\n",
    "print(f\"   ‚Ä¢ System lag > 60 seconds (pipeline falling behind)\")\n",
    "print(f\"   ‚Ä¢ Backlog > 10,000 messages (capacity issues)\")\n",
    "print(f\"   ‚Ä¢ Worker count at MAX_WORKERS (may need to increase limit)\")\n",
    "print(f\"   ‚Ä¢ Element count drops to 0 (pipeline stalled)\")\n",
    "\n",
    "# Next steps\n",
    "print(\"\\nüìù Next Steps:\")\n",
    "print(f\"   1. Review Dataflow job metrics in Cloud Console\")\n",
    "print(f\"      https://console.cloud.google.com/dataflow/jobs/{REGION}/{JOB_ID}?project={PROJECT_ID}\")\n",
    "print(f\"   2. If autoscaling threshold needs adjustment:\")\n",
    "print(f\"      ‚Ä¢ Modify MIN_WORKERS or MAX_WORKERS in deployment\")\n",
    "print(f\"      ‚Ä¢ Re-run this notebook to validate changes\")\n",
    "print(f\"   3. Implement recommended monitoring alerts\")\n",
    "print(f\"   4. Set up dashboard for real-time visibility\")\n",
    "print(f\"   5. Document findings for future capacity planning\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion_header",
   "metadata": {},
   "source": [
    "---\n",
    "## Conclusion\n",
    "\n",
    "This notebook provides a comprehensive, scientific approach to understanding Dataflow Streaming performance through systematic testing across three dimensions:\n",
    "\n",
    "**1. Message Rate** - Baseline, sustained, and peak throughput testing  \n",
    "**2. Load Patterns** - Burst, sustained, and ramping traffic patterns  \n",
    "**3. Latency Characteristics** - End-to-end breakdown and bottleneck identification\n",
    "\n",
    "### Key Insights from This Testing Framework\n",
    "\n",
    "**Understanding Dataflow Autoscaling:**\n",
    "- Dataflow scales based on **Pub/Sub backlog** and **system lag** (not just CPU like Vertex AI)\n",
    "- Worker provisioning takes **2-4 minutes** (plan for this delay)\n",
    "- Sustained load (not bursts) typically triggers autoscaling\n",
    "- Ramp tests reveal exact throughput thresholds\n",
    "\n",
    "**Latency Composition:**\n",
    "- **Window Wait**: Time from publish to window close (up to window size)\n",
    "- **Processing**: Model inference + transforms (controllable via machine type)\n",
    "- **Pub/Sub Delivery**: Usually <100ms (network overhead)\n",
    "- Smaller windows = lower latency but less batching efficiency\n",
    "\n",
    "**Bottleneck Identification:**\n",
    "- High **system lag** ‚Üí Processing capacity issues (need more workers or better machine type)\n",
    "- High **backlog** ‚Üí Ingestion rate exceeds processing capacity (autoscaling needed)\n",
    "- High **window wait %** ‚Üí Latency dominated by batching (consider smaller windows)\n",
    "- High **processing %** ‚Üí Model inference slow (consider GPU or model optimization)\n",
    "\n",
    "### Important Notes\n",
    "\n",
    "**Results are Pipeline-Specific:**  \n",
    "All results in this notebook are specific to the tested pipeline configuration:\n",
    "- PyTorch autoencoder model with 30 features\n",
    "- n1-standard-4 workers (4 vCPUs, 15 GB memory)\n",
    "- 60-second fixed windows\n",
    "- MIN=2, MAX=20 workers\n",
    "\n",
    "Your results will vary based on:\n",
    "- Model complexity and inference time\n",
    "- Machine type and resources\n",
    "- Window size configuration\n",
    "- Message size and format\n",
    "\n",
    "**Always Test Your Own Pipeline:**  \n",
    "Before production deployment:\n",
    "1. Run this notebook with your pipeline and representative data\n",
    "2. Test with realistic traffic patterns\n",
    "3. Adjust worker configuration based on results\n",
    "4. Monitor production metrics continuously\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "**If This Notebook Revealed Issues:**\n",
    "1. **Autoscaling too slow** ‚Üí Increase MIN_WORKERS for pre-warmed capacity\n",
    "2. **High system lag** ‚Üí Use c2-standard-4 or increase MAX_WORKERS\n",
    "3. **High latency** ‚Üí Reduce window size or optimize model\n",
    "4. **Backlog building** ‚Üí Increase MAX_WORKERS or improve processing efficiency\n",
    "\n",
    "**Production Deployment Tasks:**\n",
    "- Set up Cloud Monitoring alerts (system lag, backlog, worker count)\n",
    "- Create dashboard for real-time visibility\n",
    "- Document capacity planning findings\n",
    "- Implement gradual rollout with monitoring\n",
    "\n",
    "**Related Notebooks:**\n",
    "- [Dataflow Streaming RunInference](./dataflow-streaming-runinference.ipynb) - Deploy the pipeline tested here\n",
    "- [Dataflow Batch RunInference](./dataflow-batch-runinference.ipynb) - Batch processing alternative\n",
    "- [Vertex AI Endpoint Scale Tests](./scale-tests-vertex-ai-endpoints.ipynb) - Compare with endpoint serving\n",
    "\n",
    "### Resources\n",
    "\n",
    "**Dataflow Documentation:**\n",
    "- [Streaming Pipelines](https://cloud.google.com/dataflow/docs/concepts/streaming-pipelines)\n",
    "- [Autoscaling](https://cloud.google.com/dataflow/docs/guides/deploying-a-pipeline#autoscaling)\n",
    "- [Monitoring](https://cloud.google.com/dataflow/docs/guides/using-cloud-monitoring)\n",
    "\n",
    "**Apache Beam:**\n",
    "- [RunInference](https://beam.apache.org/documentation/ml/about-ml/)\n",
    "- [Windowing](https://beam.apache.org/documentation/programming-guide/#windowing)\n",
    "- [PyTorch Handler](https://beam.apache.org/documentation/ml/pytorch-inference/)\n",
    "\n",
    "**Cloud Monitoring:**\n",
    "- [Dataflow Metrics](https://cloud.google.com/dataflow/docs/guides/using-cloud-monitoring#available_dataflow_metrics)\n",
    "- [Setting Up Alerts](https://cloud.google.com/monitoring/alerts)\n",
    "\n",
    "---\n",
    "\n",
    "**Testing Framework Created:** This comprehensive testing infrastructure (`scale_testing_dataflow_utils.py` + this notebook) can be reused for testing any Dataflow Streaming pipeline. Simply update the job ID and test parameters."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (frameworks-pytorch)",
   "language": "python",
   "name": "frameworks-pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
