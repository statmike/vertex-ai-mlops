{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![tracker](https://us-central1-vertex-ai-mlops-369716.cloudfunctions.net/pixel-tracking?path=statmike%2Fvertex-ai-mlops%2FFramework+Workflows%2FPyTorch%2Fserving&file=vertex-ai-endpoint-prebuilt-container.ipynb)\n",
        "<!--- header table --->\n",
        "<table align=\"left\">\n",
        "<tr>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%20Workflows/PyTorch/serving/vertex-ai-endpoint-prebuilt-container.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://www.svgrepo.com/download/217753/github.svg\" alt=\"GitHub logo\">\n",
        "      <br>View on<br>GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://colab.research.google.com/github/statmike/vertex-ai-mlops/blob/main/Framework%20Workflows/PyTorch/serving/vertex-ai-endpoint-prebuilt-container.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://www.gstatic.com/pantheon/images/bigquery/welcome_page/colab-logo.svg\" alt=\"Google Colaboratory logo\">\n",
        "      <br>Run in<br>Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https%3A%2F%2Fraw.githubusercontent.com%2Fstatmike%2Fvertex-ai-mlops%2Fmain%2FFramework%2520Workflows%2FPyTorch%2Fserving%2Fvertex-ai-endpoint-prebuilt-container.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\">\n",
        "      <br>Run in<br>Colab Enterprise\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/bigquery/import?url=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%20Workflows/PyTorch/serving/vertex-ai-endpoint-prebuilt-container.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://www.gstatic.com/images/branding/gcpiconscolors/bigquery/v1/32px.svg\" alt=\"BigQuery logo\">\n",
        "      <br>Open in<br>BigQuery Studio\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/statmike/vertex-ai-mlops/main/Framework%20Workflows/PyTorch/serving/vertex-ai-endpoint-prebuilt-container.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://www.gstatic.com/images/branding/gcpiconscolors/vertexai/v1/32px.svg\" alt=\"Vertex AI logo\">\n",
        "      <br>Open in<br>Vertex AI Workbench\n",
        "    </a>\n",
        "  </td>\n",
        "</tr>\n",
        "<tr>\n",
        "  <td colspan=\"5\" style=\"text-align: right\">\n",
        "    <b>Share This On: </b> \n",
        "    <a href=\"https://www.linkedin.com/sharing/share-offsite/?url=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%2520Workflows/PyTorch/serving/vertex-ai-endpoint-prebuilt-container.ipynb\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"Linkedin Logo\" width=\"20px\"></a> \n",
        "    <a href=\"https://reddit.com/submit?url=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%2520Workflows/PyTorch/serving/vertex-ai-endpoint-prebuilt-container.ipynb\"><img src=\"https://redditinc.com/hubfs/Reddit%20Inc/Brand/Reddit_Logo.png\" alt=\"Reddit Logo\" width=\"20px\"></a> \n",
        "    <a href=\"https://bsky.app/intent/compose?text=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%2520Workflows/PyTorch/serving/vertex-ai-endpoint-prebuilt-container.ipynb\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"BlueSky Logo\" width=\"20px\"></a> \n",
        "    <a href=\"https://twitter.com/intent/tweet?url=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%2520Workflows/PyTorch/serving/vertex-ai-endpoint-prebuilt-container.ipynb\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/5/5a/X_icon_2.svg\" alt=\"X (Twitter) Logo\" width=\"20px\"></a> \n",
        "  </td>\n",
        "</tr>\n",
        "<tr>\n",
        "  <td colspan=\"5\" style=\"text-align: right\">\n",
        "    <b>Connect With Author On: </b> \n",
        "    <a href=\"https://www.linkedin.com/in/statmike\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"Linkedin Logo\" width=\"20px\"></a>\n",
        "    <a href=\"https://www.github.com/statmike\"><img src=\"https://www.svgrepo.com/download/217753/github.svg\" alt=\"GitHub Logo\" width=\"20px\"></a> \n",
        "    <a href=\"https://www.youtube.com/@statmike-channel\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/f/fd/YouTube_full-color_icon_%282024%29.svg\" alt=\"YouTube Logo\" width=\"20px\"></a>\n",
        "    <a href=\"https://bsky.app/profile/statmike.bsky.social\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"BlueSky Logo\" width=\"20px\"></a> \n",
        "    <a href=\"https://x.com/statmike\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/5/5a/X_icon_2.svg\" alt=\"X (Twitter) Logo\" width=\"20px\"></a>\n",
        "  </td>\n",
        "</tr>\n",
        "</table>"
      ],
      "id": "f9cd9f98"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Deploy PyTorch Model to Vertex AI Endpoint\n",
        "\n",
        "This notebook demonstrates how to deploy a PyTorch model (saved as a .mar file) to a Vertex AI Endpoint for online predictions.\n",
        "\n",
        "## What You'll Learn\n",
        "\n",
        "This workflow covers:\n",
        "\n",
        "1. **Upload Model to Cloud Storage**: Move the .mar file to GCS for Vertex AI access\n",
        "2. **Register Model in Vertex AI Model Registry**: Create a Model resource with pre-built PyTorch container\n",
        "3. **Deploy to Endpoint**: Create and deploy the model to a Vertex AI Endpoint\n",
        "4. **Make Online Predictions**: Test the deployed model with sample data\n",
        "5. **Clean Up Resources**: Remove endpoint and model to avoid ongoing costs\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "- Completed the `pytorch-autoencoder.ipynb` notebook (created the .mar file)\n",
        "- .mar file located at: `../files/pytorch-autoencoder/pytorch_autoencoder.mar`\n",
        "- Google Cloud project with Vertex AI API enabled\n",
        "- Cloud Storage bucket (automatically created if using project ID as bucket name)\n",
        "\n",
        "## Core Concepts\n",
        "\n",
        "**Vertex AI Model Registry**: A centralized repository for managing ML models. Models can be versioned, organized, and deployed to endpoints.\n",
        "\n",
        "**Vertex AI Endpoint**: A deployed model that serves online predictions via HTTP. Endpoints handle:\n",
        "- Traffic splitting across model versions\n",
        "- Autoscaling based on load\n",
        "- Health monitoring and logging\n",
        "\n",
        "**Pre-built Containers**: Google-provided Docker containers optimized for serving PyTorch models. These containers include:\n",
        "- TorchServe for model serving\n",
        "- Automatic request/response handling\n",
        "- Health check endpoints\n",
        "- GPU/CPU support\n",
        "\n",
        "**Model Archive (.mar)**: TorchServe's deployment format containing:\n",
        "- Serialized model (TorchScript traced model)\n",
        "- Custom handler for pre/post-processing\n",
        "- Model metadata and configuration\n",
        "\n",
        "## Deployment Architecture\n",
        "\n",
        "```\n",
        "Local .mar file \u2192 Cloud Storage \u2192 Vertex AI Model \u2192 Vertex AI Endpoint \u2192 Online Predictions\n",
        "```\n",
        "\n",
        "**Why This Approach?**\n",
        "- \u2705 Fully managed infrastructure (no server management)\n",
        "- \u2705 Automatic scaling based on traffic\n",
        "- \u2705 Built-in monitoring and logging\n",
        "- \u2705 Easy integration with other GCP services\n",
        "- \u2705 Support for A/B testing and canary deployments"
      ],
      "id": "overview"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Environment Setup\n",
        "\n",
        "This section will authenticate your session, enable required Google Cloud APIs, and install necessary Python packages.\n",
        "\n",
        "**Package Installation Options (`REQ_TYPE`):**\n",
        "- `PRIMARY`: Installs only the main packages. Faster, but pip resolves sub-dependencies which may result in different versions than development.\n",
        "- `ALL` (Default): Installs exact versions of all packages and dependencies. Best for perfectly reproducing the development environment.\n",
        "- `COLAB`: Installs a Colab-optimized list that excludes pre-installed packages like `ipython` and `ipykernel`.\n",
        "\n",
        "**Installation Tool Options (`INSTALL_TOOL`):**\n",
        "- `pip` (Default): Uses pip for package installation. Standard Python package installer.\n",
        "- `uv`: Modern, fast Python package installer. Must be installed separately. See: https://github.com/astral-sh/uv\n",
        "- `poetry`: Dependency management tool. Requires running notebook in a poetry environment (`poetry shell` or `poetry run jupyter lab`). Uses `pyproject.toml` instead of requirements.txt.\n",
        "\n",
        "> **Note:** If running in Google Colab, the script will automatically detect this and set `REQ_TYPE = 'COLAB'` to prevent package conflicts, overriding any manual setting."
      ],
      "id": "env_setup"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Set Your Project ID\n",
        "\n",
        "\u26a0\ufe0f **Action Required:** Replace the `PROJECT_ID` value below with your Google Cloud project ID before running this cell."
      ],
      "id": "set_project"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "PROJECT_ID = 'statmike-mlops-349915' # replace with GCP project ID\n",
        "REQ_TYPE = 'ALL' # Specify PRIMARY or ALL or COLAB\n",
        "INSTALL_TOOL = 'poetry' # Specify pip, uv, or poetry (all implemented)"
      ],
      "id": "project_config"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Configuration\n\nThis cell defines the requirements files and Google Cloud APIs needed for this notebook. Run as-is without modification."
      ],
      "id": "config_header"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "REQUIREMENTS_URL = 'https://raw.githubusercontent.com/statmike/vertex-ai-mlops/refs/heads/main/Framework%20Workflows/PyTorch/requirements.txt'\n",
        "\n",
        "REQUIRED_APIS = [\n",
        "    \"aiplatform.googleapis.com\",\n",
        "    \"storage.googleapis.com\",\n",
        "]"
      ],
      "id": "config"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Run Setup\n",
        "\n",
        "This cell downloads the centralized setup code and configures your environment. It will:\n",
        "- Authenticate your session with Google Cloud\n",
        "- Enable required APIs for this notebook\n",
        "- Install necessary Python packages\n",
        "- Display a setup summary with your project information\n",
        "\n",
        "> **Note:** In Colab, if packages are installed, the kernel will automatically restart. After restart, continue from the next cell without re-running earlier cells."
      ],
      "id": "run_setup_header"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, urllib.request\n",
        "\n",
        "# Download and import setup code\n",
        "url = 'https://raw.githubusercontent.com/statmike/vertex-ai-mlops/refs/heads/main/core/notebook-template/python_setup.py'\n",
        "urllib.request.urlretrieve(url, 'python_setup_local.py')\n",
        "import python_setup_local as python_setup\n",
        "os.remove('python_setup_local.py')\n",
        "\n",
        "# Run setup\n",
        "setup_info = python_setup.setup_environment(PROJECT_ID, REQ_TYPE, REQUIREMENTS_URL, REQUIRED_APIS, INSTALL_TOOL)"
      ],
      "id": "run_setup"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n## Python Setup"
      ],
      "id": "python_setup"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Imports"
      ],
      "id": "imports_header"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import subprocess\n",
        "from google.cloud import aiplatform\n",
        "from google.cloud import storage\n",
        "import json"
      ],
      "id": "imports"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Variables - User Set"
      ],
      "id": "vars_user"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "REGION = 'us-central1'\n",
        "SERIES = 'frameworks'\n",
        "EXPERIMENT = 'pytorch-autoencoder'\n",
        "\n",
        "# Model configuration\n",
        "MODEL_DISPLAY_NAME = EXPERIMENT\n",
        "ENDPOINT_DISPLAY_NAME = f\"{EXPERIMENT}-endpoint\"\n",
        "\n",
        "# Machine configuration\n",
        "MACHINE_TYPE = 'n1-standard-4'\n",
        "MIN_REPLICA_COUNT = 1\n",
        "MAX_REPLICA_COUNT = 4\n",
        "\n",
        "# Container image for PyTorch CPU serving\n",
        "# See: https://cloud.google.com/vertex-ai/docs/predictions/pre-built-containers#pytorch\n",
        "DEPLOY_IMAGE = 'us-docker.pkg.dev/vertex-ai/prediction/pytorch-cpu.1-13:latest'\n",
        "\n",
        "# Local path to .mar file (from pytorch-autoencoder notebook)\n",
        "LOCAL_MAR_PATH = '../files/pytorch-autoencoder/pytorch_autoencoder.mar'"
      ],
      "id": "vars_user_code"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Variables - Auto Set"
      ],
      "id": "vars_auto"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "PROJECT_ID = subprocess.run(['gcloud', 'config', 'get-value', 'project'], capture_output=True, text=True, check=True).stdout.strip()\n",
        "PROJECT_NUMBER = subprocess.run(['gcloud', 'projects', 'describe', PROJECT_ID, '--format=value(projectNumber)'], capture_output=True, text=True, check=True).stdout.strip()\n",
        "\n",
        "print(f\"\\n{'='*50}\\nGoogle Cloud Project Information\\n{'='*50}\\nPROJECT_ID     = {PROJECT_ID}\\nPROJECT_NUMBER = {PROJECT_NUMBER}\\n{'='*50}\\n\")"
      ],
      "id": "vars_auto_code"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Configurations"
      ],
      "id": "configs"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# GCS configuration\n",
        "BUCKET_NAME = PROJECT_ID\n",
        "BUCKET_URI = f\"gs://{BUCKET_NAME}\"\n",
        "MODEL_ARTIFACT_DIR = f\"{BUCKET_URI}/{SERIES}/{EXPERIMENT}\"\n",
        "\n",
        "# Model artifact in GCS\n",
        "GCS_MAR_PATH = f\"{MODEL_ARTIFACT_DIR}/pytorch_autoencoder.mar\"\n",
        "\n",
        "print(f\"Bucket: {BUCKET_URI}\")\n",
        "print(f\"Model artifacts: {MODEL_ARTIFACT_DIR}\")\n",
        "print(f\"MAR file will be at: {GCS_MAR_PATH}\")"
      ],
      "id": "configs_code"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Initialize Vertex AI SDK"
      ],
      "id": "init_vertex"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_URI)\n",
        "\n",
        "print(f\"\u2705 Vertex AI initialized\")\n",
        "print(f\"   Project: {PROJECT_ID}\")\n",
        "print(f\"   Location: {REGION}\")\n",
        "print(f\"   Staging bucket: {BUCKET_URI}\")"
      ],
      "id": "init_vertex_code"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Upload Model to Cloud Storage\n",
        "\n",
        "Vertex AI requires model artifacts to be stored in Cloud Storage. We'll upload the .mar file created in the training notebook."
      ],
      "id": "gcs_upload"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Check for Local .mar File"
      ],
      "id": "check_mar"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "if os.path.exists(LOCAL_MAR_PATH):\n",
        "    file_size = os.path.getsize(LOCAL_MAR_PATH)\n",
        "    print(f\"\u2705 Found .mar file: {LOCAL_MAR_PATH}\")\n",
        "    print(f\"   Size: {file_size:,} bytes ({file_size / 1024:.2f} KB)\")\n",
        "else:\n",
        "    print(f\"\u274c .mar file not found at: {LOCAL_MAR_PATH}\")\n",
        "    print(\"   Please run the pytorch-autoencoder.ipynb notebook first to create the .mar file\")"
      ],
      "id": "check_mar_code"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create GCS Bucket (if needed)"
      ],
      "id": "create_bucket"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "storage_client = storage.Client(project=PROJECT_ID)\n",
        "\n",
        "try:\n",
        "    bucket = storage_client.get_bucket(BUCKET_NAME)\n",
        "    print(f\"\u2705 Bucket already exists: {BUCKET_URI}\")\n",
        "except:\n",
        "    bucket = storage_client.create_bucket(BUCKET_NAME, location=REGION)\n",
        "    print(f\"\u2705 Created new bucket: {BUCKET_URI}\")"
      ],
      "id": "create_bucket_code"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Upload .mar File to GCS"
      ],
      "id": "upload_mar"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Upload the .mar file\n",
        "blob = bucket.blob(f\"{SERIES}/{EXPERIMENT}/pytorch_autoencoder.mar\")\n",
        "blob.upload_from_filename(LOCAL_MAR_PATH)\n",
        "\n",
        "print(f\"\u2705 Uploaded .mar file to: {GCS_MAR_PATH}\")\n",
        "print(f\"   GCS size: {blob.size:,} bytes ({blob.size / 1024:.2f} KB)\")"
      ],
      "id": "upload_mar_code"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Register Model in Vertex AI Model Registry\n",
        "\n",
        "Upload the model to Vertex AI Model Registry using a pre-built PyTorch serving container.\n",
        "\n",
        "### Pre-built Container Information\n",
        "\n",
        "We are using Google's pre-built PyTorch container which includes:\n",
        "- **TorchServe**: Production-ready PyTorch model server\n",
        "- **Automatic .mar handling**: Container knows how to load and serve .mar files\n",
        "- **Health checks**: Built-in endpoints for monitoring\n",
        "- **Request handling**: Automatic JSON to Tensor conversion\n",
        "\n",
        "**Container used**: `us-docker.pkg.dev/vertex-ai/prediction/pytorch-cpu.1-13:latest`\n",
        "\n",
        "### CPU vs GPU Deployment\n",
        "\n",
        "**For this notebook, we are using CPU** because:\n",
        "- \u2705 Our autoencoder is small (~1,362 parameters)\n",
        "- \u2705 Inference is very fast on CPU\n",
        "- \u2705 CPU is more cost-effective for small models\n",
        "- \u2705 No GPU transfer overhead\n",
        "\n",
        "**When to use GPU:**\n",
        "- Large models (millions of parameters)\n",
        "- Batch inference with many instances\n",
        "- Complex models (transformers, large CNNs)\n",
        "- High throughput requirements\n",
        "\n",
        "**To switch to GPU deployment:**\n",
        "```python\n",
        "# Change these variables:\n",
        "DEPLOY_IMAGE = \"us-docker.pkg.dev/vertex-ai/prediction/pytorch-gpu.1-13:latest\"\n",
        "MACHINE_TYPE = \"n1-standard-4\"\n",
        "ACCELERATOR_TYPE = \"NVIDIA_TESLA_L4\"\n",
        "ACCELERATOR_COUNT = 1\n",
        "```\n",
        "\n",
        "See [available GPU types](https://cloud.google.com/vertex-ai/docs/predictions/configure-compute#gpu-config) for more options."
      ],
      "id": "model_registry"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Check for Existing Model"
      ],
      "id": "check_model"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# List existing models\n",
        "models = aiplatform.Model.list(filter=f\"display_name={MODEL_DISPLAY_NAME}\")\n",
        "\n",
        "if models:\n",
        "    print(f\"Found {len(models)} existing model(s) with name {MODEL_DISPLAY_NAME}:\")\n",
        "    for model in models:\n",
        "        print(f\"  - {model.resource_name}\")\n",
        "        print(f\"    Created: {model.create_time}\")\n",
        "        print(f\"    Version: {model.version_id}\")\n",
        "else:\n",
        "    print(f\"No existing models found with name {MODEL_DISPLAY_NAME}\")"
      ],
      "id": "check_model_code"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Upload Model to Registry"
      ],
      "id": "upload_model"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check if model already exists\n",
        "existing_models = aiplatform.Model.list(filter=f\"display_name={MODEL_DISPLAY_NAME}\")\n",
        "\n",
        "if existing_models:\n",
        "    model = existing_models[0]\n",
        "    print(f\"\u2705 Using existing model:\")\n",
        "    print(f\"   Resource name: {model.resource_name}\")\n",
        "    print(f\"   Display name: {model.display_name}\")\n",
        "    print(f\"   Version: {model.version_id}\")\n",
        "else:\n",
        "    # Upload model\n",
        "    model = aiplatform.Model.upload(\n",
        "        display_name=MODEL_DISPLAY_NAME,\n",
        "        artifact_uri=MODEL_ARTIFACT_DIR,\n",
        "        serving_container_image_uri=DEPLOY_IMAGE,\n",
        "        serving_container_environment_variables={\n",
        "            \"MODEL_NAME\": \"pytorch_autoencoder\",\n",
        "        },\n",
        "        serving_container_ports=[7080],\n",
        "        description=\"PyTorch autoencoder for anomaly detection (fraud transactions)\"\n",
        "    )\n",
        "\n",
        "    print(f\"\u2705 Model uploaded to registry:\")\n",
        "    print(f\"   Resource name: {model.resource_name}\")\n",
        "    print(f\"   Display name: {model.display_name}\")\n",
        "    print(f\"   Artifact URI: {model.uri}\")"
      ],
      "id": "upload_model_code"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Deploy Model to Endpoint\n",
        "\n",
        "Create a Vertex AI Endpoint and deploy the model for online predictions.\n",
        "\n",
        "### Endpoint Configuration\n",
        "\n",
        "- **Machine type**: `n1-standard-4` (4 vCPUs, 15 GB memory)\n",
        "- **Autoscaling**: Min 1 replica, Max 4 replicas\n",
        "- **Traffic split**: 100% to this model\n",
        "\n",
        "### Deployment Time\n",
        "\n",
        "Deployment typically takes 10-15 minutes as Vertex AI:\n",
        "1. Creates the endpoint infrastructure\n",
        "2. Pulls the pre-built container image\n",
        "3. Downloads the .mar file from GCS\n",
        "4. Initializes TorchServe\n",
        "5. Performs health checks\n",
        "6. Marks the endpoint as ready"
      ],
      "id": "endpoint_deploy"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Check for Existing Endpoint"
      ],
      "id": "check_endpoint"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# List existing endpoints\n",
        "endpoints = aiplatform.Endpoint.list(filter=f\"display_name={ENDPOINT_DISPLAY_NAME}\")\n",
        "\n",
        "if endpoints:\n",
        "    print(f\"Found {len(endpoints)} existing endpoint(s) with name {ENDPOINT_DISPLAY_NAME}:\")\n",
        "    for ep in endpoints:\n",
        "        print(f\"  - {ep.resource_name}\")\n",
        "        print(f\"    Created: {ep.create_time}\")\n",
        "else:\n",
        "    print(f\"No existing endpoints found with name {ENDPOINT_DISPLAY_NAME}\")"
      ],
      "id": "check_endpoint_code"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create Endpoint"
      ],
      "id": "create_endpoint"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check if endpoint already exists\n",
        "existing_endpoints = aiplatform.Endpoint.list(filter=f\"display_name={ENDPOINT_DISPLAY_NAME}\")\n",
        "\n",
        "if existing_endpoints:\n",
        "    endpoint = existing_endpoints[0]\n",
        "    print(f\"\u2705 Using existing endpoint:\")\n",
        "    print(f\"   Resource name: {endpoint.resource_name}\")\n",
        "    print(f\"   Display name: {endpoint.display_name}\")\n",
        "else:\n",
        "    # Create endpoint\n",
        "    endpoint = aiplatform.Endpoint.create(\n",
        "        display_name=ENDPOINT_DISPLAY_NAME,\n",
        "        description=\"Endpoint for PyTorch autoencoder anomaly detection\"\n",
        "    )\n",
        "\n",
        "    print(f\"\u2705 Endpoint created:\")\n",
        "    print(f\"   Resource name: {endpoint.resource_name}\")\n",
        "    print(f\"   Display name: {endpoint.display_name}\")"
      ],
      "id": "create_endpoint_code"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Deploy Model to Endpoint"
      ],
      "id": "deploy_model"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Deploy model to endpoint\n",
        "print(\"Deploying model to endpoint (this will take 10-15 minutes)...\")\n",
        "print(\"You can monitor progress in the Cloud Console:\")\n",
        "print(f\"https://console.cloud.google.com/vertex-ai/endpoints/{endpoint.name.split('/')[-1]}?project={PROJECT_ID}\")\n",
        "\n",
        "endpoint.deploy(\n",
        "    model=model,\n",
        "    deployed_model_display_name=MODEL_DISPLAY_NAME,\n",
        "    machine_type=MACHINE_TYPE,\n",
        "    min_replica_count=MIN_REPLICA_COUNT,\n",
        "    max_replica_count=MAX_REPLICA_COUNT,\n",
        "    traffic_percentage=100,\n",
        "    sync=True\n",
        ")\n",
        "\n",
        "print(f\"\\n\u2705 Model deployed successfully!\")\n",
        "print(f\"   Endpoint: {endpoint.display_name}\")\n",
        "print(f\"   Resource name: {endpoint.resource_name}\")"
      ],
      "id": "deploy_model_code"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Prepare Test Data\n",
        "\n",
        "Retrieve test records from the same BigQuery source used in training."
      ],
      "id": "test_data"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Import BigQuery Client"
      ],
      "id": "import_bq"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.cloud import bigquery\n",
        "import pandas as pd\n",
        "\n",
        "bq = bigquery.Client(project=PROJECT_ID)"
      ],
      "id": "import_bq_code"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Retrieve Test Instances"
      ],
      "id": "get_test_data"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# BigQuery source (same as training notebook)\n",
        "BQ_PROJECT = PROJECT_ID\n",
        "BQ_DATASET = SERIES.replace('-', '_')\n",
        "BQ_TABLE = SERIES\n",
        "\n",
        "# Get a few test instances\n",
        "query = f\"\"\"\n",
        "SELECT * EXCEPT(splits, transaction_id, Class)\n",
        "FROM `{BQ_PROJECT}.{BQ_DATASET}.{BQ_TABLE}`\n",
        "WHERE splits = \"TEST\" AND Class = 0\n",
        "LIMIT 3\n",
        "\"\"\"\n",
        "\n",
        "test_df = bq.query(query).to_dataframe()\n",
        "print(f\"Retrieved {len(test_df)} test instances\")\n",
        "print(f\"\\nFeatures: {list(test_df.columns)}\")\n",
        "test_df.head()"
      ],
      "id": "get_test_data_code"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Format Data for Prediction"
      ],
      "id": "format_data"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert to list of lists (format expected by endpoint)\n",
        "test_instances = test_df.values.tolist()\n",
        "\n",
        "print(f\"Prepared {len(test_instances)} instances for prediction\")\n",
        "print(f\"\\nFirst instance (30 features):\")\n",
        "print(test_instances[0])"
      ],
      "id": "format_data_code"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Make Online Predictions\n",
        "\n",
        "Test the deployed model using both the Vertex AI SDK and direct REST API calls."
      ],
      "id": "predictions"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Method 1: Vertex AI SDK\n",
        "\n",
        "The simplest way to make predictions using the Python SDK."
      ],
      "id": "sdk_pred"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Make prediction using SDK\n",
        "response = endpoint.predict(instances=test_instances)\n",
        "\n",
        "print(f\"\u2705 Received predictions for {len(response.predictions)} instances\")\n",
        "print(f\"\\nPrediction structure (first instance):\")\n",
        "\n",
        "# The response contains the full dictionary output from our model\n",
        "first_prediction = response.predictions[0]\n",
        "print(f\"\\nAvailable keys: {list(first_prediction.keys())}\")\n",
        "print(f\"\\nAnomaly score (denormalized_MAE): {first_prediction['denormalized_MAE']}\")\n",
        "print(f\"Encoded representation (latent space): {first_prediction['encoded']}\")"
      ],
      "id": "sdk_pred_code"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Analyze Predictions"
      ],
      "id": "analyze_pred"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract key metrics from all predictions\n",
        "results = []\n",
        "for i, pred in enumerate(response.predictions):\n",
        "    results.append({\n",
        "        \"instance\": i,\n",
        "        \"anomaly_score\": pred[\"denormalized_MAE\"],\n",
        "        \"denorm_rmse\": pred[\"denormalized_RMSE\"],\n",
        "        \"denorm_mse\": pred[\"denormalized_MSE\"],\n",
        "        \"encoded\": pred[\"encoded\"]\n",
        "    })\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "print(\"Prediction Summary:\")\n",
        "results_df"
      ],
      "id": "analyze_pred_code"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Method 2: REST API with Requests\n",
        "\n",
        "Make predictions using direct HTTP calls. This is useful for:\n",
        "- Non-Python clients\n",
        "- Testing from other services\n",
        "- Understanding the raw API format"
      ],
      "id": "rest_api"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "import google.auth\n",
        "import google.auth.transport.requests\n",
        "\n",
        "# Get authentication token\n",
        "credentials, project = google.auth.default()\n",
        "auth_req = google.auth.transport.requests.Request()\n",
        "credentials.refresh(auth_req)\n",
        "access_token = credentials.token\n",
        "\n",
        "# Construct endpoint URL\n",
        "endpoint_url = f\"https://{REGION}-aiplatform.googleapis.com/v1/{endpoint.resource_name}:predict\"\n",
        "\n",
        "print(f\"Endpoint URL: {endpoint_url}\")"
      ],
      "id": "rest_api_setup"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare request payload\n",
        "payload = {\n",
        "    \"instances\": test_instances\n",
        "}\n",
        "\n",
        "# Make REST API request\n",
        "headers = {\n",
        "    \"Authorization\": f\"Bearer {access_token}\",\n",
        "    \"Content-Type\": \"application/json\"\n",
        "}\n",
        "\n",
        "response = requests.post(endpoint_url, headers=headers, json=payload)\n",
        "\n",
        "if response.status_code == 200:\n",
        "    rest_predictions = response.json()[\"predictions\"]\n",
        "    print(f\"\u2705 REST API prediction successful\")\n",
        "    print(f\"   Received {len(rest_predictions)} predictions\")\n",
        "    print(f\"\\nFirst prediction anomaly score: {rest_predictions[0]['denormalized_MAE']}\")\n",
        "else:\n",
        "    print(f\"\u274c Error: {response.status_code}\")\n",
        "    print(response.text)"
      ],
      "id": "rest_api_predict"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Compare SDK vs REST API Results"
      ],
      "id": "compare_results"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify both methods return the same results\n",
        "sdk_score = response.predictions[0][\"denormalized_MAE\"]\n",
        "rest_score = rest_predictions[0][\"denormalized_MAE\"]\n",
        "\n",
        "print(f\"SDK anomaly score:  {sdk_score}\")\n",
        "print(f\"REST anomaly score: {rest_score}\")\n",
        "print(f\"\\nResults match: {abs(sdk_score - rest_score) < 0.001}\")"
      ],
      "id": "compare_results_code"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Clean Up Resources\n",
        "\n",
        "**Important**: Deployed endpoints incur charges even when not making predictions. Always undeploy and delete endpoints when done.\n",
        "\n",
        "### Cost Information\n",
        "\n",
        "With our configuration (`n1-standard-4`, min 1 replica):\n",
        "- **Hourly cost**: ~$0.20/hour (varies by region)\n",
        "- **Daily cost**: ~$4.80/day if left running\n",
        "- **Monthly cost**: ~$144/month\n",
        "\n",
        "See [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing#prediction-prices) for details.\n",
        "\n",
        "### Cleanup Options\n",
        "\n",
        "1. **Undeploy only**: Keeps endpoint but removes model (no charges)\n",
        "2. **Delete endpoint**: Removes everything (recommended for this demo)\n",
        "3. **Keep model in registry**: Model storage in GCS has minimal cost"
      ],
      "id": "cleanup"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Option 1: Undeploy Model (Keep Endpoint)"
      ],
      "id": "undeploy"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Undeploy the model from the endpoint\n",
        "# endpoint.undeploy_all()\n",
        "# print(\"\u2705 Model undeployed from endpoint\")\n",
        "# print(\"   Endpoint still exists but serves no traffic\")\n",
        "print(\"Uncomment the code above to undeploy the model\")"
      ],
      "id": "undeploy_code"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Option 2: Delete Endpoint (Recommended)"
      ],
      "id": "delete_endpoint"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Delete the endpoint (recommended to avoid charges)\n",
        "# endpoint.delete(force=True)\n",
        "# print(\"\u2705 Endpoint deleted\")\n",
        "# print(\"   All resources cleaned up\")\n",
        "print(\"Uncomment the code above to delete the endpoint\")"
      ],
      "id": "delete_endpoint_code"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Option 3: Delete Model from Registry"
      ],
      "id": "delete_model"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Delete the model from Model Registry\n",
        "# model.delete()\n",
        "# print(\"\u2705 Model deleted from registry\")\n",
        "# print(\"   .mar file still exists in GCS\")\n",
        "print(\"Uncomment the code above to delete the model from registry\")"
      ],
      "id": "delete_model_code"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Verify Cleanup"
      ],
      "id": "verify_cleanup"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check remaining resources\n",
        "endpoints = aiplatform.Endpoint.list(filter=f\"display_name={ENDPOINT_DISPLAY_NAME}\")\n",
        "models = aiplatform.Model.list(filter=f\"display_name={MODEL_DISPLAY_NAME}\")\n",
        "\n",
        "print(f\"Endpoints with name {ENDPOINT_DISPLAY_NAME}: {len(endpoints)}\")\n",
        "print(f\"Models with name {MODEL_DISPLAY_NAME}: {len(models)}\")\n",
        "\n",
        "if endpoints:\n",
        "    print(\"\\n\u26a0\ufe0f  Endpoint still exists - remember to delete to avoid charges\")\n",
        "else:\n",
        "    print(\"\\n\u2705 No active endpoints\")"
      ],
      "id": "verify_cleanup_code"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Summary\n",
        "\n",
        "In this notebook, you:\n",
        "\n",
        "\u2705 Uploaded a PyTorch .mar file to Cloud Storage\n",
        "\n",
        "\u2705 Registered the model in Vertex AI Model Registry\n",
        "\n",
        "\u2705 Deployed the model to a Vertex AI Endpoint\n",
        "\n",
        "\u2705 Made online predictions using both SDK and REST API\n",
        "\n",
        "\u2705 Analyzed anomaly scores from the autoencoder\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "- **Batch Predictions**: Use Vertex AI Batch Prediction for processing large datasets\n",
        "- **Model Monitoring**: Set up alerts for prediction drift and anomalies\n",
        "- **A/B Testing**: Deploy multiple model versions and split traffic\n",
        "- **Dataflow Integration**: Use the model in streaming pipelines with Apache Beam RunInference\n",
        "\n",
        "### Related Notebooks\n",
        "\n",
        "- [PyTorch Autoencoder Training](../pytorch-autoencoder.ipynb) - Train the model\n",
        "- [Dataflow RunInference](./dataflow-runinference.ipynb) - Batch and streaming inference (coming soon)\n",
        "\n",
        "### Resources\n",
        "\n",
        "- [Vertex AI Prediction Documentation](https://cloud.google.com/vertex-ai/docs/predictions/overview)\n",
        "- [Pre-built PyTorch Containers](https://cloud.google.com/vertex-ai/docs/predictions/pre-built-containers#pytorch)\n",
        "- [TorchServe Documentation](https://pytorch.org/serve/)\n",
        "- [Vertex AI Pricing](https://cloud.google.com/vertex-ai/pricing)"
      ],
      "id": "summary"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.13.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}