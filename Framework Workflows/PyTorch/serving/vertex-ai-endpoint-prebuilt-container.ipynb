{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9cd9f98",
   "metadata": {},
   "source": [
    "![tracker](https://us-central1-vertex-ai-mlops-369716.cloudfunctions.net/pixel-tracking?path=statmike%2Fvertex-ai-mlops%2FFramework+Workflows%2FPyTorch%2Fserving&file=vertex-ai-endpoint-prebuilt-container.ipynb)\n",
    "<!--- header table --->\n",
    "<table align=\"left\">\n",
    "<tr>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%20Workflows/PyTorch/serving/vertex-ai-endpoint-prebuilt-container.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.svgrepo.com/download/217753/github.svg\" alt=\"GitHub logo\">\n",
    "      <br>View on<br>GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://colab.research.google.com/github/statmike/vertex-ai-mlops/blob/main/Framework%20Workflows/PyTorch/serving/vertex-ai-endpoint-prebuilt-container.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.gstatic.com/pantheon/images/bigquery/welcome_page/colab-logo.svg\" alt=\"Google Colaboratory logo\">\n",
    "      <br>Run in<br>Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https%3A%2F%2Fraw.githubusercontent.com%2Fstatmike%2Fvertex-ai-mlops%2Fmain%2FFramework%2520Workflows%2FPyTorch%2Fserving%2Fvertex-ai-endpoint-prebuilt-container.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\">\n",
    "      <br>Run in<br>Colab Enterprise\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/bigquery/import?url=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%20Workflows/PyTorch/serving/vertex-ai-endpoint-prebuilt-container.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.gstatic.com/images/branding/gcpiconscolors/bigquery/v1/32px.svg\" alt=\"BigQuery logo\">\n",
    "      <br>Open in<br>BigQuery Studio\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/statmike/vertex-ai-mlops/main/Framework%20Workflows/PyTorch/serving/vertex-ai-endpoint-prebuilt-container.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.gstatic.com/images/branding/gcpiconscolors/vertexai/v1/32px.svg\" alt=\"Vertex AI logo\">\n",
    "      <br>Open in<br>Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td colspan=\"5\" style=\"text-align: right\">\n",
    "    <b>Share This On: </b> \n",
    "    <a href=\"https://www.linkedin.com/sharing/share-offsite/?url=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%2520Workflows/PyTorch/serving/vertex-ai-endpoint-prebuilt-container.ipynb\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"Linkedin Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://reddit.com/submit?url=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%2520Workflows/PyTorch/serving/vertex-ai-endpoint-prebuilt-container.ipynb\"><img src=\"https://redditinc.com/hubfs/Reddit%20Inc/Brand/Reddit_Logo.png\" alt=\"Reddit Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://bsky.app/intent/compose?text=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%2520Workflows/PyTorch/serving/vertex-ai-endpoint-prebuilt-container.ipynb\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"BlueSky Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://twitter.com/intent/tweet?url=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%2520Workflows/PyTorch/serving/vertex-ai-endpoint-prebuilt-container.ipynb\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/5/5a/X_icon_2.svg\" alt=\"X (Twitter) Logo\" width=\"20px\"></a> \n",
    "  </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td colspan=\"5\" style=\"text-align: right\">\n",
    "    <b>Connect With Author On: </b> \n",
    "    <a href=\"https://www.linkedin.com/in/statmike\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"Linkedin Logo\" width=\"20px\"></a>\n",
    "    <a href=\"https://www.github.com/statmike\"><img src=\"https://www.svgrepo.com/download/217753/github.svg\" alt=\"GitHub Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://www.youtube.com/@statmike-channel\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/f/fd/YouTube_full-color_icon_%282024%29.svg\" alt=\"YouTube Logo\" width=\"20px\"></a>\n",
    "    <a href=\"https://bsky.app/profile/statmike.bsky.social\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"BlueSky Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://x.com/statmike\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/5/5a/X_icon_2.svg\" alt=\"X (Twitter) Logo\" width=\"20px\"></a>\n",
    "  </td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overview",
   "metadata": {},
   "source": [
    "# Deploy PyTorch Model to Vertex AI Endpoint\n",
    "\n",
    "This notebook demonstrates how to deploy a PyTorch model (saved as a .mar file) to a Vertex AI Endpoint for online predictions.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "This workflow covers:\n",
    "\n",
    "1. **Upload Model to Cloud Storage**: Move the .mar file to GCS for Vertex AI access\n",
    "2. **Register Model in Vertex AI Model Registry**: Create a Model resource with pre-built PyTorch container\n",
    "3. **Deploy to Endpoint**: Create and deploy the model to a Vertex AI Endpoint\n",
    "4. **Make Online Predictions**: Test the deployed model with sample data\n",
    "5. **Clean Up Resources**: Remove endpoint and model to avoid ongoing costs\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Completed the `pytorch-autoencoder.ipynb` notebook (created the .mar file)\n",
    "- .mar file located at: `../files/pytorch-autoencoder/pytorch_autoencoder.mar`\n",
    "- Google Cloud project with Vertex AI API enabled\n",
    "- Cloud Storage bucket (automatically created if using project ID as bucket name)\n",
    "\n",
    "## Core Concepts\n",
    "\n",
    "**Vertex AI Model Registry**: A centralized repository for managing ML models. Models can be versioned, organized, and deployed to endpoints.\n",
    "\n",
    "**Vertex AI Endpoint**: A deployed model that serves online predictions via HTTP. Endpoints handle:\n",
    "- Traffic splitting across model versions\n",
    "- Autoscaling based on load\n",
    "- Health monitoring and logging\n",
    "\n",
    "**Pre-built Containers**: Google-provided Docker containers optimized for serving PyTorch models. These containers include:\n",
    "- TorchServe for model serving\n",
    "- Automatic request/response handling\n",
    "- Health check endpoints\n",
    "- GPU/CPU support\n",
    "\n",
    "**Model Archive (.mar)**: TorchServe's deployment format containing:\n",
    "- Serialized model (TorchScript traced model)\n",
    "- Custom handler for pre/post-processing\n",
    "- Model metadata and configuration\n",
    "\n",
    "## Deployment Architecture\n",
    "\n",
    "```\n",
    "Local .mar file → Cloud Storage → Vertex AI Model → Vertex AI Endpoint → Online Predictions\n",
    "```\n",
    "\n",
    "**Why This Approach?**\n",
    "- ✅ Fully managed infrastructure (no server management)\n",
    "- ✅ Automatic scaling based on traffic\n",
    "- ✅ Built-in monitoring and logging\n",
    "- ✅ Easy integration with other GCP services\n",
    "- ✅ Support for A/B testing and canary deployments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "env_setup",
   "metadata": {},
   "source": [
    "---\n",
    "## Environment Setup\n",
    "\n",
    "This section will authenticate your session, enable required Google Cloud APIs, and install necessary Python packages.\n",
    "\n",
    "**Package Installation Options (`REQ_TYPE`):**\n",
    "- `PRIMARY`: Installs only the main packages. Faster, but pip resolves sub-dependencies which may result in different versions than development.\n",
    "- `ALL` (Default): Installs exact versions of all packages and dependencies. Best for perfectly reproducing the development environment.\n",
    "- `COLAB`: Installs a Colab-optimized list that excludes pre-installed packages like `ipython` and `ipykernel`.\n",
    "\n",
    "**Installation Tool Options (`INSTALL_TOOL`):**\n",
    "- `pip` (Default): Uses pip for package installation. Standard Python package installer.\n",
    "- `uv`: Modern, fast Python package installer. Must be installed separately. See: https://github.com/astral-sh/uv\n",
    "- `poetry`: Dependency management tool. Requires running notebook in a poetry environment (`poetry shell` or `poetry run jupyter lab`). Uses `pyproject.toml` instead of requirements.txt.\n",
    "\n",
    "> **Note:** If running in Google Colab, the script will automatically detect this and set `REQ_TYPE = 'COLAB'` to prevent package conflicts, overriding any manual setting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "set_project",
   "metadata": {},
   "source": [
    "### Set Your Project ID\n",
    "\n",
    "⚠️ **Action Required:** Replace the `PROJECT_ID` value below with your Google Cloud project ID before running this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "project_config",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = 'statmike-mlops-349915' # replace with GCP project ID\n",
    "REQ_TYPE = 'ALL' # Specify PRIMARY or ALL or COLAB\n",
    "INSTALL_TOOL = 'poetry' # Specify pip, uv, or poetry (all implemented)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config_header",
   "metadata": {},
   "source": [
    "### Configuration\n",
    "\n",
    "This cell defines the requirements files and Google Cloud APIs needed for this notebook. Run as-is without modification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": [
    "REQUIREMENTS_URL = 'https://raw.githubusercontent.com/statmike/vertex-ai-mlops/refs/heads/main/Framework%20Workflows/PyTorch/requirements.txt'\n",
    "\n",
    "REQUIRED_APIS = [\n",
    "    \"aiplatform.googleapis.com\",\n",
    "    \"storage.googleapis.com\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "run_setup_header",
   "metadata": {},
   "source": [
    "### Run Setup\n",
    "\n",
    "This cell downloads the centralized setup code and configures your environment. It will:\n",
    "- Authenticate your session with Google Cloud\n",
    "- Enable required APIs for this notebook\n",
    "- Install necessary Python packages\n",
    "- Display a setup summary with your project information\n",
    "\n",
    "> **Note:** In Colab, if packages are installed, the kernel will automatically restart. After restart, continue from the next cell without re-running earlier cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "run_setup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "PYTHON GCP ENVIRONMENT SETUP\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "AUTHENTICATION\n",
      "==================================================\n",
      "Checking for existing ADC...\n",
      "✅ Existing ADC found.\n",
      "✅ Project is correctly set to 'statmike-mlops-349915'.\n",
      "\n",
      "==================================================\n",
      "API CHECK & ENABLE\n",
      "==================================================\n",
      "✅ aiplatform.googleapis.com is already enabled.\n",
      "✅ storage.googleapis.com is already enabled.\n",
      "\n",
      "==================================================\n",
      "PACKAGE MANAGEMENT\n",
      "==================================================\n",
      "Installation Tool: poetry\n",
      "✅ Found poetry at: /usr/local/google/home/statmike/.local/bin/poetry\n",
      "✅ Running in poetry environment: /usr/local/google/home/statmike/.cache/pypoetry/virtualenvs/frameworks-pytorch-0KVJlKeQ-py3.13\n",
      "ℹ️  Poetry mode: Installing from pyproject.toml (REQUIREMENTS_URL ignored)\n",
      "✅ Found pyproject.toml at: /usr/local/google/home/statmike/Git/vertex-ai-mlops/Framework Workflows/PyTorch/pyproject.toml\n",
      "   Changed working directory to: /usr/local/google/home/statmike/Git/vertex-ai-mlops/Framework Workflows/PyTorch\n",
      "Running poetry install...\n",
      "   Restored working directory to: /usr/local/google/home/statmike/Git/vertex-ai-mlops/Framework Workflows/PyTorch/serving\n",
      "✅ All packages are already installed and up to date.\n",
      "\n",
      "==================================================\n",
      "Google Cloud Project Information\n",
      "==================================================\n",
      "PROJECT_ID     = statmike-mlops-349915\n",
      "PROJECT_NUMBER = 1026793852137\n",
      "==================================================\n",
      "\n",
      "\n",
      "==================================================\n",
      "SETUP SUMMARY\n",
      "==================================================\n",
      "✅ Authentication:    Success\n",
      "✅ API Configuration: Success\n",
      "✅ Package Install:   Already up to date\n",
      "✅ Installation Tool: poetry\n",
      "✅ Project ID:        statmike-mlops-349915\n",
      "✅ Project Number:    1026793852137\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os, urllib.request\n",
    "\n",
    "# Download and import setup code\n",
    "url = 'https://raw.githubusercontent.com/statmike/vertex-ai-mlops/refs/heads/main/core/notebook-template/python_setup.py'\n",
    "urllib.request.urlretrieve(url, 'python_setup_local.py')\n",
    "import python_setup_local as python_setup\n",
    "os.remove('python_setup_local.py')\n",
    "\n",
    "# Run setup\n",
    "setup_info = python_setup.setup_environment(PROJECT_ID, REQ_TYPE, REQUIREMENTS_URL, REQUIRED_APIS, INSTALL_TOOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "python_setup",
   "metadata": {},
   "source": [
    "---\n",
    "## Python Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imports_header",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "imports",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/google/home/statmike/.cache/pypoetry/virtualenvs/frameworks-pytorch-0KVJlKeQ-py3.13/lib/python3.13/site-packages/google/cloud/aiplatform/models.py:52: FutureWarning: Support for google-cloud-storage < 3.0.0 will be removed in a future version of google-cloud-aiplatform. Please upgrade to google-cloud-storage >= 3.0.0.\n",
      "  from google.cloud.aiplatform.utils import gcs_utils\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud import storage\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vars_user",
   "metadata": {},
   "source": [
    "### Variables - User Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "vars_user_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION = 'us-central1'\n",
    "SERIES = 'frameworks'\n",
    "EXPERIMENT = 'pytorch-autoencoder'\n",
    "\n",
    "# Model configuration\n",
    "MODEL_DISPLAY_NAME = EXPERIMENT\n",
    "ENDPOINT_DISPLAY_NAME = f\"{EXPERIMENT}-endpoint\"\n",
    "\n",
    "# Machine configuration\n",
    "MACHINE_TYPE = 'n1-standard-4'\n",
    "MIN_REPLICA_COUNT = 1\n",
    "MAX_REPLICA_COUNT = 4\n",
    "\n",
    "# Container image for PyTorch CPU serving\n",
    "# See: https://cloud.google.com/vertex-ai/docs/predictions/pre-built-containers#pytorch\n",
    "DEPLOY_IMAGE = 'us-docker.pkg.dev/vertex-ai/prediction/pytorch-cpu.1-13:latest'\n",
    "\n",
    "# Local path to .mar file (from pytorch-autoencoder notebook)\n",
    "LOCAL_MAR_PATH = '../files/pytorch-autoencoder/pytorch_autoencoder.mar'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vars_auto",
   "metadata": {},
   "source": [
    "### Variables - Auto Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "vars_auto_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Google Cloud Project Information\n",
      "==================================================\n",
      "PROJECT_ID     = statmike-mlops-349915\n",
      "PROJECT_NUMBER = 1026793852137\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "PROJECT_ID = subprocess.run(['gcloud', 'config', 'get-value', 'project'], capture_output=True, text=True, check=True).stdout.strip()\n",
    "PROJECT_NUMBER = subprocess.run(['gcloud', 'projects', 'describe', PROJECT_ID, '--format=value(projectNumber)'], capture_output=True, text=True, check=True).stdout.strip()\n",
    "\n",
    "print(f\"\\n{'='*50}\\nGoogle Cloud Project Information\\n{'='*50}\\nPROJECT_ID     = {PROJECT_ID}\\nPROJECT_NUMBER = {PROJECT_NUMBER}\\n{'='*50}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "configs",
   "metadata": {},
   "source": [
    "### Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "configs_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket: gs://statmike-mlops-349915\n",
      "Model artifacts: gs://statmike-mlops-349915/frameworks/pytorch-autoencoder\n",
      "MAR file will be at: gs://statmike-mlops-349915/frameworks/pytorch-autoencoder/model.mar\n"
     ]
    }
   ],
   "source": [
    "# GCS configuration\n",
    "BUCKET_NAME = PROJECT_ID\n",
    "BUCKET_URI = f\"gs://{BUCKET_NAME}\"\n",
    "MODEL_ARTIFACT_DIR = f\"{BUCKET_URI}/{SERIES}/{EXPERIMENT}\"\n",
    "\n",
    "# Model artifact in GCS (pre-built container expects \"model.mar\")\n",
    "GCS_MAR_PATH = f\"{MODEL_ARTIFACT_DIR}/model.mar\"\n",
    "\n",
    "print(f\"Bucket: {BUCKET_URI}\")\n",
    "print(f\"Model artifacts: {MODEL_ARTIFACT_DIR}\")\n",
    "print(f\"MAR file will be at: {GCS_MAR_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "init_vertex",
   "metadata": {},
   "source": [
    "### Initialize Vertex AI SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "init_vertex_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Vertex AI initialized\n",
      "   Project: statmike-mlops-349915\n",
      "   Location: us-central1\n",
      "   Staging bucket: gs://statmike-mlops-349915\n"
     ]
    }
   ],
   "source": [
    "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_URI)\n",
    "\n",
    "print(f\"✅ Vertex AI initialized\")\n",
    "print(f\"   Project: {PROJECT_ID}\")\n",
    "print(f\"   Location: {REGION}\")\n",
    "print(f\"   Staging bucket: {BUCKET_URI}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gcs_upload",
   "metadata": {},
   "source": [
    "---\n",
    "## Upload Model to Cloud Storage\n",
    "\n",
    "Vertex AI requires model artifacts to be stored in Cloud Storage. We'll upload the .mar file created in the training notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "check_mar",
   "metadata": {},
   "source": [
    "### Check for Local .mar File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "check_mar_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Found .mar file: ../files/pytorch-autoencoder/pytorch_autoencoder.mar\n",
      "   Size: 29,851 bytes (29.15 KB)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "if os.path.exists(LOCAL_MAR_PATH):\n",
    "    file_size = os.path.getsize(LOCAL_MAR_PATH)\n",
    "    print(f\"✅ Found .mar file: {LOCAL_MAR_PATH}\")\n",
    "    print(f\"   Size: {file_size:,} bytes ({file_size / 1024:.2f} KB)\")\n",
    "else:\n",
    "    print(f\"❌ .mar file not found at: {LOCAL_MAR_PATH}\")\n",
    "    print(\"   Please run the pytorch-autoencoder.ipynb notebook first to create the .mar file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "create_bucket",
   "metadata": {},
   "source": [
    "### Create GCS Bucket (if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "create_bucket_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Bucket already exists: gs://statmike-mlops-349915\n"
     ]
    }
   ],
   "source": [
    "storage_client = storage.Client(project=PROJECT_ID)\n",
    "\n",
    "try:\n",
    "    bucket = storage_client.get_bucket(BUCKET_NAME)\n",
    "    print(f\"✅ Bucket already exists: {BUCKET_URI}\")\n",
    "except:\n",
    "    bucket = storage_client.create_bucket(BUCKET_NAME, location=REGION)\n",
    "    print(f\"✅ Created new bucket: {BUCKET_URI}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "upload_mar",
   "metadata": {},
   "source": [
    "### Upload .mar File to GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "upload_mar_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded .mar file to: gs://statmike-mlops-349915/frameworks/pytorch-autoencoder/model.mar\n",
      "   GCS size: 29,851 bytes (29.15 KB)\n",
      "   Note: Renamed from pytorch_autoencoder.mar to model.mar (required by pre-built container)\n"
     ]
    }
   ],
   "source": [
    "# Upload the .mar file (rename to model.mar for pre-built container)\n",
    "blob = bucket.blob(f\"{SERIES}/{EXPERIMENT}/model.mar\")\n",
    "blob.upload_from_filename(LOCAL_MAR_PATH)\n",
    "\n",
    "print(f\"✅ Uploaded .mar file to: {GCS_MAR_PATH}\")\n",
    "print(f\"   GCS size: {blob.size:,} bytes ({blob.size / 1024:.2f} KB)\")\n",
    "print(f\"   Note: Renamed from {os.path.basename(LOCAL_MAR_PATH)} to model.mar (required by pre-built container)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model_registry",
   "metadata": {},
   "source": [
    "---\n",
    "## Register Model in Vertex AI Model Registry\n",
    "\n",
    "Upload the model to Vertex AI Model Registry using a pre-built PyTorch serving container.\n",
    "\n",
    "### Pre-built Container Information\n",
    "\n",
    "We are using Google's pre-built PyTorch container which includes:\n",
    "- **TorchServe**: Production-ready PyTorch model server\n",
    "- **Automatic .mar handling**: Container knows how to load and serve .mar files\n",
    "- **Health checks**: Built-in endpoints for monitoring\n",
    "- **Request handling**: Automatic JSON to Tensor conversion\n",
    "\n",
    "**Container used**: `us-docker.pkg.dev/vertex-ai/prediction/pytorch-cpu.1-13:latest`\n",
    "\n",
    "### CPU vs GPU Deployment\n",
    "\n",
    "**For this notebook, we are using CPU** because:\n",
    "- ✅ Our autoencoder is small (~1,362 parameters)\n",
    "- ✅ Inference is very fast on CPU\n",
    "- ✅ CPU is more cost-effective for small models\n",
    "- ✅ No GPU transfer overhead\n",
    "\n",
    "**When to use GPU:**\n",
    "- Large models (millions of parameters)\n",
    "- Batch inference with many instances\n",
    "- Complex models (transformers, large CNNs)\n",
    "- High throughput requirements\n",
    "\n",
    "**To switch to GPU deployment:**\n",
    "```python\n",
    "# Change these variables:\n",
    "DEPLOY_IMAGE = \"us-docker.pkg.dev/vertex-ai/prediction/pytorch-gpu.1-13:latest\"\n",
    "MACHINE_TYPE = \"n1-standard-4\"\n",
    "ACCELERATOR_TYPE = \"NVIDIA_TESLA_L4\"\n",
    "ACCELERATOR_COUNT = 1\n",
    "```\n",
    "\n",
    "See [available GPU types](https://cloud.google.com/vertex-ai/docs/predictions/configure-compute#gpu-config) for more options."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "check_model",
   "metadata": {},
   "source": [
    "### Check for Existing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "check_model_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No existing models found with name pytorch-autoencoder\n"
     ]
    }
   ],
   "source": [
    "# List existing models\n",
    "models = aiplatform.Model.list(filter=f\"display_name={MODEL_DISPLAY_NAME}\")\n",
    "\n",
    "if models:\n",
    "    print(f\"Found {len(models)} existing model(s) with name {MODEL_DISPLAY_NAME}:\")\n",
    "    for model in models:\n",
    "        print(f\"  - {model.resource_name}\")\n",
    "        print(f\"    Created: {model.create_time}\")\n",
    "        print(f\"    Version: {model.version_id}\")\n",
    "else:\n",
    "    print(f\"No existing models found with name {MODEL_DISPLAY_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "upload_model",
   "metadata": {},
   "source": [
    "### Upload Model to Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "upload_model_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Model\n",
      "Create Model backing LRO: projects/1026793852137/locations/us-central1/models/3963725124481318912/operations/5977118330132103168\n",
      "Model created. Resource name: projects/1026793852137/locations/us-central1/models/3963725124481318912@1\n",
      "To use this Model in another session:\n",
      "model = aiplatform.Model('projects/1026793852137/locations/us-central1/models/3963725124481318912@1')\n",
      "✅ Model uploaded to registry:\n",
      "   Resource name: projects/1026793852137/locations/us-central1/models/3963725124481318912\n",
      "   Display name: pytorch-autoencoder\n",
      "   Artifact URI: gs://statmike-mlops-349915/frameworks/pytorch-autoencoder\n"
     ]
    }
   ],
   "source": [
    "# Check if model already exists\n",
    "existing_models = aiplatform.Model.list(filter=f\"display_name={MODEL_DISPLAY_NAME}\")\n",
    "\n",
    "if existing_models:\n",
    "    model = existing_models[0]\n",
    "    print(f\"✅ Using existing model:\")\n",
    "    print(f\"   Resource name: {model.resource_name}\")\n",
    "    print(f\"   Display name: {model.display_name}\")\n",
    "    print(f\"   Version: {model.version_id}\")\n",
    "else:\n",
    "    # Upload model\n",
    "    model = aiplatform.Model.upload(\n",
    "        display_name=MODEL_DISPLAY_NAME,\n",
    "        artifact_uri=MODEL_ARTIFACT_DIR,\n",
    "        serving_container_image_uri=DEPLOY_IMAGE,\n",
    "        # Note: No MODEL_NAME env var needed - pre-built container auto-detects from model.mar\n",
    "        serving_container_ports=[7080],\n",
    "        description=\"PyTorch autoencoder for anomaly detection (fraud transactions)\"\n",
    "    )\n",
    "\n",
    "    print(f\"✅ Model uploaded to registry:\")\n",
    "    print(f\"   Resource name: {model.resource_name}\")\n",
    "    print(f\"   Display name: {model.display_name}\")\n",
    "    print(f\"   Artifact URI: {model.uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "endpoint_deploy",
   "metadata": {},
   "source": [
    "---\n",
    "## Deploy Model to Endpoint\n",
    "\n",
    "Create a Vertex AI Endpoint and deploy the model for online predictions.\n",
    "\n",
    "### Endpoint Configuration\n",
    "\n",
    "- **Machine type**: `n1-standard-4` (4 vCPUs, 15 GB memory)\n",
    "- **Autoscaling**: Min 1 replica, Max 4 replicas\n",
    "- **Traffic split**: 100% to this model\n",
    "\n",
    "### Deployment Time\n",
    "\n",
    "Deployment typically takes 10-15 minutes as Vertex AI:\n",
    "1. Creates the endpoint infrastructure\n",
    "2. Pulls the pre-built container image\n",
    "3. Downloads the .mar file from GCS\n",
    "4. Initializes TorchServe\n",
    "5. Performs health checks\n",
    "6. Marks the endpoint as ready"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "check_endpoint",
   "metadata": {},
   "source": [
    "### Check for Existing Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "check_endpoint_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No existing endpoints found with name pytorch-autoencoder-endpoint\n"
     ]
    }
   ],
   "source": [
    "# List existing endpoints\n",
    "endpoints = aiplatform.Endpoint.list(filter=f\"display_name={ENDPOINT_DISPLAY_NAME}\")\n",
    "\n",
    "if endpoints:\n",
    "    print(f\"Found {len(endpoints)} existing endpoint(s) with name {ENDPOINT_DISPLAY_NAME}:\")\n",
    "    for ep in endpoints:\n",
    "        print(f\"  - {ep.resource_name}\")\n",
    "        print(f\"    Created: {ep.create_time}\")\n",
    "else:\n",
    "    print(f\"No existing endpoints found with name {ENDPOINT_DISPLAY_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "create_endpoint",
   "metadata": {},
   "source": [
    "### Create Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "create_endpoint_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Endpoint\n",
      "Create Endpoint backing LRO: projects/1026793852137/locations/us-central1/endpoints/2741468416626917376/operations/890302551017127936\n",
      "Endpoint created. Resource name: projects/1026793852137/locations/us-central1/endpoints/2741468416626917376\n",
      "To use this Endpoint in another session:\n",
      "endpoint = aiplatform.Endpoint('projects/1026793852137/locations/us-central1/endpoints/2741468416626917376')\n",
      "✅ Endpoint created:\n",
      "   Resource name: projects/1026793852137/locations/us-central1/endpoints/2741468416626917376\n",
      "   Display name: pytorch-autoencoder-endpoint\n"
     ]
    }
   ],
   "source": [
    "# Check if endpoint already exists\n",
    "existing_endpoints = aiplatform.Endpoint.list(filter=f\"display_name={ENDPOINT_DISPLAY_NAME}\")\n",
    "\n",
    "if existing_endpoints:\n",
    "    endpoint = existing_endpoints[0]\n",
    "    print(f\"✅ Using existing endpoint:\")\n",
    "    print(f\"   Resource name: {endpoint.resource_name}\")\n",
    "    print(f\"   Display name: {endpoint.display_name}\")\n",
    "else:\n",
    "    # Create endpoint\n",
    "    endpoint = aiplatform.Endpoint.create(\n",
    "        display_name=ENDPOINT_DISPLAY_NAME,\n",
    "        description=\"Endpoint for PyTorch autoencoder anomaly detection\"\n",
    "    )\n",
    "\n",
    "    print(f\"✅ Endpoint created:\")\n",
    "    print(f\"   Resource name: {endpoint.resource_name}\")\n",
    "    print(f\"   Display name: {endpoint.display_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deploy_model",
   "metadata": {},
   "source": [
    "### Deploy Model to Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deploy_model_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deploying model to endpoint (this will take 10-15 minutes)...\n",
      "You can monitor progress in the Cloud Console:\n",
      "https://console.cloud.google.com/vertex-ai/endpoints/2741468416626917376?project=statmike-mlops-349915\n",
      "Deploying Model projects/1026793852137/locations/us-central1/models/3963725124481318912 to Endpoint : projects/1026793852137/locations/us-central1/endpoints/2741468416626917376\n",
      "Deploy Endpoint model backing LRO: projects/1026793852137/locations/us-central1/endpoints/2741468416626917376/operations/4839959424221052928\n"
     ]
    }
   ],
   "source": [
    "# Check if model is already deployed to this endpoint\n",
    "deployed_models = endpoint.list_models()\n",
    "model_already_deployed = any(dm.model == model.resource_name for dm in deployed_models)\n",
    "\n",
    "if model_already_deployed:\n",
    "    print(f\"✅ Model already deployed to endpoint:\")\n",
    "    print(f\"   Endpoint: {endpoint.display_name}\")\n",
    "    print(f\"   Model: {model.display_name}\")\n",
    "    print(f\"   Skipping redeployment\")\n",
    "else:\n",
    "    # Deploy model to endpoint\n",
    "    print(\"Deploying model to endpoint (this will take 10-15 minutes)...\")\n",
    "    print(\"You can monitor progress in the Cloud Console:\")\n",
    "    print(f\"https://console.cloud.google.com/vertex-ai/endpoints/{endpoint.name.split('/')[-1]}?project={PROJECT_ID}\")\n",
    "\n",
    "    endpoint.deploy(\n",
    "        model=model,\n",
    "        deployed_model_display_name=MODEL_DISPLAY_NAME,\n",
    "        machine_type=MACHINE_TYPE,\n",
    "        min_replica_count=MIN_REPLICA_COUNT,\n",
    "        max_replica_count=MAX_REPLICA_COUNT,\n",
    "        traffic_percentage=100,\n",
    "        sync=True\n",
    "    )\n",
    "\n",
    "    print(f\"\\n✅ Model deployed successfully!\")\n",
    "    print(f\"   Endpoint: {endpoint.display_name}\")\n",
    "    print(f\"   Resource name: {endpoint.resource_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "test_data",
   "metadata": {},
   "source": [
    "---\n",
    "## Prepare Test Data\n",
    "\n",
    "Retrieve test records from the same BigQuery source used in training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "import_bq",
   "metadata": {},
   "source": [
    "### Import BigQuery Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "import_bq_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "import pandas as pd\n",
    "\n",
    "bq = bigquery.Client(project=PROJECT_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "get_test_data",
   "metadata": {},
   "source": [
    "### Retrieve Test Instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "get_test_data_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 3 test instances\n",
      "\n",
      "Features: ['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>122959.0</td>\n",
       "      <td>-1.327297</td>\n",
       "      <td>0.422904</td>\n",
       "      <td>1.617505</td>\n",
       "      <td>2.291196</td>\n",
       "      <td>2.375055</td>\n",
       "      <td>0.411735</td>\n",
       "      <td>0.213517</td>\n",
       "      <td>0.424743</td>\n",
       "      <td>-1.809624</td>\n",
       "      <td>...</td>\n",
       "      <td>0.068572</td>\n",
       "      <td>-0.337966</td>\n",
       "      <td>-1.461959</td>\n",
       "      <td>0.192604</td>\n",
       "      <td>0.068281</td>\n",
       "      <td>-0.245725</td>\n",
       "      <td>-0.697654</td>\n",
       "      <td>0.038216</td>\n",
       "      <td>0.150059</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>122312.0</td>\n",
       "      <td>-1.988557</td>\n",
       "      <td>-0.720301</td>\n",
       "      <td>0.863204</td>\n",
       "      <td>3.114494</td>\n",
       "      <td>1.847474</td>\n",
       "      <td>0.255881</td>\n",
       "      <td>0.580362</td>\n",
       "      <td>-0.083756</td>\n",
       "      <td>-0.939044</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.857829</td>\n",
       "      <td>-0.620289</td>\n",
       "      <td>-1.075636</td>\n",
       "      <td>1.564951</td>\n",
       "      <td>0.546312</td>\n",
       "      <td>-0.548531</td>\n",
       "      <td>-0.746620</td>\n",
       "      <td>-0.748016</td>\n",
       "      <td>0.410640</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>119592.0</td>\n",
       "      <td>2.139741</td>\n",
       "      <td>0.245651</td>\n",
       "      <td>-2.654856</td>\n",
       "      <td>0.178287</td>\n",
       "      <td>1.336991</td>\n",
       "      <td>-0.724664</td>\n",
       "      <td>0.906032</td>\n",
       "      <td>-0.436125</td>\n",
       "      <td>-0.528015</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.160988</td>\n",
       "      <td>0.296681</td>\n",
       "      <td>1.036285</td>\n",
       "      <td>-0.216033</td>\n",
       "      <td>0.345316</td>\n",
       "      <td>0.747103</td>\n",
       "      <td>0.700184</td>\n",
       "      <td>-0.123739</td>\n",
       "      <td>-0.099989</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Time        V1        V2        V3        V4        V5        V6  \\\n",
       "0  122959.0 -1.327297  0.422904  1.617505  2.291196  2.375055  0.411735   \n",
       "1  122312.0 -1.988557 -0.720301  0.863204  3.114494  1.847474  0.255881   \n",
       "2  119592.0  2.139741  0.245651 -2.654856  0.178287  1.336991 -0.724664   \n",
       "\n",
       "         V7        V8        V9  ...       V20       V21       V22       V23  \\\n",
       "0  0.213517  0.424743 -1.809624  ...  0.068572 -0.337966 -1.461959  0.192604   \n",
       "1  0.580362 -0.083756 -0.939044  ... -0.857829 -0.620289 -1.075636  1.564951   \n",
       "2  0.906032 -0.436125 -0.528015  ... -0.160988  0.296681  1.036285 -0.216033   \n",
       "\n",
       "        V24       V25       V26       V27       V28  Amount  \n",
       "0  0.068281 -0.245725 -0.697654  0.038216  0.150059     0.0  \n",
       "1  0.546312 -0.548531 -0.746620 -0.748016  0.410640     0.0  \n",
       "2  0.345316  0.747103  0.700184 -0.123739 -0.099989     0.0  \n",
       "\n",
       "[3 rows x 30 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BigQuery source (same as training notebook)\n",
    "BQ_PROJECT = PROJECT_ID\n",
    "BQ_DATASET = SERIES.replace('-', '_')\n",
    "BQ_TABLE = SERIES\n",
    "\n",
    "# Get a few test instances\n",
    "query = f\"\"\"\n",
    "SELECT * EXCEPT(splits, transaction_id, Class)\n",
    "FROM `{BQ_PROJECT}.{BQ_DATASET}.{BQ_TABLE}`\n",
    "WHERE splits = \"TEST\" AND Class = 0\n",
    "LIMIT 3\n",
    "\"\"\"\n",
    "\n",
    "test_df = bq.query(query).to_dataframe()\n",
    "print(f\"Retrieved {len(test_df)} test instances\")\n",
    "print(f\"\\nFeatures: {list(test_df.columns)}\")\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "format_data",
   "metadata": {},
   "source": [
    "### Format Data for Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "format_data_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared 3 instances for prediction\n",
      "\n",
      "First instance (30 features):\n",
      "[122959.0, -1.3272968090323798, 0.422904408477484, 1.61750487263453, 2.29119608568876, 2.37505507675092, 0.411734608148703, 0.213516912086352, 0.42474305658616796, -1.8096243240512202, 0.563424163577428, -0.215715613427969, 0.255745374601212, 0.14372086809316198, 0.0664174538762353, -2.19269325676012, 1.5000437460373202, -1.48305134433897, -0.0973547276746839, -1.36598161864692, 0.0685717012577449, -0.337965984584885, -1.4619590513124, 0.19260414415618501, 0.0682811106065862, -0.24572504736969603, -0.697654195893893, 0.0382157166420934, 0.15005927811162398, 0.0]\n"
     ]
    }
   ],
   "source": [
    "# Convert to list of lists (format expected by endpoint)\n",
    "test_instances = test_df.values.tolist()\n",
    "\n",
    "print(f\"Prepared {len(test_instances)} instances for prediction\")\n",
    "print(f\"\\nFirst instance (30 features):\")\n",
    "print(test_instances[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "predictions",
   "metadata": {},
   "source": [
    "---\n",
    "## Make Online Predictions\n",
    "\n",
    "Test the deployed model using both the Vertex AI SDK and direct REST API calls."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sdk_pred",
   "metadata": {},
   "source": [
    "### Method 1: Vertex AI SDK\n",
    "\n",
    "The simplest way to make predictions using the Python SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "sdk_pred_code",
   "metadata": {},
   "outputs": [
    {
     "ename": "ServiceUnavailable",
     "evalue": "503 {\n  \"code\": 503,\n  \"type\": \"InternalServerException\",\n  \"message\": \"Prediction failed\"\n}\n [detail: \"[ORIGINAL ERROR] generic::unavailable: {\\n  \\\"code\\\": 503,\\n  \\\"type\\\": \\\"InternalServerException\\\",\\n  \\\"message\\\": \\\"Prediction failed\\\"\\n}\\n [google.rpc.error_details_ext] { message: \\\"{\\\\n  \\\\\\\"code\\\\\\\": 503,\\\\n  \\\\\\\"type\\\\\\\": \\\\\\\"InternalServerException\\\\\\\",\\\\n  \\\\\\\"message\\\\\\\": \\\\\\\"Prediction failed\\\\\\\"\\\\n}\\\\n\\\" details { [type.googleapis.com/google.rpc.Status] { details { [type.googleapis.com/google.api.HttpBody] { content_type: \\\"application/json\\\" data: \\\"{\\\\n  \\\\\\\"code\\\\\\\": 503,\\\\n  \\\\\\\"type\\\\\\\": \\\\\\\"InternalServerException\\\\\\\",\\\\n  \\\\\\\"message\\\\\\\": \\\\\\\"Prediction failed\\\\\\\"\\\\n}\\\\n\\\" extensions { [type.googleapis.com/google.rpc.context.HttpHeaderContext] { response_code: 503 } } } } } } }\"\n]",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31m_InactiveRpcError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/frameworks-pytorch-0KVJlKeQ-py3.13/lib/python3.13/site-packages/google/api_core/grpc_helpers.py:75\u001b[39m, in \u001b[36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     74\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m75\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcallable_\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     76\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m grpc.RpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/frameworks-pytorch-0KVJlKeQ-py3.13/lib/python3.13/site-packages/grpc/_interceptor.py:276\u001b[39m, in \u001b[36m_UnaryUnaryMultiCallable.__call__\u001b[39m\u001b[34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[39m\n\u001b[32m    267\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\n\u001b[32m    268\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    269\u001b[39m     request: Any,\n\u001b[32m   (...)\u001b[39m\u001b[32m    274\u001b[39m     compression: Optional[grpc.Compression] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    275\u001b[39m ) -> Any:\n\u001b[32m--> \u001b[39m\u001b[32m276\u001b[39m     response, ignored_call = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_with_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    277\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    278\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    279\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    280\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcredentials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcredentials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    281\u001b[39m \u001b[43m        \u001b[49m\u001b[43mwait_for_ready\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwait_for_ready\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    282\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    283\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    284\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/frameworks-pytorch-0KVJlKeQ-py3.13/lib/python3.13/site-packages/grpc/_interceptor.py:331\u001b[39m, in \u001b[36m_UnaryUnaryMultiCallable._with_call\u001b[39m\u001b[34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[39m\n\u001b[32m    328\u001b[39m call = \u001b[38;5;28mself\u001b[39m._interceptor.intercept_unary_unary(\n\u001b[32m    329\u001b[39m     continuation, client_call_details, request\n\u001b[32m    330\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m331\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, call\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/frameworks-pytorch-0KVJlKeQ-py3.13/lib/python3.13/site-packages/grpc/_channel.py:438\u001b[39m, in \u001b[36m_InactiveRpcError.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    437\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"See grpc.Future.result.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m438\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/frameworks-pytorch-0KVJlKeQ-py3.13/lib/python3.13/site-packages/grpc/_interceptor.py:314\u001b[39m, in \u001b[36m_UnaryUnaryMultiCallable._with_call.<locals>.continuation\u001b[39m\u001b[34m(new_details, request)\u001b[39m\n\u001b[32m    313\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m314\u001b[39m     response, call = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_thunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_method\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwith_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    315\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    316\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    317\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    318\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcredentials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_credentials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    319\u001b[39m \u001b[43m        \u001b[49m\u001b[43mwait_for_ready\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_wait_for_ready\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    320\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_compression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    321\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    322\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _UnaryOutcome(response, call)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/frameworks-pytorch-0KVJlKeQ-py3.13/lib/python3.13/site-packages/grpc/_channel.py:1180\u001b[39m, in \u001b[36m_UnaryUnaryMultiCallable.with_call\u001b[39m\u001b[34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[39m\n\u001b[32m   1177\u001b[39m state, call = \u001b[38;5;28mself\u001b[39m._blocking(\n\u001b[32m   1178\u001b[39m     request, timeout, metadata, credentials, wait_for_ready, compression\n\u001b[32m   1179\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1180\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_end_unary_response_blocking\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcall\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/frameworks-pytorch-0KVJlKeQ-py3.13/lib/python3.13/site-packages/grpc/_channel.py:996\u001b[39m, in \u001b[36m_end_unary_response_blocking\u001b[39m\u001b[34m(state, call, with_call, deadline)\u001b[39m\n\u001b[32m    995\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m state.response\n\u001b[32m--> \u001b[39m\u001b[32m996\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m _InactiveRpcError(state)\n",
      "\u001b[31m_InactiveRpcError\u001b[39m: <_InactiveRpcError of RPC that terminated with:\n\tstatus = StatusCode.UNAVAILABLE\n\tdetails = \"{\n  \"code\": 503,\n  \"type\": \"InternalServerException\",\n  \"message\": \"Prediction failed\"\n}\n\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:173.194.194.95:443 {grpc_status:14, grpc_message:\"{\\n  \\\"code\\\": 503,\\n  \\\"type\\\": \\\"InternalServerException\\\",\\n  \\\"message\\\": \\\"Prediction failed\\\"\\n}\\n\"}\"\n>",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mServiceUnavailable\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Make prediction using SDK\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m response = \u001b[43mendpoint\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstances\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtest_instances\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m✅ Received predictions for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(response.predictions)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m instances\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mPrediction structure (first instance):\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/frameworks-pytorch-0KVJlKeQ-py3.13/lib/python3.13/site-packages/google/cloud/aiplatform/models.py:2540\u001b[39m, in \u001b[36mEndpoint.predict\u001b[39m\u001b[34m(self, instances, parameters, timeout, use_raw_predict, use_dedicated_endpoint)\u001b[39m\n\u001b[32m   2525\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m Prediction(\n\u001b[32m   2526\u001b[39m         predictions=json_response[\u001b[33m\"\u001b[39m\u001b[33mpredictions\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m   2527\u001b[39m         metadata=json_response.get(\u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m   (...)\u001b[39m\u001b[32m   2536\u001b[39m         ),\n\u001b[32m   2537\u001b[39m     )\n\u001b[32m   2539\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dedicated_endpoint_enabled:\n\u001b[32m-> \u001b[39m\u001b[32m2540\u001b[39m     prediction_response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_prediction_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2541\u001b[39m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_gca_resource\u001b[49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2542\u001b[39m \u001b[43m        \u001b[49m\u001b[43minstances\u001b[49m\u001b[43m=\u001b[49m\u001b[43minstances\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2543\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2544\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2545\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2546\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m prediction_response._pb.metadata:\n\u001b[32m   2547\u001b[39m         metadata = json_format.MessageToDict(prediction_response._pb.metadata)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/frameworks-pytorch-0KVJlKeQ-py3.13/lib/python3.13/site-packages/google/cloud/aiplatform_v1/services/prediction_service/client.py:994\u001b[39m, in \u001b[36mPredictionServiceClient.predict\u001b[39m\u001b[34m(self, request, endpoint, instances, parameters, retry, timeout, metadata)\u001b[39m\n\u001b[32m    991\u001b[39m \u001b[38;5;28mself\u001b[39m._validate_universe_domain()\n\u001b[32m    993\u001b[39m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m994\u001b[39m response = \u001b[43mrpc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    995\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    996\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretry\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    997\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    998\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    999\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1001\u001b[39m \u001b[38;5;66;03m# Done; return the response.\u001b[39;00m\n\u001b[32m   1002\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/frameworks-pytorch-0KVJlKeQ-py3.13/lib/python3.13/site-packages/google/api_core/gapic_v1/method.py:131\u001b[39m, in \u001b[36m_GapicCallable.__call__\u001b[39m\u001b[34m(self, timeout, retry, compression, *args, **kwargs)\u001b[39m\n\u001b[32m    128\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compression \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    129\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mcompression\u001b[39m\u001b[33m\"\u001b[39m] = compression\n\u001b[32m--> \u001b[39m\u001b[32m131\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/frameworks-pytorch-0KVJlKeQ-py3.13/lib/python3.13/site-packages/google/api_core/grpc_helpers.py:77\u001b[39m, in \u001b[36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     75\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m callable_(*args, **kwargs)\n\u001b[32m     76\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m grpc.RpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions.from_grpc_error(exc) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n",
      "\u001b[31mServiceUnavailable\u001b[39m: 503 {\n  \"code\": 503,\n  \"type\": \"InternalServerException\",\n  \"message\": \"Prediction failed\"\n}\n [detail: \"[ORIGINAL ERROR] generic::unavailable: {\\n  \\\"code\\\": 503,\\n  \\\"type\\\": \\\"InternalServerException\\\",\\n  \\\"message\\\": \\\"Prediction failed\\\"\\n}\\n [google.rpc.error_details_ext] { message: \\\"{\\\\n  \\\\\\\"code\\\\\\\": 503,\\\\n  \\\\\\\"type\\\\\\\": \\\\\\\"InternalServerException\\\\\\\",\\\\n  \\\\\\\"message\\\\\\\": \\\\\\\"Prediction failed\\\\\\\"\\\\n}\\\\n\\\" details { [type.googleapis.com/google.rpc.Status] { details { [type.googleapis.com/google.api.HttpBody] { content_type: \\\"application/json\\\" data: \\\"{\\\\n  \\\\\\\"code\\\\\\\": 503,\\\\n  \\\\\\\"type\\\\\\\": \\\\\\\"InternalServerException\\\\\\\",\\\\n  \\\\\\\"message\\\\\\\": \\\\\\\"Prediction failed\\\\\\\"\\\\n}\\\\n\\\" extensions { [type.googleapis.com/google.rpc.context.HttpHeaderContext] { response_code: 503 } } } } } } }\"\n]"
     ]
    }
   ],
   "source": [
    "# Make prediction using SDK\n",
    "response = endpoint.predict(instances=test_instances)\n",
    "\n",
    "print(f\"✅ Received predictions for {len(response.predictions)} instances\")\n",
    "print(f\"\\nPrediction structure (first instance):\")\n",
    "\n",
    "# The response contains the full dictionary output from our model\n",
    "first_prediction = response.predictions[0]\n",
    "print(f\"\\nAvailable keys: {list(first_prediction.keys())}\")\n",
    "print(f\"\\nAnomaly score (denormalized_MAE): {first_prediction['denormalized_MAE']}\")\n",
    "print(f\"Encoded representation (latent space): {first_prediction['encoded']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "analyze_pred",
   "metadata": {},
   "source": [
    "### Analyze Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analyze_pred_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract key metrics from all predictions\n",
    "results = []\n",
    "for i, pred in enumerate(response.predictions):\n",
    "    results.append({\n",
    "        \"instance\": i,\n",
    "        \"anomaly_score\": pred[\"denormalized_MAE\"],\n",
    "        \"denorm_rmse\": pred[\"denormalized_RMSE\"],\n",
    "        \"denorm_mse\": pred[\"denormalized_MSE\"],\n",
    "        \"encoded\": pred[\"encoded\"]\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"Prediction Summary:\")\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rest_api",
   "metadata": {},
   "source": [
    "### Method 2: REST API with Requests\n",
    "\n",
    "Make predictions using direct HTTP calls. This is useful for:\n",
    "- Non-Python clients\n",
    "- Testing from other services\n",
    "- Understanding the raw API format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rest_api_setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import google.auth\n",
    "import google.auth.transport.requests\n",
    "\n",
    "# Get authentication token\n",
    "credentials, project = google.auth.default()\n",
    "auth_req = google.auth.transport.requests.Request()\n",
    "credentials.refresh(auth_req)\n",
    "access_token = credentials.token\n",
    "\n",
    "# Construct endpoint URL\n",
    "endpoint_url = f\"https://{REGION}-aiplatform.googleapis.com/v1/{endpoint.resource_name}:predict\"\n",
    "\n",
    "print(f\"Endpoint URL: {endpoint_url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rest_api_predict",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare request payload\n",
    "payload = {\n",
    "    \"instances\": test_instances\n",
    "}\n",
    "\n",
    "# Make REST API request\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {access_token}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "response = requests.post(endpoint_url, headers=headers, json=payload)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    rest_predictions = response.json()[\"predictions\"]\n",
    "    print(f\"✅ REST API prediction successful\")\n",
    "    print(f\"   Received {len(rest_predictions)} predictions\")\n",
    "    print(f\"\\nFirst prediction anomaly score: {rest_predictions[0]['denormalized_MAE']}\")\n",
    "else:\n",
    "    print(f\"❌ Error: {response.status_code}\")\n",
    "    print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compare_results",
   "metadata": {},
   "source": [
    "### Compare SDK vs REST API Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compare_results_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify both methods return the same results\n",
    "sdk_score = response.predictions[0][\"denormalized_MAE\"]\n",
    "rest_score = rest_predictions[0][\"denormalized_MAE\"]\n",
    "\n",
    "print(f\"SDK anomaly score:  {sdk_score}\")\n",
    "print(f\"REST anomaly score: {rest_score}\")\n",
    "print(f\"\\nResults match: {abs(sdk_score - rest_score) < 0.001}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cleanup",
   "metadata": {},
   "source": [
    "---\n",
    "## Clean Up Resources\n",
    "\n",
    "**Important**: Deployed endpoints incur charges even when not making predictions. Always undeploy and delete endpoints when done.\n",
    "\n",
    "### Cost Information\n",
    "\n",
    "With our configuration (`n1-standard-4`, min 1 replica):\n",
    "- **Hourly cost**: ~$0.20/hour (varies by region)\n",
    "- **Daily cost**: ~$4.80/day if left running\n",
    "- **Monthly cost**: ~$144/month\n",
    "\n",
    "See [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing#prediction-prices) for details.\n",
    "\n",
    "### Cleanup Options\n",
    "\n",
    "1. **Undeploy only**: Keeps endpoint but removes model (no charges)\n",
    "2. **Delete endpoint**: Removes everything (recommended for this demo)\n",
    "3. **Keep model in registry**: Model storage in GCS has minimal cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "undeploy",
   "metadata": {},
   "source": [
    "### Option 1: Undeploy Model (Keep Endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "undeploy_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Undeploy the model from the endpoint\n",
    "# endpoint.undeploy_all()\n",
    "# print(\"✅ Model undeployed from endpoint\")\n",
    "# print(\"   Endpoint still exists but serves no traffic\")\n",
    "print(\"Uncomment the code above to undeploy the model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "delete_endpoint",
   "metadata": {},
   "source": [
    "### Option 2: Delete Endpoint (Recommended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "delete_endpoint_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the endpoint (recommended to avoid charges)\n",
    "# endpoint.delete(force=True)\n",
    "# print(\"✅ Endpoint deleted\")\n",
    "# print(\"   All resources cleaned up\")\n",
    "print(\"Uncomment the code above to delete the endpoint\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "delete_model",
   "metadata": {},
   "source": [
    "### Option 3: Delete Model from Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "delete_model_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the model from Model Registry\n",
    "# model.delete()\n",
    "# print(\"✅ Model deleted from registry\")\n",
    "# print(\"   .mar file still exists in GCS\")\n",
    "print(\"Uncomment the code above to delete the model from registry\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verify_cleanup",
   "metadata": {},
   "source": [
    "### Verify Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verify_cleanup_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check remaining resources\n",
    "endpoints = aiplatform.Endpoint.list(filter=f\"display_name={ENDPOINT_DISPLAY_NAME}\")\n",
    "models = aiplatform.Model.list(filter=f\"display_name={MODEL_DISPLAY_NAME}\")\n",
    "\n",
    "print(f\"Endpoints with name {ENDPOINT_DISPLAY_NAME}: {len(endpoints)}\")\n",
    "print(f\"Models with name {MODEL_DISPLAY_NAME}: {len(models)}\")\n",
    "\n",
    "if endpoints:\n",
    "    print(\"\\n⚠️  Endpoint still exists - remember to delete to avoid charges\")\n",
    "else:\n",
    "    print(\"\\n✅ No active endpoints\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "In this notebook, you:\n",
    "\n",
    "✅ Uploaded a PyTorch .mar file to Cloud Storage\n",
    "\n",
    "✅ Registered the model in Vertex AI Model Registry\n",
    "\n",
    "✅ Deployed the model to a Vertex AI Endpoint\n",
    "\n",
    "✅ Made online predictions using both SDK and REST API\n",
    "\n",
    "✅ Analyzed anomaly scores from the autoencoder\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- **Batch Predictions**: Use Vertex AI Batch Prediction for processing large datasets\n",
    "- **Model Monitoring**: Set up alerts for prediction drift and anomalies\n",
    "- **A/B Testing**: Deploy multiple model versions and split traffic\n",
    "- **Dataflow Integration**: Use the model in streaming pipelines with Apache Beam RunInference\n",
    "\n",
    "### Related Notebooks\n",
    "\n",
    "- [PyTorch Autoencoder Training](../pytorch-autoencoder.ipynb) - Train the model\n",
    "- [Custom Container Deployment](./vertex-ai-endpoint-custom-container.ipynb) - Compare approaches\n",
    "\n",
    "### Resources\n",
    "\n",
    "- [Vertex AI Prediction Documentation](https://cloud.google.com/vertex-ai/docs/predictions/overview)\n",
    "- [Pre-built PyTorch Containers](https://cloud.google.com/vertex-ai/docs/predictions/pre-built-containers#pytorch)\n",
    "- [TorchServe Documentation](https://pytorch.org/serve/)\n",
    "- [Vertex AI Pricing](https://cloud.google.com/vertex-ai/pricing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f477398",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (frameworks-pytorch)",
   "language": "python",
   "name": "frameworks-pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
