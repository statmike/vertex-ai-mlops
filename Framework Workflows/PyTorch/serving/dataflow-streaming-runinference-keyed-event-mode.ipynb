{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1351230",
   "metadata": {},
   "source": [
    "![tracker](https://us-central1-vertex-ai-mlops-369716.cloudfunctions.net/pixel-tracking?path=statmike%2Fvertex-ai-mlops%2FFramework+Workflows%2FPyTorch%2Fserving&file=dataflow-streaming-runinference-keyed-event-mode.ipynb)\n",
    "<!--- header table --->\n",
    "<table align=\"left\">\n",
    "<tr>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%20Workflows/PyTorch/serving/dataflow-streaming-runinference-keyed-event-mode.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.svgrepo.com/download/217753/github.svg\" alt=\"GitHub logo\">\n",
    "      <br>View on<br>GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://colab.research.google.com/github/statmike/vertex-ai-mlops/blob/main/Framework%20Workflows/PyTorch/serving/dataflow-streaming-runinference-keyed-event-mode.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.gstatic.com/pantheon/images/bigquery/welcome_page/colab-logo.svg\" alt=\"Google Colaboratory logo\">\n",
    "      <br>Run in<br>Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https%3A%2F%2Fraw.githubusercontent.com%2Fstatmike%2Fvertex-ai-mlops%2Fmain%2FFramework%2520Workflows%2FPyTorch%2Fserving%2Fdataflow-streaming-runinference-keyed-event-mode.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\">\n",
    "      <br>Run in<br>Colab Enterprise\n",
    "    </a>\n",
    "  </td>      \n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/bigquery/import?url=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%20Workflows/PyTorch/serving/dataflow-streaming-runinference-keyed-event-mode.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.gstatic.com/images/branding/gcpiconscolors/bigquery/v1/32px.svg\" alt=\"BigQuery logo\">\n",
    "      <br>Open in<br>BigQuery Studio\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/statmike/vertex-ai-mlops/main/Framework%20Workflows/PyTorch/serving/dataflow-streaming-runinference-keyed-event-mode.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.gstatic.com/images/branding/gcpiconscolors/vertexai/v1/32px.svg\" alt=\"Vertex AI logo\">\n",
    "      <br>Open in<br>Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td colspan=\"5\" style=\"text-align: right\">\n",
    "    <b>Share This On: </b> \n",
    "    <a href=\"https://www.linkedin.com/sharing/share-offsite/?url=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%2520Workflows/PyTorch/serving/dataflow-streaming-runinference-keyed-event-mode.ipynb\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"Linkedin Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://reddit.com/submit?url=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%2520Workflows/PyTorch/serving/dataflow-streaming-runinference-keyed-event-mode.ipynb\"><img src=\"https://redditinc.com/hubfs/Reddit%20Inc/Brand/Reddit_Logo.png\" alt=\"Reddit Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://bsky.app/intent/compose?text=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%2520Workflows/PyTorch/serving/dataflow-streaming-runinference-keyed-event-mode.ipynb\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"BlueSky Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://twitter.com/intent/tweet?url=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%2520Workflows/PyTorch/serving/dataflow-streaming-runinference-keyed-event-mode.ipynb\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/5/5a/X_icon_2.svg\" alt=\"X (Twitter) Logo\" width=\"20px\"></a> \n",
    "  </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td colspan=\"5\" style=\"text-align: right\">\n",
    "    <b>Connect With Author On: </b> \n",
    "    <a href=\"https://www.linkedin.com/in/statmike\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"Linkedin Logo\" width=\"20px\"></a>\n",
    "    <a href=\"https://www.github.com/statmike\"><img src=\"https://www.svgrepo.com/download/217753/github.svg\" alt=\"GitHub Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://www.youtube.com/@statmike-channel\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/f/fd/YouTube_full-color_icon_%282024%29.svg\" alt=\"YouTube Logo\" width=\"20px\"></a>\n",
    "    <a href=\"https://bsky.app/profile/statmike.bsky.social\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"BlueSky Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://x.com/statmike\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/5/5a/X_icon_2.svg\" alt=\"X (Twitter) Logo\" width=\"20px\"></a>\n",
    "  </td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ep1wl4q7e6",
   "metadata": {},
   "source": [
    "# Dataflow Streaming Inference with RunInference + KeyedModelHandler + Event-Mode Model Hot-Swap\n",
    "\n",
    "This notebook demonstrates **ultra-low-latency streaming** inference using Dataflow with Apache Beam's `RunInference` transform, `KeyedModelHandler` for metadata passthrough, and **event-mode model hot-swap** via Pub/Sub.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "This workflow covers:\n",
    "\n",
    "1. **Create Custom ModelHandler**: Wrap PyTorch model with `PredictionResult` and `model_id` tracking\n",
    "2. **Use KeyedModelHandler**: Beam-native metadata passthrough via key serialization\n",
    "3. **Enable Event-Mode Model Hot-Swap**: Use `model_metadata_pcollection` to reload models at runtime\n",
    "4. **Run on Dataflow**: Execute continuous pipeline with model update capability\n",
    "5. **Trigger a Model Update**: Publish to Pub/Sub to swap models without restarting the pipeline\n",
    "6. **Monitor and Verify**: Confirm model swap via `model_id` column in BigQuery\n",
    "7. **Clean Up**: Stop streaming job to avoid ongoing charges\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Completed `dataflow-setup.ipynb` - This sets up:\n",
    "  - Model .pt file extracted from .mar and uploaded to GCS\n",
    "  - BigQuery tables created (including `pytorch_autoencoder_streaming_results_keyed`)\n",
    "  - Pub/Sub topics and subscriptions created (including model-update topic/subscription)\n",
    "\n",
    "## Relationship to Other Notebooks\n",
    "\n",
    "This notebook extends the **keyed RunInference** approach with runtime model hot-swap:\n",
    "\n",
    "```\n",
    "dataflow-streaming-runinference.ipynb                       EXISTING\n",
    "  - Explicit BatchElements + custom RunInferenceOnBatch DoFn\n",
    "  - 7 pipeline steps\n",
    "  - Metadata flows as (features_tensor, metadata_dict) tuples\n",
    "\n",
    "dataflow-streaming-runinference-keyed.ipynb                 KEYED\n",
    "  - RunInference + KeyedModelHandler (Beam-native)\n",
    "  - 5 pipeline steps (BatchElements + custom DoFn absorbed into RunInference)\n",
    "  - Metadata flows as JSON string key through KeyedModelHandler\n",
    "  - PredictionResult.model_id gives GCS path of model used\n",
    "  - Same behavior, same latency, more idiomatic Beam\n",
    "\n",
    "dataflow-streaming-runinference-keyed-event-mode.ipynb      THIS NOTEBOOK\n",
    "  - Everything above + model_metadata_pcollection parameter\n",
    "  - Pub/Sub side input triggers model hot-swap at runtime\n",
    "  - model_id in output changes when model swaps (= version tracking)\n",
    "  - One parameter addition from keyed notebook\n",
    "```\n",
    "\n",
    "## How Event-Mode Model Hot-Swap Works\n",
    "\n",
    "When a model update event is published to a dedicated Pub/Sub topic, all pipeline workers reload the model from the new GCS path **without restarting the pipeline**. The `model_id` field in the output automatically reflects which model produced each prediction, providing built-in version tracking.\n",
    "\n",
    "### Pipeline Architecture\n",
    "\n",
    "```\n",
    "                                     Pub/Sub Model Update Topic\n",
    "                                              |\n",
    "                                              v\n",
    "                                    Parse to ModelMetadata\n",
    "                                              |\n",
    "                                              v\n",
    "                                    GlobalWindow + trigger\n",
    "                                              |\n",
    "                                              v (side input via model_metadata_pcollection)\n",
    "Pub/Sub Data Input --> parse_json --> Window(1s) --> RunInference(KeyedModelHandler) --> format_result\n",
    "                       (key, tensor)                  - BatchElements internally          |\n",
    "                                                      - model swap on side input event    |\n",
    "                                                      - PredictionResult.model_id updates v\n",
    "                                                                                   BigQuery + Pub/Sub\n",
    "                                                                                   (model_id column\n",
    "                                                                                    tracks which model)\n",
    "```\n",
    "\n",
    "### What Happens During a Model Swap\n",
    "\n",
    "1. You publish a JSON message to the model update Pub/Sub topic with a new `model_path`\n",
    "2. The side input PCollection parses this into a `ModelMetadata` object\n",
    "3. `_RunInferenceDoFn` detects the new `model_id` differs from the current one\n",
    "4. Each worker calls `update_model_path()` + `load_model()` with a thread lock\n",
    "5. `self._torch_script_model_path` updates on the handler\n",
    "6. All subsequent `PredictionResult.model_id` values reflect the new path\n",
    "7. `format_result` writes the new path to the `model_id` column in BigQuery\n",
    "\n",
    "### Key Design Choices\n",
    "\n",
    "**1. GCS Path as Version Identifier**\n",
    "- Each model version lives at a distinct GCS path\n",
    "- No separate version number needed -- the path IS the version\n",
    "- Rollback = publish the old GCS path again\n",
    "\n",
    "**2. Worker Consistency During Transitions**\n",
    "- Different workers swap at slightly different times\n",
    "- Brief window where some workers use the old model, others the new one\n",
    "- The `model_id` column in every output row tells you exactly which model produced each prediction\n",
    "\n",
    "**3. Memory During Model Swap**\n",
    "- Two models briefly in memory during transition\n",
    "- For this autoencoder (~47KB) this is negligible\n",
    "- For larger models, worth considering worker memory allocation\n",
    "\n",
    "---\n",
    "\n",
    "## RunInference + KeyedModelHandler Benefits\n",
    "\n",
    "- **In-process**: Model loaded directly in workers (no network calls)\n",
    "- **Beam-native batching**: BatchElements absorbed into RunInference (fewer pipeline steps)\n",
    "- **Metadata passthrough**: KeyedModelHandler preserves metadata as JSON string key\n",
    "- **Model tracking**: `PredictionResult.model_id` gives GCS path of model used\n",
    "- **Event-mode hot-swap**: One parameter addition enables runtime model updates\n",
    "- **Ultra-low-latency**: Same ~40ms average latency as explicit approach\n",
    "- **Cost-effective**: Efficient batching reduces inference costs\n",
    "\n",
    "## Timing Expectations\n",
    "\n",
    "**Total time from start to results: ~8-10 minutes**\n",
    "\n",
    "1. **Start Dataflow job**: Instant\n",
    "2. **Wait for workers**: 3-5 minutes (worker provisioning)\n",
    "3. **Send test messages**: Instant\n",
    "4. **Wait for processing**: ~1-2 minutes (trigger + processing)\n",
    "5. **View results**: Check BigQuery and Pub/Sub\n",
    "6. **Trigger model swap**: Instant (publish to model update topic)\n",
    "\n",
    "## What This Pipeline Does\n",
    "\n",
    "1. Read transactions from Pub/Sub input topic\n",
    "2. Parse JSON and create (metadata_key, features_tensor) tuples\n",
    "3. Window data into 1-second windows (for watermark progression)\n",
    "4. Run inference via RunInference with KeyedModelHandler (batching handled internally)\n",
    "5. Monitor model update side input for hot-swap events\n",
    "6. Extract relevant outputs (score + embeddings + model_id)\n",
    "7. Write results to BigQuery (for analysis, including model version tracking)\n",
    "8. Publish to Pub/Sub output (for downstream systems)\n",
    "9. Job runs continuously until cancelled, accepting model updates at any time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58e8bb9",
   "metadata": {},
   "source": [
    "---\n",
    "## Environment Setup\n",
    "\n",
    "This section will authenticate your session, enable required Google Cloud APIs, and install necessary Python packages.\n",
    "\n",
    "**Package Installation Options (`REQ_TYPE`)**:\n",
    "- `PRIMARY`: Installs only the main packages. Faster, but pip resolves sub-dependencies which may result in different versions than development.\n",
    "- `ALL` (Default): Installs exact versions of all packages and dependencies. Best for perfectly reproducing the development environment.\n",
    "- `COLAB`: Installs a Colab-optimized list that excludes pre-installed packages like `ipython` and `ipykernel`.\n",
    "\n",
    "**Installation Tool Options (`INSTALL_TOOL`)**:\n",
    "- `pip` (Default): Uses pip for package installation. Standard Python package installer.\n",
    "- `uv`: Modern, fast Python package installer. Must be installed separately. See: https://github.com/astral-sh/uv\n",
    "- `poetry`: Dependency management tool. Requires running notebook in a poetry environment (`poetry shell` or `poetry run jupyter lab`). Uses `pyproject.toml` instead of requirements.txt.\n",
    "\n",
    "> **Note:** If running in Google Colab, the script will automatically detect this and set `REQ_TYPE = 'COLAB'` to prevent package conflicts, overriding any manual setting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2692647a",
   "metadata": {},
   "source": [
    "### Set Your Project ID\n",
    "\n",
    "**Action Required:** Replace the `PROJECT_ID` value below with your Google Cloud project ID before running this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a601990",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = 'statmike-mlops-349915' # replace with GCP project ID\n",
    "REQ_TYPE = 'ALL' # Specify PRIMARY or ALL or COLAB\n",
    "INSTALL_TOOL = 'poetry' # Specify pip, uv, or poetry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75112889",
   "metadata": {},
   "source": [
    "### Configuration\n",
    "\n",
    "This cell defines the requirements files and Google Cloud APIs needed for this notebook. Run as-is without modification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ff39312",
   "metadata": {},
   "outputs": [],
   "source": [
    "REQUIREMENTS_URL = 'https://raw.githubusercontent.com/statmike/vertex-ai-mlops/refs/heads/main/Framework%20Workflows/PyTorch/requirements.txt'\n",
    "\n",
    "REQUIRED_APIS = [\n",
    "    \"dataflow.googleapis.com\",\n",
    "    \"pubsub.googleapis.com\",\n",
    "    \"bigquery.googleapis.com\",\n",
    "    \"storage.googleapis.com\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481d3c9d",
   "metadata": {},
   "source": [
    "### Run Setup\n",
    "\n",
    "This cell downloads the centralized setup code and configures your environment. It will:\n",
    "- Authenticate your session with Google Cloud\n",
    "- Enable required APIs for this notebook\n",
    "- Install necessary Python packages\n",
    "- Display a setup summary with your project information\n",
    "\n",
    "> **Note:** In Colab, if packages are installed, the kernel will automatically restart. After restart, continue from the next cell without re-running earlier cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acb9666d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "PYTHON GCP ENVIRONMENT SETUP\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "AUTHENTICATION\n",
      "==================================================\n",
      "Checking for existing ADC...\n",
      "✅ Existing ADC found.\n",
      "✅ Project is correctly set to 'statmike-mlops-349915'.\n",
      "\n",
      "==================================================\n",
      "API CHECK & ENABLE\n",
      "==================================================\n",
      "✅ dataflow.googleapis.com is already enabled.\n",
      "✅ pubsub.googleapis.com is already enabled.\n",
      "✅ bigquery.googleapis.com is already enabled.\n",
      "✅ storage.googleapis.com is already enabled.\n",
      "\n",
      "==================================================\n",
      "PACKAGE MANAGEMENT\n",
      "==================================================\n",
      "Installation Tool: poetry\n",
      "✅ Found poetry at: /usr/local/google/home/statmike/.local/bin/poetry\n",
      "✅ Running in poetry environment: /usr/local/google/home/statmike/.cache/pypoetry/virtualenvs/frameworks-pytorch-0KVJlKeQ-py3.13\n",
      "ℹ️  Poetry mode: Installing from pyproject.toml (REQUIREMENTS_URL ignored)\n",
      "✅ Found pyproject.toml at: /usr/local/google/home/statmike/Git/vertex-ai-mlops/Framework Workflows/PyTorch/pyproject.toml\n",
      "   Changed working directory to: /usr/local/google/home/statmike/Git/vertex-ai-mlops/Framework Workflows/PyTorch\n",
      "Running poetry install...\n",
      "   Restored working directory to: /usr/local/google/home/statmike/Git/vertex-ai-mlops/Framework Workflows/PyTorch/serving\n",
      "✅ All packages are already installed and up to date.\n",
      "\n",
      "==================================================\n",
      "Google Cloud Project Information\n",
      "==================================================\n",
      "PROJECT_ID     = statmike-mlops-349915\n",
      "PROJECT_NUMBER = 1026793852137\n",
      "==================================================\n",
      "\n",
      "\n",
      "==================================================\n",
      "SETUP SUMMARY\n",
      "==================================================\n",
      "✅ Authentication:    Success\n",
      "✅ API Configuration: Success\n",
      "✅ Package Install:   Already up to date\n",
      "✅ Installation Tool: poetry\n",
      "✅ Project ID:        statmike-mlops-349915\n",
      "✅ Project Number:    1026793852137\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os, urllib.request\n",
    "\n",
    "# Download and import setup code\n",
    "url = 'https://raw.githubusercontent.com/statmike/vertex-ai-mlops/refs/heads/main/core/notebook-template/python_setup.py'\n",
    "urllib.request.urlretrieve(url, 'python_setup_local.py')\n",
    "import python_setup_local as python_setup\n",
    "os.remove('python_setup_local.py')\n",
    "\n",
    "# Run setup\n",
    "setup_info = python_setup.setup_environment(PROJECT_ID, REQ_TYPE, REQUIREMENTS_URL, REQUIRED_APIS, INSTALL_TOOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "py",
   "metadata": {},
   "source": [
    "---\n",
    "## Python Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "imp",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import apache_beam as beam\n",
    "from apache_beam import window\n",
    "from apache_beam.ml.inference.base import RunInference, PredictionResult, KeyedModelHandler, ModelMetadata\n",
    "from apache_beam.ml.inference.pytorch_inference import PytorchModelHandlerTensor\n",
    "from apache_beam.options.pipeline_options import PipelineOptions, StandardOptions\n",
    "from apache_beam.io.gcp.pubsub import ReadFromPubSub, WriteToPubSub\n",
    "from apache_beam.io.gcp.bigquery import WriteToBigQuery\n",
    "import torch\n",
    "import json\n",
    "from datetime import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "vars",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project: statmike-mlops-349915\n",
      "Source model: gs://statmike-mlops-349915/frameworks/pytorch-autoencoder/dataflow/final_model_traced.pt\n",
      "Model v1: gs://statmike-mlops-349915/frameworks/pytorch-autoencoder/dataflow/v1/final_model_traced.pt\n",
      "Model v2: gs://statmike-mlops-349915/frameworks/pytorch-autoencoder/dataflow/v2/final_model_traced.pt\n",
      "Pipeline starts with: v1\n",
      "Input subscription: projects/statmike-mlops-349915/subscriptions/pytorch-autoencoder-input-sub-local\n",
      "Output topic: projects/statmike-mlops-349915/topics/pytorch-autoencoder-output-local\n",
      "Model update topic: projects/statmike-mlops-349915/topics/pytorch-autoencoder-model-update\n",
      "Results table: statmike-mlops-349915.frameworks.pytorch_autoencoder_streaming_results_keyed\n"
     ]
    }
   ],
   "source": [
    "PROJECT_ID = subprocess.run([\"gcloud\", \"config\", \"get-value\", \"project\"], capture_output=True, text=True, check=True).stdout.strip()\n",
    "\n",
    "# Configuration\n",
    "REGION = \"us-central1\"\n",
    "SERIES = \"frameworks\"\n",
    "EXPERIMENT = \"pytorch-autoencoder\"\n",
    "\n",
    "# GCS paths (aligned with dataflow-setup.ipynb)\n",
    "BUCKET_NAME = PROJECT_ID\n",
    "BUCKET_URI = f\"gs://{BUCKET_NAME}\"\n",
    "\n",
    "# Model paths\n",
    "# Source model created by dataflow-setup.ipynb\n",
    "MODEL_SOURCE_PATH = f\"{BUCKET_URI}/{SERIES}/{EXPERIMENT}/dataflow/final_model_traced.pt\"\n",
    "# Versioned paths for Model Registry (v1 = initial, v2 = swap target)\n",
    "MODEL_PATH_V1 = f\"{BUCKET_URI}/{SERIES}/{EXPERIMENT}/dataflow/v1/final_model_traced.pt\"\n",
    "MODEL_PATH_V2 = f\"{BUCKET_URI}/{SERIES}/{EXPERIMENT}/dataflow/v2/final_model_traced.pt\"\n",
    "# Pipeline starts with v1\n",
    "MODEL_PATH = MODEL_PATH_V1\n",
    "\n",
    "# Pub/Sub configuration - using LOCAL-specific topics and subscriptions\n",
    "INPUT_SUB = f\"projects/{PROJECT_ID}/subscriptions/{EXPERIMENT}-input-sub-local\"\n",
    "OUTPUT_TOPIC = f\"projects/{PROJECT_ID}/topics/{EXPERIMENT}-output-local\"\n",
    "\n",
    "# Model update Pub/Sub (event-mode model hot-swap)\n",
    "MODEL_UPDATE_SUB = f\"projects/{PROJECT_ID}/subscriptions/{EXPERIMENT}-model-update-sub\"\n",
    "MODEL_UPDATE_TOPIC = f\"projects/{PROJECT_ID}/topics/{EXPERIMENT}-model-update\"\n",
    "\n",
    "# BigQuery configuration\n",
    "BQ_DATASET = SERIES.replace(\"-\", \"_\")\n",
    "BQ_TABLE = f\"{EXPERIMENT.replace('-', '_')}_streaming_results_keyed\"\n",
    "\n",
    "# Dataflow configuration\n",
    "DATAFLOW_STAGING = f\"{BUCKET_URI}/dataflow/staging\"\n",
    "DATAFLOW_TEMP = f\"{BUCKET_URI}/dataflow/temp\"\n",
    "\n",
    "print(f\"Project: {PROJECT_ID}\")\n",
    "print(f\"Source model: {MODEL_SOURCE_PATH}\")\n",
    "print(f\"Model v1: {MODEL_PATH_V1}\")\n",
    "print(f\"Model v2: {MODEL_PATH_V2}\")\n",
    "print(f\"Pipeline starts with: v1\")\n",
    "print(f\"Input subscription: {INPUT_SUB}\")\n",
    "print(f\"Output topic: {OUTPUT_TOPIC}\")\n",
    "print(f\"Model update topic: {MODEL_UPDATE_TOPIC}\")\n",
    "print(f\"Results table: {PROJECT_ID}.{BQ_DATASET}.{BQ_TABLE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da56815",
   "metadata": {},
   "source": [
    "---\n",
    "## Register Model Versions in Vertex AI Model Registry\n",
    "\n",
    "This notebook uses **Vertex AI Model Registry** as the source of truth for model versions. Instead of hardcoding GCS paths for model updates, we:\n",
    "\n",
    "1. **Copy the model** to versioned GCS directories (v1/, v2/)\n",
    "2. **Register both versions** in Model Registry under a single model resource\n",
    "3. **Retrieve version artifact URIs** from the registry\n",
    "4. **Use registry paths** when publishing model swap events via Pub/Sub\n",
    "\n",
    "In production, your training pipeline would register new model versions in the registry. The event-mode mechanism in this notebook consumes those versions by reading their `artifact_uri` and publishing it to the model update topic.\n",
    "\n",
    "### Why Model Registry?\n",
    "\n",
    "- **Single source of truth**: All model versions tracked in one place\n",
    "- **Artifact URIs**: Each version stores its GCS path -- no hardcoding needed\n",
    "- **Rollback**: Look up any previous version's path to roll back instantly\n",
    "- **Audit trail**: Version history with descriptions and metadata\n",
    "- **CI/CD integration**: Automated pipelines register new versions after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "912d9313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Preparing versioned model artifacts in GCS\n",
      "============================================================\n",
      "  Exists:    gs://statmike-mlops-349915/frameworks/pytorch-autoencoder/dataflow/v1/final_model_traced.pt\n",
      "  Exists:    gs://statmike-mlops-349915/frameworks/pytorch-autoencoder/dataflow/v2/final_model_traced.pt\n",
      "\n",
      "Step 2: Registering in Vertex AI Model Registry\n",
      "============================================================\n",
      "  Creating model 'pytorch-autoencoder-dataflow'...\n",
      "Creating Model\n",
      "Create Model backing LRO: projects/1026793852137/locations/us-central1/models/6150948421244026880/operations/7901847579543994368\n",
      "Model created. Resource name: projects/1026793852137/locations/us-central1/models/6150948421244026880@1\n",
      "To use this Model in another session:\n",
      "model = aiplatform.Model('projects/1026793852137/locations/us-central1/models/6150948421244026880@1')\n",
      "  v1 registered (version_id=1)\n",
      "Creating Model\n",
      "Create Model backing LRO: projects/1026793852137/locations/us-central1/models/6150948421244026880/operations/3960072005687967744\n",
      "Model created. Resource name: projects/1026793852137/locations/us-central1/models/6150948421244026880@2\n",
      "To use this Model in another session:\n",
      "model = aiplatform.Model('projects/1026793852137/locations/us-central1/models/6150948421244026880@2')\n",
      "  v2 registered (version_id=2)\n",
      "\n",
      "Step 3: Retrieving version metadata\n",
      "============================================================\n",
      "Getting versions for projects/1026793852137/locations/us-central1/models/6150948421244026880\n",
      "\n",
      "  Version 1:\n",
      "    Artifact URI: gs://statmike-mlops-349915/frameworks/pytorch-autoencoder/dataflow/v1\n",
      "    Model path:   gs://statmike-mlops-349915/frameworks/pytorch-autoencoder/dataflow/v1/final_model_traced.pt\n",
      "\n",
      "  Version 2:\n",
      "    Artifact URI: gs://statmike-mlops-349915/frameworks/pytorch-autoencoder/dataflow/v2\n",
      "    Model path:   gs://statmike-mlops-349915/frameworks/pytorch-autoencoder/dataflow/v2/final_model_traced.pt\n",
      "\n",
      "  Pipeline will start with v1: gs://statmike-mlops-349915/frameworks/pytorch-autoencoder/dataflow/v1/final_model_traced.pt\n",
      "  Event-mode swap target v2:   gs://statmike-mlops-349915/frameworks/pytorch-autoencoder/dataflow/v2/final_model_traced.pt\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import aiplatform, storage\n",
    "\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION)\n",
    "\n",
    "# ======================================================\n",
    "# Step 1: Copy model to versioned GCS directories\n",
    "# ======================================================\n",
    "# In production, each version would be a different trained model.\n",
    "# Here we copy the same model to two paths to demonstrate version tracking.\n",
    "# The model_id in output will differ (different GCS paths), proving the swap worked.\n",
    "\n",
    "print(\"Step 1: Preparing versioned model artifacts in GCS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "storage_client = storage.Client(project=PROJECT_ID)\n",
    "bucket = storage_client.bucket(BUCKET_NAME)\n",
    "\n",
    "source_blob_name = f\"{SERIES}/{EXPERIMENT}/dataflow/final_model_traced.pt\"\n",
    "source_blob = bucket.blob(source_blob_name)\n",
    "\n",
    "for version_path in [MODEL_PATH_V1, MODEL_PATH_V2]:\n",
    "    dest_blob_name = version_path.replace(f\"gs://{BUCKET_NAME}/\", \"\")\n",
    "    dest_blob = bucket.blob(dest_blob_name)\n",
    "    if not dest_blob.exists():\n",
    "        bucket.copy_blob(source_blob, bucket, dest_blob_name)\n",
    "        print(f\"  Copied to: {version_path}\")\n",
    "    else:\n",
    "        print(f\"  Exists:    {version_path}\")\n",
    "\n",
    "# ======================================================\n",
    "# Step 2: Register versions in Vertex AI Model Registry\n",
    "# ======================================================\n",
    "print(f\"\\nStep 2: Registering in Vertex AI Model Registry\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "MODEL_DISPLAY_NAME = \"pytorch-autoencoder-dataflow\"\n",
    "\n",
    "# Use a non-prediction container to bypass artifact format validation.\n",
    "# Prebuilt prediction containers (us-docker.pkg.dev/vertex-ai/prediction/*)\n",
    "# validate artifacts against the framework's expected format — PyTorch\n",
    "# containers require .mar (TorchServe Model Archive) files. Since we use\n",
    "# Model Registry for version tracking only (RunInference loads .pt files\n",
    "# directly from GCS), we register with a generic container image.\n",
    "SERVING_CONTAINER = \"gcr.io/cloud-builders/gcloud\"\n",
    "\n",
    "# Clean up any existing model for idempotent re-runs\n",
    "existing_models = aiplatform.Model.list(\n",
    "    filter=f'display_name=\"{MODEL_DISPLAY_NAME}\"',\n",
    "    order_by=\"create_time desc\"\n",
    ")\n",
    "for model in existing_models:\n",
    "    try:\n",
    "        model.delete()\n",
    "        print(f\"  Deleted existing model: {model.resource_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Warning: Could not delete {model.resource_name}: {e}\")\n",
    "\n",
    "# Register v1 (creates the model resource)\n",
    "print(f\"  Creating model '{MODEL_DISPLAY_NAME}'...\")\n",
    "model_v1 = aiplatform.Model.upload(\n",
    "    display_name=MODEL_DISPLAY_NAME,\n",
    "    artifact_uri=f\"{BUCKET_URI}/{SERIES}/{EXPERIMENT}/dataflow/v1\",\n",
    "    serving_container_image_uri=SERVING_CONTAINER,\n",
    "    serving_container_predict_route=\"/predict\",\n",
    "    serving_container_health_route=\"/health\",\n",
    "    version_aliases=[\"v1\"],\n",
    "    version_description=\"Version 1 - initial model\",\n",
    ")\n",
    "print(f\"  v1 registered (version_id={model_v1.version_id})\")\n",
    "\n",
    "# Register v2 (adds version to existing model)\n",
    "model_v2 = aiplatform.Model.upload(\n",
    "    display_name=MODEL_DISPLAY_NAME,\n",
    "    parent_model=model_v1.resource_name,\n",
    "    artifact_uri=f\"{BUCKET_URI}/{SERIES}/{EXPERIMENT}/dataflow/v2\",\n",
    "    serving_container_image_uri=SERVING_CONTAINER,\n",
    "    serving_container_predict_route=\"/predict\",\n",
    "    serving_container_health_route=\"/health\",\n",
    "    version_aliases=[\"v2\"],\n",
    "    version_description=\"Version 2 - updated model\",\n",
    ")\n",
    "print(f\"  v2 registered (version_id={model_v2.version_id})\")\n",
    "\n",
    "parent_model = model_v1\n",
    "\n",
    "# ======================================================\n",
    "# Step 3: Retrieve version metadata from registry\n",
    "# ======================================================\n",
    "print(f\"\\nStep 3: Retrieving version metadata\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "registry = aiplatform.models.ModelRegistry(model=parent_model)\n",
    "versions = registry.list_versions()\n",
    "\n",
    "for v in versions:\n",
    "    version_model = registry.get_model(version=v.version_id)\n",
    "    artifact_uri = version_model.gca_resource.artifact_uri\n",
    "    print(f\"\\n  Version {v.version_id}:\")\n",
    "    print(f\"    Artifact URI: {artifact_uri}\")\n",
    "    print(f\"    Model path:   {artifact_uri}/final_model_traced.pt\")\n",
    "\n",
    "# Confirm MODEL_PATH is set to v1\n",
    "print(f\"\\n  Pipeline will start with v1: {MODEL_PATH}\")\n",
    "print(f\"  Event-mode swap target v2:   {MODEL_PATH_V2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handler",
   "metadata": {},
   "source": [
    "---\n",
    "## Create ModelHandler\n",
    "\n",
    "The ModelHandler wraps the PyTorch model for use with Apache Beam's `RunInference` transform. This handler returns `PredictionResult` objects (not plain dicts) so that `model_id` is available in the output.\n",
    "\n",
    "### Key Differences from the Existing Streaming Notebook\n",
    "\n",
    "1. **Returns `PredictionResult`** (not plain dict) -- enables `model_id` tracking\n",
    "2. **Batching params in constructor** (not explicit `BatchElements`) -- `RunInference` handles batching internally\n",
    "3. **`model_id` set to `self._torch_script_model_path`** -- auto-updates when model swaps via event-mode\n",
    "\n",
    "### How model_id Updates During Hot-Swap\n",
    "\n",
    "When a model update event arrives via the side input:\n",
    "1. `_RunInferenceDoFn` calls `update_model_path(new_path)` on the handler\n",
    "2. This updates `self._torch_script_model_path` to the new GCS path\n",
    "3. `load_model()` is called to download and load the new model\n",
    "4. All subsequent calls to `run_inference` return `PredictionResult` with the new `model_id`\n",
    "\n",
    "This is automatic -- no code changes needed in the handler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "handler_c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelHandler created with KeyedModelHandler wrapper\n",
      "   Model: gs://statmike-mlops-349915/frameworks/pytorch-autoencoder/dataflow/v1/final_model_traced.pt\n",
      "   Device: cpu\n",
      "   Batching: min=1, max=50, max_wait=10ms\n",
      "   Metadata passthrough: via JSON string key\n",
      "   Model tracking: PredictionResult.model_id = GCS path\n"
     ]
    }
   ],
   "source": [
    "class PyTorchAutoencoderHandler(PytorchModelHandlerTensor):\n",
    "    \"\"\"\n",
    "    Custom ModelHandler for PyTorch autoencoder inference.\n",
    "    \n",
    "    Changes from existing notebook:\n",
    "    1. Returns PredictionResult (not plain dict) -- enables model_id tracking\n",
    "    2. Batching params in constructor (not explicit BatchElements)\n",
    "    3. model_id set to self._torch_script_model_path -- auto-updates on model swap\n",
    "    \n",
    "    Key Design Choices:\n",
    "    \n",
    "    1. TorchScript Model Loading:\n",
    "       - Uses torch_script_model_path (not state_dict_path)\n",
    "       - TorchScript models (.pt) have architecture embedded\n",
    "       - No need to provide model_class parameter\n",
    "    \n",
    "    2. Batch Processing:\n",
    "       - Input: List of tensors (one per instance)\n",
    "       - Stack into single batch tensor with torch.stack()\n",
    "       - Enables efficient GPU/CPU vectorized operations\n",
    "    \n",
    "    3. Output Format:\n",
    "       - Returns list of PredictionResult objects\n",
    "       - Each PredictionResult contains example (input), inference (output), and model_id\n",
    "       - model_id = self._torch_script_model_path (GCS path = version identifier)\n",
    "       - When model swaps via event-mode, model_id updates automatically\n",
    "    \n",
    "    4. Model Output:\n",
    "       - Autoencoder returns dict: {\\\"denormalized_MAE\\\": tensor, \\\"encoded\\\": tensor, ...}\n",
    "       - Extract only needed fields (denormalized_MAE for anomaly score, encoded for embeddings)\n",
    "       - Keep as tensors in run_inference, convert to Python types later\n",
    "    \"\"\"\n",
    "    \n",
    "    def run_inference(self, batch, model, inference_args=None):\n",
    "        \"\"\"\n",
    "        Run inference on a batch of input tensors.\n",
    "        \n",
    "        Args:\n",
    "            batch: List of torch.Tensor, each shape (30,) for our autoencoder\n",
    "            model: Loaded TorchScript model\n",
    "            inference_args: Optional additional arguments (unused)\n",
    "            \n",
    "        Returns:\n",
    "            List of PredictionResult, one per input, with model_id set to GCS path\n",
    "        \"\"\"\n",
    "        # Stack list of tensors into single batch tensor\n",
    "        # Input: [tensor(30,), tensor(30,), ...] -> Output: tensor(batch_size, 30)\n",
    "        batch_tensor = torch.stack(batch)\n",
    "\n",
    "        # Run model inference without gradient computation\n",
    "        with torch.no_grad():\n",
    "            predictions = model(batch_tensor)\n",
    "\n",
    "        # Convert batch output to list of PredictionResult objects\n",
    "        results = []\n",
    "        for i in range(len(batch)):\n",
    "            results.append(PredictionResult(\n",
    "                example=batch[i],\n",
    "                inference={\n",
    "                    \"denormalized_MAE\": predictions[\"denormalized_MAE\"][i],\n",
    "                    \"encoded\": predictions[\"encoded\"][i]\n",
    "                },\n",
    "                model_id=self._torch_script_model_path  # GCS path = version\n",
    "            ))\n",
    "        return results\n",
    "\n",
    "\n",
    "# Batching params in constructor (replaces explicit BatchElements)\n",
    "model_handler = PyTorchAutoencoderHandler(\n",
    "    torch_script_model_path=MODEL_PATH,\n",
    "    device=\"cpu\",\n",
    "    min_batch_size=1,\n",
    "    max_batch_size=50,\n",
    "    max_batch_duration_secs=0.01\n",
    ")\n",
    "\n",
    "# Wrap for metadata passthrough\n",
    "keyed_handler = KeyedModelHandler(model_handler)\n",
    "\n",
    "print(\"ModelHandler created with KeyedModelHandler wrapper\")\n",
    "print(f\"   Model: {MODEL_PATH}\")\n",
    "print(f\"   Device: cpu\")\n",
    "print(f\"   Batching: min=1, max=50, max_wait=10ms\")\n",
    "print(f\"   Metadata passthrough: via JSON string key\")\n",
    "print(f\"   Model tracking: PredictionResult.model_id = GCS path\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pipeline",
   "metadata": {},
   "source": [
    "---\n",
    "## Build Streaming Pipeline with Event-Mode Model Hot-Swap\n",
    "\n",
    "This section builds the streaming pipeline using `RunInference` with `KeyedModelHandler` and a model update side input:\n",
    "\n",
    "1. **1-second fixed windows** (for watermark progression)\n",
    "2. **1-second early triggers** (release messages early)\n",
    "3. **RunInference with KeyedModelHandler** (batching handled internally, metadata via key)\n",
    "4. **model_metadata_pcollection** (side input for runtime model updates)\n",
    "\n",
    "### Why KeyedModelHandler + Event-Mode?\n",
    "\n",
    "**Beam-Native:**\n",
    "- `RunInference` handles batching internally (fewer pipeline steps)\n",
    "- `KeyedModelHandler` provides metadata passthrough without custom DoFn\n",
    "- `model_metadata_pcollection` is Beam's built-in mechanism for model updates\n",
    "\n",
    "**Model Version Tracking:**\n",
    "- `PredictionResult.model_id` gives the GCS path of the model used\n",
    "- When model swaps, `model_id` changes automatically\n",
    "- BigQuery `model_id` column shows exactly which model produced each prediction\n",
    "\n",
    "**Simple Extension:**\n",
    "- Only one parameter addition from the keyed notebook: `model_metadata_pcollection=model_updates`\n",
    "- Handler, parse_json, format_result are all identical to the keyed notebook\n",
    "\n",
    "### Pipeline Comparison\n",
    "\n",
    "**Existing (7 steps):**\n",
    "```\n",
    "ReadPubSub -> parse_json -> Window -> BatchElements -> ParDo(RunInferenceOnBatch) -> format_result -> BQ+PubSub\n",
    "```\n",
    "\n",
    "**Keyed (5 steps):**\n",
    "```\n",
    "ReadPubSub -> parse_json -> Window -> RunInference(KeyedModelHandler) -> format_result -> BQ+PubSub\n",
    "```\n",
    "\n",
    "**Event-Mode (5 steps + side input):**\n",
    "```\n",
    "ReadPubSub -> parse_json -> Window -> RunInference(KeyedModelHandler, model_metadata_pcollection) -> format_result -> BQ+PubSub\n",
    "                                       + model_updates side input from Pub/Sub\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "qhkd671nl6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "WORKER COMPUTE CONFIGURATION\n",
      "============================================================\n",
      "Machine Type: n1-standard-4\n",
      "  - vCPUs: 4\n",
      "  - Memory: 15 GB\n",
      "\n",
      "Autoscaling:\n",
      "  - Min Workers: 2\n",
      "  - Max Workers: 20\n",
      "  - Estimated capacity: ~200-10000 transactions/sec\n",
      "\n",
      "GPU Support: Not configured (CPU inference)\n",
      "  - To enable GPU: Add --worker_gpu_type and --worker_gpu_count\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Worker Compute Configuration\n",
    "# =============================\n",
    "# These settings control the machine type and autoscaling behavior for Dataflow workers.\n",
    "# Proper configuration ensures efficient resource utilization and cost management.\n",
    "\n",
    "# Machine Type: n1-standard-4\n",
    "# - 4 vCPUs, 15 GB memory\n",
    "# - Suitable for PyTorch inference workloads that need moderate CPU and memory\n",
    "# - Each worker can handle multiple inference requests concurrently\n",
    "# - Alternative: n1-standard-2 (smaller), n1-standard-8 (larger), or custom machine types\n",
    "\n",
    "MACHINE_TYPE = \"n1-standard-4\"\n",
    "\n",
    "# Autoscaling for Streaming Pipelines\n",
    "# - min_workers=2: Ensures pipeline remains responsive even with low traffic\n",
    "# - max_workers=20: Handles traffic spikes without overwhelming resources\n",
    "# - Dataflow autoscales based on Pub/Sub backlog and processing latency\n",
    "\n",
    "MIN_WORKERS = 2\n",
    "MAX_WORKERS = 20\n",
    "\n",
    "# Why These Settings for Streaming?\n",
    "# ----------------------------------\n",
    "# 1. **Minimum Workers (2)**:\n",
    "#    - Provides redundancy (if one worker fails, pipeline continues)\n",
    "#    - Reduces cold start latency when traffic arrives\n",
    "#    - Maintains low end-to-end latency for real-time processing\n",
    "#\n",
    "# 2. **Maximum Workers (20)**:\n",
    "#    - Allows scaling to handle traffic bursts\n",
    "#    - Each n1-standard-4 worker processes ~100-500 transactions/sec\n",
    "#    - 20 workers can handle ~2,000-10,000 transactions/sec\n",
    "#    - Prevents runaway costs from unlimited scaling\n",
    "#\n",
    "# 3. **Machine Type (n1-standard-4)**:\n",
    "#    - PyTorch model loading requires ~2-4 GB memory per worker\n",
    "#    - 15 GB allows model + batch processing overhead\n",
    "#    - 4 vCPUs enable parallel batch inference\n",
    "#    - Cost-effective for moderate throughput requirements\n",
    "\n",
    "# When to Adjust These Settings:\n",
    "# -------------------------------\n",
    "# - **Higher Traffic**: Increase max_workers (e.g., 50-100)\n",
    "# - **Lower Latency**: Increase min_workers (e.g., 5-10) to pre-warm capacity\n",
    "# - **Cost Optimization**: Use smaller machine type (n1-standard-2) if memory permits\n",
    "# - **Larger Models**: Use n1-standard-8 or n1-highmem-4 for memory-intensive models\n",
    "# - **CPU-Intensive Models**: Use c2-standard-4 for compute-optimized instances\n",
    "\n",
    "# GPU Support (Optional)\n",
    "# ----------------------\n",
    "# Dataflow supports GPU workers for accelerated inference:\n",
    "# - Machine type: n1-standard-4 (host machine)\n",
    "# - GPU type: nvidia-tesla-t4, nvidia-tesla-v100, etc.\n",
    "# - GPU count: 1-4 per worker\n",
    "# - Requirements:\n",
    "#   1. Add --worker_machine_type=n1-standard-4\n",
    "#   2. Add --worker_gpu_type=nvidia-tesla-t4\n",
    "#   3. Add --worker_gpu_count=1\n",
    "#   4. Update model handler to use device=\"cuda\"\n",
    "#   5. Ensure PyTorch is GPU-enabled in requirements file\n",
    "#\n",
    "# Note: GPU workers are significantly more expensive than CPU workers.\n",
    "# Only use for models where GPU acceleration provides meaningful speedup.\n",
    "# For small models like this autoencoder, CPU inference is more cost-effective.\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"WORKER COMPUTE CONFIGURATION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Machine Type: {MACHINE_TYPE}\")\n",
    "print(f\"  - vCPUs: 4\")\n",
    "print(f\"  - Memory: 15 GB\")\n",
    "print(f\"\\nAutoscaling:\")\n",
    "print(f\"  - Min Workers: {MIN_WORKERS}\")\n",
    "print(f\"  - Max Workers: {MAX_WORKERS}\")\n",
    "print(f\"  - Estimated capacity: ~{MIN_WORKERS * 100}-{MAX_WORKERS * 500} transactions/sec\")\n",
    "print(f\"\\nGPU Support: Not configured (CPU inference)\")\n",
    "print(f\"  - To enable GPU: Add --worker_gpu_type and --worker_gpu_count\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "pipeline_c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streaming pipeline with event-mode model hot-swap configured\n",
      "   Job will run in: us-central1\n",
      "   Staging: gs://statmike-mlops-349915/dataflow/staging\n",
      "   Requirements: /tmp/tmpmktrmdhk.txt\n",
      "   Machine type: n1-standard-4\n",
      "   Worker scaling: 2-20 workers\n",
      "   Model update subscription: projects/statmike-mlops-349915/subscriptions/pytorch-autoencoder-model-update-sub\n",
      "   Target latency: <50ms end-to-end\n"
     ]
    }
   ],
   "source": [
    "def parse_json(message):\n",
    "    \"\"\"\n",
    "    Parse Pub/Sub message and return (metadata_key_str, features_tensor).\n",
    "\n",
    "    KeyedModelHandler requires (key, element) tuples. Metadata is serialized\n",
    "    as a JSON string key. Overhead: ~1-2 microseconds per message (negligible\n",
    "    at 40ms total pipeline latency).\n",
    "\n",
    "    Args:\n",
    "        message: Bytes from Pub/Sub containing JSON with 'features' key\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (str, torch.Tensor):\n",
    "        - metadata_key: JSON string containing test_id, message_id, etc.\n",
    "        - features: Shape (30,) tensor for model input\n",
    "    \"\"\"\n",
    "    data = json.loads(message.decode(\"utf-8\"))\n",
    "    features = torch.tensor(data[\"features\"], dtype=torch.float32)\n",
    "    metadata_key = json.dumps({\n",
    "        \"test_id\": data.get(\"test_id\"),\n",
    "        \"message_id\": data.get(\"message_id\"),\n",
    "        \"publish_time\": data.get(\"publish_time\"),\n",
    "        \"sequence\": data.get(\"sequence\")\n",
    "    })\n",
    "    return (metadata_key, features)\n",
    "\n",
    "\n",
    "def format_result(element, window=beam.DoFn.WindowParam):\n",
    "    \"\"\"\n",
    "    Format RunInference output for BigQuery/Pub/Sub.\n",
    "\n",
    "    Element is (metadata_key_str, PredictionResult) from KeyedModelHandler.\n",
    "    PredictionResult.model_id gives the GCS path of the model used.\n",
    "\n",
    "    Args:\n",
    "        element: Tuple of (metadata_key_str, PredictionResult)\n",
    "        window: Beam window parameter for extracting window boundaries\n",
    "\n",
    "    Returns:\n",
    "        Dict with Python types suitable for BigQuery/JSON serialization\n",
    "    \"\"\"\n",
    "    metadata_key, prediction_result = element\n",
    "    metadata = json.loads(metadata_key)\n",
    "    prediction = prediction_result.inference\n",
    "    model_id = prediction_result.model_id       # GCS path = version identifier\n",
    "\n",
    "    result = {\n",
    "        \"instance_id\": str(hash(str(prediction[\"denormalized_MAE\"].item()))),\n",
    "        \"anomaly_score\": float(prediction[\"denormalized_MAE\"].item()),\n",
    "        \"encoded\": prediction[\"encoded\"].tolist(),\n",
    "        \"timestamp\": datetime.utcnow().isoformat(),\n",
    "        \"window_start\": window.start.to_utc_datetime().timestamp(),\n",
    "        \"window_end\": window.end.to_utc_datetime().timestamp(),\n",
    "        \"pipeline_output_time\": time.time(),\n",
    "        \"model_id\": model_id,\n",
    "    }\n",
    "\n",
    "    if metadata:\n",
    "        if metadata.get(\"test_id\"):\n",
    "            result[\"test_id\"] = metadata[\"test_id\"]\n",
    "        if metadata.get(\"message_id\"):\n",
    "            result[\"message_id\"] = metadata[\"message_id\"]\n",
    "        if metadata.get(\"publish_time\"):\n",
    "            result[\"publish_time\"] = metadata[\"publish_time\"]\n",
    "        if metadata.get(\"sequence\") is not None:\n",
    "            result[\"sequence\"] = metadata[\"sequence\"]\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def to_json(element):\n",
    "    \"\"\"Convert dict to JSON bytes for Pub/Sub publication.\"\"\"\n",
    "    return json.dumps(element).encode(\"utf-8\")\n",
    "\n",
    "\n",
    "# Create requirements file for Dataflow workers\n",
    "# Workers need PyTorch installed to load and run the model\n",
    "import tempfile\n",
    "requirements_content = \"--extra-index-url https://pypi.org/simple/\\ntorch>=2.0.0\\n\"\n",
    "\n",
    "with tempfile.NamedTemporaryFile(mode='w', suffix='.txt', delete=False) as f:\n",
    "    f.write(requirements_content)\n",
    "    requirements_file = f.name\n",
    "\n",
    "# Configure Dataflow pipeline options\n",
    "from apache_beam.transforms import trigger\n",
    "\n",
    "options = PipelineOptions([\n",
    "    f\"--project={PROJECT_ID}\",\n",
    "    f\"--region={REGION}\",\n",
    "    \"--runner=DataflowRunner\",  # Run on Google Cloud (not locally)\n",
    "    f\"--temp_location={DATAFLOW_TEMP}\",\n",
    "    f\"--staging_location={DATAFLOW_STAGING}\",\n",
    "    f\"--requirements_file={requirements_file}\",  # Install PyTorch in workers\n",
    "    f\"--job_name=pytorch-streaming-keyed-event-{datetime.now().strftime('%Y%m%d-%H%M%S')}\",\n",
    "    \"--streaming\",  # Enable streaming mode\n",
    "    \"--save_main_session\",  # Serialize global imports and variables\n",
    "    # Worker compute configuration\n",
    "    f\"--machine_type={MACHINE_TYPE}\",\n",
    "    f\"--num_workers={MIN_WORKERS}\",\n",
    "    f\"--max_num_workers={MAX_WORKERS}\",\n",
    "    # Low-latency pipeline optimization\n",
    "    \"--experiments=enable_streaming_engine\",  # Faster Dataflow execution engine\n",
    "    \"--experiments=use_runner_v2\",  # Latest runner with performance improvements\n",
    "])\n",
    "\n",
    "print(\"Streaming pipeline with event-mode model hot-swap configured\")\n",
    "print(f\"   Job will run in: {REGION}\")\n",
    "print(f\"   Staging: {DATAFLOW_STAGING}\")\n",
    "print(f\"   Requirements: {requirements_file}\")\n",
    "print(f\"   Machine type: {MACHINE_TYPE}\")\n",
    "print(f\"   Worker scaling: {MIN_WORKERS}-{MAX_WORKERS} workers\")\n",
    "print(f\"   Model update subscription: {MODEL_UPDATE_SUB}\")\n",
    "print(f\"   Target latency: <50ms end-to-end\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "run",
   "metadata": {},
   "source": [
    "---\n",
    "## Run Streaming Job with Event-Mode Model Hot-Swap\n",
    "\n",
    "This cell builds and executes the pipeline with two PCollections:\n",
    "\n",
    "### Main Data Flow\n",
    "```\n",
    "Read from Pub/Sub\n",
    "  | (metadata_key_str, features_tensor)\n",
    "Parse JSON\n",
    "  |\n",
    "Window (1 sec, trigger every 1 sec)\n",
    "  |\n",
    "RunInference (KeyedModelHandler + model_metadata_pcollection)\n",
    "  | (metadata_key_str, PredictionResult)  <-- model_id updates on swap\n",
    "Format results\n",
    "  | Dict with model_id column\n",
    "Write to BigQuery + Pub/Sub\n",
    "```\n",
    "\n",
    "### Model Update Side Input\n",
    "```\n",
    "Read from Model Update Pub/Sub\n",
    "  |\n",
    "Parse to ModelMetadata(model_id=path, model_name=filename)\n",
    "  |\n",
    "GlobalWindow + AfterCount(1) trigger\n",
    "  |\n",
    "Side input to RunInference (triggers model reload)\n",
    "```\n",
    "\n",
    "### What Happens\n",
    "\n",
    "1. **Messages arrive** at Pub/Sub data topic\n",
    "2. **Every 1 second**: Trigger fires, releasing accumulated messages\n",
    "3. **RunInference**: Batches elements internally, runs inference with KeyedModelHandler\n",
    "4. **Model update arrives** at model update topic (optional, at any time)\n",
    "5. **Workers detect** new model_id in side input, reload model with thread lock\n",
    "6. **Subsequent predictions** use new model, model_id updates in output\n",
    "7. **Results flow**: Each prediction (with model_id) goes to BigQuery + Pub/Sub\n",
    "8. **Continuous**: Job runs until cancelled, accepting model updates at any time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ekiwbgilhr",
   "metadata": {},
   "source": [
    "---\n",
    "## Check for Running Streaming Jobs\n",
    "\n",
    "All local-model streaming notebooks read from the same Pub/Sub subscription (`pytorch-autoencoder-input-sub-local`). If two run simultaneously, they compete for messages -- each gets a random subset, producing incorrect results silently.\n",
    "\n",
    "This cell checks for any running Dataflow streaming jobs that use the same subscription before launching a new one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4obuwsmedw8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No conflicting streaming jobs running.\n",
      "   Subscription projects/statmike-mlops-349915/subscriptions/pytorch-autoencoder-input-sub-local is available.\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import dataflow_v1beta3\n",
    "\n",
    "# Check for running streaming jobs that use the same Pub/Sub subscription\n",
    "dataflow_client = dataflow_v1beta3.JobsV1Beta3Client()\n",
    "request = dataflow_v1beta3.ListJobsRequest(\n",
    "    project_id=PROJECT_ID,\n",
    "    location=REGION\n",
    ")\n",
    "\n",
    "all_jobs = list(dataflow_client.list_jobs(request=request))\n",
    "active_streaming_jobs = [\n",
    "    job for job in all_jobs\n",
    "    if job.current_state in [\n",
    "        dataflow_v1beta3.JobState.JOB_STATE_RUNNING,\n",
    "        dataflow_v1beta3.JobState.JOB_STATE_PENDING\n",
    "    ]\n",
    "    and job.name.startswith(\"pytorch-streaming\")\n",
    "    and \"-vertex-\" not in job.name  # vertex jobs use different subscriptions\n",
    "]\n",
    "\n",
    "if active_streaming_jobs:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"WARNING: RUNNING STREAMING JOBS DETECTED\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"\\nFound {len(active_streaming_jobs)} active job(s) using the same subscription:\")\n",
    "    for job in active_streaming_jobs:\n",
    "        print(f\"  - {job.name}\")\n",
    "        print(f\"    State: {job.current_state.name}\")\n",
    "        print(f\"    Started: {job.create_time}\")\n",
    "    print(f\"\\nSubscription: {INPUT_SUB}\")\n",
    "    print(\"Running multiple pipelines on the same subscription causes message competition.\")\n",
    "    print(\"\\nSet CANCEL_EXISTING_JOBS = True in the next cell to cancel these jobs.\")\n",
    "    print(\"Or use dataflow-cleanup.ipynb for comprehensive cleanup.\")\n",
    "    print(\"=\" * 60)\n",
    "else:\n",
    "    print(\"No conflicting streaming jobs running.\")\n",
    "    print(f\"   Subscription {INPUT_SUB} is available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cbzc1lt6t3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No jobs to cancel. Proceeding.\n"
     ]
    }
   ],
   "source": [
    "CANCEL_EXISTING_JOBS = False  # Set to True to cancel running jobs\n",
    "\n",
    "if active_streaming_jobs and CANCEL_EXISTING_JOBS:\n",
    "    for job in active_streaming_jobs:\n",
    "        try:\n",
    "            cancel_request = dataflow_v1beta3.UpdateJobRequest(\n",
    "                project_id=PROJECT_ID,\n",
    "                location=REGION,\n",
    "                job_id=job.id,\n",
    "                job=dataflow_v1beta3.Job(\n",
    "                    requested_state=dataflow_v1beta3.JobState.JOB_STATE_CANCELLED\n",
    "                )\n",
    "            )\n",
    "            dataflow_client.update_job(request=cancel_request)\n",
    "            print(f\"Cancelled: {job.name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to cancel {job.name}: {e}\")\n",
    "\n",
    "    # Wait for cancellation to take effect\n",
    "    import time as _time\n",
    "    print(\"\\nWaiting 30 seconds for cancellation to complete...\")\n",
    "    _time.sleep(30)\n",
    "\n",
    "    # Verify cancellation\n",
    "    request = dataflow_v1beta3.ListJobsRequest(\n",
    "        project_id=PROJECT_ID,\n",
    "        location=REGION\n",
    "    )\n",
    "    still_active = [\n",
    "        job for job in dataflow_client.list_jobs(request=request)\n",
    "        if job.current_state == dataflow_v1beta3.JobState.JOB_STATE_RUNNING\n",
    "        and job.name.startswith(\"pytorch-streaming\")\n",
    "        and \"-vertex-\" not in job.name\n",
    "    ]\n",
    "    if still_active:\n",
    "        print(f\"{len(still_active)} job(s) still running. May need more time to cancel.\")\n",
    "    else:\n",
    "        print(\"All conflicting jobs cancelled. Safe to proceed.\")\n",
    "elif active_streaming_jobs:\n",
    "    print(\"Existing jobs not cancelled. Set CANCEL_EXISTING_JOBS = True to cancel.\")\n",
    "else:\n",
    "    print(\"No jobs to cancel. Proceeding.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "run_c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:apache_beam.options.pipeline_options:Bucket specified in temp_location has soft-delete policy enabled. To avoid being billed for unnecessary storage costs, turn off the soft delete feature on buckets that your Dataflow jobs use for temporary and staging storage. For more information, see https://cloud.google.com/storage/docs/use-soft-delete#remove-soft-delete-policy.\n",
      "WARNING:apache_beam.options.pipeline_options:Bucket specified in staging_location has soft-delete policy enabled. To avoid being billed for unnecessary storage costs, turn off the soft delete feature on buckets that your Dataflow jobs use for temporary and staging storage. For more information, see https://cloud.google.com/storage/docs/use-soft-delete#remove-soft-delete-policy.\n",
      "WARNING:apache_beam.runners.interactive.interactive_environment:Dependencies required for Interactive Beam PCollection visualization are not available, please use: `pip install apache-beam[interactive]` to install necessary dependencies to enable all data visualization features.\n"
     ]
    },
    {
     "data": {
      "application/javascript": "\n        if (typeof window.interactive_beam_jquery == 'undefined') {\n          var jqueryScript = document.createElement('script');\n          jqueryScript.src = 'https://code.jquery.com/jquery-3.4.1.slim.min.js';\n          jqueryScript.type = 'text/javascript';\n          jqueryScript.onload = function() {\n            var datatableScript = document.createElement('script');\n            datatableScript.src = 'https://cdn.datatables.net/1.10.20/js/jquery.dataTables.min.js';\n            datatableScript.type = 'text/javascript';\n            datatableScript.onload = function() {\n              window.interactive_beam_jquery = jQuery.noConflict(true);\n              window.interactive_beam_jquery(document).ready(function($){\n                \n              });\n            }\n            document.head.appendChild(datatableScript);\n          };\n          document.head.appendChild(jqueryScript);\n        } else {\n          window.interactive_beam_jquery(document).ready(function($){\n            \n          });\n        }"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    div.alert {\n",
       "      white-space: pre-line;\n",
       "    }\n",
       "  </style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <link rel=\"stylesheet\" href=\"https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/css/bootstrap.min.css\" integrity=\"sha384-Vkoo8x4CGsO3+Hhxv8T/Q5PaXtkKtu6ug5TOeNV6gBiFeWPGFN9MuhOf23Q9Ifjh\" crossorigin=\"anonymous\">\n",
       "            <div class=\"alert alert-info\">No cache_root detected. Defaulting to staging_location gs://statmike-mlops-349915/dataflow/staging for cache location.</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:apache_beam.transforms.core:Key coder FastPrimitivesCoder for transform <ParDo(PTransform) label=[ParDo(_StatefulBatchElementsDoFn)]> with stateful DoFn may not be deterministic. This may cause incorrect behavior for complex key types. Consider adding an input type hint for this transform.\n",
      "WARNING:apache_beam.transforms.core:('No iterator is returned by the process method in %s.', <class 'apache_beam.io.gcp.pubsub._PubSubWriteDoFn'>)\n",
      "WARNING:apache_beam.options.pipeline_options:Bucket specified in temp_location has soft-delete policy enabled. To avoid being billed for unnecessary storage costs, turn off the soft delete feature on buckets that your Dataflow jobs use for temporary and staging storage. For more information, see https://cloud.google.com/storage/docs/use-soft-delete#remove-soft-delete-policy.\n",
      "WARNING:apache_beam.options.pipeline_options:Bucket specified in staging_location has soft-delete policy enabled. To avoid being billed for unnecessary storage costs, turn off the soft delete feature on buckets that your Dataflow jobs use for temporary and staging storage. For more information, see https://cloud.google.com/storage/docs/use-soft-delete#remove-soft-delete-policy.\n",
      "WARNING:apache_beam.transforms.core:('No iterator is returned by the process method in %s.', <class 'apache_beam.io.gcp.pubsub._PubSubWriteDoFn'>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Streaming job with event-mode model hot-swap started!\n",
      "Monitor: https://console.cloud.google.com/dataflow/jobs/us-central1?project=statmike-mlops-349915\n",
      "\n",
      "======================================================================\n",
      "IMPORTANT: Wait 3-5 minutes for Dataflow workers to start\n",
      "======================================================================\n",
      "The pipeline needs time to:\n",
      "  1. Provision workers (2-3 minutes)\n",
      "  2. Initialize the environment and load PyTorch model\n",
      "  3. Connect to Pub/Sub subscriptions (data + model updates)\n",
      "\n",
      "Once workers are running, you can:\n",
      "  - Send test data to the input topic\n",
      "  - Publish model update events to swap models at runtime\n",
      "\n",
      "This pipeline achieves ~40ms average latency:\n",
      "   - Trigger fires: Every 1 second\n",
      "   - Batch formation: 0-10ms (RunInference internal BatchElements)\n",
      "   - Model inference: ~25ms (batch processing)\n",
      "   - Pub/Sub delivery: ~5ms\n",
      "\n",
      "Model hot-swap: Publish to model update topic to swap models\n",
      "   Model update subscription: projects/statmike-mlops-349915/subscriptions/pytorch-autoencoder-model-update-sub\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "from apache_beam.transforms import trigger\n",
    "from apache_beam.ml.inference.base import ModelMetadata\n",
    "\n",
    "# Build the pipeline\n",
    "p = beam.Pipeline(options=options)\n",
    "\n",
    "# Model update side input\n",
    "# Reads from a dedicated Pub/Sub subscription for model update events.\n",
    "# Each message contains {\"model_path\": \"gs://...\"} which is parsed into ModelMetadata.\n",
    "# Uses GlobalWindows with AfterCount(1) trigger so every update is processed immediately.\n",
    "# This PCollection is passed as a side input to RunInference via model_metadata_pcollection.\n",
    "model_updates = (\n",
    "    p\n",
    "    | \"Read Model Updates\" >> ReadFromPubSub(subscription=MODEL_UPDATE_SUB)\n",
    "    | \"Parse Model Update\" >> beam.Map(lambda msg: (\n",
    "        lambda d: ModelMetadata(\n",
    "            model_id=d[\"model_path\"],\n",
    "            model_name=d[\"model_path\"].split(\"/\")[-1]\n",
    "        )\n",
    "    )(json.loads(msg.decode(\"utf-8\"))))\n",
    "    | \"Global Window\" >> beam.WindowInto(\n",
    "          window.GlobalWindows(),\n",
    "          trigger=trigger.Repeatedly(trigger.AfterCount(1)),\n",
    "          accumulation_mode=trigger.AccumulationMode.DISCARDING\n",
    "      )\n",
    ")\n",
    "\n",
    "# Main data pipeline\n",
    "results = (\n",
    "    p\n",
    "    # Step 1: Read messages from Pub/Sub subscription (unbounded stream)\n",
    "    | \"Read from Pub/Sub\" >> ReadFromPubSub(subscription=INPUT_SUB)\n",
    "    \n",
    "    # Step 2: Parse JSON bytes to (metadata_key_str, features_tensor)\n",
    "    | \"Parse JSON\" >> beam.Map(parse_json)\n",
    "    \n",
    "    # Step 3: Assign 1-second windows with 1-second early triggers\n",
    "    # - Windows provide watermark progression (required for streaming)\n",
    "    # - Triggers fire every 1 second to release messages early\n",
    "    # - RunInference handles batching internally via handler constructor params\n",
    "    | \"Window (1 sec, trigger 1 sec)\" >> beam.WindowInto(\n",
    "        window.FixedWindows(1),\n",
    "        trigger=trigger.Repeatedly(\n",
    "            trigger.AfterProcessingTime(1)\n",
    "        ),\n",
    "        accumulation_mode=trigger.AccumulationMode.DISCARDING\n",
    "    )\n",
    "    \n",
    "    # Step 4: RunInference with KeyedModelHandler + event-mode model updates\n",
    "    # - KeyedModelHandler wraps handler to accept (key, element) tuples\n",
    "    # - BatchElements applied internally (min=1, max=50, max_wait=10ms)\n",
    "    # - model_metadata_pcollection provides side input for model hot-swap\n",
    "    # - PredictionResult.model_id tracks which model produced each prediction\n",
    "    | \"RunInference\" >> RunInference(\n",
    "          model_handler=keyed_handler,\n",
    "          model_metadata_pcollection=model_updates\n",
    "      )\n",
    "    \n",
    "    # Step 5: Format predictions for output\n",
    "    # - Unpacks (metadata_key_str, PredictionResult)\n",
    "    # - Converts tensor outputs to Python types\n",
    "    # - Includes model_id for version tracking\n",
    "    | \"Format results\" >> beam.Map(format_result)\n",
    ")\n",
    "\n",
    "# Write to Pub/Sub output topic (for downstream real-time processing)\n",
    "_ = (\n",
    "    results \n",
    "    | \"To JSON\" >> beam.Map(to_json) \n",
    "    | \"Write to Pub/Sub\" >> WriteToPubSub(topic=OUTPUT_TOPIC)\n",
    ")\n",
    "\n",
    "# Write to BigQuery (for storage and analysis, including model version tracking)\n",
    "_ = (\n",
    "    results \n",
    "    | \"Write to BigQuery\" >> WriteToBigQuery(\n",
    "        table=f\"{PROJECT_ID}:{BQ_DATASET}.{BQ_TABLE}\",\n",
    "        write_disposition=beam.io.BigQueryDisposition.WRITE_APPEND\n",
    "    )\n",
    ")\n",
    "\n",
    "result = p.run()\n",
    "print(\"\\nStreaming job with event-mode model hot-swap started!\")\n",
    "print(f\"Monitor: https://console.cloud.google.com/dataflow/jobs/{REGION}?project={PROJECT_ID}\")\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"IMPORTANT: Wait 3-5 minutes for Dataflow workers to start\")\n",
    "print(\"=\" * 70)\n",
    "print(\"The pipeline needs time to:\")\n",
    "print(\"  1. Provision workers (2-3 minutes)\")\n",
    "print(\"  2. Initialize the environment and load PyTorch model\")\n",
    "print(\"  3. Connect to Pub/Sub subscriptions (data + model updates)\")\n",
    "print(\"\\nOnce workers are running, you can:\")\n",
    "print(\"  - Send test data to the input topic\")\n",
    "print(\"  - Publish model update events to swap models at runtime\")\n",
    "print(\"\\nThis pipeline achieves ~40ms average latency:\")\n",
    "print(\"   - Trigger fires: Every 1 second\")\n",
    "print(\"   - Batch formation: 0-10ms (RunInference internal BatchElements)\")\n",
    "print(\"   - Model inference: ~25ms (batch processing)\")\n",
    "print(\"   - Pub/Sub delivery: ~5ms\")\n",
    "print(\"\\nModel hot-swap: Publish to model update topic to swap models\")\n",
    "print(f\"   Model update subscription: {MODEL_UPDATE_SUB}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "simulate",
   "metadata": {},
   "source": [
    "---\n",
    "## Simulate Streaming Data\n",
    "\n",
    "**Wait 3-5 minutes** after starting the Dataflow job before running this cell. Check the [Dataflow Console](https://console.cloud.google.com/dataflow/jobs/us-central1) to verify workers are running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a578076b",
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(5*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "simulate_c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Published message 1\n",
      "Published message 2\n",
      "Published message 3\n",
      "Published message 4\n",
      "Published message 5\n",
      "\n",
      "Sent 5 test messages to LOCAL input topic: projects/statmike-mlops-349915/topics/pytorch-autoencoder-input-local\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import pubsub_v1\n",
    "import time\n",
    "\n",
    "publisher = pubsub_v1.PublisherClient()\n",
    "# Publish to LOCAL-specific input topic\n",
    "topic_path = publisher.topic_path(PROJECT_ID, f\"{EXPERIMENT}-input-local\")\n",
    "\n",
    "# Send test messages\n",
    "for i in range(5):\n",
    "    message = {\"features\": [0.1] * 30}  # Dummy transaction\n",
    "    publisher.publish(topic_path, json.dumps(message).encode(\"utf-8\"))\n",
    "    print(f\"Published message {i+1}\")\n",
    "    time.sleep(2)\n",
    "\n",
    "print(f\"\\nSent 5 test messages to LOCAL input topic: {topic_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "monitor",
   "metadata": {},
   "source": [
    "### Monitor Results\n",
    "\n",
    "**Wait 2-3 minutes** after sending test messages for the pipeline to process them.\n",
    "\n",
    "Monitor results from both output destinations: BigQuery (storage/analysis) and Pub/Sub (downstream processing).\n",
    "\n",
    "Note the `model_id` column in BigQuery results -- this shows the GCS path of the model that produced each prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f44ad1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "monitor_c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "BIGQUERY RESULTS (Storage & Analysis)\n",
      "============================================================\n",
      "Found 10 results in BigQuery\n",
      "\n",
      "Model ID (GCS path of model used):\n",
      "  gs://statmike-mlops-349915/frameworks/pytorch-autoencoder/dataflow/v1/final_model_traced.pt: 5 predictions\n",
      "  gs://statmike-mlops-349915/frameworks/pytorch-autoencoder/dataflow/final_model_traced.pt: 5 predictions\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instance_id</th>\n",
       "      <th>anomaly_score</th>\n",
       "      <th>encoded</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>window_start</th>\n",
       "      <th>window_end</th>\n",
       "      <th>pipeline_output_time</th>\n",
       "      <th>model_id</th>\n",
       "      <th>test_id</th>\n",
       "      <th>message_id</th>\n",
       "      <th>publish_time</th>\n",
       "      <th>sequence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9042935888746843648</td>\n",
       "      <td>2324.92627</td>\n",
       "      <td>[0.0, 0.0, 5.395322322845459, 0.0]</td>\n",
       "      <td>2026-02-11 18:50:37.937586+00:00</td>\n",
       "      <td>2026-02-11 18:48:42+00:00</td>\n",
       "      <td>2026-02-11 18:48:43+00:00</td>\n",
       "      <td>1.770836e+09</td>\n",
       "      <td>gs://statmike-mlops-349915/frameworks/pytorch-...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1199747387048409500</td>\n",
       "      <td>2324.92627</td>\n",
       "      <td>[0.0, 0.0, 5.395322322845459, 0.0]</td>\n",
       "      <td>2026-02-11 18:50:36.483784+00:00</td>\n",
       "      <td>2026-02-11 18:48:40+00:00</td>\n",
       "      <td>2026-02-11 18:48:41+00:00</td>\n",
       "      <td>1.770836e+09</td>\n",
       "      <td>gs://statmike-mlops-349915/frameworks/pytorch-...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-483126712892260412</td>\n",
       "      <td>2324.92627</td>\n",
       "      <td>[0.0, 0.0, 5.395322322845459, 0.0]</td>\n",
       "      <td>2026-02-11 18:50:36.482364+00:00</td>\n",
       "      <td>2026-02-11 18:48:35+00:00</td>\n",
       "      <td>2026-02-11 18:48:36+00:00</td>\n",
       "      <td>1.770836e+09</td>\n",
       "      <td>gs://statmike-mlops-349915/frameworks/pytorch-...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9042935888746843648</td>\n",
       "      <td>2324.92627</td>\n",
       "      <td>[0.0, 0.0, 5.395322322845459, 0.0]</td>\n",
       "      <td>2026-02-11 18:50:36.472241+00:00</td>\n",
       "      <td>2026-02-11 18:48:38+00:00</td>\n",
       "      <td>2026-02-11 18:48:39+00:00</td>\n",
       "      <td>1.770836e+09</td>\n",
       "      <td>gs://statmike-mlops-349915/frameworks/pytorch-...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1920919322843888324</td>\n",
       "      <td>2324.92627</td>\n",
       "      <td>[0.0, 0.0, 5.395322322845459, 0.0]</td>\n",
       "      <td>2026-02-11 18:50:36.472241+00:00</td>\n",
       "      <td>2026-02-11 18:48:36+00:00</td>\n",
       "      <td>2026-02-11 18:48:37+00:00</td>\n",
       "      <td>1.770836e+09</td>\n",
       "      <td>gs://statmike-mlops-349915/frameworks/pytorch-...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4476922047404426470</td>\n",
       "      <td>2324.92627</td>\n",
       "      <td>[0.0, 0.0, 5.395322322845459, 0.0]</td>\n",
       "      <td>2026-02-11 17:01:54.967571+00:00</td>\n",
       "      <td>2026-02-11 17:01:52+00:00</td>\n",
       "      <td>2026-02-11 17:01:53+00:00</td>\n",
       "      <td>1.770829e+09</td>\n",
       "      <td>gs://statmike-mlops-349915/frameworks/pytorch-...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>8200601297703644321</td>\n",
       "      <td>2324.92627</td>\n",
       "      <td>[0.0, 0.0, 5.395322322845459, 0.0]</td>\n",
       "      <td>2026-02-11 17:01:52.836684+00:00</td>\n",
       "      <td>2026-02-11 17:01:50+00:00</td>\n",
       "      <td>2026-02-11 17:01:51+00:00</td>\n",
       "      <td>1.770829e+09</td>\n",
       "      <td>gs://statmike-mlops-349915/frameworks/pytorch-...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8200601297703644321</td>\n",
       "      <td>2324.92627</td>\n",
       "      <td>[0.0, 0.0, 5.395322322845459, 0.0]</td>\n",
       "      <td>2026-02-11 17:01:52.828933+00:00</td>\n",
       "      <td>2026-02-11 17:01:48+00:00</td>\n",
       "      <td>2026-02-11 17:01:49+00:00</td>\n",
       "      <td>1.770829e+09</td>\n",
       "      <td>gs://statmike-mlops-349915/frameworks/pytorch-...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8200601297703644321</td>\n",
       "      <td>2324.92627</td>\n",
       "      <td>[0.0, 0.0, 5.395322322845459, 0.0]</td>\n",
       "      <td>2026-02-11 17:01:52.174930+00:00</td>\n",
       "      <td>2026-02-11 17:01:45+00:00</td>\n",
       "      <td>2026-02-11 17:01:46+00:00</td>\n",
       "      <td>1.770829e+09</td>\n",
       "      <td>gs://statmike-mlops-349915/frameworks/pytorch-...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4476922047404426470</td>\n",
       "      <td>2324.92627</td>\n",
       "      <td>[0.0, 0.0, 5.395322322845459, 0.0]</td>\n",
       "      <td>2026-02-11 17:01:51.674222+00:00</td>\n",
       "      <td>2026-02-11 17:01:46+00:00</td>\n",
       "      <td>2026-02-11 17:01:47+00:00</td>\n",
       "      <td>1.770829e+09</td>\n",
       "      <td>gs://statmike-mlops-349915/frameworks/pytorch-...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           instance_id  anomaly_score                             encoded  \\\n",
       "0  9042935888746843648     2324.92627  [0.0, 0.0, 5.395322322845459, 0.0]   \n",
       "1  1199747387048409500     2324.92627  [0.0, 0.0, 5.395322322845459, 0.0]   \n",
       "2  -483126712892260412     2324.92627  [0.0, 0.0, 5.395322322845459, 0.0]   \n",
       "3  9042935888746843648     2324.92627  [0.0, 0.0, 5.395322322845459, 0.0]   \n",
       "4  1920919322843888324     2324.92627  [0.0, 0.0, 5.395322322845459, 0.0]   \n",
       "5  4476922047404426470     2324.92627  [0.0, 0.0, 5.395322322845459, 0.0]   \n",
       "6  8200601297703644321     2324.92627  [0.0, 0.0, 5.395322322845459, 0.0]   \n",
       "7  8200601297703644321     2324.92627  [0.0, 0.0, 5.395322322845459, 0.0]   \n",
       "8  8200601297703644321     2324.92627  [0.0, 0.0, 5.395322322845459, 0.0]   \n",
       "9  4476922047404426470     2324.92627  [0.0, 0.0, 5.395322322845459, 0.0]   \n",
       "\n",
       "                         timestamp              window_start  \\\n",
       "0 2026-02-11 18:50:37.937586+00:00 2026-02-11 18:48:42+00:00   \n",
       "1 2026-02-11 18:50:36.483784+00:00 2026-02-11 18:48:40+00:00   \n",
       "2 2026-02-11 18:50:36.482364+00:00 2026-02-11 18:48:35+00:00   \n",
       "3 2026-02-11 18:50:36.472241+00:00 2026-02-11 18:48:38+00:00   \n",
       "4 2026-02-11 18:50:36.472241+00:00 2026-02-11 18:48:36+00:00   \n",
       "5 2026-02-11 17:01:54.967571+00:00 2026-02-11 17:01:52+00:00   \n",
       "6 2026-02-11 17:01:52.836684+00:00 2026-02-11 17:01:50+00:00   \n",
       "7 2026-02-11 17:01:52.828933+00:00 2026-02-11 17:01:48+00:00   \n",
       "8 2026-02-11 17:01:52.174930+00:00 2026-02-11 17:01:45+00:00   \n",
       "9 2026-02-11 17:01:51.674222+00:00 2026-02-11 17:01:46+00:00   \n",
       "\n",
       "                 window_end  pipeline_output_time  \\\n",
       "0 2026-02-11 18:48:43+00:00          1.770836e+09   \n",
       "1 2026-02-11 18:48:41+00:00          1.770836e+09   \n",
       "2 2026-02-11 18:48:36+00:00          1.770836e+09   \n",
       "3 2026-02-11 18:48:39+00:00          1.770836e+09   \n",
       "4 2026-02-11 18:48:37+00:00          1.770836e+09   \n",
       "5 2026-02-11 17:01:53+00:00          1.770829e+09   \n",
       "6 2026-02-11 17:01:51+00:00          1.770829e+09   \n",
       "7 2026-02-11 17:01:49+00:00          1.770829e+09   \n",
       "8 2026-02-11 17:01:46+00:00          1.770829e+09   \n",
       "9 2026-02-11 17:01:47+00:00          1.770829e+09   \n",
       "\n",
       "                                            model_id test_id message_id  \\\n",
       "0  gs://statmike-mlops-349915/frameworks/pytorch-...    None       None   \n",
       "1  gs://statmike-mlops-349915/frameworks/pytorch-...    None       None   \n",
       "2  gs://statmike-mlops-349915/frameworks/pytorch-...    None       None   \n",
       "3  gs://statmike-mlops-349915/frameworks/pytorch-...    None       None   \n",
       "4  gs://statmike-mlops-349915/frameworks/pytorch-...    None       None   \n",
       "5  gs://statmike-mlops-349915/frameworks/pytorch-...    None       None   \n",
       "6  gs://statmike-mlops-349915/frameworks/pytorch-...    None       None   \n",
       "7  gs://statmike-mlops-349915/frameworks/pytorch-...    None       None   \n",
       "8  gs://statmike-mlops-349915/frameworks/pytorch-...    None       None   \n",
       "9  gs://statmike-mlops-349915/frameworks/pytorch-...    None       None   \n",
       "\n",
       "   publish_time  sequence  \n",
       "0           NaN      <NA>  \n",
       "1           NaN      <NA>  \n",
       "2           NaN      <NA>  \n",
       "3           NaN      <NA>  \n",
       "4           NaN      <NA>  \n",
       "5           NaN      <NA>  \n",
       "6           NaN      <NA>  \n",
       "7           NaN      <NA>  \n",
       "8           NaN      <NA>  \n",
       "9           NaN      <NA>  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "PUB/SUB OUTPUT TOPIC (Downstream Processing - LOCAL)\n",
      "============================================================\n",
      "Pulling messages from: projects/statmike-mlops-349915/subscriptions/pytorch-autoencoder-output-sub-local\n",
      "Found 5 messages in output topic\n",
      "\n",
      "Sample messages:\n",
      "\n",
      "Message 1:\n",
      "  Anomaly Score: 2324.92626953125\n",
      "  Model ID: gs://statmike-mlops-349915/frameworks/pytorch-autoencoder/dataflow/v1/final_model_traced.pt\n",
      "  Timestamp: 2026-02-11T18:50:37.937586\n",
      "\n",
      "Message 2:\n",
      "  Anomaly Score: 2324.92626953125\n",
      "  Model ID: gs://statmike-mlops-349915/frameworks/pytorch-autoencoder/dataflow/v1/final_model_traced.pt\n",
      "  Timestamp: 2026-02-11T18:50:36.483784\n",
      "\n",
      "Message 3:\n",
      "  Anomaly Score: 2324.92626953125\n",
      "  Model ID: gs://statmike-mlops-349915/frameworks/pytorch-autoencoder/dataflow/v1/final_model_traced.pt\n",
      "  Timestamp: 2026-02-11T18:50:36.472241\n",
      "\n",
      "Acknowledged 5 messages\n",
      "\n",
      "============================================================\n",
      "Pipeline Status Summary\n",
      "============================================================\n",
      "Pipeline is working correctly!\n",
      "   Total results in BigQuery: 10\n",
      "   Results are being written to both:\n",
      "     - BigQuery table: statmike-mlops-349915.frameworks.pytorch_autoencoder_streaming_results_keyed\n",
      "     - Pub/Sub topic: projects/statmike-mlops-349915/topics/pytorch-autoencoder-output-local\n",
      "   Model tracking: model_id column shows GCS path of model used\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import bigquery\n",
    "from google.cloud import pubsub_v1\n",
    "\n",
    "# Monitor BigQuery results\n",
    "print(\"=\" * 60)\n",
    "print(\"BIGQUERY RESULTS (Storage & Analysis)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "bq = bigquery.Client(project=PROJECT_ID)\n",
    "query = f\"SELECT * FROM `{PROJECT_ID}.{BQ_DATASET}.{BQ_TABLE}` ORDER BY timestamp DESC LIMIT 10\"\n",
    "df = bq.query(query).to_dataframe()\n",
    "\n",
    "if len(df) > 0:\n",
    "    print(f\"Found {len(df)} results in BigQuery\")\n",
    "    print(f\"\\nModel ID (GCS path of model used):\")\n",
    "    for model_id in df['model_id'].unique():\n",
    "        count = len(df[df['model_id'] == model_id])\n",
    "        print(f\"  {model_id}: {count} predictions\")\n",
    "    display(df)\n",
    "else:\n",
    "    print(\"No results yet in BigQuery\")\n",
    "    print(\"   Wait a few minutes for the pipeline to process data\")\n",
    "\n",
    "# Monitor Pub/Sub output topic (LOCAL-specific)\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PUB/SUB OUTPUT TOPIC (Downstream Processing - LOCAL)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "subscriber = pubsub_v1.SubscriberClient()\n",
    "output_sub_path = subscriber.subscription_path(PROJECT_ID, f\"{EXPERIMENT}-output-sub-local\")\n",
    "\n",
    "print(f\"Pulling messages from: {output_sub_path}\")\n",
    "\n",
    "# Pull messages from output subscription\n",
    "try:\n",
    "    response = subscriber.pull(\n",
    "        request={\"subscription\": output_sub_path, \"max_messages\": 5},\n",
    "        timeout=10\n",
    "    )\n",
    "    \n",
    "    if response.received_messages:\n",
    "        print(f\"Found {len(response.received_messages)} messages in output topic\")\n",
    "        print(\"\\nSample messages:\")\n",
    "        for i, msg in enumerate(response.received_messages[:3], 1):\n",
    "            data = json.loads(msg.message.data.decode(\"utf-8\"))\n",
    "            print(f\"\\nMessage {i}:\")\n",
    "            print(f\"  Anomaly Score: {data.get('anomaly_score', 'N/A')}\")\n",
    "            print(f\"  Model ID: {data.get('model_id', 'N/A')}\")\n",
    "            print(f\"  Timestamp: {data.get('timestamp', 'N/A')}\")\n",
    "        \n",
    "        # Acknowledge messages (optional - remove if you want to keep them)\n",
    "        ack_ids = [msg.ack_id for msg in response.received_messages]\n",
    "        subscriber.acknowledge(request={\"subscription\": output_sub_path, \"ack_ids\": ack_ids})\n",
    "        print(f\"\\nAcknowledged {len(ack_ids)} messages\")\n",
    "    else:\n",
    "        print(\"No messages currently in output subscription\")\n",
    "        print(\"   Messages may have been consumed already or not yet published\")\n",
    "        \n",
    "except Exception as e:\n",
    "    if \"DeadlineExceeded\" in str(type(e).__name__):\n",
    "        print(\"No messages available in output subscription (timeout)\")\n",
    "        print(\"   This is normal - messages are consumed quickly or not yet available\")\n",
    "    else:\n",
    "        print(f\"Error pulling messages: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Pipeline Status Summary\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Get total count from BigQuery\n",
    "count_query = f\"SELECT COUNT(*) as total FROM `{PROJECT_ID}.{BQ_DATASET}.{BQ_TABLE}`\"\n",
    "count_result = bq.query(count_query).to_dataframe()\n",
    "total_results = count_result['total'].iloc[0] if len(count_result) > 0 else 0\n",
    "\n",
    "if total_results > 0:\n",
    "    print(f\"Pipeline is working correctly!\")\n",
    "    print(f\"   Total results in BigQuery: {total_results}\")\n",
    "    print(f\"   Results are being written to both:\")\n",
    "    print(f\"     - BigQuery table: {PROJECT_ID}.{BQ_DATASET}.{BQ_TABLE}\")\n",
    "    print(f\"     - Pub/Sub topic: {OUTPUT_TOPIC}\")\n",
    "    print(f\"   Model tracking: model_id column shows GCS path of model used\")\n",
    "else:\n",
    "    print(\"No results yet in BigQuery\")\n",
    "    print(\"   Pipeline may still be processing or waiting for data\")\n",
    "    \n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model_update_section",
   "metadata": {},
   "source": [
    "---\n",
    "## Swap to Version 2 (via Model Registry)\n",
    "\n",
    "Event-mode hot-swap in action: publish a new model path to the update topic to swap the model **without restarting the pipeline**. The model path comes from **Vertex AI Model Registry**.\n",
    "\n",
    "### Production Workflow\n",
    "\n",
    "1. Training pipeline produces a new model and uploads artifacts to GCS\n",
    "2. CI/CD registers it as a new version in Model Registry\n",
    "3. Operator (or automation) retrieves the version's `artifact_uri` from the registry\n",
    "4. Publish the path to the model update Pub/Sub topic\n",
    "5. All pipeline workers swap to the new model without restart\n",
    "\n",
    "### What Happens During the Swap\n",
    "\n",
    "1. Message published to model update topic with v2's GCS path (from registry)\n",
    "2. Side input PCollection parses it into `ModelMetadata`\n",
    "3. Workers detect new `model_id`, call `update_model_path()` + `load_model()` with thread lock\n",
    "4. All subsequent `PredictionResult.model_id` values reflect v2's path\n",
    "5. BigQuery `model_id` column tracks the transition\n",
    "\n",
    "### Worker Consistency During Transition\n",
    "\n",
    "Different workers swap at slightly different times. During the brief transition:\n",
    "- Some workers may still use v1, others have already loaded v2\n",
    "- The `model_id` column in every output row tells you exactly which model was used\n",
    "- This is by design -- no downtime, graceful transition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "model_update_c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Published model update to v2:\n",
      "  Source: Vertex AI Model Registry 'pytorch-autoencoder-dataflow'\n",
      "  Path:   gs://statmike-mlops-349915/frameworks/pytorch-autoencoder/dataflow/v2/final_model_traced.pt\n",
      "\n",
      "Waiting 30 seconds for model swap to propagate...\n",
      "Published message 1 (post-swap)\n",
      "Published message 2 (post-swap)\n",
      "Published message 3 (post-swap)\n",
      "Published message 4 (post-swap)\n",
      "Published message 5 (post-swap)\n",
      "\n",
      "Waiting 2 minutes for results to appear in BigQuery...\n",
      "Ready to verify the swap.\n"
     ]
    }
   ],
   "source": [
    "# Swap to Version 2 using path from Vertex AI Model Registry\n",
    "from google.cloud import pubsub_v1\n",
    "\n",
    "publisher = pubsub_v1.PublisherClient()\n",
    "topic_path = publisher.topic_path(PROJECT_ID, MODEL_UPDATE_TOPIC.split(\"/\")[-1])\n",
    "\n",
    "# MODEL_PATH_V2 was retrieved from Vertex AI Model Registry\n",
    "update_event = {\"model_path\": MODEL_PATH_V2}\n",
    "\n",
    "publisher.publish(topic_path, json.dumps(update_event).encode(\"utf-8\"))\n",
    "print(f\"Published model update to v2:\")\n",
    "print(f\"  Source: Vertex AI Model Registry '{MODEL_DISPLAY_NAME}'\")\n",
    "print(f\"  Path:   {MODEL_PATH_V2}\")\n",
    "\n",
    "# Wait for swap to propagate, then send test data\n",
    "print(f\"\\nWaiting 30 seconds for model swap to propagate...\")\n",
    "time.sleep(30)\n",
    "\n",
    "# Send test data to capture predictions with new model\n",
    "topic_input = publisher.topic_path(PROJECT_ID, f\"{EXPERIMENT}-input-local\")\n",
    "for i in range(5):\n",
    "    message = {\"features\": [0.1] * 30}\n",
    "    publisher.publish(topic_input, json.dumps(message).encode(\"utf-8\"))\n",
    "    print(f\"Published message {i+1} (post-swap)\")\n",
    "    time.sleep(2)\n",
    "\n",
    "print(f\"\\nWaiting 2 minutes for results to appear in BigQuery...\")\n",
    "time.sleep(120)\n",
    "print(\"Ready to verify the swap.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verify_swap_md",
   "metadata": {},
   "source": [
    "### Verify Model Swap in BigQuery\n",
    "\n",
    "The previous cell published a model update event (v2 from Model Registry) and sent test data. The pipeline should now have predictions from both v1 and v2.\n",
    "\n",
    "Query BigQuery to see the `model_id` transition -- you should see two distinct `model_id` values (GCS paths corresponding to the two Model Registry versions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "verify_swap_c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "MODEL VERSION TRACKING\n",
      "============================================================\n",
      "Found 2 distinct model version(s):\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_id</th>\n",
       "      <th>predictions</th>\n",
       "      <th>first</th>\n",
       "      <th>last</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gs://statmike-mlops-349915/frameworks/pytorch-...</td>\n",
       "      <td>5</td>\n",
       "      <td>2026-02-11 17:01:51.674222+00:00</td>\n",
       "      <td>2026-02-11 17:01:54.967571+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gs://statmike-mlops-349915/frameworks/pytorch-...</td>\n",
       "      <td>10</td>\n",
       "      <td>2026-02-11 18:50:36.472241+00:00</td>\n",
       "      <td>2026-02-11 18:51:32.893198+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            model_id  predictions  \\\n",
       "0  gs://statmike-mlops-349915/frameworks/pytorch-...            5   \n",
       "1  gs://statmike-mlops-349915/frameworks/pytorch-...           10   \n",
       "\n",
       "                             first                             last  \n",
       "0 2026-02-11 17:01:51.674222+00:00 2026-02-11 17:01:54.967571+00:00  \n",
       "1 2026-02-11 18:50:36.472241+00:00 2026-02-11 18:51:32.893198+00:00  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model swap detected! Multiple model versions used.\n",
      "Each row shows a different model (GCS path) and when it was active.\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import bigquery\n",
    "\n",
    "bq = bigquery.Client(project=PROJECT_ID)\n",
    "\n",
    "# Query to see model versions used over time\n",
    "query = f\"\"\"\n",
    "SELECT \n",
    "    model_id, \n",
    "    COUNT(*) as predictions, \n",
    "    MIN(timestamp) as first, \n",
    "    MAX(timestamp) as last\n",
    "FROM `{PROJECT_ID}.{BQ_DATASET}.{BQ_TABLE}`\n",
    "GROUP BY model_id\n",
    "ORDER BY first\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"MODEL VERSION TRACKING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "df = bq.query(query).to_dataframe()\n",
    "\n",
    "if len(df) > 0:\n",
    "    print(f\"Found {len(df)} distinct model version(s):\\n\")\n",
    "    display(df)\n",
    "    \n",
    "    if len(df) > 1:\n",
    "        print(\"\\nModel swap detected! Multiple model versions used.\")\n",
    "        print(\"Each row shows a different model (GCS path) and when it was active.\")\n",
    "    else:\n",
    "        print(\"\\nOnly one model version seen so far.\")\n",
    "        print(\"Publish a model update event and send more data to see the swap.\")\n",
    "else:\n",
    "    print(\"No results yet in BigQuery.\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rollback_md",
   "metadata": {},
   "source": [
    "### Rollback to Version 1\n",
    "\n",
    "A key benefit of event-mode: **rollback is simply publishing the previous version's GCS path**. No pipeline restart, no redeployment.\n",
    "\n",
    "The Model Registry stores all version paths, so rolling back means:\n",
    "1. Look up v1's `artifact_uri` in the registry (already retrieved above as `MODEL_PATH_V1`)\n",
    "2. Publish it to the model update topic\n",
    "3. Workers reload v1 -- done\n",
    "\n",
    "This cell publishes v1's path, sends test data, and waits for results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "rollback_c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Published rollback to v1:\n",
      "  Source: Vertex AI Model Registry 'pytorch-autoencoder-dataflow'\n",
      "  Path:   gs://statmike-mlops-349915/frameworks/pytorch-autoencoder/dataflow/v1/final_model_traced.pt\n",
      "\n",
      "Waiting 30 seconds for model swap to propagate...\n",
      "Published message 1 (post-rollback)\n",
      "Published message 2 (post-rollback)\n",
      "Published message 3 (post-rollback)\n",
      "Published message 4 (post-rollback)\n",
      "Published message 5 (post-rollback)\n",
      "\n",
      "Waiting 2 minutes for results to appear in BigQuery...\n",
      "Ready to verify the rollback.\n"
     ]
    }
   ],
   "source": [
    "# Rollback to Version 1 using path from Vertex AI Model Registry\n",
    "publisher = pubsub_v1.PublisherClient()\n",
    "topic_path = publisher.topic_path(PROJECT_ID, MODEL_UPDATE_TOPIC.split(\"/\")[-1])\n",
    "\n",
    "# MODEL_PATH_V1 was retrieved from Vertex AI Model Registry\n",
    "rollback_event = {\"model_path\": MODEL_PATH_V1}\n",
    "\n",
    "publisher.publish(topic_path, json.dumps(rollback_event).encode(\"utf-8\"))\n",
    "print(f\"Published rollback to v1:\")\n",
    "print(f\"  Source: Vertex AI Model Registry '{MODEL_DISPLAY_NAME}'\")\n",
    "print(f\"  Path:   {MODEL_PATH_V1}\")\n",
    "\n",
    "# Wait for swap to propagate, then send test data\n",
    "print(f\"\\nWaiting 30 seconds for model swap to propagate...\")\n",
    "time.sleep(30)\n",
    "\n",
    "# Send test data to capture predictions with rolled-back model\n",
    "topic_input = publisher.topic_path(PROJECT_ID, f\"{EXPERIMENT}-input-local\")\n",
    "for i in range(5):\n",
    "    message = {\"features\": [0.1] * 30}\n",
    "    publisher.publish(topic_input, json.dumps(message).encode(\"utf-8\"))\n",
    "    print(f\"Published message {i+1} (post-rollback)\")\n",
    "    time.sleep(2)\n",
    "\n",
    "print(f\"\\nWaiting 2 minutes for results to appear in BigQuery...\")\n",
    "time.sleep(120)\n",
    "print(\"Ready to verify the rollback.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rollback_verify_md",
   "metadata": {},
   "source": [
    "### Verify Rollback in BigQuery\n",
    "\n",
    "After the rollback, BigQuery should show predictions from three phases:\n",
    "1. **v1** (initial): Predictions before the first swap\n",
    "2. **v2** (swap): Predictions after swapping to version 2\n",
    "3. **v1** (rollback): Predictions after rolling back to version 1\n",
    "\n",
    "The `model_id` column tracks the full lifecycle: v1 -> v2 -> v1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "rollback_verify_c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "MODEL VERSION SUMMARY (After Rollback)\n",
      "============================================================\n",
      "Found 2 distinct model version(s):\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_id</th>\n",
       "      <th>predictions</th>\n",
       "      <th>first_seen</th>\n",
       "      <th>last_seen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gs://statmike-mlops-349915/frameworks/pytorch-...</td>\n",
       "      <td>5</td>\n",
       "      <td>2026-02-11 17:01:51.674222+00:00</td>\n",
       "      <td>2026-02-11 17:01:54.967571+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gs://statmike-mlops-349915/frameworks/pytorch-...</td>\n",
       "      <td>15</td>\n",
       "      <td>2026-02-11 18:50:36.472241+00:00</td>\n",
       "      <td>2026-02-11 18:54:16.853125+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            model_id  predictions  \\\n",
       "0  gs://statmike-mlops-349915/frameworks/pytorch-...            5   \n",
       "1  gs://statmike-mlops-349915/frameworks/pytorch-...           15   \n",
       "\n",
       "                        first_seen                        last_seen  \n",
       "0 2026-02-11 17:01:51.674222+00:00 2026-02-11 17:01:54.967571+00:00  \n",
       "1 2026-02-11 18:50:36.472241+00:00 2026-02-11 18:54:16.853125+00:00  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Both model versions visible:\n",
      "  v1 predictions span initial run AND after rollback\n",
      "  v2 predictions span only the period between swaps\n",
      "\n",
      "============================================================\n",
      "TIMELINE (Last 20 predictions)\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>model_id</th>\n",
       "      <th>anomaly_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2026-02-11 18:54:16.853125+00:00</td>\n",
       "      <td>gs://statmike-mlops-349915/frameworks/pytorch-...</td>\n",
       "      <td>2324.92627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2026-02-11 18:54:15.000796+00:00</td>\n",
       "      <td>gs://statmike-mlops-349915/frameworks/pytorch-...</td>\n",
       "      <td>2324.92627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2026-02-11 18:54:11.975828+00:00</td>\n",
       "      <td>gs://statmike-mlops-349915/frameworks/pytorch-...</td>\n",
       "      <td>2324.92627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2026-02-11 18:54:10.799836+00:00</td>\n",
       "      <td>gs://statmike-mlops-349915/frameworks/pytorch-...</td>\n",
       "      <td>2324.92627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2026-02-11 18:54:10.473979+00:00</td>\n",
       "      <td>gs://statmike-mlops-349915/frameworks/pytorch-...</td>\n",
       "      <td>2324.92627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2026-02-11 18:51:32.893198+00:00</td>\n",
       "      <td>gs://statmike-mlops-349915/frameworks/pytorch-...</td>\n",
       "      <td>2324.92627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2026-02-11 18:51:30.855428+00:00</td>\n",
       "      <td>gs://statmike-mlops-349915/frameworks/pytorch-...</td>\n",
       "      <td>2324.92627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2026-02-11 18:51:29.119211+00:00</td>\n",
       "      <td>gs://statmike-mlops-349915/frameworks/pytorch-...</td>\n",
       "      <td>2324.92627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2026-02-11 18:51:26.888372+00:00</td>\n",
       "      <td>gs://statmike-mlops-349915/frameworks/pytorch-...</td>\n",
       "      <td>2324.92627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2026-02-11 18:51:24.793045+00:00</td>\n",
       "      <td>gs://statmike-mlops-349915/frameworks/pytorch-...</td>\n",
       "      <td>2324.92627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2026-02-11 18:50:37.937586+00:00</td>\n",
       "      <td>gs://statmike-mlops-349915/frameworks/pytorch-...</td>\n",
       "      <td>2324.92627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2026-02-11 18:50:36.483784+00:00</td>\n",
       "      <td>gs://statmike-mlops-349915/frameworks/pytorch-...</td>\n",
       "      <td>2324.92627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2026-02-11 18:50:36.482364+00:00</td>\n",
       "      <td>gs://statmike-mlops-349915/frameworks/pytorch-...</td>\n",
       "      <td>2324.92627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2026-02-11 18:50:36.472241+00:00</td>\n",
       "      <td>gs://statmike-mlops-349915/frameworks/pytorch-...</td>\n",
       "      <td>2324.92627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2026-02-11 18:50:36.472241+00:00</td>\n",
       "      <td>gs://statmike-mlops-349915/frameworks/pytorch-...</td>\n",
       "      <td>2324.92627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2026-02-11 17:01:54.967571+00:00</td>\n",
       "      <td>gs://statmike-mlops-349915/frameworks/pytorch-...</td>\n",
       "      <td>2324.92627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2026-02-11 17:01:52.836684+00:00</td>\n",
       "      <td>gs://statmike-mlops-349915/frameworks/pytorch-...</td>\n",
       "      <td>2324.92627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2026-02-11 17:01:52.828933+00:00</td>\n",
       "      <td>gs://statmike-mlops-349915/frameworks/pytorch-...</td>\n",
       "      <td>2324.92627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2026-02-11 17:01:52.174930+00:00</td>\n",
       "      <td>gs://statmike-mlops-349915/frameworks/pytorch-...</td>\n",
       "      <td>2324.92627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2026-02-11 17:01:51.674222+00:00</td>\n",
       "      <td>gs://statmike-mlops-349915/frameworks/pytorch-...</td>\n",
       "      <td>2324.92627</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          timestamp  \\\n",
       "0  2026-02-11 18:54:16.853125+00:00   \n",
       "1  2026-02-11 18:54:15.000796+00:00   \n",
       "2  2026-02-11 18:54:11.975828+00:00   \n",
       "3  2026-02-11 18:54:10.799836+00:00   \n",
       "4  2026-02-11 18:54:10.473979+00:00   \n",
       "5  2026-02-11 18:51:32.893198+00:00   \n",
       "6  2026-02-11 18:51:30.855428+00:00   \n",
       "7  2026-02-11 18:51:29.119211+00:00   \n",
       "8  2026-02-11 18:51:26.888372+00:00   \n",
       "9  2026-02-11 18:51:24.793045+00:00   \n",
       "10 2026-02-11 18:50:37.937586+00:00   \n",
       "11 2026-02-11 18:50:36.483784+00:00   \n",
       "12 2026-02-11 18:50:36.482364+00:00   \n",
       "13 2026-02-11 18:50:36.472241+00:00   \n",
       "14 2026-02-11 18:50:36.472241+00:00   \n",
       "15 2026-02-11 17:01:54.967571+00:00   \n",
       "16 2026-02-11 17:01:52.836684+00:00   \n",
       "17 2026-02-11 17:01:52.828933+00:00   \n",
       "18 2026-02-11 17:01:52.174930+00:00   \n",
       "19 2026-02-11 17:01:51.674222+00:00   \n",
       "\n",
       "                                             model_id  anomaly_score  \n",
       "0   gs://statmike-mlops-349915/frameworks/pytorch-...     2324.92627  \n",
       "1   gs://statmike-mlops-349915/frameworks/pytorch-...     2324.92627  \n",
       "2   gs://statmike-mlops-349915/frameworks/pytorch-...     2324.92627  \n",
       "3   gs://statmike-mlops-349915/frameworks/pytorch-...     2324.92627  \n",
       "4   gs://statmike-mlops-349915/frameworks/pytorch-...     2324.92627  \n",
       "5   gs://statmike-mlops-349915/frameworks/pytorch-...     2324.92627  \n",
       "6   gs://statmike-mlops-349915/frameworks/pytorch-...     2324.92627  \n",
       "7   gs://statmike-mlops-349915/frameworks/pytorch-...     2324.92627  \n",
       "8   gs://statmike-mlops-349915/frameworks/pytorch-...     2324.92627  \n",
       "9   gs://statmike-mlops-349915/frameworks/pytorch-...     2324.92627  \n",
       "10  gs://statmike-mlops-349915/frameworks/pytorch-...     2324.92627  \n",
       "11  gs://statmike-mlops-349915/frameworks/pytorch-...     2324.92627  \n",
       "12  gs://statmike-mlops-349915/frameworks/pytorch-...     2324.92627  \n",
       "13  gs://statmike-mlops-349915/frameworks/pytorch-...     2324.92627  \n",
       "14  gs://statmike-mlops-349915/frameworks/pytorch-...     2324.92627  \n",
       "15  gs://statmike-mlops-349915/frameworks/pytorch-...     2324.92627  \n",
       "16  gs://statmike-mlops-349915/frameworks/pytorch-...     2324.92627  \n",
       "17  gs://statmike-mlops-349915/frameworks/pytorch-...     2324.92627  \n",
       "18  gs://statmike-mlops-349915/frameworks/pytorch-...     2324.92627  \n",
       "19  gs://statmike-mlops-349915/frameworks/pytorch-...     2324.92627  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model transitions detected at 1 point(s) in this window\n",
      "This confirms the v1 -> v2 -> v1 lifecycle.\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import bigquery\n",
    "\n",
    "bq = bigquery.Client(project=PROJECT_ID)\n",
    "\n",
    "# Summary: model versions used\n",
    "query_summary = f\"\"\"\n",
    "SELECT\n",
    "    model_id,\n",
    "    COUNT(*) as predictions,\n",
    "    MIN(timestamp) as first_seen,\n",
    "    MAX(timestamp) as last_seen\n",
    "FROM `{PROJECT_ID}.{BQ_DATASET}.{BQ_TABLE}`\n",
    "GROUP BY model_id\n",
    "ORDER BY first_seen\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"MODEL VERSION SUMMARY (After Rollback)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "df = bq.query(query_summary).to_dataframe()\n",
    "\n",
    "if len(df) > 0:\n",
    "    print(f\"Found {len(df)} distinct model version(s):\\n\")\n",
    "    display(df)\n",
    "\n",
    "    if len(df) >= 2:\n",
    "        print(\"\\nBoth model versions visible:\")\n",
    "        print(\"  v1 predictions span initial run AND after rollback\")\n",
    "        print(\"  v2 predictions span only the period between swaps\")\n",
    "    else:\n",
    "        print(\"\\nOnly one model version seen.\")\n",
    "        print(\"Both swaps may not have completed yet.\")\n",
    "else:\n",
    "    print(\"No results yet in BigQuery.\")\n",
    "\n",
    "# Timeline: recent predictions showing v1 -> v2 -> v1 transitions\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TIMELINE (Last 20 predictions)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "query_timeline = f\"\"\"\n",
    "SELECT\n",
    "    timestamp,\n",
    "    model_id,\n",
    "    anomaly_score\n",
    "FROM `{PROJECT_ID}.{BQ_DATASET}.{BQ_TABLE}`\n",
    "ORDER BY timestamp DESC\n",
    "LIMIT 20\n",
    "\"\"\"\n",
    "\n",
    "df_timeline = bq.query(query_timeline).to_dataframe()\n",
    "\n",
    "if len(df_timeline) > 0:\n",
    "    display(df_timeline)\n",
    "\n",
    "    # Show transition points\n",
    "    model_ids = df_timeline['model_id'].tolist()\n",
    "    transitions = [i for i in range(1, len(model_ids)) if model_ids[i] != model_ids[i-1]]\n",
    "    if transitions:\n",
    "        print(f\"\\nModel transitions detected at {len(transitions)} point(s) in this window\")\n",
    "        print(\"This confirms the v1 -> v2 -> v1 lifecycle.\")\n",
    "    else:\n",
    "        print(\"\\nNo transitions in this window (all predictions from same model)\")\n",
    "        print(\"Try querying a wider time range if swaps happened earlier.\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "j4k8lillxb",
   "metadata": {},
   "source": [
    "---\n",
    "## Understanding Scaling and Performance\n",
    "\n",
    "Now that your streaming pipeline is deployed, understanding how it scales and performs under load is critical for production deployments.\n",
    "\n",
    "### Factors Affecting Performance\n",
    "\n",
    "**Service-Side Factors**:\n",
    "- **Worker Count**: Number of Dataflow workers processing data (min/max worker settings)\n",
    "- **Machine Type**: CPU and memory resources per worker\n",
    "- **Autoscaling Configuration**: How quickly new workers are provisioned under backlog\n",
    "- **Worker Startup Time**: Time for new workers to become ready and process data\n",
    "\n",
    "**Usage-Side Factors**:\n",
    "- **Message Rate**: Number of messages published to Pub/Sub per second\n",
    "- **Window Size**: Duration of windows for micro-batching (affects latency vs. throughput)\n",
    "- **Message Size**: Size of payload per message\n",
    "- **Traffic Pattern**: Constant flow vs. spikes vs. bursts\n",
    "\n",
    "**Event-Mode Factors**:\n",
    "- **Model Swap Frequency**: How often model updates are published\n",
    "- **Model Size**: Larger models take longer to download and load during swap\n",
    "- **Worker Count During Swap**: More workers = longer transition window where different workers use different models\n",
    "- **Memory During Swap**: Two models briefly in memory per worker during transition\n",
    "\n",
    "### Current Configuration\n",
    "\n",
    "Your pipeline is currently configured with:\n",
    "- **Machine Type**: `n1-standard-4` (4 vCPUs, 15 GB memory)\n",
    "- **Min Workers**: 2 (baseline parallelism)\n",
    "- **Max Workers**: 20 (autoscaling limit)\n",
    "- **Window Size**: 1 second (ultra-low latency)\n",
    "- **Model**: Local PyTorch model loaded in-process\n",
    "- **Model Updates**: Via Pub/Sub side input (event-mode)\n",
    "\n",
    "This configuration provides:\n",
    "- Ultra-low latency for real-time processing (1-second windows, 1s triggers)\n",
    "- Autoscaling for traffic spikes (up to 20x capacity)\n",
    "- Cost-effective for variable traffic (2 workers baseline)\n",
    "- High throughput (no network calls to external endpoints)\n",
    "- Runtime model updates without pipeline restart\n",
    "\n",
    "### Performance Testing\n",
    "\n",
    "To understand how this deployment scales under different load patterns and identify performance thresholds, see:\n",
    "\n",
    "**[scale-tests-dataflow-streaming-runinference.ipynb](./scale-tests-dataflow-streaming-runinference.ipynb)**\n",
    "\n",
    "This notebook demonstrates:\n",
    "- **Progressive Load Testing**: Find where pipeline performance degrades\n",
    "- **Latency Analysis**: Measure end-to-end latency breakdown\n",
    "- **Autoscaling Behavior**: Understand worker scaling patterns\n",
    "- **Tuning Recommendations**: Data-driven configuration guidance\n",
    "\n",
    "### When to Run Scale Testing\n",
    "\n",
    "Run scale tests when:\n",
    "- **Before production launch**: Understand capacity and set appropriate worker limits\n",
    "- **After model changes**: New models may have different inference latency\n",
    "- **For capacity planning**: Estimate costs for expected message volumes\n",
    "- **During incidents**: Diagnose backlog issues and identify bottlenecks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cleanup",
   "metadata": {},
   "source": [
    "---\n",
    "## Clean Up\n",
    "\n",
    "**IMPORTANT: Streaming jobs run continuously until explicitly cancelled**, incurring ongoing costs for workers and resources.\n",
    "\n",
    "### Centralized Cleanup Notebook\n",
    "\n",
    "For comprehensive cleanup of all Dataflow resources, use the centralized cleanup notebook:\n",
    "\n",
    "**[dataflow-cleanup.ipynb](./dataflow-cleanup.ipynb)**\n",
    "\n",
    "This notebook provides:\n",
    "- **Stop Dataflow Jobs**: Cancel running streaming/batch jobs created by these notebooks\n",
    "- **Clean BigQuery Tables**: Truncate or delete result tables\n",
    "- **Clean Pub/Sub Resources**: Delete topics and subscriptions (including model-update topic)\n",
    "- **Clean GCS Files**: Delete model files uploaded for Dataflow\n",
    "- **Granular Control**: Use flags to choose exactly what to clean up\n",
    "- **Safety Checks**: Warnings for risky operations\n",
    "- **Confirmation Prompts**: Review before executing\n",
    "\n",
    "### Why Use Centralized Cleanup?\n",
    "\n",
    "- **One location**: Manage all Dataflow infrastructure from a single notebook\n",
    "- **Comprehensive**: Clean up jobs, tables, Pub/Sub, and GCS files together\n",
    "- **Flexible**: Truncate tables without deleting schema (useful for testing)\n",
    "- **Safe**: Built-in safety checks and confirmation prompts\n",
    "- **Efficient**: Clean up resources from all Dataflow notebooks at once\n",
    "\n",
    "### Quick Cleanup (This Job Only)\n",
    "\n",
    "If you only need to stop the streaming job created by this notebook, uncomment and run the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "h5od5nznrw",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the line below to stop the streaming job created by this notebook\n",
    "#result.cancel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "In this notebook, you successfully:\n",
    "\n",
    "**Created Custom PyTorch ModelHandler with PredictionResult**\n",
    "- Wrapped TorchScript model for RunInference\n",
    "- Returns `PredictionResult` objects with `model_id` set to GCS path\n",
    "- `model_id` updates automatically when model swaps via event-mode\n",
    "\n",
    "**Used KeyedModelHandler for Metadata Passthrough**\n",
    "- Metadata serialized as JSON string key (no custom DoFn needed)\n",
    "- Batching handled internally by RunInference (fewer pipeline steps)\n",
    "- Same ~40ms latency as explicit BatchElements approach\n",
    "\n",
    "**Enabled Event-Mode Model Hot-Swap**\n",
    "- Dedicated Pub/Sub topic/subscription for model update events\n",
    "- `model_metadata_pcollection` side input triggers model reload on all workers\n",
    "- No pipeline restart needed -- model swaps happen at runtime\n",
    "- `model_id` column in BigQuery tracks which model produced each prediction\n",
    "\n",
    "**Deployed Streaming Dataflow Pipeline**\n",
    "- Real-time data ingestion from Pub/Sub\n",
    "- 1-second fixed windows with early triggers\n",
    "- Dual output to BigQuery and Pub/Sub\n",
    "- Continuous operation with model update capability\n",
    "\n",
    "## Key Learnings\n",
    "\n",
    "**ModelHandler Design:**\n",
    "- Return `PredictionResult` (not plain dict) to enable `model_id` tracking\n",
    "- Set `model_id=self._torch_script_model_path` -- auto-updates on model swap\n",
    "- Batching params in handler constructor replaces explicit `BatchElements`\n",
    "\n",
    "**KeyedModelHandler:**\n",
    "- Accepts `(key, element)` tuples, passes key through unchanged\n",
    "- JSON string serialization for metadata: ~1-2 microseconds overhead (negligible)\n",
    "- Delegates `batch_elements_kwargs()` to inner handler (batching preserved)\n",
    "\n",
    "**Event-Mode Model Hot-Swap:**\n",
    "- `ModelMetadata(model_id=path, model_name=filename)` parsed from Pub/Sub\n",
    "- `model_id` triggers model reload; `model_name` is for metrics namespace only\n",
    "- `GlobalWindows` + `AfterCount(1)` trigger ensures immediate processing\n",
    "- Thread lock prevents concurrent model loads on the same worker\n",
    "- GCS path IS the version identifier -- no separate version number needed\n",
    "- Rollback = publish the old GCS path again\n",
    "\n",
    "**Streaming Considerations:**\n",
    "- Workers need 3-5 minutes to provision and initialize\n",
    "- Model swap takes seconds per worker (download + load)\n",
    "- Brief transition window where different workers may use different models\n",
    "- `model_id` column provides exact version tracking for every prediction\n",
    "- Continuous jobs require manual cancellation to stop charges\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "### Continue with Dataflow Workflows:\n",
    "\n",
    "**Keyed RunInference (without event-mode):**\n",
    "- [dataflow-streaming-runinference-keyed.ipynb](./dataflow-streaming-runinference-keyed.ipynb)\n",
    "  - Same KeyedModelHandler approach without model hot-swap\n",
    "  - Simpler setup for static model deployments\n",
    "  - Good starting point before adding event-mode\n",
    "\n",
    "**Batch Inference with Vertex Endpoint:**\n",
    "- [dataflow-batch-runinference-vertex.ipynb](./dataflow-batch-runinference-vertex.ipynb)\n",
    "  - Process historical data via Vertex AI Endpoint\n",
    "  - Centralized model updates without redeploying pipeline\n",
    "\n",
    "**Streaming Inference with Vertex Endpoint:**\n",
    "- [dataflow-streaming-runinference-vertex.ipynb](./dataflow-streaming-runinference-vertex.ipynb)\n",
    "  - Real-time endpoint calls from Dataflow\n",
    "  - Shared model across multiple services\n",
    "\n",
    "**Batch Inference with Local Model:**\n",
    "- [dataflow-batch-runinference.ipynb](./dataflow-batch-runinference.ipynb)\n",
    "  - Historical data processing with in-process model\n",
    "  - Higher throughput for bounded datasets\n",
    "\n",
    "### Production Enhancements:\n",
    "\n",
    "1. **Error Handling**: Add dead letter queues for failed predictions\n",
    "2. **Monitoring**: Set up Cloud Monitoring alerts for model swap events and job failures\n",
    "3. **Autoscaling**: Configure min/max workers based on traffic patterns\n",
    "4. **Model Versioning**: Automate model update publishing from CI/CD pipelines\n",
    "5. **Data Validation**: Add schema validation for input messages and model update events\n",
    "6. **A/B Testing**: Use model_id to route traffic percentages to different model versions\n",
    "\n",
    "### Resources\n",
    "\n",
    "- [Dataflow Streaming Pipelines](https://cloud.google.com/dataflow/docs/concepts/streaming-pipelines)\n",
    "- [Apache Beam RunInference](https://beam.apache.org/documentation/ml/about-ml/)\n",
    "- [PyTorch RunInference](https://beam.apache.org/documentation/ml/pytorch-inference/)\n",
    "- [Beam Side Inputs](https://beam.apache.org/documentation/programming-guide/#side-inputs)\n",
    "- [Pub/Sub Best Practices](https://cloud.google.com/pubsub/docs/publisher)\n",
    "- [Windowing in Beam](https://beam.apache.org/documentation/programming-guide/#windowing)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (frameworks-pytorch)",
   "language": "python",
   "name": "frameworks-pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
