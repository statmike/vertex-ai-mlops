{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8ebb79e",
   "metadata": {},
   "source": [
    "![tracker](https://us-central1-vertex-ai-mlops-369716.cloudfunctions.net/pixel-tracking?path=statmike%2Fvertex-ai-mlops%2FFramework+Workflows%2FPyTorch%2Fserving&file=spanner-vertex-ai-endpoint.ipynb)\n",
    "<!--- header table --->\n",
    "<table align=\"left\">\n",
    "<tr>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%20Workflows/PyTorch/serving/spanner-vertex-ai-endpoint.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.svgrepo.com/download/217753/github.svg\" alt=\"GitHub logo\">\n",
    "      <br>View on<br>GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://colab.research.google.com/github/statmike/vertex-ai-mlops/blob/main/Framework%20Workflows/PyTorch/serving/spanner-vertex-ai-endpoint.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.gstatic.com/pantheon/images/bigquery/welcome_page/colab-logo.svg\" alt=\"Google Colaboratory logo\">\n",
    "      <br>Run in<br>Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https%3A%2F%2Fraw.githubusercontent.com%2Fstatmike%2Fvertex-ai-mlops%2Fmain%2FFramework%2520Workflows%2FPyTorch%2Fserving%2Fspanner-vertex-ai-endpoint.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\">\n",
    "      <br>Run in<br>Colab Enterprise\n",
    "    </a>\n",
    "  </td>      \n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/bigquery/import?url=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%20Workflows/PyTorch/serving/spanner-vertex-ai-endpoint.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.gstatic.com/images/branding/gcpiconscolors/bigquery/v1/32px.svg\" alt=\"BigQuery logo\">\n",
    "      <br>Open in<br>BigQuery Studio\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/statmike/vertex-ai-mlops/main/Framework%20Workflows/PyTorch/serving/spanner-vertex-ai-endpoint.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.gstatic.com/images/branding/gcpiconscolors/vertexai/v1/32px.svg\" alt=\"Vertex AI logo\">\n",
    "      <br>Open in<br>Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td colspan=\"5\" style=\"text-align: right\">\n",
    "    <b>Share This On: </b> \n",
    "    <a href=\"https://www.linkedin.com/sharing/share-offsite/?url=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%2520Workflows/PyTorch/serving/spanner-vertex-ai-endpoint.ipynb\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"Linkedin Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://reddit.com/submit?url=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%2520Workflows/PyTorch/serving/spanner-vertex-ai-endpoint.ipynb\"><img src=\"https://redditinc.com/hubfs/Reddit%20Inc/Brand/Reddit_Logo.png\" alt=\"Reddit Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://bsky.app/intent/compose?text=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%2520Workflows/PyTorch/serving/spanner-vertex-ai-endpoint.ipynb\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"BlueSky Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://twitter.com/intent/tweet?url=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%2520Workflows/PyTorch/serving/spanner-vertex-ai-endpoint.ipynb\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/5/5a/X_icon_2.svg\" alt=\"X (Twitter) Logo\" width=\"20px\"></a> \n",
    "  </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td colspan=\"5\" style=\"text-align: right\">\n",
    "    <b>Connect With Author On: </b> \n",
    "    <a href=\"https://www.linkedin.com/in/statmike\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"Linkedin Logo\" width=\"20px\"></a>\n",
    "    <a href=\"https://www.github.com/statmike\"><img src=\"https://www.svgrepo.com/download/217753/github.svg\" alt=\"GitHub Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://www.youtube.com/@statmike-channel\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/f/fd/YouTube_full-color_icon_%282024%29.svg\" alt=\"YouTube Logo\" width=\"20px\"></a>\n",
    "    <a href=\"https://bsky.app/profile/statmike.bsky.social\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"BlueSky Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://x.com/statmike\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/5/5a/X_icon_2.svg\" alt=\"X (Twitter) Logo\" width=\"20px\"></a>\n",
    "  </td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overview",
   "metadata": {},
   "source": [
    "# Spanner SQL-Based ML Inference with Vertex AI Endpoints\n",
    "\n",
    "This notebook demonstrates how to use **Cloud Spanner with GoogleSQL dialect** to call Vertex AI endpoints directly from SQL for ML inference. Spanner's ML integration enables database-native predictions, bringing AI capabilities into your globally distributed database.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "This workflow covers:\n",
    "\n",
    "1. **Prerequisites Check**: Verify deployed Vertex AI endpoint and test predictions\n",
    "2. **Spanner Infrastructure**: Create minimal instance for demo (reusable across notebooks)\n",
    "3. **GoogleSQL Database Setup**: Create database for ML inference\n",
    "4. **Load Test Data**: Import transaction data from BigQuery into Spanner\n",
    "5. **Register Vertex AI Model**: Use `CREATE MODEL` to register the endpoint\n",
    "6. **SQL-Based Predictions**: Call models with `ML.PREDICT()` \n",
    "7. **Multi-Stage Inference**: Simple predictions → Field extraction → Business logic → Batch scoring\n",
    "8. **Cleanup**: Remove all Spanner resources to avoid charges\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "This notebook requires an existing Vertex AI Endpoint with a deployed PyTorch model.\n",
    "\n",
    "**Create an endpoint using either:**\n",
    "- [vertex-ai-endpoint-prebuilt-container.ipynb](./vertex-ai-endpoint-prebuilt-container.ipynb) - Pre-built TorchServe container (recommended)\n",
    "- [vertex-ai-endpoint-custom-container.ipynb](./vertex-ai-endpoint-custom-container.ipynb) - Custom FastAPI container\n",
    "\n",
    "Both endpoints work with this notebook. The first section will find and test your deployed endpoint.\n",
    "\n",
    "## Why GoogleSQL Instead of PostgreSQL?\n",
    "\n",
    "Spanner supports two SQL dialects (GoogleSQL and PostgreSQL), but this notebook uses **GoogleSQL only** for ML inference.\n",
    "\n",
    "**After extensive testing, we found critical limitations** in Spanner's PostgreSQL dialect for ML workloads:\n",
    "\n",
    "### PostgreSQL Limitations ❌\n",
    "- Missing `json_build_object()`, `json_build_array()` functions\n",
    "- Cannot convert numeric arrays to JSON strings (`array_to_string()`)\n",
    "- No `format()` or `TO_JSON_STRING()` functions\n",
    "- Cannot concatenate text with numeric types\n",
    "- `spanner.ml_predict_row()` only works with hard-coded literal values\n",
    "- **Cannot dynamically build JSON payloads from table data**\n",
    "\n",
    "### Why This Matters\n",
    "The Spanner documentation shows PostgreSQL ML examples with **literal JSONB strings**, but provides no way to build these from table columns. This makes it impractical for:\n",
    "- Querying predictions from database tables\n",
    "- Batch scoring operations\n",
    "- Any real-world ML workflow\n",
    "\n",
    "### GoogleSQL Advantages ✅\n",
    "- Full `ML.PREDICT()` support with seamless table integration\n",
    "- Set-based operations (like BigQuery ML)\n",
    "- `CREATE MODEL` for endpoint registration\n",
    "- Works with any table query\n",
    "- Production-ready for batch scoring\n",
    "- **No JSON construction limitations**\n",
    "\n",
    "### If You Need PostgreSQL Compatibility for ML\n",
    "Consider these alternatives:\n",
    "1. **AlloyDB for PostgreSQL** - Full `google_ml.predict_row()` support with proper table integration\n",
    "2. **BigQuery ML** - PostgreSQL-compatible SQL with `ML.PREDICT()`\n",
    "3. **Export → Score → Import** - Export from Spanner PostgreSQL, score via Python/Dataflow, import results back\n",
    "\n",
    "This notebook demonstrates the **recommended approach**: GoogleSQL dialect for Spanner ML inference.\n",
    "\n",
    "## What is Cloud Spanner?\n",
    "\n",
    "[**Cloud Spanner**](https://cloud.google.com/spanner) is Google Cloud's fully managed, globally distributed, horizontally scalable relational database service. It uniquely combines:\n",
    "\n",
    "- **Global distribution** with strong consistency\n",
    "- **Horizontal scalability** to petabyte scale\n",
    "- **High availability** (99.999% SLA for multi-region)\n",
    "- **SQL capabilities** with ACID transactions\n",
    "- **Two SQL dialects**: GoogleSQL and PostgreSQL\n",
    "\n",
    "### Spanner Architecture\n",
    "\n",
    "Spanner is organized hierarchically:\n",
    "\n",
    "```\n",
    "Instance (regional or multi-region)\n",
    "  └── Database (GoogleSQL or PostgreSQL dialect)\n",
    "      └── Tables, Indexes, Views\n",
    "```\n",
    "\n",
    "**Instance**: Compute and storage allocation\n",
    "- **Regional**: Single region, lower cost, 99.99% SLA\n",
    "- **Multi-region**: Globally distributed, highest availability, 99.999% SLA\n",
    "- Processing capacity measured in nodes (1000 processing units = 1 node)\n",
    "- **Editions**: Enterprise, Enterprise Plus, Standard (ML requires Enterprise or higher)\n",
    "\n",
    "**Database**: SQL interface with chosen dialect\n",
    "- **Dialect choice is permanent** - set at database creation\n",
    "- Can have multiple databases with different dialects on same instance\n",
    "- Each database is independent with its own schema\n",
    "\n",
    "### Spanner Capabilities\n",
    "\n",
    "**Global Distribution:**\n",
    "- ✅ **Multi-region configurations** for global apps\n",
    "- ✅ **Automatic replication** across regions\n",
    "- ✅ **Strong consistency** worldwide\n",
    "- ✅ **Low-latency reads** via regional replicas\n",
    "\n",
    "**Scalability:**\n",
    "- ✅ **Horizontal scaling** by adding nodes\n",
    "- ✅ **Petabyte-scale** databases\n",
    "- ✅ **Automatic sharding** and rebalancing\n",
    "- ✅ **Unlimited storage** per database\n",
    "\n",
    "**High Availability:**\n",
    "- ✅ **99.999% SLA** for multi-region (5 nines)\n",
    "- ✅ **99.99% SLA** for regional (4 nines)\n",
    "- ✅ **Automatic failover** with zero data loss\n",
    "- ✅ **Point-in-time recovery** (PITR)\n",
    "\n",
    "**AI & ML Integration:**\n",
    "- ✅ **Built-in Vertex AI integration** for ML predictions from SQL (GoogleSQL)\n",
    "- ✅ **No IAM permissions required** (unlike AlloyDB)\n",
    "- ✅ **Vector search** with GoogleSQL (Enterprise edition)\n",
    "\n",
    "**SQL Compatibility:**\n",
    "- ✅ **Two dialect choices**: GoogleSQL or PostgreSQL\n",
    "- ✅ **ANSI SQL** standard support\n",
    "- ✅ **ACID transactions** with serializable isolation\n",
    "- ✅ **Interleaved tables** for performance\n",
    "\n",
    "### Minimal Setup for This Notebook\n",
    "\n",
    "To keep costs low while demonstrating Spanner's ML capabilities, this notebook uses:\n",
    "\n",
    "- **Instance Config**: Regional (us-central1)\n",
    "- **Edition**: Enterprise (required for ML integration)\n",
    "- **Compute**: 1 node (1000 processing units)\n",
    "- **Database**: One GoogleSQL database\n",
    "- **Replication**: Single region (no multi-region)\n",
    "\n",
    "**Cost Estimate** (approximate, `us-central1` region):\n",
    "- ~$0.90/hour for 1 node Enterprise regional instance\n",
    "- ~$21.60/day if left running 24/7\n",
    "- ~$648/month if not deleted\n",
    "\n",
    "**Cost Sharing Tip**: This notebook uses the same instance naming (`PROJECT_ID`) as the Spanner vector search example. If you run both notebooks, they can **share the same instance** with different databases, reducing costs for workshops and demos.\n",
    "\n",
    "**Important**: The cleanup section at the end of this notebook will delete all resources to avoid ongoing charges.\n",
    "\n",
    "## Spanner vs AlloyDB vs BigQuery for ML\n",
    "\n",
    "| Feature | Spanner (GoogleSQL) | AlloyDB | BigQuery |\n",
    "|---------|---------|---------|----------|\n",
    "| **Primary Use Case** | Global applications | Operational databases | Data warehousing |\n",
    "| **Workload** | OLTP (global) | OLTP + OLAP | OLAP |\n",
    "| **Scale** | Petabytes | Up to 64 TB | Petabytes+ |\n",
    "| **Distribution** | Multi-region | Single region | Global (query only) |\n",
    "| **Latency** | <10ms (regional) | Sub-millisecond | Seconds |\n",
    "| **SQL Dialect** | GoogleSQL | PostgreSQL | GoogleSQL |\n",
    "| **ML Function** | `ML.PREDICT()` | `google_ml.predict_row()` | `ML.PREDICT()` |\n",
    "| **IAM for ML** | None required | Requires aiplatform.user | Requires connection | \n",
    "| **Best For ML** | Global inference | Low-latency inference | Batch scoring |\n",
    "\n",
    "## Spanner ML Integration with GoogleSQL\n",
    "\n",
    "Spanner's **Vertex AI integration** enables SQL-based ML inference:\n",
    "\n",
    "**Key Functions:**\n",
    "- `CREATE MODEL ... REMOTE OPTIONS (endpoint = '...')` - Register Vertex AI endpoint\n",
    "- `ML.PREDICT(MODEL model, TABLE data)` - Batch predictions with table integration\n",
    "\n",
    "**Key Advantage: No IAM Permissions**\n",
    "\n",
    "Unlike AlloyDB, Spanner ML integration **does not require** granting IAM permissions to service accounts. Model registration and inference work immediately after `CREATE MODEL`.\n",
    "\n",
    "**Example:**\n",
    "```sql\n",
    "-- Register endpoint\n",
    "CREATE MODEL my_model\n",
    "INPUT (feature1 FLOAT64, feature2 FLOAT64)\n",
    "OUTPUT (prediction FLOAT64)\n",
    "REMOTE OPTIONS (\n",
    "  endpoint = 'https://us-central1-aiplatform.googleapis.com/v1/projects/.../endpoints/...'\n",
    ");\n",
    "\n",
    "-- Make predictions\n",
    "SELECT * FROM ML.PREDICT(\n",
    "  MODEL my_model,\n",
    "  (SELECT * FROM my_table)\n",
    ");\n",
    "```\n",
    "\n",
    "## Resources\n",
    "\n",
    "**Documentation:**\n",
    "- [Spanner ML Integration Overview](https://cloud.google.com/spanner/docs/ml)\n",
    "- [Spanner ML Tutorial](https://cloud.google.com/spanner/docs/ml-tutorial)\n",
    "- [ML.PREDICT Function (GoogleSQL)](https://cloud.google.com/spanner/docs/reference/standard-sql/ml-functions#mlpredict)\n",
    "- [Choosing GoogleSQL vs PostgreSQL](https://cloud.google.com/spanner/docs/choose-googlesql-or-postgres)\n",
    "- [Spanner Pricing](https://cloud.google.com/spanner/pricing)\n",
    "\n",
    "**Related Notebooks:**\n",
    "- [vertex-ai-endpoint-prebuilt-container.ipynb](./vertex-ai-endpoint-prebuilt-container.ipynb) - Deploy endpoint with pre-built container\n",
    "- [vertex-ai-endpoint-custom-container.ipynb](./vertex-ai-endpoint-custom-container.ipynb) - Deploy endpoint with custom container\n",
    "- [alloydb-vertex-ai-endpoint.ipynb](./alloydb-vertex-ai-endpoint.ipynb) - Compare with AlloyDB (PostgreSQL with ML)\n",
    "- [bigquery-bqml-remote-model-vertex.ipynb](./bigquery-bqml-remote-model-vertex.ipynb) - Compare with BigQuery ML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "env_setup",
   "metadata": {},
   "source": [
    "---\n",
    "## Environment Setup\n",
    "\n",
    "This section will authenticate your session, enable required Google Cloud APIs, and install necessary Python packages.\n",
    "\n",
    "**Package Installation Options (`REQ_TYPE`):**\n",
    "- `PRIMARY`: Installs only the main packages. Faster, but pip resolves sub-dependencies which may result in different versions than development.\n",
    "- `ALL` (Default): Installs exact versions of all packages and dependencies. Best for perfectly reproducing the development environment.\n",
    "- `COLAB`: Installs a Colab-optimized list that excludes pre-installed packages like `ipython` and `ipykernel`.\n",
    "\n",
    "**Installation Tool Options (`INSTALL_TOOL`):**\n",
    "- `pip` (Default): Uses pip for package installation. Standard Python package installer.\n",
    "- `uv`: Modern, fast Python package installer. Must be installed separately. See: https://github.com/astral-sh/uv\n",
    "- `poetry`: Dependency management tool. Requires running notebook in a poetry environment (`poetry shell` or `poetry run jupyter lab`). Uses `pyproject.toml` instead of requirements.txt.\n",
    "\n",
    "> **Note:** If running in Google Colab, the script will automatically detect this and set `REQ_TYPE = 'COLAB'` to prevent package conflicts, overriding any manual setting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "set_project",
   "metadata": {},
   "source": [
    "### Set Your Project ID\n",
    "\n",
    "⚠️ **Action Required:** Replace the `PROJECT_ID` value below with your Google Cloud project ID before running this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "project_config",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = 'statmike-mlops-349915' # replace with GCP project ID\n",
    "REQ_TYPE = 'ALL' # Specify PRIMARY or ALL or COLAB\n",
    "INSTALL_TOOL = 'poetry' # Specify pip, uv, or poetry\n",
    "POETRY_GROUPS = 'spanner' # Optional: specify poetry dependency groups to install with main (only used when INSTALL_TOOL='poetry'): None | 'group_name' | 'group1,group2'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config_header",
   "metadata": {},
   "source": [
    "### Configuration\n",
    "\n",
    "This cell defines the requirements files and Google Cloud APIs needed for this notebook. Run as-is without modification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": [
    "REQUIREMENTS_URL = 'https://raw.githubusercontent.com/statmike/vertex-ai-mlops/refs/heads/main/Framework%20Workflows/PyTorch/requirements.txt'\n",
    "\n",
    "REQUIRED_APIS = [\n",
    "    \"aiplatform.googleapis.com\",\n",
    "    \"spanner.googleapis.com\",\n",
    "    \"bigquery.googleapis.com\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "run_setup_header",
   "metadata": {},
   "source": [
    "### Run Setup\n",
    "\n",
    "This cell downloads the centralized setup code and configures your environment. It will:\n",
    "- Authenticate your session with Google Cloud\n",
    "- Enable required APIs for this notebook\n",
    "- Install necessary Python packages\n",
    "- Display a setup summary with your project information\n",
    "\n",
    "> **Note:** In Colab, if packages are installed, the kernel will automatically restart. After restart, continue from the next cell without re-running earlier cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "run_setup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "PYTHON GCP ENVIRONMENT SETUP\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "AUTHENTICATION\n",
      "==================================================\n",
      "Checking for existing ADC...\n",
      "✅ Existing ADC found.\n",
      "✅ Project is correctly set to 'statmike-mlops-349915'.\n",
      "\n",
      "==================================================\n",
      "API CHECK & ENABLE\n",
      "==================================================\n",
      "✅ aiplatform.googleapis.com is already enabled.\n",
      "✅ spanner.googleapis.com is already enabled.\n",
      "✅ bigquery.googleapis.com is already enabled.\n",
      "\n",
      "==================================================\n",
      "PACKAGE MANAGEMENT\n",
      "==================================================\n",
      "Installation Tool: poetry\n",
      "✅ Found poetry at: /usr/local/google/home/statmike/.local/bin/poetry\n",
      "✅ Running in poetry environment: /usr/local/google/home/statmike/.cache/pypoetry/virtualenvs/frameworks-pytorch-0KVJlKeQ-py3.13\n",
      "ℹ️  Poetry mode: Installing from pyproject.toml (REQUIREMENTS_URL ignored)\n",
      "✅ Found pyproject.toml at: /usr/local/google/home/statmike/Git/vertex-ai-mlops/Framework Workflows/PyTorch/pyproject.toml\n",
      "   Changed working directory to: /usr/local/google/home/statmike/Git/vertex-ai-mlops/Framework Workflows/PyTorch\n",
      "ℹ️  Installing with optional group(s): spanner\n",
      "Running poetry install...\n",
      "   Restored working directory to: /usr/local/google/home/statmike/Git/vertex-ai-mlops/Framework Workflows/PyTorch/serving\n",
      "✅ All packages are already installed and up to date.\n",
      "\n",
      "==================================================\n",
      "Google Cloud Project Information\n",
      "==================================================\n",
      "PROJECT_ID     = statmike-mlops-349915\n",
      "PROJECT_NUMBER = 1026793852137\n",
      "==================================================\n",
      "\n",
      "\n",
      "==================================================\n",
      "SETUP SUMMARY\n",
      "==================================================\n",
      "✅ Authentication:    Success\n",
      "✅ API Configuration: Success\n",
      "✅ Package Install:   Already up to date\n",
      "✅ Installation Tool: poetry\n",
      "✅ Project ID:        statmike-mlops-349915\n",
      "✅ Project Number:    1026793852137\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os, urllib.request\n",
    "\n",
    "# Download and import setup code\n",
    "url = 'https://raw.githubusercontent.com/statmike/vertex-ai-mlops/refs/heads/main/core/notebook-template/python_setup.py'\n",
    "urllib.request.urlretrieve(url, 'python_setup_local.py')\n",
    "import python_setup_local as python_setup\n",
    "os.remove('python_setup_local.py')\n",
    "\n",
    "# Run setup\n",
    "setup_info = python_setup.setup_environment(PROJECT_ID, REQ_TYPE, REQUIREMENTS_URL, REQUIRED_APIS, INSTALL_TOOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "python_setup_header",
   "metadata": {},
   "source": [
    "---\n",
    "## Python Setup\n",
    "\n",
    "Import necessary libraries and initialize clients for Vertex AI, BigQuery, and Spanner."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imports_header",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "imports",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/google/home/statmike/.cache/pypoetry/virtualenvs/frameworks-pytorch-0KVJlKeQ-py3.13/lib/python3.13/site-packages/google/cloud/aiplatform/models.py:52: FutureWarning: Support for google-cloud-storage < 3.0.0 will be removed in a future version of google-cloud-aiplatform. Please upgrade to google-cloud-storage >= 3.0.0.\n",
      "  from google.cloud.aiplatform.utils import gcs_utils\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import json\n",
    "import time\n",
    "\n",
    "# Vertex AI\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "# BigQuery\n",
    "from google.cloud import bigquery\n",
    "\n",
    "# Spanner\n",
    "from google.cloud import spanner\n",
    "from google.cloud import spanner_admin_instance_v1\n",
    "from google.cloud import spanner_admin_database_v1\n",
    "import sqlalchemy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "variables_header",
   "metadata": {},
   "source": [
    "### Variables - User Set\n",
    "\n",
    "Configure Spanner and Vertex AI endpoint settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "variables_user",
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION = 'us-central1'\n",
    "SERIES = 'frameworks'\n",
    "EXPERIMENT = 'pytorch-autoencoder-spanner'\n",
    "\n",
    "# Vertex AI Endpoint (must already be deployed)\n",
    "ENDPOINT_DISPLAY_NAME = 'pytorch-autoencoder-endpoint'\n",
    "\n",
    "# Spanner Configuration\n",
    "# Instance name uses PROJECT_ID to enable sharing with vector search notebook\n",
    "SPANNER_INSTANCE_NAME = PROJECT_ID\n",
    "SPANNER_DATABASE_GOOGLESQL = SERIES  # GoogleSQL database\n",
    "SPANNER_TABLE_NAME = EXPERIMENT.replace('-', '_')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "variables_auto_header",
   "metadata": {},
   "source": [
    "### Variables - Auto Set\n",
    "\n",
    "Retrieve project information from the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "variables_auto",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project ID: statmike-mlops-349915\n",
      "Project Number: 1026793852137\n",
      "Region: us-central1\n"
     ]
    }
   ],
   "source": [
    "# Get project information\n",
    "if not PROJECT_ID:\n",
    "    shell_output = subprocess.run(['gcloud', 'config', 'list', '--format', 'value(core.project)'], capture_output=True)\n",
    "    PROJECT_ID = shell_output.stdout.decode('utf-8').strip()\n",
    "\n",
    "shell_output = subprocess.run(['gcloud', 'projects', 'describe', PROJECT_ID, '--format', 'value(projectNumber)'], capture_output=True)\n",
    "PROJECT_NUMBER = shell_output.stdout.decode('utf-8').strip()\n",
    "\n",
    "print(f'Project ID: {PROJECT_ID}')\n",
    "print(f'Project Number: {PROJECT_NUMBER}')\n",
    "print(f'Region: {REGION}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clients_header",
   "metadata": {},
   "source": [
    "### Initialize Clients\n",
    "\n",
    "Create clients for Vertex AI, BigQuery, and Spanner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "clients",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Vertex AI\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION)\n",
    "\n",
    "# Initialize BigQuery\n",
    "bq = bigquery.Client(project=PROJECT_ID)\n",
    "\n",
    "# Initialize Spanner Admin Clients\n",
    "spanner_client = spanner.Client(project=PROJECT_ID)\n",
    "spanner_instance_client = spanner_admin_instance_v1.InstanceAdminClient()\n",
    "spanner_database_client = spanner_admin_database_v1.DatabaseAdminClient()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prerequisites_header",
   "metadata": {},
   "source": [
    "---\n",
    "## Prerequisites: Verify Deployed Endpoint\n",
    "\n",
    "This notebook requires a deployed Vertex AI Endpoint. Use either:\n",
    "- [vertex-ai-endpoint-prebuilt-container.ipynb](./vertex-ai-endpoint-prebuilt-container.ipynb) - Pre-built TorchServe container\n",
    "- [vertex-ai-endpoint-custom-container.ipynb](./vertex-ai-endpoint-custom-container.ipynb) - Custom FastAPI container\n",
    "\n",
    "This section will find the endpoint and test predictions to verify it's working."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "find_endpoint_header",
   "metadata": {},
   "source": [
    "### Find Endpoint\n",
    "\n",
    "Search for the deployed endpoint by display name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "find_endpoint",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found endpoint: pytorch-autoencoder-endpoint\n",
      "Resource name: projects/1026793852137/locations/us-central1/endpoints/5971323405637517312\n",
      "Endpoint ID: 5971323405637517312\n"
     ]
    }
   ],
   "source": [
    "# Find endpoint by display name\n",
    "endpoints = aiplatform.Endpoint.list(\n",
    "    filter=f'display_name=\"{ENDPOINT_DISPLAY_NAME}\"',\n",
    "    order_by='create_time desc'\n",
    ")\n",
    "\n",
    "if not endpoints:\n",
    "    raise ValueError(\n",
    "        f\"No endpoint found with display_name='{ENDPOINT_DISPLAY_NAME}'. \"\n",
    "        f\"Please deploy an endpoint first using one of the prerequisite notebooks.\"\n",
    "    )\n",
    "\n",
    "endpoint = endpoints[0]\n",
    "print(f'Found endpoint: {endpoint.display_name}')\n",
    "print(f'Resource name: {endpoint.resource_name}')\n",
    "print(f'Endpoint ID: {endpoint.name.split(\"/\")[-1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "test_endpoint_header",
   "metadata": {},
   "source": [
    "### Test Predictions\n",
    "\n",
    "Make a test prediction to verify the endpoint is working and detect its type (pre-built or custom container)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "test_endpoint",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test prediction successful!\n",
      "Prediction type: <class 'google.cloud.aiplatform.models.Prediction'>\n",
      "Prediction structure: <class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "# Sample test instance (30 features matching the autoencoder model)\n",
    "test_instance = [\n",
    "    92.35, -0.26, 0.13, -1.15, 0.93, -0.60, -0.20, 0.57, \n",
    "    0.82, 0.30, -0.58, 0.07, -0.18, 0.42, -0.22, 0.43, \n",
    "    -0.24, -0.03, 0.15, 0.06, -0.14, 0.24, -0.05, 0.29, \n",
    "    0.54, -0.33, 0.21, 0.09, -0.02, 15.47\n",
    "]\n",
    "\n",
    "# Make prediction\n",
    "prediction = endpoint.predict(instances=[test_instance])\n",
    "\n",
    "print('Test prediction successful!')\n",
    "print(f'Prediction type: {type(prediction)}')\n",
    "print(f'Prediction structure: {type(prediction.predictions[0])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "detect_endpoint_type_header",
   "metadata": {},
   "source": [
    "### Detect Endpoint Type\n",
    "\n",
    "Determine if this is a pre-built container (13 output fields) or custom container (2 output fields)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "detect_endpoint_type",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Detected: Pre-built Container Endpoint\n",
      "  - Output fields: ['normalized_reconstruction', 'encoded', 'normalized_RMSE', 'denormalized_MSE', 'normalized_reconstruction_errors', 'denormalized_reconstruction', 'denormalized_reconstruction_errors', 'denormalized_MSLE', 'normalized_MSE', 'denormalized_MAE', 'normalized_MSLE', 'denormalized_RMSE', 'normalized_MAE']\n",
      "  - Denormalized MAE: 2413.40673828125\n"
     ]
    }
   ],
   "source": [
    "# Detect endpoint type by checking response structure\n",
    "first_prediction = prediction.predictions[0]\n",
    "\n",
    "if isinstance(first_prediction, dict):\n",
    "    # Custom container - returns dict with specific fields\n",
    "    if 'anomaly_score' in first_prediction and 'encoded' in first_prediction:\n",
    "        ENDPOINT_TYPE = 'custom'\n",
    "        print('✓ Detected: Custom Container Endpoint')\n",
    "        print(f'  - Output fields: {list(first_prediction.keys())}')\n",
    "        print(f'  - Anomaly score: {first_prediction[\"anomaly_score\"]:.2f}')\n",
    "        print(f'  - Encoded representation: {first_prediction[\"encoded\"][:2]}...')\n",
    "    else:\n",
    "        # Pre-built container - returns dict with 13 fields\n",
    "        ENDPOINT_TYPE = 'prebuilt'\n",
    "        print('✓ Detected: Pre-built Container Endpoint')\n",
    "        print(f'  - Output fields: {list(first_prediction.keys())}')\n",
    "        print(f'  - Denormalized MAE: {first_prediction.get(\"denormalized_MAE\", \"N/A\")}')\n",
    "else:\n",
    "    raise ValueError(f'Unexpected prediction format: {type(first_prediction)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spanner_infra_header",
   "metadata": {},
   "source": [
    "---\n",
    "## Spanner Infrastructure Setup\n",
    "\n",
    "Create minimal Spanner infrastructure for this demonstration:\n",
    "- **Instance**: Regional, Enterprise edition, 1 node (shared across notebooks)\n",
    "- **Database**: One GoogleSQL database for ML inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "create_instance_header",
   "metadata": {},
   "source": [
    "### Create/Retrieve Spanner Instance\n",
    "\n",
    "Create a minimal Spanner instance (or retrieve existing one).\n",
    "\n",
    "**Instance Naming**: Uses `PROJECT_ID` as instance name, allowing this notebook to **share the same instance** with the Spanner vector search notebook. This reduces costs for workshops and demos where both notebooks may be run together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "create_instance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Found existing instance: projects/statmike-mlops-349915/instances/statmike-mlops-349915\n",
      "  Edition: ENTERPRISE\n",
      "  Node count: 1\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    spanner_instance = spanner_instance_client.get_instance(\n",
    "        name=f'projects/{PROJECT_ID}/instances/{SPANNER_INSTANCE_NAME}'\n",
    "    )\n",
    "    print(f'✓ Found existing instance: {spanner_instance.name}')\n",
    "    print(f'  Edition: {spanner_instance.edition.name}')\n",
    "    print(f'  Node count: {spanner_instance.node_count}')\n",
    "except Exception:\n",
    "    print(f'Creating Spanner instance: {SPANNER_INSTANCE_NAME}')\n",
    "    print('  Configuration: regional-us-central1, Enterprise edition, 1 node')\n",
    "    print('  This may take 10-15 minutes...')\n",
    "    \n",
    "    create_instance = spanner_instance_client.create_instance(\n",
    "        parent=f\"projects/{PROJECT_ID}\",\n",
    "        instance_id=SPANNER_INSTANCE_NAME,\n",
    "        instance=spanner_admin_instance_v1.Instance(\n",
    "            name=f'projects/{PROJECT_ID}/instances/{SPANNER_INSTANCE_NAME}',\n",
    "            config=f'projects/{PROJECT_ID}/instanceConfigs/regional-{REGION}',\n",
    "            display_name=SPANNER_INSTANCE_NAME,\n",
    "            node_count=1,\n",
    "            edition='ENTERPRISE'  # Required for ML integration\n",
    "        )\n",
    "    )\n",
    "    spanner_instance = create_instance.result()\n",
    "    spanner_instance = spanner_instance_client.get_instance(\n",
    "        name=f'projects/{PROJECT_ID}/instances/{SPANNER_INSTANCE_NAME}'\n",
    "    )\n",
    "    print(f'✓ Created instance: {spanner_instance.name}')\n",
    "    print(f'  Edition: {spanner_instance.edition.name}')\n",
    "    print(f'  Node count: {spanner_instance.node_count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "connection_setup_header",
   "metadata": {},
   "source": [
    "---\n",
    "## Database Connection Setup\n",
    "\n",
    "Set up connections to Spanner databases using the Python Client and SQLAlchemy.\n",
    "\n",
    "Spanner uses public API endpoints and is accessible from Google Cloud environments (Cloud Shell, Vertex AI Workbench, Colab Enterprise, GCE VMs) and local workstations with proper authentication."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "connection_functions_header",
   "metadata": {},
   "source": [
    "### Connection Helper Functions\n",
    "\n",
    "Create functions to connect to Spanner databases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "connection_functions",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Connection functions defined\n"
     ]
    }
   ],
   "source": [
    "# Create connection pool function\n",
    "def get_spanner_engine(database_id):\n",
    "    \"\"\"Create SQLAlchemy engine for Spanner database.\"\"\"\n",
    "    instance = spanner_client.instance(SPANNER_INSTANCE_NAME)\n",
    "    database = instance.database(database_id)\n",
    "    \n",
    "    engine = sqlalchemy.create_engine(\n",
    "        f\"spanner+spanner:///{database.name}\",\n",
    "        connect_args=dict(client=spanner_client)\n",
    "    )\n",
    "    # Set autocommit mode\n",
    "    autocommit_engine = engine.execution_options(isolation_level=\"AUTOCOMMIT\")\n",
    "    return autocommit_engine\n",
    "\n",
    "print('✓ Connection functions defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "query_helper_header",
   "metadata": {},
   "source": [
    "### Query Helper Function\n",
    "\n",
    "Create a helper function to execute SQL queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "query_helper",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Query helper function defined\n"
     ]
    }
   ],
   "source": [
    "def run_query(query, engine):\n",
    "    \"\"\"Execute a SQL query and return results.\"\"\"\n",
    "    with engine.connect() as connection:\n",
    "        result = connection.execute(query)\n",
    "    \n",
    "    # Prepare the response\n",
    "    rows = []\n",
    "    try:\n",
    "        for row in result:\n",
    "            rows.append(dict(zip(result.keys(), row)))\n",
    "    except Exception:\n",
    "        pass\n",
    "    \n",
    "    # Return the response\n",
    "    return rows[0] if len(rows) == 1 else rows\n",
    "\n",
    "print('✓ Query helper function defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "database_setup_header",
   "metadata": {},
   "source": [
    "---\n",
    "## Database Setup - GoogleSQL Dialect\n",
    "\n",
    "This notebook uses the **GoogleSQL dialect** for ML inference.\n",
    "\n",
    "**Why GoogleSQL Instead of PostgreSQL?**\n",
    "\n",
    "After extensive testing, Spanner's PostgreSQL dialect has critical limitations for ML workloads:\n",
    "- ❌ Missing JSON construction functions (`json_build_object`, `json_build_array`)\n",
    "- ❌ Cannot dynamically build JSONB payloads from table data\n",
    "- ❌ `spanner.ml_predict_row()` only works with hard-coded literal values\n",
    "- ❌ Impractical for real-world batch scoring or table-based predictions\n",
    "\n",
    "**GoogleSQL Advantages for ML:**\n",
    "- ✅ Full `ML.PREDICT()` support with seamless table integration\n",
    "- ✅ Set-based operations (like BigQuery ML)\n",
    "- ✅ CREATE MODEL for endpoint registration\n",
    "- ✅ Works with any table query\n",
    "- ✅ Production-ready for batch scoring\n",
    "\n",
    "**If you need PostgreSQL compatibility for ML**, consider:\n",
    "- **AlloyDB for PostgreSQL** - Full `google_ml.predict_row()` support with table integration\n",
    "- **BigQuery ML** - PostgreSQL-compatible SQL with `ML.PREDICT()`\n",
    "\n",
    "This notebook demonstrates ML inference with GoogleSQL, which is the recommended approach for Spanner."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "create_googlesql_db_header",
   "metadata": {},
   "source": [
    "### Create GoogleSQL Database\n",
    "\n",
    "Create a database with GoogleSQL dialect for `ML.PREDICT()` demonstrations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "create_googlesql_db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Found existing GoogleSQL database: projects/statmike-mlops-349915/instances/statmike-mlops-349915/databases/frameworks\n",
      "  Dialect: GOOGLE_STANDARD_SQL\n",
      "  State: READY\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    googlesql_database = spanner_database_client.get_database(\n",
    "        name=f\"{spanner_instance.name}/databases/{SPANNER_DATABASE_GOOGLESQL}\"\n",
    "    )\n",
    "    print(f'✓ Found existing GoogleSQL database: {googlesql_database.name}')\n",
    "except Exception:\n",
    "    print(f'Creating GoogleSQL database: {SPANNER_DATABASE_GOOGLESQL}')\n",
    "    print('  Dialect: GoogleSQL (GOOGLE_STANDARD_SQL)')\n",
    "    print('  This may take 5-10 minutes...')\n",
    "    \n",
    "    create_database = spanner_database_client.create_database(\n",
    "        request=spanner_admin_database_v1.types.CreateDatabaseRequest(\n",
    "            parent=spanner_instance.name,\n",
    "            create_statement=f'CREATE DATABASE `{SPANNER_DATABASE_GOOGLESQL}`',\n",
    "            database_dialect=spanner_admin_database_v1.DatabaseDialect.GOOGLE_STANDARD_SQL,\n",
    "            extra_statements=[]\n",
    "        )\n",
    "    )\n",
    "    googlesql_database = create_database.result()\n",
    "    googlesql_database = spanner_database_client.get_database(\n",
    "        name=f\"{spanner_instance.name}/databases/{SPANNER_DATABASE_GOOGLESQL}\"\n",
    "    )\n",
    "    print(f'✓ Created GoogleSQL database: {googlesql_database.name}')\n",
    "\n",
    "print(f'  Dialect: {googlesql_database.database_dialect.name}')\n",
    "print(f'  State: {googlesql_database.state.name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "create_engines_header",
   "metadata": {},
   "source": [
    "### Create Database Engine\n",
    "\n",
    "Create SQLAlchemy engine for the GoogleSQL database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "create_engines",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Created SQLAlchemy engine\n",
      "  Database: frameworks (GoogleSQL dialect)\n"
     ]
    }
   ],
   "source": [
    "# Create engine for GoogleSQL database\n",
    "engine = get_spanner_engine(SPANNER_DATABASE_GOOGLESQL)\n",
    "\n",
    "print('✓ Created SQLAlchemy engine')\n",
    "print(f'  Database: {SPANNER_DATABASE_GOOGLESQL} (GoogleSQL dialect)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "test_connections_header",
   "metadata": {},
   "source": [
    "### Test Connection\n",
    "\n",
    "Execute a test query to verify connectivity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "test_connections",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created multiplexed session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Connection successful!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/google/home/statmike/.cache/pypoetry/virtualenvs/frameworks-pytorch-0KVJlKeQ-py3.13/lib/python3.13/site-packages/google/cloud/sqlalchemy_spanner/sqlalchemy_spanner.py:1825: UserWarning: This method is non-operational as a transaction has not been started.\n",
      "  dbapi_connection.rollback()\n"
     ]
    }
   ],
   "source": [
    "# Test connection\n",
    "result = run_query(\n",
    "    sqlalchemy.text(\"SELECT 'Connection successful!' as status\"),\n",
    "    engine\n",
    ")\n",
    "print(f\"✓ {result['status']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load_data_header",
   "metadata": {},
   "source": [
    "---\n",
    "## Load Test Data\n",
    "\n",
    "Load transaction data from BigQuery into Spanner."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "query_bigquery_header",
   "metadata": {},
   "source": [
    "### Query Test Data from BigQuery\n",
    "\n",
    "Retrieve a sample of transactions for testing predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "query_bigquery",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Retrieved 1000 test transactions from BigQuery\n",
      "  Columns: 33\n",
      "\n",
      "Sample data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "      <th>transaction_id</th>\n",
       "      <th>splits</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>122959.0</td>\n",
       "      <td>-1.327297</td>\n",
       "      <td>0.422904</td>\n",
       "      <td>1.617505</td>\n",
       "      <td>2.291196</td>\n",
       "      <td>2.375055</td>\n",
       "      <td>0.411735</td>\n",
       "      <td>0.213517</td>\n",
       "      <td>0.424743</td>\n",
       "      <td>-1.809624</td>\n",
       "      <td>...</td>\n",
       "      <td>0.192604</td>\n",
       "      <td>0.068281</td>\n",
       "      <td>-0.245725</td>\n",
       "      <td>-0.697654</td>\n",
       "      <td>0.038216</td>\n",
       "      <td>0.150059</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>c97b6e2f-603a-4dbe-9bca-0add881f2084</td>\n",
       "      <td>TEST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>122312.0</td>\n",
       "      <td>-1.988557</td>\n",
       "      <td>-0.720301</td>\n",
       "      <td>0.863204</td>\n",
       "      <td>3.114494</td>\n",
       "      <td>1.847474</td>\n",
       "      <td>0.255881</td>\n",
       "      <td>0.580362</td>\n",
       "      <td>-0.083756</td>\n",
       "      <td>-0.939044</td>\n",
       "      <td>...</td>\n",
       "      <td>1.564951</td>\n",
       "      <td>0.546312</td>\n",
       "      <td>-0.548531</td>\n",
       "      <td>-0.746620</td>\n",
       "      <td>-0.748016</td>\n",
       "      <td>0.410640</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>791e403e-d59f-491d-b0b7-d8f8710c07fb</td>\n",
       "      <td>TEST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>119592.0</td>\n",
       "      <td>2.139741</td>\n",
       "      <td>0.245651</td>\n",
       "      <td>-2.654856</td>\n",
       "      <td>0.178287</td>\n",
       "      <td>1.336991</td>\n",
       "      <td>-0.724664</td>\n",
       "      <td>0.906032</td>\n",
       "      <td>-0.436125</td>\n",
       "      <td>-0.528015</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.216033</td>\n",
       "      <td>0.345316</td>\n",
       "      <td>0.747103</td>\n",
       "      <td>0.700184</td>\n",
       "      <td>-0.123739</td>\n",
       "      <td>-0.099989</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>d9e720c5-311d-4cf7-95cb-2256823803ba</td>\n",
       "      <td>TEST</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Time        V1        V2        V3        V4        V5        V6  \\\n",
       "0  122959.0 -1.327297  0.422904  1.617505  2.291196  2.375055  0.411735   \n",
       "1  122312.0 -1.988557 -0.720301  0.863204  3.114494  1.847474  0.255881   \n",
       "2  119592.0  2.139741  0.245651 -2.654856  0.178287  1.336991 -0.724664   \n",
       "\n",
       "         V7        V8        V9  ...       V23       V24       V25       V26  \\\n",
       "0  0.213517  0.424743 -1.809624  ...  0.192604  0.068281 -0.245725 -0.697654   \n",
       "1  0.580362 -0.083756 -0.939044  ...  1.564951  0.546312 -0.548531 -0.746620   \n",
       "2  0.906032 -0.436125 -0.528015  ... -0.216033  0.345316  0.747103  0.700184   \n",
       "\n",
       "        V27       V28  Amount  Class                        transaction_id  \\\n",
       "0  0.038216  0.150059     0.0      0  c97b6e2f-603a-4dbe-9bca-0add881f2084   \n",
       "1 -0.748016  0.410640     0.0      0  791e403e-d59f-491d-b0b7-d8f8710c07fb   \n",
       "2 -0.123739 -0.099989     0.0      0  d9e720c5-311d-4cf7-95cb-2256823803ba   \n",
       "\n",
       "   splits  \n",
       "0    TEST  \n",
       "1    TEST  \n",
       "2    TEST  \n",
       "\n",
       "[3 rows x 33 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Query sample data from BigQuery\n",
    "query = f\"\"\"\n",
    "SELECT *\n",
    "FROM `{PROJECT_ID}.{SERIES}.{SERIES}`\n",
    "WHERE splits = 'TEST'\n",
    "LIMIT 1000\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "df = bq.query(query).to_dataframe()\n",
    "print(f'✓ Retrieved {len(df)} test transactions from BigQuery')\n",
    "print(f'  Columns: {df.shape[1]}')\n",
    "\n",
    "# Display sample\n",
    "print('\\nSample data:')\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "create_tables_header",
   "metadata": {},
   "source": [
    "### Create Table in Spanner\n",
    "\n",
    "Create table with GoogleSQL syntax to store transaction data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "create_tables",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/google/home/statmike/.cache/pypoetry/virtualenvs/frameworks-pytorch-0KVJlKeQ-py3.13/lib/python3.13/site-packages/google/cloud/sqlalchemy_spanner/sqlalchemy_spanner.py:1825: UserWarning: This method is non-operational as a transaction has not been started.\n",
      "  dbapi_connection.rollback()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped existing table: pytorch_autoencoder_spanner\n",
      "✓ Created table: pytorch_autoencoder_spanner\n"
     ]
    }
   ],
   "source": [
    "# Drop table if exists (for demo purposes)\n",
    "try:\n",
    "    run_query(sqlalchemy.text(f'DROP TABLE `{SPANNER_TABLE_NAME}`'), engine)\n",
    "    print(f'Dropped existing table: {SPANNER_TABLE_NAME}')\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# Create table with schema matching BigQuery\n",
    "# GoogleSQL syntax (backticks)\n",
    "create_table_sql = f\"\"\"\n",
    "CREATE TABLE `{SPANNER_TABLE_NAME}` (\n",
    "    transaction_id STRING(MAX) NOT NULL,\n",
    "    Time FLOAT64,\n",
    "    V1 FLOAT64, V2 FLOAT64, V3 FLOAT64, V4 FLOAT64, V5 FLOAT64,\n",
    "    V6 FLOAT64, V7 FLOAT64, V8 FLOAT64, V9 FLOAT64, V10 FLOAT64,\n",
    "    V11 FLOAT64, V12 FLOAT64, V13 FLOAT64, V14 FLOAT64, V15 FLOAT64,\n",
    "    V16 FLOAT64, V17 FLOAT64, V18 FLOAT64, V19 FLOAT64, V20 FLOAT64,\n",
    "    V21 FLOAT64, V22 FLOAT64, V23 FLOAT64, V24 FLOAT64, V25 FLOAT64,\n",
    "    V26 FLOAT64, V27 FLOAT64, V28 FLOAT64,\n",
    "    Amount FLOAT64,\n",
    "    Class INT64,\n",
    "    splits STRING(MAX)\n",
    ") PRIMARY KEY (transaction_id)\n",
    "\"\"\"\n",
    "\n",
    "run_query(sqlalchemy.text(create_table_sql), engine)\n",
    "print(f'✓ Created table: {SPANNER_TABLE_NAME}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load_data_both_header",
   "metadata": {},
   "source": [
    "### Load Data into Spanner\n",
    "\n",
    "Insert test data into the GoogleSQL database.\n",
    "\n",
    "**Note**: Using batch INSERT statements for efficient loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "load_data_both",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 1000 rows into Spanner...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created multiplexed session.\n",
      "Created multiplexed session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded 1000 rows into Spanner database\n"
     ]
    }
   ],
   "source": [
    "# Prepare data for insertion\n",
    "# Reorder columns to match table schema (transaction_id first)\n",
    "columns_order = ['transaction_id', 'Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10',\n",
    "                 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20',\n",
    "                 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount', 'Class', 'splits']\n",
    "df_ordered = df[columns_order]\n",
    "\n",
    "# Load into Spanner GoogleSQL database\n",
    "print(f'Loading {len(df_ordered)} rows into Spanner...')\n",
    "instance = spanner_client.instance(SPANNER_INSTANCE_NAME)\n",
    "database = instance.database(SPANNER_DATABASE_GOOGLESQL)\n",
    "\n",
    "with database.batch() as batch:\n",
    "    batch.insert(\n",
    "        table=SPANNER_TABLE_NAME,\n",
    "        columns=columns_order,\n",
    "        values=df_ordered.values.tolist()\n",
    "    )\n",
    "print(f'✓ Loaded {len(df_ordered)} rows into Spanner database')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verify_load_header",
   "metadata": {},
   "source": [
    "### Verify Data Load\n",
    "\n",
    "Check that data was loaded successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "verify_load",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row count: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/google/home/statmike/.cache/pypoetry/virtualenvs/frameworks-pytorch-0KVJlKeQ-py3.13/lib/python3.13/site-packages/google/cloud/sqlalchemy_spanner/sqlalchemy_spanner.py:1825: UserWarning: This method is non-operational as a transaction has not been started.\n",
      "  dbapi_connection.rollback()\n"
     ]
    }
   ],
   "source": [
    "# Count rows in Spanner database\n",
    "result = run_query(\n",
    "    sqlalchemy.text(f'SELECT COUNT(*) as count FROM `{SPANNER_TABLE_NAME}`'),\n",
    "    engine\n",
    ")\n",
    "print(f'Row count: {result[\"count\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "register_models_header",
   "metadata": {},
   "source": [
    "---\n",
    "## Register Vertex AI Model in Spanner\n",
    "\n",
    "Use `CREATE MODEL` to register the Vertex AI endpoint for SQL-based inference.\n",
    "\n",
    "**Key Advantage: No IAM Permissions Required**\n",
    "\n",
    "Unlike AlloyDB, Spanner ML integration **does not require** granting IAM permissions to service accounts. Model registration and inference work immediately after `CREATE MODEL`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "register_googlesql_header",
   "metadata": {},
   "source": [
    "### Register Model\n",
    "\n",
    "Use `CREATE MODEL` with GoogleSQL syntax to register the Vertex AI endpoint.\n",
    "\n",
    "**Note**: The INPUT and OUTPUT schemas must match your model. For the PyTorch autoencoder:\n",
    "- **INPUT**: 30 FLOAT64 features (Time, V1-V28, Amount)\n",
    "- **OUTPUT**: Varies by endpoint type (prebuilt has 13 fields, custom has 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "register_googlesql",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped existing model: pytorch_autoencoder_endpoint\n",
      "Registering model in Spanner...\n",
      "  Model name: pytorch_autoencoder_endpoint (hyphens replaced with underscores)\n",
      "✓ Registered model: pytorch_autoencoder_endpoint\n",
      "  Endpoint: https://us-central1-aiplatform.googleapis.com/v1/projects/1026793852137/locations/us-central1/endpoints/5971323405637517312\n",
      "  Database: frameworks (GoogleSQL)\n"
     ]
    }
   ],
   "source": [
    "# Drop model if exists (for demo purposes)\n",
    "# Model names cannot contain hyphens, so replace with underscores\n",
    "MODEL_NAME = ENDPOINT_DISPLAY_NAME.replace('-', '_')\n",
    "\n",
    "try:\n",
    "    run_query(\n",
    "        sqlalchemy.text(f'DROP MODEL `{MODEL_NAME}`'),\n",
    "        engine\n",
    "    )\n",
    "    print(f'Dropped existing model: {MODEL_NAME}')\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# Construct endpoint URL - use the full resource path WITHOUT :predict suffix\n",
    "endpoint_url = f'https://{REGION}-aiplatform.googleapis.com/v1/{endpoint.resource_name}'\n",
    "\n",
    "# Determine OUTPUT schema based on endpoint type\n",
    "if ENDPOINT_TYPE == 'custom':\n",
    "    output_schema = '''\n",
    "        anomaly_score FLOAT64,\n",
    "        encoded ARRAY<FLOAT64>\n",
    "    '''\n",
    "else:\n",
    "    output_schema = '''\n",
    "        denormalized_MAE FLOAT64,\n",
    "        denormalized_RMSE FLOAT64,\n",
    "        denormalized_MSE FLOAT64,\n",
    "        denormalized_MSLE FLOAT64,\n",
    "        normalized_MAE FLOAT64,\n",
    "        normalized_RMSE FLOAT64,\n",
    "        normalized_MSE FLOAT64,\n",
    "        normalized_MSLE FLOAT64,\n",
    "        encoded ARRAY<FLOAT64>,\n",
    "        normalized_reconstruction ARRAY<FLOAT64>,\n",
    "        normalized_reconstruction_errors ARRAY<FLOAT64>,\n",
    "        denormalized_reconstruction ARRAY<FLOAT64>,\n",
    "        denormalized_reconstruction_errors ARRAY<FLOAT64>\n",
    "    '''\n",
    "\n",
    "# Register model with GoogleSQL syntax\n",
    "register_model_sql = f'''\n",
    "CREATE MODEL `{MODEL_NAME}`\n",
    "INPUT (\n",
    "    Time FLOAT64,\n",
    "    V1 FLOAT64, V2 FLOAT64, V3 FLOAT64, V4 FLOAT64, V5 FLOAT64,\n",
    "    V6 FLOAT64, V7 FLOAT64, V8 FLOAT64, V9 FLOAT64, V10 FLOAT64,\n",
    "    V11 FLOAT64, V12 FLOAT64, V13 FLOAT64, V14 FLOAT64, V15 FLOAT64,\n",
    "    V16 FLOAT64, V17 FLOAT64, V18 FLOAT64, V19 FLOAT64, V20 FLOAT64,\n",
    "    V21 FLOAT64, V22 FLOAT64, V23 FLOAT64, V24 FLOAT64, V25 FLOAT64,\n",
    "    V26 FLOAT64, V27 FLOAT64, V28 FLOAT64,\n",
    "    Amount FLOAT64\n",
    ")\n",
    "OUTPUT ({output_schema})\n",
    "REMOTE\n",
    "OPTIONS (\n",
    "    endpoint = '{endpoint_url}'\n",
    ")\n",
    "'''\n",
    "\n",
    "print(f'Registering model in Spanner...')\n",
    "print(f'  Model name: {MODEL_NAME} (hyphens replaced with underscores)')\n",
    "run_query(sqlalchemy.text(register_model_sql), engine)\n",
    "print(f'✓ Registered model: {MODEL_NAME}')\n",
    "print(f'  Endpoint: {endpoint_url}')\n",
    "print(f'  Database: {SPANNER_DATABASE_GOOGLESQL} (GoogleSQL)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sql_inference_googlesql_header",
   "metadata": {},
   "source": [
    "---\n",
    "## SQL-Based ML Inference\n",
    "\n",
    "Make predictions using the `ML.PREDICT()` function with GoogleSQL.\n",
    "\n",
    "**Key Features:**\n",
    "- **Set-based operations**: `ML.PREDICT(MODEL model, TABLE data)`\n",
    "- **Similar to BigQuery ML**: Familiar syntax for BigQuery users\n",
    "- **Batch predictions**: Processes entire tables or subqueries\n",
    "\n",
    "This section demonstrates a multi-stage progression:\n",
    "1. **Simple Prediction**: Basic `ML.PREDICT()` call\n",
    "2. **Extract Fields**: Parse specific output fields\n",
    "3. **Business Logic**: Add CASE statements for classification\n",
    "4. **Batch Scoring**: Create results table with all predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "googlesql_stage1_header",
   "metadata": {},
   "source": [
    "### Stage 1: Simple Prediction with ML.PREDICT()\n",
    "\n",
    "Make a basic prediction using `ML.PREDICT()` with GoogleSQL syntax.\n",
    "\n",
    "**Syntax**: `ML.PREDICT(MODEL model_name, TABLE input_relation)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "googlesql_stage1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making prediction with ML.PREDICT() (this may take 10-30 seconds)...\n",
      "Simple prediction result:\n",
      "  Transaction ID: 0018196f-3775-45a2-8222-bcf0d0ef5a35\n",
      "  Amount: $0.0\n",
      "  Actual class: 0\n",
      "  Denormalized MAE: 28.76\n",
      "  Encoded (first 2): [0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "# Simple prediction using ML.PREDICT()\n",
    "# Note: This may take 10-30 seconds per row due to endpoint latency\n",
    "print('Making prediction with ML.PREDICT() (this may take 10-30 seconds)...')\n",
    "\n",
    "simple_prediction_sql = f'''\n",
    "SELECT\n",
    "    transaction_id,\n",
    "    Time,\n",
    "    Amount,\n",
    "    Class,\n",
    "    *\n",
    "FROM ML.PREDICT(\n",
    "    MODEL `{MODEL_NAME}`,\n",
    "    (\n",
    "        SELECT\n",
    "            transaction_id,\n",
    "            Time, V1, V2, V3, V4, V5, V6, V7, V8, V9, V10,\n",
    "            V11, V12, V13, V14, V15, V16, V17, V18, V19, V20,\n",
    "            V21, V22, V23, V24, V25, V26, V27, V28, Amount, Class\n",
    "        FROM `{SPANNER_TABLE_NAME}`\n",
    "        LIMIT 1\n",
    "    )\n",
    ")\n",
    "'''\n",
    "\n",
    "result = run_query(sqlalchemy.text(simple_prediction_sql), engine)\n",
    "print('Simple prediction result:')\n",
    "print(f'  Transaction ID: {result[\"transaction_id\"]}')\n",
    "print(f'  Amount: ${result[\"Amount\"]}')\n",
    "print(f'  Actual class: {result[\"Class\"]}')\n",
    "\n",
    "# Display prediction fields based on endpoint type\n",
    "if ENDPOINT_TYPE == 'custom':\n",
    "    print(f'  Anomaly score: {result[\"anomaly_score\"]:.2f}')\n",
    "    print(f'  Encoded (first 2): {result[\"encoded\"][:2]}')\n",
    "else:\n",
    "    print(f'  Denormalized MAE: {result[\"denormalized_MAE\"]:.2f}')\n",
    "    print(f'  Encoded (first 2): {result[\"encoded\"][:2]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "googlesql_stage2_header",
   "metadata": {},
   "source": [
    "### Stage 2: Extract Specific Fields\n",
    "\n",
    "Select only the fields needed for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "googlesql_stage2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted fields from 5 predictions:\n",
      "\n",
      "Row 1:\n",
      "  Amount: $0.69\n",
      "  Anomaly score: 1209.65\n",
      "  Encoded (first 2): [0.0, 0.0]\n",
      "\n",
      "Row 2:\n",
      "  Amount: $0.0\n",
      "  Anomaly score: 1130.49\n",
      "  Encoded (first 2): [0.0, 0.0]\n",
      "\n",
      "Row 3:\n",
      "  Amount: $0.77\n",
      "  Anomaly score: 1093.59\n",
      "  Encoded (first 2): [0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "# Determine which field to extract based on endpoint type\n",
    "if ENDPOINT_TYPE == 'custom':\n",
    "    anomaly_field = 'anomaly_score'\n",
    "else:\n",
    "    anomaly_field = 'denormalized_MAE'\n",
    "\n",
    "extract_fields_sql = f'''\n",
    "SELECT\n",
    "    transaction_id,\n",
    "    Time,\n",
    "    Amount,\n",
    "    Class,\n",
    "    {anomaly_field} AS anomaly_score,\n",
    "    encoded\n",
    "FROM ML.PREDICT(\n",
    "    MODEL `{MODEL_NAME}`,\n",
    "    (\n",
    "        SELECT\n",
    "            transaction_id,\n",
    "            Time, V1, V2, V3, V4, V5, V6, V7, V8, V9, V10,\n",
    "            V11, V12, V13, V14, V15, V16, V17, V18, V19, V20,\n",
    "            V21, V22, V23, V24, V25, V26, V27, V28, Amount, Class\n",
    "        FROM `{SPANNER_TABLE_NAME}`\n",
    "        LIMIT 5\n",
    "    )\n",
    ")\n",
    "ORDER BY anomaly_score DESC\n",
    "'''\n",
    "\n",
    "results = run_query(sqlalchemy.text(extract_fields_sql), engine)\n",
    "print(f'Extracted fields from {len(results)} predictions:')\n",
    "for i, result in enumerate(results[:3]):\n",
    "    print(f'\\nRow {i+1}:')\n",
    "    print(f'  Amount: ${result[\"Amount\"]}')\n",
    "    print(f'  Anomaly score: {result[\"anomaly_score\"]:.2f}')\n",
    "    print(f'  Encoded (first 2): {result[\"encoded\"][:2]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "googlesql_stage3_header",
   "metadata": {},
   "source": [
    "### Stage 3: Apply Business Logic\n",
    "\n",
    "Add CASE statements to classify transactions based on anomaly scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "googlesql_stage3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Business logic applied to 10 predictions:\n",
      "\n",
      "Top 5 by anomaly score:\n",
      "1. Amount: $0.69 | Score: 1209.65 | Risk: HIGH_RISK | Actual: Legitimate\n",
      "2. Amount: $0.0 | Score: 1130.49 | Risk: HIGH_RISK | Actual: Legitimate\n",
      "3. Amount: $0.77 | Score: 1093.59 | Risk: HIGH_RISK | Actual: Legitimate\n",
      "4. Amount: $0.77 | Score: 1018.69 | Risk: HIGH_RISK | Actual: Legitimate\n",
      "5. Amount: $0.77 | Score: 866.30 | Risk: HIGH_RISK | Actual: Legitimate\n"
     ]
    }
   ],
   "source": [
    "business_logic_sql = f'''\n",
    "WITH predictions AS (\n",
    "    SELECT\n",
    "        transaction_id,\n",
    "        Time,\n",
    "        Amount,\n",
    "        Class,\n",
    "        {anomaly_field} AS anomaly_score\n",
    "    FROM ML.PREDICT(\n",
    "        MODEL `{MODEL_NAME}`,\n",
    "        (\n",
    "            SELECT\n",
    "                transaction_id,\n",
    "                Time, V1, V2, V3, V4, V5, V6, V7, V8, V9, V10,\n",
    "                V11, V12, V13, V14, V15, V16, V17, V18, V19, V20,\n",
    "                V21, V22, V23, V24, V25, V26, V27, V28, Amount, Class\n",
    "            FROM `{SPANNER_TABLE_NAME}`\n",
    "            LIMIT 10\n",
    "        )\n",
    "    )\n",
    ")\n",
    "SELECT\n",
    "    transaction_id,\n",
    "    Amount,\n",
    "    Class,\n",
    "    anomaly_score,\n",
    "    CASE\n",
    "        WHEN anomaly_score > 200 THEN 'HIGH_RISK'\n",
    "        WHEN anomaly_score > 100 THEN 'MEDIUM_RISK'\n",
    "        ELSE 'NORMAL'\n",
    "    END AS risk_category,\n",
    "    CASE\n",
    "        WHEN Class = 1 THEN 'Fraud'\n",
    "        ELSE 'Legitimate'\n",
    "    END AS actual_label\n",
    "FROM predictions\n",
    "ORDER BY anomaly_score DESC\n",
    "'''\n",
    "\n",
    "results = run_query(sqlalchemy.text(business_logic_sql), engine)\n",
    "print(f'Business logic applied to {len(results)} predictions:')\n",
    "print('\\nTop 5 by anomaly score:')\n",
    "for i, result in enumerate(results[:5]):\n",
    "    print(f'{i+1}. Amount: ${result[\"Amount\"]} | '\n",
    "          f'Score: {result[\"anomaly_score\"]:.2f} | '\n",
    "          f'Risk: {result[\"risk_category\"]} | '\n",
    "          f'Actual: {result[\"actual_label\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "googlesql_stage4_header",
   "metadata": {},
   "source": [
    "### Stage 4: Batch Scoring with Results Table\n",
    "\n",
    "Create a table to store prediction results for all transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "googlesql_stage4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating batch predictions table...\n",
      "\n",
      "✓ Created predictions table: pytorch_autoencoder_spanner_predictions\n",
      "  Total predictions: 100\n",
      "  High risk: 83\n",
      "  Medium risk: 10\n",
      "  Normal: 7\n",
      "  Actual fraud cases: 3\n"
     ]
    }
   ],
   "source": [
    "# Create results table\n",
    "results_table = f'{SPANNER_TABLE_NAME}_predictions'\n",
    "\n",
    "# Drop if exists\n",
    "try:\n",
    "    run_query(\n",
    "        sqlalchemy.text(f'DROP TABLE `{results_table}`'),\n",
    "        engine\n",
    "    )\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# Create table with explicit schema (Spanner doesn't support CREATE TABLE AS SELECT in DDL)\n",
    "create_table_sql = f'''\n",
    "CREATE TABLE `{results_table}` (\n",
    "    transaction_id STRING(MAX) NOT NULL,\n",
    "    Time FLOAT64,\n",
    "    Amount FLOAT64,\n",
    "    Class INT64,\n",
    "    anomaly_score FLOAT64,\n",
    "    risk_category STRING(MAX),\n",
    "    actual_label STRING(MAX)\n",
    ") PRIMARY KEY (transaction_id)\n",
    "'''\n",
    "\n",
    "print('Creating batch predictions table...')\n",
    "run_query(sqlalchemy.text(create_table_sql), engine)\n",
    "\n",
    "# Insert prediction results\n",
    "insert_sql = f'''\n",
    "INSERT INTO `{results_table}` (transaction_id, Time, Amount, Class, anomaly_score, risk_category, actual_label)\n",
    "WITH predictions AS (\n",
    "    SELECT\n",
    "        transaction_id,\n",
    "        Time,\n",
    "        Amount,\n",
    "        Class,\n",
    "        {anomaly_field} AS anomaly_score\n",
    "    FROM ML.PREDICT(\n",
    "        MODEL `{MODEL_NAME}`,\n",
    "        (\n",
    "            SELECT\n",
    "                transaction_id,\n",
    "                Time, V1, V2, V3, V4, V5, V6, V7, V8, V9, V10,\n",
    "                V11, V12, V13, V14, V15, V16, V17, V18, V19, V20,\n",
    "                V21, V22, V23, V24, V25, V26, V27, V28, Amount, Class\n",
    "            FROM `{SPANNER_TABLE_NAME}`\n",
    "            LIMIT 100\n",
    "        )\n",
    "    )\n",
    ")\n",
    "SELECT\n",
    "    transaction_id,\n",
    "    Time,\n",
    "    Amount,\n",
    "    Class,\n",
    "    anomaly_score,\n",
    "    CASE\n",
    "        WHEN anomaly_score > 200 THEN 'HIGH_RISK'\n",
    "        WHEN anomaly_score > 100 THEN 'MEDIUM_RISK'\n",
    "        ELSE 'NORMAL'\n",
    "    END AS risk_category,\n",
    "    CASE\n",
    "        WHEN Class = 1 THEN 'Fraud'\n",
    "        ELSE 'Legitimate'\n",
    "    END AS actual_label\n",
    "FROM predictions\n",
    "'''\n",
    "\n",
    "run_query(sqlalchemy.text(insert_sql), engine)\n",
    "\n",
    "# Get statistics\n",
    "stats = run_query(sqlalchemy.text(f'''\n",
    "    SELECT\n",
    "        COUNT(*) as total_predictions,\n",
    "        COUNTIF(risk_category = 'HIGH_RISK') as high_risk_count,\n",
    "        COUNTIF(risk_category = 'MEDIUM_RISK') as medium_risk_count,\n",
    "        COUNTIF(risk_category = 'NORMAL') as normal_count,\n",
    "        COUNTIF(Class = 1) as actual_fraud_count\n",
    "    FROM `{results_table}`\n",
    "'''), engine)\n",
    "\n",
    "print(f'\\n✓ Created predictions table: {results_table}')\n",
    "print(f'  Total predictions: {stats[\"total_predictions\"]}')\n",
    "print(f'  High risk: {stats[\"high_risk_count\"]}')\n",
    "print(f'  Medium risk: {stats[\"medium_risk_count\"]}')\n",
    "print(f'  Normal: {stats[\"normal_count\"]}')\n",
    "print(f'  Actual fraud cases: {stats[\"actual_fraud_count\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cleanup_header",
   "metadata": {},
   "source": [
    "---\n",
    "## Cleanup\n",
    "\n",
    "Remove all Spanner resources to avoid ongoing charges. This will:\n",
    "1. Drop model from database\n",
    "2. Drop tables from database\n",
    "3. Delete the database\n",
    "4. Delete the Spanner instance (optional)\n",
    "\n",
    "**Warning**: This will permanently delete all data. Make sure you've saved any results you need.\n",
    "\n",
    "**Note on Instance Deletion**: The Spanner instance is named using `PROJECT_ID` to enable sharing with other notebooks (like the vector search example). If you want to keep the instance for other notebooks, set `DELETE_INSTANCE = False` below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enable_cleanup_header",
   "metadata": {},
   "source": [
    "### Enable Cleanup\n",
    "\n",
    "**IMPORTANT:** Cleanup is disabled by default to prevent accidental deletion.\n",
    "\n",
    "To enable cleanup and delete resources, change `ENABLE_CLEANUP` to `True` in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "enable_cleanup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  CLEANUP IS DISABLED\n",
      "   To delete resources, change ENABLE_CLEANUP to True above and re-run this cell.\n",
      "   Then proceed with the cleanup cells below.\n"
     ]
    }
   ],
   "source": [
    "# Change to True to delete resources\n",
    "ENABLE_CLEANUP = False\n",
    "\n",
    "# Set to False to keep the instance for other notebooks (like vector search)\n",
    "DELETE_INSTANCE = True\n",
    "\n",
    "# Safety check - do not modify below this line\n",
    "if not ENABLE_CLEANUP:\n",
    "    print('⚠️  CLEANUP IS DISABLED')\n",
    "    print('   To delete resources, change ENABLE_CLEANUP to True above and re-run this cell.')\n",
    "    print('   Then proceed with the cleanup cells below.')\n",
    "else:\n",
    "    print('✓ CLEANUP IS ENABLED')\n",
    "    print('  All cleanup cells will execute and delete resources.')\n",
    "    print('  Estimated resources to be deleted:')\n",
    "    print(f'    - GoogleSQL Database: {SPANNER_DATABASE_GOOGLESQL}')\n",
    "    if DELETE_INSTANCE:\n",
    "        print(f'    - Spanner Instance: {SPANNER_INSTANCE_NAME}')\n",
    "    else:\n",
    "        print(f'    - Instance will be KEPT: {SPANNER_INSTANCE_NAME}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cleanup_models_tables_header",
   "metadata": {},
   "source": [
    "### Drop Model and Tables\n",
    "\n",
    "Remove model and tables from the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cleanup_models_tables",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Skipping: Cleanup is disabled. Set ENABLE_CLEANUP = True to proceed.\n"
     ]
    }
   ],
   "source": [
    "if not ENABLE_CLEANUP:\n",
    "    print('⚠️  Skipping: Cleanup is disabled. Set ENABLE_CLEANUP = True to proceed.')\n",
    "else:\n",
    "    # Model name uses underscores (hyphens not allowed in Spanner model names)\n",
    "    MODEL_NAME = ENDPOINT_DISPLAY_NAME.replace('-', '_')\n",
    "    \n",
    "    # Drop model\n",
    "    print('Dropping model...')\n",
    "    try:\n",
    "        run_query(\n",
    "            sqlalchemy.text(f'DROP MODEL `{MODEL_NAME}`'),\n",
    "            engine\n",
    "        )\n",
    "        print(f'  ✓ Dropped model: {MODEL_NAME}')\n",
    "    except Exception as e:\n",
    "        print(f'  Model drop skipped: {e}')\n",
    "    \n",
    "    # Drop tables\n",
    "    for table in [SPANNER_TABLE_NAME, f'{SPANNER_TABLE_NAME}_predictions']:\n",
    "        try:\n",
    "            run_query(\n",
    "                sqlalchemy.text(f'DROP TABLE `{table}`'),\n",
    "                engine\n",
    "            )\n",
    "            print(f'  ✓ Dropped table: {table}')\n",
    "        except Exception as e:\n",
    "            print(f'  Table drop skipped: {e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cleanup_connections_header",
   "metadata": {},
   "source": [
    "### Close Database Connection\n",
    "\n",
    "Close the database connection engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cleanup_connections",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Skipping: Cleanup is disabled. Set ENABLE_CLEANUP = True to proceed.\n"
     ]
    }
   ],
   "source": [
    "if not ENABLE_CLEANUP:\n",
    "    print('⚠️  Skipping: Cleanup is disabled. Set ENABLE_CLEANUP = True to proceed.')\n",
    "else:\n",
    "    # Close connection engine\n",
    "    try:\n",
    "        engine.dispose()\n",
    "        print('✓ Closed database engine')\n",
    "    except Exception as e:\n",
    "        print(f'Engine close skipped: {e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cleanup_databases_header",
   "metadata": {},
   "source": [
    "### Delete Database\n",
    "\n",
    "Delete the GoogleSQL database.\n",
    "\n",
    "**Note**: This may take 5-10 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cleanup_databases",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Skipping: Cleanup is disabled. Set ENABLE_CLEANUP = True to proceed.\n"
     ]
    }
   ],
   "source": [
    "if not ENABLE_CLEANUP:\n",
    "    print('⚠️  Skipping: Cleanup is disabled. Set ENABLE_CLEANUP = True to proceed.')\n",
    "else:\n",
    "    # Delete GoogleSQL database\n",
    "    print(f'Deleting GoogleSQL database: {SPANNER_DATABASE_GOOGLESQL}')\n",
    "    print('  This may take 5-10 minutes...')\n",
    "    try:\n",
    "        spanner_database_client.drop_database(\n",
    "            database=googlesql_database.name\n",
    "        )\n",
    "        print(f'✓ Deleted GoogleSQL database: {SPANNER_DATABASE_GOOGLESQL}')\n",
    "    except Exception as e:\n",
    "        print(f'GoogleSQL database deletion failed: {e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cleanup_instance_header",
   "metadata": {},
   "source": [
    "### Delete Spanner Instance (Optional)\n",
    "\n",
    "Delete the Spanner instance if `DELETE_INSTANCE = True`.\n",
    "\n",
    "**Note**: If you're sharing this instance with other notebooks (like the vector search example), set `DELETE_INSTANCE = False` above to keep the instance.\n",
    "\n",
    "**Important**: This may take 10-15 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cleanup_instance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Skipping: Cleanup is disabled. Set ENABLE_CLEANUP = True to proceed.\n"
     ]
    }
   ],
   "source": [
    "if not ENABLE_CLEANUP:\n",
    "    print('⚠️  Skipping: Cleanup is disabled. Set ENABLE_CLEANUP = True to proceed.')\n",
    "elif not DELETE_INSTANCE:\n",
    "    print('⚠️  Instance deletion skipped (DELETE_INSTANCE = False)')\n",
    "    print(f'   Instance will be kept: {SPANNER_INSTANCE_NAME}')\n",
    "    print('   You can use this instance for other notebooks (e.g., vector search)')\n",
    "else:\n",
    "    print(f'Deleting Spanner instance: {SPANNER_INSTANCE_NAME}')\n",
    "    print('  This may take 10-15 minutes...')\n",
    "    \n",
    "    try:\n",
    "        spanner_instance_client.delete_instance(\n",
    "            name=spanner_instance.name\n",
    "        )\n",
    "        print(f'✓ Deleted instance: {SPANNER_INSTANCE_NAME}')\n",
    "        print('\\nAll Spanner resources have been deleted!')\n",
    "    except Exception as e:\n",
    "        print(f'Instance deletion failed: {e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cleanup_summary",
   "metadata": {},
   "source": [
    "---\n",
    "## Cleanup Complete\n",
    "\n",
    "If cleanup was enabled and executed successfully:\n",
    "\n",
    "✓ **Removed:**\n",
    "- Model from database\n",
    "- Tables from database\n",
    "- GoogleSQL database\n",
    "- Spanner instance (if `DELETE_INSTANCE = True`)\n",
    "\n",
    "**No ongoing charges** will accrue for the deleted resources.\n",
    "\n",
    "**Next Steps:**\n",
    "- Review the [readme.md](./readme.md) for other serving approaches\n",
    "- Compare with [AlloyDB](./alloydb-vertex-ai-endpoint.ipynb) or [BigQuery ML](./bigquery-bqml-remote-model-vertex.ipynb)\n",
    "- Explore [Spanner vector search](../../Applied%20GenAI/Retrieval/Retrieval%20-%20Spanner.ipynb) with the same instance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32900021",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (frameworks-pytorch)",
   "language": "python",
   "name": "frameworks-pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
