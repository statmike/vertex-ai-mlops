{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![tracker](https://us-central1-vertex-ai-mlops-369716.cloudfunctions.net/pixel-tracking?path=statmike%2Fvertex-ai-mlops%2FFramework+Workflows%2FPyTorch%2Fserving&file=dataflow-batch-runinference.ipynb)\n",
        "<!--- header table --->\n",
        "<table align=\"left\">\n",
        "<tr>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%20Workflows/PyTorch/serving/dataflow-batch-runinference.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://www.svgrepo.com/download/217753/github.svg\" alt=\"GitHub logo\">\n",
        "      <br>View on<br>GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://colab.research.google.com/github/statmike/vertex-ai-mlops/blob/main/Framework%20Workflows/PyTorch/serving/dataflow-batch-runinference.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://www.gstatic.com/pantheon/images/bigquery/welcome_page/colab-logo.svg\" alt=\"Google Colaboratory logo\">\n",
        "      <br>Run in<br>Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https%3A%2F%2Fraw.githubusercontent.com%2Fstatmike%2Fvertex-ai-mlops%2Fmain%2FFramework%2520Workflows%2FPyTorch%2Fserving%2Fdataflow-batch-runinference.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\">\n",
        "      <br>Run in<br>Colab Enterprise\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/bigquery/import?url=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%20Workflows/PyTorch/serving/dataflow-batch-runinference.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://www.gstatic.com/images/branding/gcpiconscolors/bigquery/v1/32px.svg\" alt=\"BigQuery logo\">\n",
        "      <br>Open in<br>BigQuery Studio\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/statmike/vertex-ai-mlops/main/Framework%20Workflows/PyTorch/serving/dataflow-batch-runinference.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://www.gstatic.com/images/branding/gcpiconscolors/vertexai/v1/32px.svg\" alt=\"Vertex AI logo\">\n",
        "      <br>Open in<br>Vertex AI Workbench\n",
        "    </a>\n",
        "  </td>\n",
        "</tr>\n",
        "<tr>\n",
        "  <td colspan=\"5\" style=\"text-align: right\">\n",
        "    <b>Share This On: </b> \n",
        "    <a href=\"https://www.linkedin.com/sharing/share-offsite/?url=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%2520Workflows/PyTorch/serving/dataflow-batch-runinference.ipynb\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"Linkedin Logo\" width=\"20px\"></a> \n",
        "    <a href=\"https://reddit.com/submit?url=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%2520Workflows/PyTorch/serving/dataflow-batch-runinference.ipynb\"><img src=\"https://redditinc.com/hubfs/Reddit%20Inc/Brand/Reddit_Logo.png\" alt=\"Reddit Logo\" width=\"20px\"></a> \n",
        "    <a href=\"https://bsky.app/intent/compose?text=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%2520Workflows/PyTorch/serving/dataflow-batch-runinference.ipynb\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"BlueSky Logo\" width=\"20px\"></a> \n",
        "    <a href=\"https://twitter.com/intent/tweet?url=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%2520Workflows/PyTorch/serving/dataflow-batch-runinference.ipynb\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/5/5a/X_icon_2.svg\" alt=\"X (Twitter) Logo\" width=\"20px\"></a> \n",
        "  </td>\n",
        "</tr>\n",
        "<tr>\n",
        "  <td colspan=\"5\" style=\"text-align: right\">\n",
        "    <b>Connect With Author On: </b> \n",
        "    <a href=\"https://www.linkedin.com/in/statmike\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"Linkedin Logo\" width=\"20px\"></a>\n",
        "    <a href=\"https://www.github.com/statmike\"><img src=\"https://www.svgrepo.com/download/217753/github.svg\" alt=\"GitHub Logo\" width=\"20px\"></a> \n",
        "    <a href=\"https://www.youtube.com/@statmike-channel\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/f/fd/YouTube_full-color_icon_%282024%29.svg\" alt=\"YouTube Logo\" width=\"20px\"></a>\n",
        "    <a href=\"https://bsky.app/profile/statmike.bsky.social\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"BlueSky Logo\" width=\"20px\"></a> \n",
        "    <a href=\"https://x.com/statmike\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/5/5a/X_icon_2.svg\" alt=\"X (Twitter) Logo\" width=\"20px\"></a>\n",
        "  </td>\n",
        "</tr>\n",
        "</table>"
      ],
      "id": "header"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Dataflow Batch Inference with RunInference\n",
        "\n",
        "This notebook demonstrates batch processing of transactions using Dataflow with Apache Beam RunInference.\n",
        "\n",
        "## What You'll Learn\n",
        "\n",
        "This workflow covers:\n",
        "\n",
        "1. **Create Custom ModelHandler**: Wrap PyTorch model for RunInference\n",
        "2. **Build Batch Pipeline**: Read from BigQuery, apply model, write results\n",
        "3. **Run on Dataflow**: Execute pipeline on Google Cloud\n",
        "4. **Monitor Job**: Track progress in Cloud Console\n",
        "5. **Analyze Results**: Query and visualize anomaly scores\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "- Completed `dataflow-setup.ipynb` (infrastructure ready)\n",
        "- Model .pt file in GCS\n",
        "- BigQuery tables created\n",
        "\n",
        "## Batch vs Streaming\n",
        "\n",
        "**Batch Processing (This Notebook)**:\n",
        "- \u2705 Process historical data\n",
        "- \u2705 Bounded dataset (has a start and end)\n",
        "- \u2705 Results available when job completes\n",
        "- \u2705 Cost-effective for large datasets\n",
        "- Example: Analyze all transactions from last month\n",
        "\n",
        "**Streaming Processing (Next Notebook)**:\n",
        "- \u2705 Process real-time data\n",
        "- \u2705 Unbounded dataset (continuous)\n",
        "- \u2705 Results available immediately\n",
        "- \u2705 Low-latency anomaly detection\n",
        "- Example: Flag suspicious transactions as they occur\n",
        "\n",
        "## Architecture\n",
        "\n",
        "```\n",
        "BigQuery Source Table\n",
        "  \u2193 Read test transactions\n",
        "Dataflow Pipeline\n",
        "  \u2193 Format instances\n",
        "RunInference (PyTorch Model)\n",
        "  \u2193 Generate anomaly scores\n",
        "Transform Results\n",
        "  \u2193 Extract scores and embeddings\n",
        "BigQuery Results Table\n",
        "```\n",
        "\n",
        "## RunInference Benefits\n",
        "\n",
        "- **In-process**: Model loaded directly in workers (no network calls)\n",
        "- **Automatic batching**: Combines instances for efficient inference\n",
        "- **Scalable**: Scales with pipeline workers\n",
        "- **Cost-effective**: Pay only when job runs\n",
        "\n",
        "## What This Pipeline Does\n",
        "\n",
        "1. Read TEST transactions from BigQuery\n",
        "2. Format data for PyTorch model\n",
        "3. Load TorchScript model in workers\n",
        "4. Run inference to get anomaly scores\n",
        "5. Extract relevant outputs (score + embeddings)\n",
        "6. Write results to BigQuery\n",
        "7. Job completes when all data processed"
      ],
      "id": "overview"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n## Environment Setup"
      ],
      "id": "env"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Set Your Project ID"
      ],
      "id": "proj"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "PROJECT_ID = 'statmike-mlops-349915'\nREQ_TYPE = 'ALL'\nINSTALL_TOOL = 'poetry'"
      ],
      "id": "proj_c"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Configuration"
      ],
      "id": "conf"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "REQUIREMENTS_URL = 'https://raw.githubusercontent.com/statmike/vertex-ai-mlops/refs/heads/main/Framework%20Workflows/PyTorch/requirements.txt'\nREQUIRED_APIS = [\"dataflow.googleapis.com\", \"bigquery.googleapis.com\", \"storage.googleapis.com\"]"
      ],
      "id": "conf_c"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Run Setup"
      ],
      "id": "setup"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, urllib.request\nurl = 'https://raw.githubusercontent.com/statmike/vertex-ai-mlops/refs/heads/main/core/notebook-template/python_setup.py'\nurllib.request.urlretrieve(url, 'python_setup_local.py')\nimport python_setup_local as python_setup\nos.remove('python_setup_local.py')\nsetup_info = python_setup.setup_environment(PROJECT_ID, REQ_TYPE, REQUIREMENTS_URL, REQUIRED_APIS, INSTALL_TOOL)"
      ],
      "id": "setup_c"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n## Python Setup"
      ],
      "id": "py"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Imports"
      ],
      "id": "imp"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import subprocess\nimport apache_beam as beam\nfrom apache_beam.ml.inference.base import RunInference\nfrom apache_beam.ml.inference.pytorch_inference import PytorchModelHandlerTensor\nfrom apache_beam.options.pipeline_options import PipelineOptions\nfrom apache_beam.io.gcp.bigquery import ReadFromBigQuery, WriteToBigQuery\nimport torch\nfrom datetime import datetime"
      ],
      "id": "imp_c"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Variables"
      ],
      "id": "vars"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "PROJECT_ID = subprocess.run([\"gcloud\", \"config\", \"get-value\", \"project\"], capture_output=True, text=True, check=True).stdout.strip()\nREGION = \"us-central1\"\nSERIES = \"frameworks\"\nEXPERIMENT = \"pytorch-autoencoder\"\nBUCKET_URI = f\"gs://{PROJECT_ID}\"\nMODEL_PATH = f\"{BUCKET_URI}/{SERIES}/{EXPERIMENT}/dataflow/final_model_traced.pt\"\nBQ_DATASET = SERIES.replace(\"-\", \"_\")\nBQ_TABLE_RESULTS = f\"{EXPERIMENT}_batch_results\"\nprint(f\"Model: {MODEL_PATH}\")\nprint(f\"Results: {PROJECT_ID}.{BQ_DATASET}.{BQ_TABLE_RESULTS}\")"
      ],
      "id": "vars_c"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n## Create Custom ModelHandler"
      ],
      "id": "handler"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PyTorchAutoencoderHandler(PytorchModelHandlerTensor):\n",
        "    \"\"\"Custom handler that extracts specific outputs from model\"\"\"\n",
        "    \n",
        "    def run_inference(self, batch, model, inference_args=None):\n",
        "        # Get full model output\n",
        "        with torch.no_grad():\n",
        "            predictions = model(batch)\n",
        "        \n",
        "        # Extract just anomaly scores and embeddings\n",
        "        results = []\n",
        "        for i in range(len(batch)):\n",
        "            results.append({\n",
        "                \"anomaly_score\": float(predictions[\"denormalized_MAE\"][i].item()),\n",
        "                \"encoded\": predictions[\"encoded\"][i].tolist()\n",
        "            })\n",
        "        return results\n",
        "\n",
        "# Create handler\n",
        "model_handler = PyTorchAutoencoderHandler(\n",
        "    state_dict_path=MODEL_PATH,\n",
        "    model_class=None,  # TorchScript model\n",
        "    device=\"cpu\"\n",
        ")\n",
        "print(\"\u2705 ModelHandler created\")"
      ],
      "id": "handler_c"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n## Build Dataflow Pipeline"
      ],
      "id": "pipeline"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def format_for_bq(element):\n",
        "    \"\"\"Format results for BigQuery\"\"\"\n",
        "    prediction = element[1]  # RunInference returns (input, output)\n",
        "    return {\n",
        "        \"instance_id\": str(hash(str(element[0]))),\n",
        "        \"anomaly_score\": prediction[\"anomaly_score\"],\n",
        "        \"encoded\": prediction[\"encoded\"],\n",
        "        \"timestamp\": datetime.utcnow().isoformat()\n",
        "    }\n",
        "\n",
        "# Pipeline options\n",
        "options = PipelineOptions([\n",
        "    f\"--project={PROJECT_ID}\",\n",
        "    f\"--region={REGION}\",\n",
        "    \"--runner=DataflowRunner\",\n",
        "    f\"--temp_location={BUCKET_URI}/dataflow/temp\",\n",
        "    f\"--staging_location={BUCKET_URI}/dataflow/staging\",\n",
        "    f\"--job_name=pytorch-batch-{datetime.now().strftime('%Y%m%d-%H%M%S')}\",\n",
        "    \"--save_main_session=True\"\n",
        "])\n",
        "\n",
        "print(\"\u2705 Pipeline options configured\")"
      ],
      "id": "pipeline_c"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Run Batch Job"
      ],
      "id": "run"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build and run pipeline\n",
        "with beam.Pipeline(options=options) as p:\n",
        "    results = (\n",
        "        p\n",
        "        | \"Read from BigQuery\" >> ReadFromBigQuery(\n",
        "            query=f\"\"\"\n",
        "            SELECT * EXCEPT(splits, transaction_id, Class)\n",
        "            FROM `{PROJECT_ID}.{BQ_DATASET}.{SERIES}`\n",
        "            WHERE splits = \"TEST\"\n",
        "            LIMIT 1000\n",
        "            \"\"\",\n",
        "            use_standard_sql=True\n",
        "        )\n",
        "        | \"Convert to tensors\" >> beam.Map(lambda row: torch.tensor(list(row.values()), dtype=torch.float32))\n",
        "        | \"RunInference\" >> RunInference(model_handler)\n",
        "        | \"Format for BigQuery\" >> beam.Map(format_for_bq)\n",
        "        | \"Write to BigQuery\" >> WriteToBigQuery(\n",
        "            table=f\"{PROJECT_ID}:{BQ_DATASET}.{BQ_TABLE_RESULTS}\",\n",
        "            write_disposition=beam.io.BigQueryDisposition.WRITE_APPEND\n",
        "        )\n",
        "    )\n",
        "\n",
        "print(\"\\n\u2705 Dataflow job submitted!\")\n",
        "print(f\"Monitor at: https://console.cloud.google.com/dataflow/jobs/{REGION}?project={PROJECT_ID}\")"
      ],
      "id": "run_c"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n## Analyze Results"
      ],
      "id": "results"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.cloud import bigquery\n",
        "import pandas as pd\n",
        "\n",
        "bq = bigquery.Client(project=PROJECT_ID)\n",
        "query = f\"\"\"\n",
        "SELECT *\n",
        "FROM `{PROJECT_ID}.{BQ_DATASET}.{BQ_TABLE_RESULTS}`\n",
        "ORDER BY timestamp DESC\n",
        "LIMIT 10\n",
        "\"\"\"\n",
        "df = bq.query(query).to_dataframe()\n",
        "print(f\"Retrieved {len(df)} results\")\n",
        "df"
      ],
      "id": "results_c"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n## Summary\n\n",
        "\u2705 Created custom PyTorch ModelHandler\n\n",
        "\u2705 Built batch Dataflow pipeline\n\n",
        "\u2705 Processed transactions with RunInference\n\n",
        "\u2705 Wrote anomaly scores to BigQuery\n\n",
        "### Next Steps\n\n",
        "- [Streaming Inference](./dataflow-streaming-runinference.ipynb) - Real-time processing\n",
        "- [Vertex Endpoint](./dataflow-vertex-endpoint.ipynb) - Call endpoint from Dataflow"
      ],
      "id": "summary"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.13.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}