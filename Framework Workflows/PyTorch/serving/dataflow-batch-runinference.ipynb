{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e3de771",
   "metadata": {},
   "source": [
    "![tracker](https://us-central1-vertex-ai-mlops-369716.cloudfunctions.net/pixel-tracking?path=statmike%2Fvertex-ai-mlops%2FFramework+Workflows%2FPyTorch%2Fserving&file=dataflow-batch-runinference.ipynb)\n",
    "<!--- header table --->\n",
    "<table align=\"left\">\n",
    "<tr>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%20Workflows/PyTorch/serving/dataflow-batch-runinference.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.svgrepo.com/download/217753/github.svg\" alt=\"GitHub logo\">\n",
    "      <br>View on<br>GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://colab.research.google.com/github/statmike/vertex-ai-mlops/blob/main/Framework%20Workflows/PyTorch/serving/dataflow-batch-runinference.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.gstatic.com/pantheon/images/bigquery/welcome_page/colab-logo.svg\" alt=\"Google Colaboratory logo\">\n",
    "      <br>Run in<br>Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https%3A%2F%2Fraw.githubusercontent.com%2Fstatmike%2Fvertex-ai-mlops%2Fmain%2FFramework%2520Workflows%2FPyTorch%2Fserving%2Fdataflow-batch-runinference.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\">\n",
    "      <br>Run in<br>Colab Enterprise\n",
    "    </a>\n",
    "  </td>      \n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/bigquery/import?url=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%20Workflows/PyTorch/serving/dataflow-batch-runinference.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.gstatic.com/images/branding/gcpiconscolors/bigquery/v1/32px.svg\" alt=\"BigQuery logo\">\n",
    "      <br>Open in<br>BigQuery Studio\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/statmike/vertex-ai-mlops/main/Framework%20Workflows/PyTorch/serving/dataflow-batch-runinference.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.gstatic.com/images/branding/gcpiconscolors/vertexai/v1/32px.svg\" alt=\"Vertex AI logo\">\n",
    "      <br>Open in<br>Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td colspan=\"5\" style=\"text-align: right\">\n",
    "    <b>Share This On: </b> \n",
    "    <a href=\"https://www.linkedin.com/sharing/share-offsite/?url=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%2520Workflows/PyTorch/serving/dataflow-batch-runinference.ipynb\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"Linkedin Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://reddit.com/submit?url=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%2520Workflows/PyTorch/serving/dataflow-batch-runinference.ipynb\"><img src=\"https://redditinc.com/hubfs/Reddit%20Inc/Brand/Reddit_Logo.png\" alt=\"Reddit Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://bsky.app/intent/compose?text=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%2520Workflows/PyTorch/serving/dataflow-batch-runinference.ipynb\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"BlueSky Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://twitter.com/intent/tweet?url=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%2520Workflows/PyTorch/serving/dataflow-batch-runinference.ipynb\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/5/5a/X_icon_2.svg\" alt=\"X (Twitter) Logo\" width=\"20px\"></a> \n",
    "  </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td colspan=\"5\" style=\"text-align: right\">\n",
    "    <b>Connect With Author On: </b> \n",
    "    <a href=\"https://www.linkedin.com/in/statmike\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"Linkedin Logo\" width=\"20px\"></a>\n",
    "    <a href=\"https://www.github.com/statmike\"><img src=\"https://www.svgrepo.com/download/217753/github.svg\" alt=\"GitHub Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://www.youtube.com/@statmike-channel\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/f/fd/YouTube_full-color_icon_%282024%29.svg\" alt=\"YouTube Logo\" width=\"20px\"></a>\n",
    "    <a href=\"https://bsky.app/profile/statmike.bsky.social\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"BlueSky Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://x.com/statmike\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/5/5a/X_icon_2.svg\" alt=\"X (Twitter) Logo\" width=\"20px\"></a>\n",
    "  </td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overview",
   "metadata": {},
   "source": "# Dataflow Batch Inference with RunInference\n\nThis notebook demonstrates batch processing of transactions using Dataflow with Apache Beam RunInference.\n\n## What You'll Learn\n\nThis workflow covers:\n\n1. **Create Custom ModelHandler**: Wrap PyTorch model for RunInference\n2. **Configure Worker Resources**: Set machine type and autoscaling parameters\n3. **Build Batch Pipeline**: Read from BigQuery, apply model, write results\n4. **Run on Dataflow**: Execute pipeline on Google Cloud (non-blocking)\n5. **Monitor Job Progress**: Track job status programmatically until completion\n6. **Analyze Results**: Query and visualize anomaly scores\n\n## Prerequisites\n\n- Completed `dataflow-setup.ipynb` - This sets up:\n  - Model .pt file extracted from .mar and uploaded to GCS\n  - BigQuery tables created (including `pytorch_autoencoder_batch_results`)\n  - Pub/Sub topics and subscriptions created\n\n## Batch vs Streaming\n\n**Batch Processing (This Notebook)**:\n- ‚úÖ Process historical data\n- ‚úÖ Bounded dataset (has a start and end)\n- ‚úÖ Job completes automatically when data is exhausted\n- ‚úÖ Cost-effective for large datasets\n- ‚úÖ Ideal for periodic analytics and reporting\n- Example: Analyze all transactions from last month\n\n**Streaming Processing (Next Notebook)**:\n- ‚úÖ Process real-time data\n- ‚úÖ Unbounded dataset (continuous)\n- ‚úÖ Results available immediately as data arrives\n- ‚úÖ Low-latency anomaly detection\n- ‚úÖ Requires manual job cancellation\n- Example: Flag suspicious transactions as they occur\n\n## Architecture\n\n```\nBigQuery Source Table\n  ‚Üì Read test transactions (bounded)\nDataflow Pipeline\n  ‚Üì Format instances\nRunInference (PyTorch Model)\n  ‚Üì Generate anomaly scores\nTransform Results\n  ‚Üì Extract scores and embeddings\nBigQuery Results Table\n```\n\n## RunInference Benefits\n\n- **In-process**: Model loaded directly in workers (no network calls)\n- **Automatic batching**: Combines instances for efficient inference\n- **Scalable**: Workers autoscale based on data volume\n- **Cost-effective**: Pay only when job runs (no always-on infrastructure)\n\n## Timing Expectations\n\n**Total time: ~5-10 minutes**\n\n1. **Start Dataflow job**: Instant (non-blocking execution)\n2. **Worker provisioning**: 2-3 minutes\n3. **Processing 1000 records**: 2-5 minutes\n4. **Job completion**: Automatic when all data processed\n\n## What This Pipeline Does\n\n1. Read TEST transactions from BigQuery (1000 records)\n2. Convert features to PyTorch tensors\n3. Load TorchScript model in workers\n4. Run batch inference to get anomaly scores\n5. Extract relevant outputs (score + embeddings)\n6. Write results to BigQuery\n7. Job completes automatically when all data processed"
  },
  {
   "cell_type": "markdown",
   "id": "env",
   "metadata": {},
   "source": [
    "---\n",
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proj",
   "metadata": {},
   "source": [
    "### Set Your Project ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "proj_c",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = 'statmike-mlops-349915'\n",
    "REQ_TYPE = 'ALL'\n",
    "INSTALL_TOOL = 'poetry'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conf",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "conf_c",
   "metadata": {},
   "outputs": [],
   "source": [
    "REQUIREMENTS_URL = 'https://raw.githubusercontent.com/statmike/vertex-ai-mlops/refs/heads/main/Framework%20Workflows/PyTorch/requirements.txt'\n",
    "REQUIRED_APIS = [\"dataflow.googleapis.com\", \"bigquery.googleapis.com\", \"storage.googleapis.com\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup",
   "metadata": {},
   "source": [
    "### Run Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "setup_c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "PYTHON GCP ENVIRONMENT SETUP\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "AUTHENTICATION\n",
      "==================================================\n",
      "Checking for existing ADC...\n",
      "‚úÖ Existing ADC found.\n",
      "‚úÖ Project is correctly set to 'statmike-mlops-349915'.\n",
      "\n",
      "==================================================\n",
      "API CHECK & ENABLE\n",
      "==================================================\n",
      "‚úÖ dataflow.googleapis.com is already enabled.\n",
      "‚úÖ bigquery.googleapis.com is already enabled.\n",
      "‚úÖ storage.googleapis.com is already enabled.\n",
      "\n",
      "==================================================\n",
      "PACKAGE MANAGEMENT\n",
      "==================================================\n",
      "Installation Tool: poetry\n",
      "‚úÖ Found poetry at: /usr/local/google/home/statmike/.local/bin/poetry\n",
      "‚úÖ Running in poetry environment: /usr/local/google/home/statmike/.cache/pypoetry/virtualenvs/frameworks-pytorch-0KVJlKeQ-py3.13\n",
      "‚ÑπÔ∏è  Poetry mode: Installing from pyproject.toml (REQUIREMENTS_URL ignored)\n",
      "‚úÖ Found pyproject.toml at: /usr/local/google/home/statmike/Git/vertex-ai-mlops/Framework Workflows/PyTorch/pyproject.toml\n",
      "   Changed working directory to: /usr/local/google/home/statmike/Git/vertex-ai-mlops/Framework Workflows/PyTorch\n",
      "Running poetry install...\n",
      "   Restored working directory to: /usr/local/google/home/statmike/Git/vertex-ai-mlops/Framework Workflows/PyTorch/serving\n",
      "‚úÖ All packages are already installed and up to date.\n",
      "\n",
      "==================================================\n",
      "Google Cloud Project Information\n",
      "==================================================\n",
      "PROJECT_ID     = statmike-mlops-349915\n",
      "PROJECT_NUMBER = 1026793852137\n",
      "==================================================\n",
      "\n",
      "\n",
      "==================================================\n",
      "SETUP SUMMARY\n",
      "==================================================\n",
      "‚úÖ Authentication:    Success\n",
      "‚úÖ API Configuration: Success\n",
      "‚úÖ Package Install:   Already up to date\n",
      "‚úÖ Installation Tool: poetry\n",
      "‚úÖ Project ID:        statmike-mlops-349915\n",
      "‚úÖ Project Number:    1026793852137\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os, urllib.request\n",
    "url = 'https://raw.githubusercontent.com/statmike/vertex-ai-mlops/refs/heads/main/core/notebook-template/python_setup.py'\n",
    "urllib.request.urlretrieve(url, 'python_setup_local.py')\n",
    "import python_setup_local as python_setup\n",
    "os.remove('python_setup_local.py')\n",
    "setup_info = python_setup.setup_environment(PROJECT_ID, REQ_TYPE, REQUIREMENTS_URL, REQUIRED_APIS, INSTALL_TOOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "py",
   "metadata": {},
   "source": [
    "---\n",
    "## Python Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imp",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "imp_c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import apache_beam as beam\n",
    "from apache_beam.ml.inference.base import RunInference\n",
    "from apache_beam.ml.inference.pytorch_inference import PytorchModelHandlerTensor\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "from apache_beam.io.gcp.bigquery import ReadFromBigQuery, WriteToBigQuery\n",
    "import torch\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vars",
   "metadata": {},
   "source": [
    "### Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "vars_c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project: statmike-mlops-349915\n",
      "Model: gs://statmike-mlops-349915/frameworks/pytorch-autoencoder/dataflow/final_model_traced.pt\n",
      "Results table: statmike-mlops-349915.frameworks.pytorch_autoencoder_batch_results\n",
      "Dataflow staging: gs://statmike-mlops-349915/dataflow/staging\n"
     ]
    }
   ],
   "source": [
    "PROJECT_ID = subprocess.run([\"gcloud\", \"config\", \"get-value\", \"project\"], capture_output=True, text=True, check=True).stdout.strip()\n",
    "\n",
    "# Configuration\n",
    "REGION = \"us-central1\"\n",
    "SERIES = \"frameworks\"\n",
    "EXPERIMENT = \"pytorch-autoencoder\"\n",
    "\n",
    "# GCS paths (aligned with dataflow-setup.ipynb)\n",
    "BUCKET_NAME = PROJECT_ID\n",
    "BUCKET_URI = f\"gs://{BUCKET_NAME}\"\n",
    "MODEL_PATH = f\"{BUCKET_URI}/{SERIES}/{EXPERIMENT}/dataflow/final_model_traced.pt\"\n",
    "\n",
    "# BigQuery configuration\n",
    "BQ_DATASET = SERIES.replace(\"-\", \"_\")\n",
    "BQ_TABLE_RESULTS = f\"{EXPERIMENT.replace('-', '_')}_batch_results\"\n",
    "\n",
    "# Dataflow configuration\n",
    "DATAFLOW_STAGING = f\"{BUCKET_URI}/dataflow/staging\"\n",
    "DATAFLOW_TEMP = f\"{BUCKET_URI}/dataflow/temp\"\n",
    "\n",
    "print(f\"Project: {PROJECT_ID}\")\n",
    "print(f\"Model: {MODEL_PATH}\")\n",
    "print(f\"Results table: {PROJECT_ID}.{BQ_DATASET}.{BQ_TABLE_RESULTS}\")\n",
    "print(f\"Dataflow staging: {DATAFLOW_STAGING}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handler",
   "metadata": {},
   "source": [
    "---\n",
    "## Create ModelHandler\n",
    "\n",
    "The ModelHandler wraps the PyTorch model for use with Apache Beam's RunInference transform. This section explains the key design choices for working with TorchScript models in Dataflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "handler_c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ModelHandler created\n",
      "   Model: gs://statmike-mlops-349915/frameworks/pytorch-autoencoder/dataflow/final_model_traced.pt\n",
      "   Device: cpu\n"
     ]
    }
   ],
   "source": [
    "class PyTorchAutoencoderHandler(PytorchModelHandlerTensor):\n",
    "    \"\"\"\n",
    "    Custom ModelHandler for PyTorch autoencoder inference.\n",
    "    \n",
    "    Key Design Choices:\n",
    "    \n",
    "    1. TorchScript Model Loading:\n",
    "       - Uses torch_script_model_path (not state_dict_path)\n",
    "       - TorchScript models (.pt) have architecture embedded\n",
    "       - No need to provide model_class parameter\n",
    "    \n",
    "    2. Batch Processing:\n",
    "       - Input: List of tensors (one per instance)\n",
    "       - Stack into single batch tensor with torch.stack()\n",
    "       - Enables efficient GPU/CPU vectorized operations\n",
    "    \n",
    "    3. Output Format:\n",
    "       - Returns list of dicts (one per instance in batch)\n",
    "       - Each dict contains tensors for that specific instance\n",
    "       - PytorchModelHandlerTensor passes these dicts through directly\n",
    "       - Downstream format_for_bq() function converts tensors to Python types\n",
    "    \n",
    "    4. Model Output:\n",
    "       - Autoencoder returns dict: {\"denormalized_MAE\": tensor, \"encoded\": tensor, ...}\n",
    "       - Extract only needed fields (denormalized_MAE for anomaly score, encoded for embeddings)\n",
    "       - Keep as tensors in run_inference, convert to Python types later\n",
    "    \"\"\"\n",
    "    \n",
    "    def run_inference(self, batch, model, inference_args=None):\n",
    "        \"\"\"\n",
    "        Run inference on a batch of input tensors.\n",
    "        \n",
    "        Args:\n",
    "            batch: List of torch.Tensor, each shape (30,) for our autoencoder\n",
    "            model: Loaded TorchScript model\n",
    "            inference_args: Optional additional arguments (unused)\n",
    "            \n",
    "        Returns:\n",
    "            List of dicts, one per input, containing model outputs as tensors\n",
    "        \"\"\"\n",
    "        # Stack list of tensors into single batch tensor\n",
    "        # Input: [tensor(30,), tensor(30,), ...] -> Output: tensor(batch_size, 30)\n",
    "        batch_tensor = torch.stack(batch)\n",
    "        \n",
    "        # Run model inference without gradient computation\n",
    "        with torch.no_grad():\n",
    "            predictions = model(batch_tensor)\n",
    "        \n",
    "        # Convert batch output to list of individual results\n",
    "        # This matches the batch size and allows proper per-instance processing\n",
    "        results = []\n",
    "        for i in range(len(batch)):\n",
    "            results.append({\n",
    "                \"denormalized_MAE\": predictions[\"denormalized_MAE\"][i],  # Scalar tensor\n",
    "                \"encoded\": predictions[\"encoded\"][i]  # 1D tensor (embedding)\n",
    "            })\n",
    "        return results\n",
    "\n",
    "\n",
    "# Initialize ModelHandler with TorchScript model from GCS\n",
    "model_handler = PyTorchAutoencoderHandler(\n",
    "    torch_script_model_path=MODEL_PATH,  # Path to .pt file in GCS\n",
    "    device=\"cpu\"  # Run on CPU in Dataflow workers\n",
    ")\n",
    "\n",
    "print(\"‚úÖ ModelHandler created\")\n",
    "print(f\"   Model: {MODEL_PATH}\")\n",
    "print(f\"   Device: cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pipeline",
   "metadata": {},
   "source": [
    "---\n",
    "## Build Dataflow Pipeline\n",
    "\n",
    "Configure the Dataflow pipeline components:\n",
    "- **format_for_bq**: Extract predictions and convert tensors to Python types\n",
    "- **Pipeline options**: Configure Dataflow job parameters including worker requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "x41oxd3d1zj",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "WORKER COMPUTE CONFIGURATION\n",
      "============================================================\n",
      "Machine Type: n1-standard-4\n",
      "  - vCPUs: 4\n",
      "  - Memory: 15 GB\n",
      "\n",
      "Autoscaling:\n",
      "  - Min Workers: 2\n",
      "  - Max Workers: 10\n",
      "  - Estimated capacity: ~400-10000 records/sec\n",
      "\n",
      "GPU Support: Not configured (CPU inference)\n",
      "  - To enable GPU: Add --worker_gpu_type and --worker_gpu_count\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Worker Compute Configuration\n",
    "# =============================\n",
    "# These settings control the machine type and autoscaling behavior for Dataflow workers.\n",
    "# Proper configuration ensures efficient resource utilization and cost management.\n",
    "\n",
    "# Machine Type: n1-standard-4\n",
    "# - 4 vCPUs, 15 GB memory\n",
    "# - Suitable for PyTorch inference workloads that need moderate CPU and memory\n",
    "# - Each worker can handle multiple inference requests concurrently\n",
    "# - Alternative: n1-standard-2 (smaller), n1-standard-8 (larger), or custom machine types\n",
    "\n",
    "MACHINE_TYPE = \"n1-standard-4\"\n",
    "\n",
    "# Autoscaling for Batch Pipelines\n",
    "# - min_workers=2: Provides baseline parallelism for faster job completion\n",
    "# - max_workers=10: Balances throughput with cost for bounded datasets\n",
    "# - Dataflow autoscales based on data volume and processing rate\n",
    "\n",
    "MIN_WORKERS = 2\n",
    "MAX_WORKERS = 10\n",
    "\n",
    "# Why These Settings for Batch?\n",
    "# ------------------------------\n",
    "# 1. **Minimum Workers (2)**:\n",
    "#    - Establishes baseline parallelism for efficient data processing\n",
    "#    - Reduces overall job runtime by distributing work\n",
    "#    - Lower than streaming since batch jobs are temporary\n",
    "#\n",
    "# 2. **Maximum Workers (10)**:\n",
    "#    - Allows scaling for large datasets while controlling costs\n",
    "#    - Each n1-standard-4 worker processes ~200-1000 records/sec\n",
    "#    - 10 workers can process ~2,000-10,000 records/sec\n",
    "#    - Conservative limit since batch jobs complete quickly\n",
    "#\n",
    "# 3. **Machine Type (n1-standard-4)**:\n",
    "#    - PyTorch model loading requires ~2-4 GB memory per worker\n",
    "#    - 15 GB allows model + batch processing overhead\n",
    "#    - 4 vCPUs enable parallel batch inference\n",
    "#    - Cost-effective for moderate-sized batch jobs\n",
    "\n",
    "# When to Adjust These Settings:\n",
    "# -------------------------------\n",
    "# - **Large Datasets**: Increase max_workers (e.g., 50-100) for millions of records\n",
    "# - **Faster Completion**: Increase min_workers (e.g., 5-10) to reduce job runtime\n",
    "# - **Cost Optimization**: Use smaller machine type (n1-standard-2) if memory permits\n",
    "# - **Larger Models**: Use n1-standard-8 or n1-highmem-4 for memory-intensive models\n",
    "# - **CPU-Intensive Models**: Use c2-standard-4 for compute-optimized instances\n",
    "\n",
    "# GPU Support (Optional)\n",
    "# ----------------------\n",
    "# Dataflow supports GPU workers for accelerated inference:\n",
    "# - Machine type: n1-standard-4 (host machine)\n",
    "# - GPU type: nvidia-tesla-t4, nvidia-tesla-v100, etc.\n",
    "# - GPU count: 1-4 per worker\n",
    "# - Requirements:\n",
    "#   1. Add --worker_machine_type=n1-standard-4\n",
    "#   2. Add --worker_gpu_type=nvidia-tesla-t4\n",
    "#   3. Add --worker_gpu_count=1\n",
    "#   4. Update model handler to use device=\"cuda\"\n",
    "#   5. Ensure PyTorch is GPU-enabled in requirements file\n",
    "#\n",
    "# Note: GPU workers are significantly more expensive than CPU workers.\n",
    "# Only use for models where GPU acceleration provides meaningful speedup.\n",
    "# For small models like this autoencoder, CPU inference is more cost-effective.\n",
    "# For batch workloads, consider if faster per-worker inference justifies higher cost.\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"WORKER COMPUTE CONFIGURATION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Machine Type: {MACHINE_TYPE}\")\n",
    "print(f\"  - vCPUs: 4\")\n",
    "print(f\"  - Memory: 15 GB\")\n",
    "print(f\"\\nAutoscaling:\")\n",
    "print(f\"  - Min Workers: {MIN_WORKERS}\")\n",
    "print(f\"  - Max Workers: {MAX_WORKERS}\")\n",
    "print(f\"  - Estimated capacity: ~{MIN_WORKERS * 200}-{MAX_WORKERS * 1000} records/sec\")\n",
    "print(f\"\\nGPU Support: Not configured (CPU inference)\")\n",
    "print(f\"  - To enable GPU: Add --worker_gpu_type and --worker_gpu_count\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "pipeline_c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Pipeline options configured\n",
      "   Job will run in: us-central1\n",
      "   Staging: gs://statmike-mlops-349915/dataflow/staging\n",
      "   Requirements: /tmp/tmpt9495xk7.txt\n",
      "   Machine type: n1-standard-4\n",
      "   Worker scaling: 2-10 workers\n"
     ]
    }
   ],
   "source": [
    "def format_for_bq(element):\n",
    "    \"\"\"\n",
    "    Format model predictions for BigQuery output.\n",
    "    \n",
    "    Args:\n",
    "        element: Dict returned from run_inference containing tensor outputs\n",
    "        \n",
    "    Returns:\n",
    "        Dict with Python types suitable for BigQuery insertion\n",
    "        \n",
    "    Note:\n",
    "        With our custom ModelHandler, element is the dict we returned from run_inference.\n",
    "        PytorchModelHandlerTensor passes these dicts through directly without wrapping.\n",
    "        We convert tensor values to Python types here for serialization.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"instance_id\": str(hash(str(element[\"denormalized_MAE\"].item()))),\n",
    "        \"anomaly_score\": float(element[\"denormalized_MAE\"].item()),\n",
    "        \"encoded\": element[\"encoded\"].tolist(),\n",
    "        \"timestamp\": datetime.utcnow().isoformat()\n",
    "    }\n",
    "\n",
    "\n",
    "# Create requirements file for Dataflow workers\n",
    "# Workers need PyTorch installed to load and run the model\n",
    "import tempfile\n",
    "requirements_content = \"torch>=2.0.0\\n\"\n",
    "\n",
    "with tempfile.NamedTemporaryFile(mode='w', suffix='.txt', delete=False) as f:\n",
    "    f.write(requirements_content)\n",
    "    requirements_file = f.name\n",
    "\n",
    "# Configure Dataflow pipeline options\n",
    "options = PipelineOptions([\n",
    "    f\"--project={PROJECT_ID}\",\n",
    "    f\"--region={REGION}\",\n",
    "    \"--runner=DataflowRunner\",  # Run on Google Cloud (not locally)\n",
    "    f\"--temp_location={DATAFLOW_TEMP}\",\n",
    "    f\"--staging_location={DATAFLOW_STAGING}\",\n",
    "    f\"--requirements_file={requirements_file}\",  # Install PyTorch in workers\n",
    "    f\"--job_name=pytorch-batch-{datetime.now().strftime('%Y%m%d-%H%M%S')}\",\n",
    "    \"--save_main_session\",  # Serialize global imports and variables\n",
    "    # Worker compute configuration\n",
    "    f\"--machine_type={MACHINE_TYPE}\",  # Machine type for workers\n",
    "    f\"--num_workers={MIN_WORKERS}\",  # Initial/minimum number of workers\n",
    "    f\"--max_num_workers={MAX_WORKERS}\",  # Maximum workers for autoscaling\n",
    "])\n",
    "\n",
    "print(\"‚úÖ Pipeline options configured\")\n",
    "print(f\"   Job will run in: {REGION}\")\n",
    "print(f\"   Staging: {DATAFLOW_STAGING}\")\n",
    "print(f\"   Requirements: {requirements_file}\")\n",
    "print(f\"   Machine type: {MACHINE_TYPE}\")\n",
    "print(f\"   Worker scaling: {MIN_WORKERS}-{MAX_WORKERS} workers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "run",
   "metadata": {},
   "source": [
    "### Run Batch Job\n",
    "\n",
    "Build and execute the pipeline graph. The job will:\n",
    "1. Read 1000 TEST transactions from BigQuery\n",
    "2. Convert to PyTorch tensors\n",
    "3. Run model inference via RunInference\n",
    "4. Write results to BigQuery\n",
    "\n",
    "The pipeline completes automatically when all data is processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "run_c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:apache_beam.options.pipeline_options:Bucket specified in temp_location has soft-delete policy enabled. To avoid being billed for unnecessary storage costs, turn off the soft delete feature on buckets that your Dataflow jobs use for temporary and staging storage. For more information, see https://cloud.google.com/storage/docs/use-soft-delete#remove-soft-delete-policy.\n",
      "WARNING:apache_beam.options.pipeline_options:Bucket specified in staging_location has soft-delete policy enabled. To avoid being billed for unnecessary storage costs, turn off the soft delete feature on buckets that your Dataflow jobs use for temporary and staging storage. For more information, see https://cloud.google.com/storage/docs/use-soft-delete#remove-soft-delete-policy.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    div.alert {\n",
       "      white-space: pre-line;\n",
       "    }\n",
       "  </style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <link rel=\"stylesheet\" href=\"https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/css/bootstrap.min.css\" integrity=\"sha384-Vkoo8x4CGsO3+Hhxv8T/Q5PaXtkKtu6ug5TOeNV6gBiFeWPGFN9MuhOf23Q9Ifjh\" crossorigin=\"anonymous\">\n",
       "            <div class=\"alert alert-info\">No cache_root detected. Defaulting to staging_location gs://statmike-mlops-349915/dataflow/staging for cache location.</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:apache_beam.transforms.core:('No iterator is returned by the process method in %s.', <class 'apache_beam.io.gcp.bigquery_read_internal._PassThroughThenCleanup.expand.<locals>.RemoveExtractedFiles'>)\n",
      "WARNING:apache_beam.transforms.core:('No iterator is returned by the process method in %s.', <class 'apache_beam.io.gcp.bigquery_file_loads.UpdateDestinationSchema'>)\n",
      "WARNING:apache_beam.transforms.core:('No iterator is returned by the process method in %s.', <class 'apache_beam.io.gcp.bigquery_file_loads.TriggerCopyJobs'>)\n",
      "WARNING:apache_beam.transforms.core:('No iterator is returned by the process method in %s.', <class 'apache_beam.io.gcp.bigquery_file_loads.DeleteTablesFn'>)\n",
      "WARNING:apache_beam.options.pipeline_options:Bucket specified in temp_location has soft-delete policy enabled. To avoid being billed for unnecessary storage costs, turn off the soft delete feature on buckets that your Dataflow jobs use for temporary and staging storage. For more information, see https://cloud.google.com/storage/docs/use-soft-delete#remove-soft-delete-policy.\n",
      "WARNING:apache_beam.options.pipeline_options:Bucket specified in staging_location has soft-delete policy enabled. To avoid being billed for unnecessary storage costs, turn off the soft delete feature on buckets that your Dataflow jobs use for temporary and staging storage. For more information, see https://cloud.google.com/storage/docs/use-soft-delete#remove-soft-delete-policy.\n",
      "WARNING:apache_beam.transforms.core:('No iterator is returned by the process method in %s.', <class 'apache_beam.io.gcp.bigquery_read_internal._PassThroughThenCleanup.expand.<locals>.RemoveExtractedFiles'>)\n",
      "WARNING:apache_beam.transforms.core:('No iterator is returned by the process method in %s.', <class 'apache_beam.io.gcp.bigquery_file_loads.UpdateDestinationSchema'>)\n",
      "WARNING:apache_beam.transforms.core:('No iterator is returned by the process method in %s.', <class 'apache_beam.io.gcp.bigquery_file_loads.TriggerCopyJobs'>)\n",
      "WARNING:apache_beam.transforms.core:('No iterator is returned by the process method in %s.', <class 'apache_beam.io.gcp.bigquery_file_loads.DeleteTablesFn'>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Dataflow job submitted!\n",
      "Job ID: 2025-11-07_17_52_11-1328735655060961236\n",
      "Monitor at: https://console.cloud.google.com/dataflow/jobs/us-central1/2025-11-07_17_52_11-1328735655060961236?project=statmike-mlops-349915\n",
      "\n",
      "============================================================\n",
      "‚è≥ TIMING EXPECTATIONS\n",
      "============================================================\n",
      "The pipeline will take approximately 5-10 minutes:\n",
      "  1. Worker provisioning: 2-3 minutes\n",
      "  2. Processing 1000 records: 2-5 minutes\n",
      "  3. Writing to BigQuery: < 1 minute\n",
      "\n",
      "Job will complete automatically when all data is processed.\n",
      "Run the next cell to monitor job progress.\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Build and run pipeline (non-blocking)\n",
    "p = beam.Pipeline(options=options)\n",
    "\n",
    "results = (\n",
    "    p\n",
    "    | \"Read from BigQuery\" >> ReadFromBigQuery(\n",
    "        query=f\"\"\"\n",
    "        SELECT * EXCEPT(splits, transaction_id, Class)\n",
    "        FROM `{PROJECT_ID}.{BQ_DATASET}.{SERIES}`\n",
    "        WHERE splits = \"TEST\"\n",
    "        LIMIT 1000\n",
    "        \"\"\",\n",
    "        use_standard_sql=True\n",
    "    )\n",
    "    | \"Convert to tensors\" >> beam.Map(lambda row: torch.tensor(list(row.values()), dtype=torch.float32))\n",
    "    | \"RunInference\" >> RunInference(model_handler)\n",
    "    | \"Format for BigQuery\" >> beam.Map(format_for_bq)\n",
    "    | \"Write to BigQuery\" >> WriteToBigQuery(\n",
    "        table=f\"{PROJECT_ID}:{BQ_DATASET}.{BQ_TABLE_RESULTS}\",\n",
    "        write_disposition=beam.io.BigQueryDisposition.WRITE_APPEND\n",
    "    )\n",
    ")\n",
    "\n",
    "# Run pipeline asynchronously (non-blocking)\n",
    "result = p.run()\n",
    "\n",
    "print(\"\\n‚úÖ Dataflow job submitted!\")\n",
    "print(f\"Job ID: {result.job_id()}\")\n",
    "print(f\"Monitor at: https://console.cloud.google.com/dataflow/jobs/{REGION}/{result.job_id()}?project={PROJECT_ID}\")\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚è≥ TIMING EXPECTATIONS\")\n",
    "print(\"=\" * 60)\n",
    "print(\"The pipeline will take approximately 5-10 minutes:\")\n",
    "print(\"  1. Worker provisioning: 2-3 minutes\")\n",
    "print(\"  2. Processing 1000 records: 2-5 minutes\")\n",
    "print(\"  3. Writing to BigQuery: < 1 minute\")\n",
    "print(\"\\nJob will complete automatically when all data is processed.\")\n",
    "print(\"Run the next cell to monitor job progress.\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6fh36rknik",
   "metadata": {},
   "source": [
    "### Monitor Job Progress\n",
    "\n",
    "Wait for the batch job to complete by polling its status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7147093xz25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monitoring Dataflow job: 2025-11-07_17_52_11-1328735655060961236\n",
      "Console: https://console.cloud.google.com/dataflow/jobs/us-central1/2025-11-07_17_52_11-1328735655060961236?project=statmike-mlops-349915\n",
      "============================================================\n",
      "[0s] Job state: JOB_STATE_PENDING\n",
      "[30s] Job state: JOB_STATE_RUNNING\n",
      "[811s] Job state: JOB_STATE_DONE\n",
      "============================================================\n",
      "‚úÖ Job completed successfully!\n",
      "   Total time: 13 minutes\n",
      "   You can now check results in BigQuery\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import dataflow_v1beta3\n",
    "import time\n",
    "\n",
    "# Initialize Dataflow client\n",
    "dataflow_client = dataflow_v1beta3.JobsV1Beta3Client()\n",
    "\n",
    "# Get the job ID from the result object\n",
    "job_id = result.job_id()\n",
    "\n",
    "print(f\"Monitoring Dataflow job: {job_id}\")\n",
    "print(f\"Console: https://console.cloud.google.com/dataflow/jobs/{REGION}/{job_id}?project={PROJECT_ID}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Poll job status every 30 seconds\n",
    "start_time = time.time()\n",
    "last_state = None\n",
    "\n",
    "while True:\n",
    "    # Get current job status\n",
    "    request = dataflow_v1beta3.GetJobRequest(\n",
    "        project_id=PROJECT_ID,\n",
    "        location=REGION,\n",
    "        job_id=job_id\n",
    "    )\n",
    "    \n",
    "    job = dataflow_client.get_job(request=request)\n",
    "    current_state = job.current_state.name\n",
    "    \n",
    "    # Print status update if state changed\n",
    "    if current_state != last_state:\n",
    "        elapsed = int(time.time() - start_time)\n",
    "        print(f\"[{elapsed}s] Job state: {current_state}\")\n",
    "        last_state = current_state\n",
    "    \n",
    "    # Check if job is in terminal state\n",
    "    if job.current_state in [\n",
    "        dataflow_v1beta3.JobState.JOB_STATE_DONE,\n",
    "        dataflow_v1beta3.JobState.JOB_STATE_FAILED,\n",
    "        dataflow_v1beta3.JobState.JOB_STATE_CANCELLED,\n",
    "        dataflow_v1beta3.JobState.JOB_STATE_DRAINED\n",
    "    ]:\n",
    "        break\n",
    "    \n",
    "    # Wait 30 seconds before next check\n",
    "    time.sleep(30)\n",
    "\n",
    "# Final status\n",
    "elapsed_minutes = int((time.time() - start_time) / 60)\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if job.current_state == dataflow_v1beta3.JobState.JOB_STATE_DONE:\n",
    "    print(f\"‚úÖ Job completed successfully!\")\n",
    "    print(f\"   Total time: {elapsed_minutes} minutes\")\n",
    "    print(f\"   You can now check results in BigQuery\")\n",
    "elif job.current_state == dataflow_v1beta3.JobState.JOB_STATE_FAILED:\n",
    "    print(f\"‚ùå Job failed after {elapsed_minutes} minutes\")\n",
    "    print(f\"   Check logs in console for details\")\n",
    "elif job.current_state == dataflow_v1beta3.JobState.JOB_STATE_CANCELLED:\n",
    "    print(f\"‚ö†Ô∏è  Job was cancelled after {elapsed_minutes} minutes\")\n",
    "else:\n",
    "    print(f\"‚ÑπÔ∏è  Job ended with state: {job.current_state.name}\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "results",
   "metadata": {},
   "source": [
    "---\n",
    "## Analyze Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "results_c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "BIGQUERY RESULTS (Batch Inference)\n",
      "============================================================\n",
      "‚úÖ Found 10 recent results\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instance_id</th>\n",
       "      <th>anomaly_score</th>\n",
       "      <th>encoded</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-598982276668726674</td>\n",
       "      <td>2705.457275</td>\n",
       "      <td>[0.0, 0.0, 2.6281771659851074, 0.0]</td>\n",
       "      <td>2025-11-08 02:01:03.511921+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5997578589454453703</td>\n",
       "      <td>1320.756104</td>\n",
       "      <td>[3.7616047859191895, 0.0, 9.868988990783691, 0.0]</td>\n",
       "      <td>2025-11-08 02:01:03.511873+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5143318905981266681</td>\n",
       "      <td>3013.436279</td>\n",
       "      <td>[0.0, 0.0, 2.6159238815307617, 0.0]</td>\n",
       "      <td>2025-11-08 02:01:03.511825+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7328622277488711907</td>\n",
       "      <td>2542.620605</td>\n",
       "      <td>[0.0, 0.0, 0.40106064081192017, 0.0]</td>\n",
       "      <td>2025-11-08 02:01:03.511778+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2392079463149692441</td>\n",
       "      <td>106.213814</td>\n",
       "      <td>[0.012153884395956993, 0.0, 8.333025932312012,...</td>\n",
       "      <td>2025-11-08 02:01:03.511731+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-3932637324696731622</td>\n",
       "      <td>901.961731</td>\n",
       "      <td>[0.37614986300468445, 0.7020396590232849, 0.60...</td>\n",
       "      <td>2025-11-08 02:01:03.511680+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>8535226073225465373</td>\n",
       "      <td>3199.614990</td>\n",
       "      <td>[0.0, 0.0, 0.08318693935871124, 0.0]</td>\n",
       "      <td>2025-11-08 02:01:03.511633+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5220432787149206558</td>\n",
       "      <td>218.590790</td>\n",
       "      <td>[0.12951889634132385, 1.1100342273712158, 0.83...</td>\n",
       "      <td>2025-11-08 02:01:03.511582+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8093907155018534167</td>\n",
       "      <td>259.608673</td>\n",
       "      <td>[0.12567558884620667, 1.0980515480041504, 0.82...</td>\n",
       "      <td>2025-11-08 02:01:03.511529+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-2656101211283403419</td>\n",
       "      <td>954.889404</td>\n",
       "      <td>[0.1446773111820221, 1.182241439819336, 0.9164...</td>\n",
       "      <td>2025-11-08 02:01:03.511464+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            instance_id  anomaly_score  \\\n",
       "0   -598982276668726674    2705.457275   \n",
       "1   5997578589454453703    1320.756104   \n",
       "2   5143318905981266681    3013.436279   \n",
       "3   7328622277488711907    2542.620605   \n",
       "4   2392079463149692441     106.213814   \n",
       "5  -3932637324696731622     901.961731   \n",
       "6   8535226073225465373    3199.614990   \n",
       "7   5220432787149206558     218.590790   \n",
       "8   8093907155018534167     259.608673   \n",
       "9  -2656101211283403419     954.889404   \n",
       "\n",
       "                                             encoded  \\\n",
       "0                [0.0, 0.0, 2.6281771659851074, 0.0]   \n",
       "1  [3.7616047859191895, 0.0, 9.868988990783691, 0.0]   \n",
       "2                [0.0, 0.0, 2.6159238815307617, 0.0]   \n",
       "3               [0.0, 0.0, 0.40106064081192017, 0.0]   \n",
       "4  [0.012153884395956993, 0.0, 8.333025932312012,...   \n",
       "5  [0.37614986300468445, 0.7020396590232849, 0.60...   \n",
       "6               [0.0, 0.0, 0.08318693935871124, 0.0]   \n",
       "7  [0.12951889634132385, 1.1100342273712158, 0.83...   \n",
       "8  [0.12567558884620667, 1.0980515480041504, 0.82...   \n",
       "9  [0.1446773111820221, 1.182241439819336, 0.9164...   \n",
       "\n",
       "                         timestamp  \n",
       "0 2025-11-08 02:01:03.511921+00:00  \n",
       "1 2025-11-08 02:01:03.511873+00:00  \n",
       "2 2025-11-08 02:01:03.511825+00:00  \n",
       "3 2025-11-08 02:01:03.511778+00:00  \n",
       "4 2025-11-08 02:01:03.511731+00:00  \n",
       "5 2025-11-08 02:01:03.511680+00:00  \n",
       "6 2025-11-08 02:01:03.511633+00:00  \n",
       "7 2025-11-08 02:01:03.511582+00:00  \n",
       "8 2025-11-08 02:01:03.511529+00:00  \n",
       "9 2025-11-08 02:01:03.511464+00:00  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üí° Pipeline Status Summary\n",
      "============================================================\n",
      "‚úÖ Pipeline completed successfully!\n",
      "   Total results in BigQuery: 2000\n",
      "   Results table: statmike-mlops-349915.frameworks.pytorch_autoencoder_batch_results\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import bigquery\n",
    "import pandas as pd\n",
    "\n",
    "bq = bigquery.Client(project=PROJECT_ID)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"BIGQUERY RESULTS (Batch Inference)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Get recent results\n",
    "query = f\"\"\"\n",
    "SELECT *\n",
    "FROM `{PROJECT_ID}.{BQ_DATASET}.{BQ_TABLE_RESULTS}`\n",
    "ORDER BY timestamp DESC\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "df = bq.query(query).to_dataframe()\n",
    "\n",
    "if len(df) > 0:\n",
    "    print(f\"‚úÖ Found {len(df)} recent results\")\n",
    "    display(df)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No results yet in BigQuery\")\n",
    "    print(\"   Wait for Dataflow job to complete\")\n",
    "\n",
    "# Get total count\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üí° Pipeline Status Summary\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "count_query = f\"SELECT COUNT(*) as total FROM `{PROJECT_ID}.{BQ_DATASET}.{BQ_TABLE_RESULTS}`\"\n",
    "count_result = bq.query(count_query).to_dataframe()\n",
    "total_results = count_result['total'].iloc[0] if len(count_result) > 0 else 0\n",
    "\n",
    "if total_results > 0:\n",
    "    print(f\"‚úÖ Pipeline completed successfully!\")\n",
    "    print(f\"   Total results in BigQuery: {total_results}\")\n",
    "    print(f\"   Results table: {PROJECT_ID}.{BQ_DATASET}.{BQ_TABLE_RESULTS}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No results yet in BigQuery\")\n",
    "    print(\"   Check Dataflow job status for errors\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "In this notebook, you successfully:\n",
    "\n",
    "‚úÖ **Created Custom PyTorch ModelHandler**\n",
    "- Wrapped TorchScript model for RunInference\n",
    "- Implemented efficient batch processing with torch.stack()\n",
    "- Designed output format compatible with downstream processing\n",
    "\n",
    "‚úÖ **Built Batch Dataflow Pipeline**\n",
    "- Read historical data from BigQuery\n",
    "- Applied PyTorch model inference at scale\n",
    "- Automatic dependency installation (PyTorch) in workers\n",
    "\n",
    "‚úÖ **Deployed PyTorch Model In-Process**\n",
    "- Model loaded directly in Dataflow workers\n",
    "- No network calls to external endpoints\n",
    "- High-throughput inference for bounded datasets\n",
    "\n",
    "‚úÖ **Wrote Results to BigQuery**\n",
    "- Structured output for SQL analytics\n",
    "- Persistent storage for auditing and analysis\n",
    "\n",
    "## Key Learnings\n",
    "\n",
    "**ModelHandler Design:**\n",
    "- TorchScript models use `torch_script_model_path` (not `state_dict_path`)\n",
    "- Batch processing requires `torch.stack()` to combine tensors\n",
    "- Return list of dicts (one per instance) from `run_inference()`\n",
    "- Keep outputs as tensors until format stage for efficiency\n",
    "\n",
    "**Batch Processing Benefits:**\n",
    "- Process all data at once (vs. streaming micro-batches)\n",
    "- Higher throughput for large datasets\n",
    "- Simpler pipeline logic (no windowing needed)\n",
    "- Job completes when data is exhausted\n",
    "\n",
    "**Cost Considerations:**\n",
    "- In-process inference avoids endpoint API costs\n",
    "- Pay only while job runs (no always-on infrastructure)\n",
    "- Workers autoscale based on data volume\n",
    "- More cost-effective than streaming for historical analysis\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "### Continue with Dataflow Workflows:\n",
    "\n",
    "**Streaming Inference with Local Model:**\n",
    "- [dataflow-streaming-runinference.ipynb](./dataflow-streaming-runinference.ipynb)\n",
    "  - Process real-time data from Pub/Sub\n",
    "  - Same ModelHandler pattern with windowing\n",
    "  - Low-latency continuous inference\n",
    "\n",
    "**Batch Inference with Vertex Endpoint:**\n",
    "- [dataflow-batch-runinference-vertex.ipynb](./dataflow-batch-runinference-vertex.ipynb)\n",
    "  - Call deployed Vertex AI Endpoint\n",
    "  - Compare performance vs local RunInference\n",
    "  - Centralized model updates without redeploying pipeline\n",
    "\n",
    "**Streaming Inference with Vertex Endpoint:**\n",
    "- [dataflow-streaming-runinference-vertex.ipynb](./dataflow-streaming-runinference-vertex.ipynb)\n",
    "  - Real-time endpoint calls from Dataflow\n",
    "  - Shared model across multiple services\n",
    "  - Managed infrastructure with auto-scaling\n",
    "\n",
    "### Production Enhancements:\n",
    "\n",
    "1. **Error Handling**: Add try-catch blocks with dead letter queues for failed records\n",
    "2. **Monitoring**: Set up Cloud Monitoring dashboards for job metrics\n",
    "3. **Autoscaling**: Configure worker pool sizing based on data volume\n",
    "4. **Model Versioning**: Parameterize MODEL_PATH for A/B testing\n",
    "5. **Data Quality**: Add input validation before inference\n",
    "\n",
    "### Resources\n",
    "\n",
    "- [Dataflow Batch Pipelines](https://cloud.google.com/dataflow/docs/guides/batch-pipeline)\n",
    "- [Apache Beam RunInference](https://beam.apache.org/documentation/ml/about-ml/)\n",
    "- [PyTorch RunInference](https://beam.apache.org/documentation/ml/pytorch-inference/)\n",
    "- [BigQuery Best Practices](https://cloud.google.com/bigquery/docs/best-practices-performance-overview)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (frameworks-pytorch)",
   "language": "python",
   "name": "frameworks-pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}