{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e3de771",
   "metadata": {},
   "source": [
    "![tracker](https://us-central1-vertex-ai-mlops-369716.cloudfunctions.net/pixel-tracking?path=statmike%2Fvertex-ai-mlops%2FFramework+Workflows%2FPyTorch%2Fserving&file=dataflow-batch-runinference.ipynb)\n",
    "<!--- header table --->\n",
    "<table align=\"left\">\n",
    "<tr>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%20Workflows/PyTorch/serving/dataflow-batch-runinference.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.svgrepo.com/download/217753/github.svg\" alt=\"GitHub logo\">\n",
    "      <br>View on<br>GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://colab.research.google.com/github/statmike/vertex-ai-mlops/blob/main/Framework%20Workflows/PyTorch/serving/dataflow-batch-runinference.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.gstatic.com/pantheon/images/bigquery/welcome_page/colab-logo.svg\" alt=\"Google Colaboratory logo\">\n",
    "      <br>Run in<br>Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https%3A%2F%2Fraw.githubusercontent.com%2Fstatmike%2Fvertex-ai-mlops%2Fmain%2FFramework%2520Workflows%2FPyTorch%2Fserving%2Fdataflow-batch-runinference.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\">\n",
    "      <br>Run in<br>Colab Enterprise\n",
    "    </a>\n",
    "  </td>      \n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/bigquery/import?url=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%20Workflows/PyTorch/serving/dataflow-batch-runinference.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.gstatic.com/images/branding/gcpiconscolors/bigquery/v1/32px.svg\" alt=\"BigQuery logo\">\n",
    "      <br>Open in<br>BigQuery Studio\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/statmike/vertex-ai-mlops/main/Framework%20Workflows/PyTorch/serving/dataflow-batch-runinference.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.gstatic.com/images/branding/gcpiconscolors/vertexai/v1/32px.svg\" alt=\"Vertex AI logo\">\n",
    "      <br>Open in<br>Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td colspan=\"5\" style=\"text-align: right\">\n",
    "    <b>Share This On: </b> \n",
    "    <a href=\"https://www.linkedin.com/sharing/share-offsite/?url=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%2520Workflows/PyTorch/serving/dataflow-batch-runinference.ipynb\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"Linkedin Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://reddit.com/submit?url=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%2520Workflows/PyTorch/serving/dataflow-batch-runinference.ipynb\"><img src=\"https://redditinc.com/hubfs/Reddit%20Inc/Brand/Reddit_Logo.png\" alt=\"Reddit Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://bsky.app/intent/compose?text=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%2520Workflows/PyTorch/serving/dataflow-batch-runinference.ipynb\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"BlueSky Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://twitter.com/intent/tweet?url=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%2520Workflows/PyTorch/serving/dataflow-batch-runinference.ipynb\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/5/5a/X_icon_2.svg\" alt=\"X (Twitter) Logo\" width=\"20px\"></a> \n",
    "  </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td colspan=\"5\" style=\"text-align: right\">\n",
    "    <b>Connect With Author On: </b> \n",
    "    <a href=\"https://www.linkedin.com/in/statmike\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"Linkedin Logo\" width=\"20px\"></a>\n",
    "    <a href=\"https://www.github.com/statmike\"><img src=\"https://www.svgrepo.com/download/217753/github.svg\" alt=\"GitHub Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://www.youtube.com/@statmike-channel\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/f/fd/YouTube_full-color_icon_%282024%29.svg\" alt=\"YouTube Logo\" width=\"20px\"></a>\n",
    "    <a href=\"https://bsky.app/profile/statmike.bsky.social\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"BlueSky Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://x.com/statmike\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/5/5a/X_icon_2.svg\" alt=\"X (Twitter) Logo\" width=\"20px\"></a>\n",
    "  </td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overview",
   "metadata": {},
   "source": "# Dataflow Batch Inference with RunInference\n\nThis notebook demonstrates batch processing of transactions using Dataflow with Apache Beam RunInference.\n\n## What You'll Learn\n\nThis workflow covers:\n\n1. **Create Custom ModelHandler**: Wrap PyTorch model for RunInference\n2. **Build Batch Pipeline**: Read from BigQuery, apply model, write results\n3. **Run on Dataflow**: Execute pipeline on Google Cloud\n4. **Monitor Job**: Track progress in Cloud Console\n5. **Analyze Results**: Query and visualize anomaly scores\n\n## Prerequisites\n\n- Completed `dataflow-setup.ipynb` - This sets up:\n  - Model .pt file extracted from .mar and uploaded to GCS\n  - BigQuery tables created (including `pytorch_autoencoder_batch_results`)\n  - Pub/Sub topics and subscriptions created\n\n## Batch vs Streaming\n\n**Batch Processing (This Notebook)**:\n- ‚úÖ Process historical data\n- ‚úÖ Bounded dataset (has a start and end)\n- ‚úÖ Results available when job completes\n- ‚úÖ Cost-effective for large datasets\n- Example: Analyze all transactions from last month\n\n**Streaming Processing (Next Notebook)**:\n- ‚úÖ Process real-time data\n- ‚úÖ Unbounded dataset (continuous)\n- ‚úÖ Results available immediately\n- ‚úÖ Low-latency anomaly detection\n- Example: Flag suspicious transactions as they occur\n\n## Architecture\n\n```\nBigQuery Source Table\n  ‚Üì Read test transactions\nDataflow Pipeline\n  ‚Üì Format instances\nRunInference (PyTorch Model)\n  ‚Üì Generate anomaly scores\nTransform Results\n  ‚Üì Extract scores and embeddings\nBigQuery Results Table\n```\n\n## RunInference Benefits\n\n- **In-process**: Model loaded directly in workers (no network calls)\n- **Automatic batching**: Combines instances for efficient inference\n- **Scalable**: Scales with pipeline workers\n- **Cost-effective**: Pay only when job runs\n\n## Timing Expectations\n\n**Total time: ~5-10 minutes**\n\n1. **Start Dataflow job**: Instant\n2. **Worker provisioning**: 2-3 minutes\n3. **Processing 1000 records**: 2-5 minutes\n4. **View results**: Immediate after job completes\n\n## What This Pipeline Does\n\n1. Read TEST transactions from BigQuery (1000 records)\n2. Format data for PyTorch model\n3. Load TorchScript model in workers\n4. Run inference to get anomaly scores\n5. Extract relevant outputs (score + embeddings)\n6. Write results to BigQuery\n7. Job completes when all data processed"
  },
  {
   "cell_type": "markdown",
   "id": "env",
   "metadata": {},
   "source": [
    "---\n",
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proj",
   "metadata": {},
   "source": [
    "### Set Your Project ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proj_c",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = 'statmike-mlops-349915'\n",
    "REQ_TYPE = 'ALL'\n",
    "INSTALL_TOOL = 'poetry'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conf",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conf_c",
   "metadata": {},
   "outputs": [],
   "source": [
    "REQUIREMENTS_URL = 'https://raw.githubusercontent.com/statmike/vertex-ai-mlops/refs/heads/main/Framework%20Workflows/PyTorch/requirements.txt'\n",
    "REQUIRED_APIS = [\"dataflow.googleapis.com\", \"bigquery.googleapis.com\", \"storage.googleapis.com\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup",
   "metadata": {},
   "source": [
    "### Run Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup_c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, urllib.request\n",
    "url = 'https://raw.githubusercontent.com/statmike/vertex-ai-mlops/refs/heads/main/core/notebook-template/python_setup.py'\n",
    "urllib.request.urlretrieve(url, 'python_setup_local.py')\n",
    "import python_setup_local as python_setup\n",
    "os.remove('python_setup_local.py')\n",
    "setup_info = python_setup.setup_environment(PROJECT_ID, REQ_TYPE, REQUIREMENTS_URL, REQUIRED_APIS, INSTALL_TOOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "py",
   "metadata": {},
   "source": [
    "---\n",
    "## Python Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imp",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imp_c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import apache_beam as beam\n",
    "from apache_beam.ml.inference.base import RunInference\n",
    "from apache_beam.ml.inference.pytorch_inference import PytorchModelHandlerTensor\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "from apache_beam.io.gcp.bigquery import ReadFromBigQuery, WriteToBigQuery\n",
    "import torch\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vars",
   "metadata": {},
   "source": [
    "### Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vars_c",
   "metadata": {},
   "outputs": [],
   "source": "PROJECT_ID = subprocess.run([\"gcloud\", \"config\", \"get-value\", \"project\"], capture_output=True, text=True, check=True).stdout.strip()\n\n# Configuration\nREGION = \"us-central1\"\nSERIES = \"frameworks\"\nEXPERIMENT = \"pytorch-autoencoder\"\n\n# GCS paths (aligned with dataflow-setup.ipynb)\nBUCKET_NAME = PROJECT_ID\nBUCKET_URI = f\"gs://{BUCKET_NAME}\"\nMODEL_PATH = f\"{BUCKET_URI}/{SERIES}/{EXPERIMENT}/dataflow/final_model_traced.pt\"\n\n# BigQuery configuration\nBQ_DATASET = SERIES.replace(\"-\", \"_\")\nBQ_TABLE_RESULTS = f\"{EXPERIMENT.replace('-', '_')}_batch_results\"\n\n# Dataflow configuration\nDATAFLOW_STAGING = f\"{BUCKET_URI}/dataflow/staging\"\nDATAFLOW_TEMP = f\"{BUCKET_URI}/dataflow/temp\"\n\nprint(f\"Project: {PROJECT_ID}\")\nprint(f\"Model: {MODEL_PATH}\")\nprint(f\"Results table: {PROJECT_ID}.{BQ_DATASET}.{BQ_TABLE_RESULTS}\")\nprint(f\"Dataflow staging: {DATAFLOW_STAGING}\")"
  },
  {
   "cell_type": "markdown",
   "id": "handler",
   "metadata": {},
   "source": "---\n## Create ModelHandler\n\nThe ModelHandler wraps the PyTorch model for use with Apache Beam's RunInference transform. This section explains the key design choices for working with TorchScript models in Dataflow."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handler_c",
   "metadata": {},
   "outputs": [],
   "source": "class PyTorchAutoencoderHandler(PytorchModelHandlerTensor):\n    \"\"\"\n    Custom ModelHandler for PyTorch autoencoder inference.\n    \n    Key Design Choices:\n    \n    1. TorchScript Model Loading:\n       - Uses torch_script_model_path (not state_dict_path)\n       - TorchScript models (.pt) have architecture embedded\n       - No need to provide model_class parameter\n    \n    2. Batch Processing:\n       - Input: List of tensors (one per instance)\n       - Stack into single batch tensor with torch.stack()\n       - Enables efficient GPU/CPU vectorized operations\n    \n    3. Output Format:\n       - Returns list of dicts (one per instance in batch)\n       - Each dict contains tensors for that specific instance\n       - PytorchModelHandlerTensor passes these dicts through directly\n       - Downstream format_for_bq() function converts tensors to Python types\n    \n    4. Model Output:\n       - Autoencoder returns dict: {\"denormalized_MAE\": tensor, \"encoded\": tensor, ...}\n       - Extract only needed fields (denormalized_MAE for anomaly score, encoded for embeddings)\n       - Keep as tensors in run_inference, convert to Python types later\n    \"\"\"\n    \n    def run_inference(self, batch, model, inference_args=None):\n        \"\"\"\n        Run inference on a batch of input tensors.\n        \n        Args:\n            batch: List of torch.Tensor, each shape (30,) for our autoencoder\n            model: Loaded TorchScript model\n            inference_args: Optional additional arguments (unused)\n            \n        Returns:\n            List of dicts, one per input, containing model outputs as tensors\n        \"\"\"\n        # Stack list of tensors into single batch tensor\n        # Input: [tensor(30,), tensor(30,), ...] -> Output: tensor(batch_size, 30)\n        batch_tensor = torch.stack(batch)\n        \n        # Run model inference without gradient computation\n        with torch.no_grad():\n            predictions = model(batch_tensor)\n        \n        # Convert batch output to list of individual results\n        # This matches the batch size and allows proper per-instance processing\n        results = []\n        for i in range(len(batch)):\n            results.append({\n                \"denormalized_MAE\": predictions[\"denormalized_MAE\"][i],  # Scalar tensor\n                \"encoded\": predictions[\"encoded\"][i]  # 1D tensor (embedding)\n            })\n        return results\n\n\n# Initialize ModelHandler with TorchScript model from GCS\nmodel_handler = PyTorchAutoencoderHandler(\n    torch_script_model_path=MODEL_PATH,  # Path to .pt file in GCS\n    device=\"cpu\"  # Run on CPU in Dataflow workers\n)\n\nprint(\"‚úÖ ModelHandler created\")\nprint(f\"   Model: {MODEL_PATH}\")\nprint(f\"   Device: cpu\")"
  },
  {
   "cell_type": "markdown",
   "id": "pipeline",
   "metadata": {},
   "source": "---\n## Build Dataflow Pipeline\n\nConfigure the Dataflow pipeline components:\n- **format_for_bq**: Extract predictions and convert tensors to Python types\n- **Pipeline options**: Configure Dataflow job parameters including worker requirements"
  },
  {
   "cell_type": "code",
   "id": "x41oxd3d1zj",
   "source": "# Worker Compute Configuration\n# =============================\n# These settings control the machine type and autoscaling behavior for Dataflow workers.\n# Proper configuration ensures efficient resource utilization and cost management.\n\n# Machine Type: n1-standard-4\n# - 4 vCPUs, 15 GB memory\n# - Suitable for PyTorch inference workloads that need moderate CPU and memory\n# - Each worker can handle multiple inference requests concurrently\n# - Alternative: n1-standard-2 (smaller), n1-standard-8 (larger), or custom machine types\n\nMACHINE_TYPE = \"n1-standard-4\"\n\n# Autoscaling for Batch Pipelines\n# - min_workers=2: Provides baseline parallelism for faster job completion\n# - max_workers=10: Balances throughput with cost for bounded datasets\n# - Dataflow autoscales based on data volume and processing rate\n\nMIN_WORKERS = 2\nMAX_WORKERS = 10\n\n# Why These Settings for Batch?\n# ------------------------------\n# 1. **Minimum Workers (2)**:\n#    - Establishes baseline parallelism for efficient data processing\n#    - Reduces overall job runtime by distributing work\n#    - Lower than streaming since batch jobs are temporary\n#\n# 2. **Maximum Workers (10)**:\n#    - Allows scaling for large datasets while controlling costs\n#    - Each n1-standard-4 worker processes ~200-1000 records/sec\n#    - 10 workers can process ~2,000-10,000 records/sec\n#    - Conservative limit since batch jobs complete quickly\n#\n# 3. **Machine Type (n1-standard-4)**:\n#    - PyTorch model loading requires ~2-4 GB memory per worker\n#    - 15 GB allows model + batch processing overhead\n#    - 4 vCPUs enable parallel batch inference\n#    - Cost-effective for moderate-sized batch jobs\n\n# When to Adjust These Settings:\n# -------------------------------\n# - **Large Datasets**: Increase max_workers (e.g., 50-100) for millions of records\n# - **Faster Completion**: Increase min_workers (e.g., 5-10) to reduce job runtime\n# - **Cost Optimization**: Use smaller machine type (n1-standard-2) if memory permits\n# - **Larger Models**: Use n1-standard-8 or n1-highmem-4 for memory-intensive models\n# - **CPU-Intensive Models**: Use c2-standard-4 for compute-optimized instances\n\n# GPU Support (Optional)\n# ----------------------\n# Dataflow supports GPU workers for accelerated inference:\n# - Machine type: n1-standard-4 (host machine)\n# - GPU type: nvidia-tesla-t4, nvidia-tesla-v100, etc.\n# - GPU count: 1-4 per worker\n# - Requirements:\n#   1. Add --worker_machine_type=n1-standard-4\n#   2. Add --worker_gpu_type=nvidia-tesla-t4\n#   3. Add --worker_gpu_count=1\n#   4. Update model handler to use device=\"cuda\"\n#   5. Ensure PyTorch is GPU-enabled in requirements file\n#\n# Note: GPU workers are significantly more expensive than CPU workers.\n# Only use for models where GPU acceleration provides meaningful speedup.\n# For small models like this autoencoder, CPU inference is more cost-effective.\n# For batch workloads, consider if faster per-worker inference justifies higher cost.\n\nprint(\"=\" * 60)\nprint(\"WORKER COMPUTE CONFIGURATION\")\nprint(\"=\" * 60)\nprint(f\"Machine Type: {MACHINE_TYPE}\")\nprint(f\"  - vCPUs: 4\")\nprint(f\"  - Memory: 15 GB\")\nprint(f\"\\nAutoscaling:\")\nprint(f\"  - Min Workers: {MIN_WORKERS}\")\nprint(f\"  - Max Workers: {MAX_WORKERS}\")\nprint(f\"  - Estimated capacity: ~{MIN_WORKERS * 200}-{MAX_WORKERS * 1000} records/sec\")\nprint(f\"\\nGPU Support: Not configured (CPU inference)\")\nprint(f\"  - To enable GPU: Add --worker_gpu_type and --worker_gpu_count\")\nprint(\"=\" * 60)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pipeline_c",
   "metadata": {},
   "outputs": [],
   "source": "def format_for_bq(element):\n    \"\"\"\n    Format model predictions for BigQuery output.\n    \n    Args:\n        element: Dict returned from run_inference containing tensor outputs\n        \n    Returns:\n        Dict with Python types suitable for BigQuery insertion\n        \n    Note:\n        With our custom ModelHandler, element is the dict we returned from run_inference.\n        PytorchModelHandlerTensor passes these dicts through directly without wrapping.\n        We convert tensor values to Python types here for serialization.\n    \"\"\"\n    return {\n        \"instance_id\": str(hash(str(element[\"denormalized_MAE\"].item()))),\n        \"anomaly_score\": float(element[\"denormalized_MAE\"].item()),\n        \"encoded\": element[\"encoded\"].tolist(),\n        \"timestamp\": datetime.utcnow().isoformat()\n    }\n\n\n# Create requirements file for Dataflow workers\n# Workers need PyTorch installed to load and run the model\nimport tempfile\nrequirements_content = \"torch>=2.0.0\\n\"\n\nwith tempfile.NamedTemporaryFile(mode='w', suffix='.txt', delete=False) as f:\n    f.write(requirements_content)\n    requirements_file = f.name\n\n# Configure Dataflow pipeline options\noptions = PipelineOptions([\n    f\"--project={PROJECT_ID}\",\n    f\"--region={REGION}\",\n    \"--runner=DataflowRunner\",  # Run on Google Cloud (not locally)\n    f\"--temp_location={DATAFLOW_TEMP}\",\n    f\"--staging_location={DATAFLOW_STAGING}\",\n    f\"--requirements_file={requirements_file}\",  # Install PyTorch in workers\n    f\"--job_name=pytorch-batch-{datetime.now().strftime('%Y%m%d-%H%M%S')}\",\n    \"--save_main_session\",  # Serialize global imports and variables\n    # Worker compute configuration\n    f\"--machine_type={MACHINE_TYPE}\",  # Machine type for workers\n    f\"--num_workers={MIN_WORKERS}\",  # Initial/minimum number of workers\n    f\"--max_num_workers={MAX_WORKERS}\",  # Maximum workers for autoscaling\n])\n\nprint(\"‚úÖ Pipeline options configured\")\nprint(f\"   Job will run in: {REGION}\")\nprint(f\"   Staging: {DATAFLOW_STAGING}\")\nprint(f\"   Requirements: {requirements_file}\")\nprint(f\"   Machine type: {MACHINE_TYPE}\")\nprint(f\"   Worker scaling: {MIN_WORKERS}-{MAX_WORKERS} workers\")"
  },
  {
   "cell_type": "markdown",
   "id": "run",
   "metadata": {},
   "source": "### Run Batch Job\n\nBuild and execute the pipeline graph. The job will:\n1. Read 1000 TEST transactions from BigQuery\n2. Convert to PyTorch tensors\n3. Run model inference via RunInference\n4. Write results to BigQuery\n\nThe pipeline completes automatically when all data is processed."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run_c",
   "metadata": {},
   "outputs": [],
   "source": "# Build and run pipeline\nwith beam.Pipeline(options=options) as p:\n    results = (\n        p\n        | \"Read from BigQuery\" >> ReadFromBigQuery(\n            query=f\"\"\"\n            SELECT * EXCEPT(splits, transaction_id, Class)\n            FROM `{PROJECT_ID}.{BQ_DATASET}.{SERIES}`\n            WHERE splits = \"TEST\"\n            LIMIT 1000\n            \"\"\",\n            use_standard_sql=True\n        )\n        | \"Convert to tensors\" >> beam.Map(lambda row: torch.tensor(list(row.values()), dtype=torch.float32))\n        | \"RunInference\" >> RunInference(model_handler)\n        | \"Format for BigQuery\" >> beam.Map(format_for_bq)\n        | \"Write to BigQuery\" >> WriteToBigQuery(\n            table=f\"{PROJECT_ID}:{BQ_DATASET}.{BQ_TABLE_RESULTS}\",\n            write_disposition=beam.io.BigQueryDisposition.WRITE_APPEND\n        )\n    )\n\nprint(\"\\n‚úÖ Dataflow job submitted!\")\nprint(f\"Monitor at: https://console.cloud.google.com/dataflow/jobs/{REGION}?project={PROJECT_ID}\")\nprint(\"\\n\" + \"=\" * 60)\nprint(\"‚è≥ TIMING EXPECTATIONS\")\nprint(\"=\" * 60)\nprint(\"The pipeline will take approximately 5-10 minutes:\")\nprint(\"  1. Worker provisioning: 2-3 minutes\")\nprint(\"  2. Processing 1000 records: 2-5 minutes\")\nprint(\"  3. Writing to BigQuery: < 1 minute\")\nprint(\"\\nJob will complete automatically when all data is processed.\")\nprint(\"Check the Dataflow console to monitor progress.\")\nprint(\"=\" * 60)"
  },
  {
   "cell_type": "markdown",
   "id": "results",
   "metadata": {},
   "source": [
    "---\n",
    "## Analyze Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "results_c",
   "metadata": {},
   "outputs": [],
   "source": "from google.cloud import bigquery\nimport pandas as pd\n\nbq = bigquery.Client(project=PROJECT_ID)\n\nprint(\"=\" * 60)\nprint(\"BIGQUERY RESULTS (Batch Inference)\")\nprint(\"=\" * 60)\n\n# Get recent results\nquery = f\"\"\"\nSELECT *\nFROM `{PROJECT_ID}.{BQ_DATASET}.{BQ_TABLE_RESULTS}`\nORDER BY timestamp DESC\nLIMIT 10\n\"\"\"\ndf = bq.query(query).to_dataframe()\n\nif len(df) > 0:\n    print(f\"‚úÖ Found {len(df)} recent results\")\n    display(df)\nelse:\n    print(\"‚ö†Ô∏è  No results yet in BigQuery\")\n    print(\"   Wait for Dataflow job to complete\")\n\n# Get total count\nprint(\"\\n\" + \"=\" * 60)\nprint(\"üí° Pipeline Status Summary\")\nprint(\"=\" * 60)\n\ncount_query = f\"SELECT COUNT(*) as total FROM `{PROJECT_ID}.{BQ_DATASET}.{BQ_TABLE_RESULTS}`\"\ncount_result = bq.query(count_query).to_dataframe()\ntotal_results = count_result['total'].iloc[0] if len(count_result) > 0 else 0\n\nif total_results > 0:\n    print(f\"‚úÖ Pipeline completed successfully!\")\n    print(f\"   Total results in BigQuery: {total_results}\")\n    print(f\"   Results table: {PROJECT_ID}.{BQ_DATASET}.{BQ_TABLE_RESULTS}\")\nelse:\n    print(\"‚ö†Ô∏è  No results yet in BigQuery\")\n    print(\"   Check Dataflow job status for errors\")\n\nprint(\"=\" * 60)"
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": "---\n## Summary\n\nIn this notebook, you successfully:\n\n‚úÖ **Created Custom PyTorch ModelHandler**\n- Wrapped TorchScript model for RunInference\n- Implemented efficient batch processing with torch.stack()\n- Designed output format compatible with downstream processing\n\n‚úÖ **Built Batch Dataflow Pipeline**\n- Read historical data from BigQuery\n- Applied PyTorch model inference at scale\n- Automatic dependency installation (PyTorch) in workers\n\n‚úÖ **Deployed PyTorch Model In-Process**\n- Model loaded directly in Dataflow workers\n- No network calls to external endpoints\n- High-throughput inference for bounded datasets\n\n‚úÖ **Wrote Results to BigQuery**\n- Structured output for SQL analytics\n- Persistent storage for auditing and analysis\n\n## Key Learnings\n\n**ModelHandler Design:**\n- TorchScript models use `torch_script_model_path` (not `state_dict_path`)\n- Batch processing requires `torch.stack()` to combine tensors\n- Return list of dicts (one per instance) from `run_inference()`\n- Keep outputs as tensors until format stage for efficiency\n\n**Batch Processing Benefits:**\n- Process all data at once (vs. streaming micro-batches)\n- Higher throughput for large datasets\n- Simpler pipeline logic (no windowing needed)\n- Job completes when data is exhausted\n\n**Cost Considerations:**\n- In-process inference avoids endpoint API costs\n- Pay only while job runs (no always-on infrastructure)\n- Workers autoscale based on data volume\n- More cost-effective than streaming for historical analysis\n\n## Next Steps\n\n### Continue with Dataflow Workflows:\n\n**Streaming Inference with Local Model:**\n- [dataflow-streaming-runinference.ipynb](./dataflow-streaming-runinference.ipynb)\n  - Process real-time data from Pub/Sub\n  - Same ModelHandler pattern with windowing\n  - Low-latency continuous inference\n\n**Batch Inference with Vertex Endpoint:**\n- [dataflow-batch-runinference-vertex.ipynb](./dataflow-batch-runinference-vertex.ipynb)\n  - Call deployed Vertex AI Endpoint\n  - Compare performance vs local RunInference\n  - Centralized model updates without redeploying pipeline\n\n**Streaming Inference with Vertex Endpoint:**\n- [dataflow-streaming-runinference-vertex.ipynb](./dataflow-streaming-runinference-vertex.ipynb)\n  - Real-time endpoint calls from Dataflow\n  - Shared model across multiple services\n  - Managed infrastructure with auto-scaling\n\n### Production Enhancements:\n\n1. **Error Handling**: Add try-catch blocks with dead letter queues for failed records\n2. **Monitoring**: Set up Cloud Monitoring dashboards for job metrics\n3. **Autoscaling**: Configure worker pool sizing based on data volume\n4. **Model Versioning**: Parameterize MODEL_PATH for A/B testing\n5. **Data Quality**: Add input validation before inference\n\n### Resources\n\n- [Dataflow Batch Pipelines](https://cloud.google.com/dataflow/docs/guides/batch-pipeline)\n- [Apache Beam RunInference](https://beam.apache.org/documentation/ml/about-ml/)\n- [PyTorch RunInference](https://beam.apache.org/documentation/ml/pytorch-inference/)\n- [BigQuery Best Practices](https://cloud.google.com/bigquery/docs/best-practices-performance-overview)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}