{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e3de771",
   "metadata": {},
   "source": [
    "![tracker](https://us-central1-vertex-ai-mlops-369716.cloudfunctions.net/pixel-tracking?path=statmike%2Fvertex-ai-mlops%2FFramework+Workflows%2FPyTorch%2Fserving&file=dataflow-batch-runinference.ipynb)\n",
    "<!--- header table --->\n",
    "<table align=\"left\">\n",
    "<tr>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%20Workflows/PyTorch/serving/dataflow-batch-runinference.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.svgrepo.com/download/217753/github.svg\" alt=\"GitHub logo\">\n",
    "      <br>View on<br>GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://colab.research.google.com/github/statmike/vertex-ai-mlops/blob/main/Framework%20Workflows/PyTorch/serving/dataflow-batch-runinference.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.gstatic.com/pantheon/images/bigquery/welcome_page/colab-logo.svg\" alt=\"Google Colaboratory logo\">\n",
    "      <br>Run in<br>Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https%3A%2F%2Fraw.githubusercontent.com%2Fstatmike%2Fvertex-ai-mlops%2Fmain%2FFramework%2520Workflows%2FPyTorch%2Fserving%2Fdataflow-batch-runinference.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\">\n",
    "      <br>Run in<br>Colab Enterprise\n",
    "    </a>\n",
    "  </td>      \n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/bigquery/import?url=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%20Workflows/PyTorch/serving/dataflow-batch-runinference.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.gstatic.com/images/branding/gcpiconscolors/bigquery/v1/32px.svg\" alt=\"BigQuery logo\">\n",
    "      <br>Open in<br>BigQuery Studio\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/statmike/vertex-ai-mlops/main/Framework%20Workflows/PyTorch/serving/dataflow-batch-runinference.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.gstatic.com/images/branding/gcpiconscolors/vertexai/v1/32px.svg\" alt=\"Vertex AI logo\">\n",
    "      <br>Open in<br>Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td colspan=\"5\" style=\"text-align: right\">\n",
    "    <b>Share This On: </b> \n",
    "    <a href=\"https://www.linkedin.com/sharing/share-offsite/?url=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%2520Workflows/PyTorch/serving/dataflow-batch-runinference.ipynb\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"Linkedin Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://reddit.com/submit?url=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%2520Workflows/PyTorch/serving/dataflow-batch-runinference.ipynb\"><img src=\"https://redditinc.com/hubfs/Reddit%20Inc/Brand/Reddit_Logo.png\" alt=\"Reddit Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://bsky.app/intent/compose?text=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%2520Workflows/PyTorch/serving/dataflow-batch-runinference.ipynb\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"BlueSky Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://twitter.com/intent/tweet?url=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%2520Workflows/PyTorch/serving/dataflow-batch-runinference.ipynb\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/5/5a/X_icon_2.svg\" alt=\"X (Twitter) Logo\" width=\"20px\"></a> \n",
    "  </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td colspan=\"5\" style=\"text-align: right\">\n",
    "    <b>Connect With Author On: </b> \n",
    "    <a href=\"https://www.linkedin.com/in/statmike\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"Linkedin Logo\" width=\"20px\"></a>\n",
    "    <a href=\"https://www.github.com/statmike\"><img src=\"https://www.svgrepo.com/download/217753/github.svg\" alt=\"GitHub Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://www.youtube.com/@statmike-channel\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/f/fd/YouTube_full-color_icon_%282024%29.svg\" alt=\"YouTube Logo\" width=\"20px\"></a>\n",
    "    <a href=\"https://bsky.app/profile/statmike.bsky.social\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"BlueSky Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://x.com/statmike\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/5/5a/X_icon_2.svg\" alt=\"X (Twitter) Logo\" width=\"20px\"></a>\n",
    "  </td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overview",
   "metadata": {},
   "source": "# Dataflow Batch Inference with RunInference\n\nThis notebook demonstrates batch processing of transactions using Dataflow with Apache Beam RunInference.\n\n## What You'll Learn\n\nThis workflow covers:\n\n1. **Create Custom ModelHandler**: Wrap PyTorch model for RunInference\n2. **Build Batch Pipeline**: Read from BigQuery, apply model, write results\n3. **Run on Dataflow**: Execute pipeline on Google Cloud\n4. **Monitor Job**: Track progress in Cloud Console\n5. **Analyze Results**: Query and visualize anomaly scores\n\n## Prerequisites\n\n- Completed `dataflow-setup.ipynb` - This sets up:\n  - Model .pt file extracted from .mar and uploaded to GCS\n  - BigQuery tables created\n  - Pub/Sub topics and subscriptions created\n\n## Batch vs Streaming\n\n**Batch Processing (This Notebook)**:\n- ✅ Process historical data\n- ✅ Bounded dataset (has a start and end)\n- ✅ Results available when job completes\n- ✅ Cost-effective for large datasets\n- Example: Analyze all transactions from last month\n\n**Streaming Processing (Next Notebook)**:\n- ✅ Process real-time data\n- ✅ Unbounded dataset (continuous)\n- ✅ Results available immediately\n- ✅ Low-latency anomaly detection\n- Example: Flag suspicious transactions as they occur\n\n## Architecture\n\n```\nBigQuery Source Table\n  ↓ Read test transactions\nDataflow Pipeline\n  ↓ Format instances\nRunInference (PyTorch Model)\n  ↓ Generate anomaly scores\nTransform Results\n  ↓ Extract scores and embeddings\nBigQuery Results Table\n```\n\n## RunInference Benefits\n\n- **In-process**: Model loaded directly in workers (no network calls)\n- **Automatic batching**: Combines instances for efficient inference\n- **Scalable**: Scales with pipeline workers\n- **Cost-effective**: Pay only when job runs\n\n## What This Pipeline Does\n\n1. Read TEST transactions from BigQuery\n2. Format data for PyTorch model\n3. Load TorchScript model in workers\n4. Run inference to get anomaly scores\n5. Extract relevant outputs (score + embeddings)\n6. Write results to BigQuery\n7. Job completes when all data processed"
  },
  {
   "cell_type": "markdown",
   "id": "env",
   "metadata": {},
   "source": [
    "---\n",
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proj",
   "metadata": {},
   "source": [
    "### Set Your Project ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proj_c",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = 'statmike-mlops-349915'\n",
    "REQ_TYPE = 'ALL'\n",
    "INSTALL_TOOL = 'poetry'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conf",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conf_c",
   "metadata": {},
   "outputs": [],
   "source": [
    "REQUIREMENTS_URL = 'https://raw.githubusercontent.com/statmike/vertex-ai-mlops/refs/heads/main/Framework%20Workflows/PyTorch/requirements.txt'\n",
    "REQUIRED_APIS = [\"dataflow.googleapis.com\", \"bigquery.googleapis.com\", \"storage.googleapis.com\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup",
   "metadata": {},
   "source": [
    "### Run Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup_c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, urllib.request\n",
    "url = 'https://raw.githubusercontent.com/statmike/vertex-ai-mlops/refs/heads/main/core/notebook-template/python_setup.py'\n",
    "urllib.request.urlretrieve(url, 'python_setup_local.py')\n",
    "import python_setup_local as python_setup\n",
    "os.remove('python_setup_local.py')\n",
    "setup_info = python_setup.setup_environment(PROJECT_ID, REQ_TYPE, REQUIREMENTS_URL, REQUIRED_APIS, INSTALL_TOOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "py",
   "metadata": {},
   "source": [
    "---\n",
    "## Python Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imp",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imp_c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import apache_beam as beam\n",
    "from apache_beam.ml.inference.base import RunInference\n",
    "from apache_beam.ml.inference.pytorch_inference import PytorchModelHandlerTensor\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "from apache_beam.io.gcp.bigquery import ReadFromBigQuery, WriteToBigQuery\n",
    "import torch\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vars",
   "metadata": {},
   "source": [
    "### Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vars_c",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = subprocess.run([\"gcloud\", \"config\", \"get-value\", \"project\"], capture_output=True, text=True, check=True).stdout.strip()\n",
    "\n",
    "# Configuration\n",
    "REGION = \"us-central1\"\n",
    "SERIES = \"frameworks\"\n",
    "EXPERIMENT = \"pytorch-autoencoder\"\n",
    "\n",
    "# GCS paths\n",
    "BUCKET_NAME = PROJECT_ID\n",
    "BUCKET_URI = f\"gs://{BUCKET_NAME}\"\n",
    "MODEL_DIR = f\"{BUCKET_URI}/{SERIES}/{EXPERIMENT}\"\n",
    "MODEL_PATH = f\"{MODEL_DIR}/final_model_traced.pt\"\n",
    "\n",
    "# BigQuery configuration\n",
    "BQ_DATASET = SERIES.replace(\"-\", \"_\")\n",
    "BQ_TABLE_RESULTS = f\"{EXPERIMENT.replace('-', '_')}_batch_results\"\n",
    "\n",
    "# Dataflow configuration\n",
    "DATAFLOW_STAGING = f\"{BUCKET_URI}/dataflow/staging\"\n",
    "DATAFLOW_TEMP = f\"{BUCKET_URI}/dataflow/temp\"\n",
    "\n",
    "print(f\"Project: {PROJECT_ID}\")\n",
    "print(f\"Model: {MODEL_PATH}\")\n",
    "print(f\"Results table: {PROJECT_ID}.{BQ_DATASET}.{BQ_TABLE_RESULTS}\")\n",
    "print(f\"Dataflow staging: {DATAFLOW_STAGING}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handler",
   "metadata": {},
   "source": [
    "---\n",
    "## Create Custom ModelHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handler_c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PyTorchAutoencoderHandler(PytorchModelHandlerTensor):\n",
    "    \"\"\"Custom handler that extracts specific outputs from model\"\"\"\n",
    "    \n",
    "    def run_inference(self, batch, model, inference_args=None):\n",
    "        # Get full model output\n",
    "        with torch.no_grad():\n",
    "            predictions = model(batch)\n",
    "        \n",
    "        # Extract just anomaly scores and embeddings\n",
    "        results = []\n",
    "        for i in range(len(batch)):\n",
    "            results.append({\n",
    "                \"anomaly_score\": float(predictions[\"denormalized_MAE\"][i].item()),\n",
    "                \"encoded\": predictions[\"encoded\"][i].tolist()\n",
    "            })\n",
    "        return results\n",
    "\n",
    "# Create handler\n",
    "model_handler = PyTorchAutoencoderHandler(\n",
    "    state_dict_path=MODEL_PATH,\n",
    "    model_class=None,  # TorchScript model\n",
    "    device=\"cpu\"\n",
    ")\n",
    "print(\"✅ ModelHandler created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pipeline",
   "metadata": {},
   "source": [
    "---\n",
    "## Build Dataflow Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pipeline_c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_for_bq(element):\n",
    "    \"\"\"Format results for BigQuery\"\"\"\n",
    "    prediction = element[1]  # RunInference returns (input, output)\n",
    "    return {\n",
    "        \"instance_id\": str(hash(str(element[0]))),\n",
    "        \"anomaly_score\": prediction[\"anomaly_score\"],\n",
    "        \"encoded\": prediction[\"encoded\"],\n",
    "        \"timestamp\": datetime.utcnow().isoformat()\n",
    "    }\n",
    "\n",
    "# Pipeline options\n",
    "options = PipelineOptions([\n",
    "    f\"--project={PROJECT_ID}\",\n",
    "    f\"--region={REGION}\",\n",
    "    \"--runner=DataflowRunner\",\n",
    "    f\"--temp_location={DATAFLOW_TEMP}\",\n",
    "    f\"--staging_location={DATAFLOW_STAGING}\",\n",
    "    f\"--job_name=pytorch-batch-{datetime.now().strftime('%Y%m%d-%H%M%S')}\",\n",
    "    \"--save_main_session=True\"\n",
    "])\n",
    "\n",
    "print(\"✅ Pipeline options configured\")\n",
    "print(f\"   Job will run in: {REGION}\")\n",
    "print(f\"   Staging: {DATAFLOW_STAGING}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "run",
   "metadata": {},
   "source": [
    "### Run Batch Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run_c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and run pipeline\n",
    "with beam.Pipeline(options=options) as p:\n",
    "    results = (\n",
    "        p\n",
    "        | \"Read from BigQuery\" >> ReadFromBigQuery(\n",
    "            query=f\"\"\"\n",
    "            SELECT * EXCEPT(splits, transaction_id, Class)\n",
    "            FROM `{PROJECT_ID}.{BQ_DATASET}.{SERIES}`\n",
    "            WHERE splits = \"TEST\"\n",
    "            LIMIT 1000\n",
    "            \"\"\",\n",
    "            use_standard_sql=True\n",
    "        )\n",
    "        | \"Convert to tensors\" >> beam.Map(lambda row: torch.tensor(list(row.values()), dtype=torch.float32))\n",
    "        | \"RunInference\" >> RunInference(model_handler)\n",
    "        | \"Format for BigQuery\" >> beam.Map(format_for_bq)\n",
    "        | \"Write to BigQuery\" >> WriteToBigQuery(\n",
    "            table=f\"{PROJECT_ID}:{BQ_DATASET}.{BQ_TABLE_RESULTS}\",\n",
    "            write_disposition=beam.io.BigQueryDisposition.WRITE_APPEND\n",
    "        )\n",
    "    )\n",
    "\n",
    "print(\"\\n✅ Dataflow job submitted!\")\n",
    "print(f\"Monitor at: https://console.cloud.google.com/dataflow/jobs/{REGION}?project={PROJECT_ID}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "results",
   "metadata": {},
   "source": [
    "---\n",
    "## Analyze Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "results_c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "import pandas as pd\n",
    "\n",
    "bq = bigquery.Client(project=PROJECT_ID)\n",
    "query = f\"\"\"\n",
    "SELECT *\n",
    "FROM `{PROJECT_ID}.{BQ_DATASET}.{BQ_TABLE_RESULTS}`\n",
    "ORDER BY timestamp DESC\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "df = bq.query(query).to_dataframe()\n",
    "print(f\"Retrieved {len(df)} results\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "✅ Created custom PyTorch ModelHandler\n",
    "\n",
    "✅ Built batch Dataflow pipeline\n",
    "\n",
    "✅ Processed transactions with RunInference\n",
    "\n",
    "✅ Wrote anomaly scores to BigQuery\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- [Streaming Inference](./dataflow-streaming-runinference.ipynb) - Real-time processing\n",
    "- [Vertex Endpoint](./dataflow-vertex-endpoint.ipynb) - Call endpoint from Dataflow"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}