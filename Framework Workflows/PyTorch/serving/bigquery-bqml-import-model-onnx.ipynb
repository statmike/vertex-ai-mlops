{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "![tracker](https://us-central1-vertex-ai-mlops-369716.cloudfunctions.net/pixel-tracking?path=statmike%2Fvertex-ai-mlops%2FFramework+Workflows%2FPyTorch%2Fserving&file=bigquery-bqml-import-model-onnx.ipynb)\n",
    "<!--- header table --->\n",
    "<table align=\"left\">\n",
    "<tr>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%20Workflows/PyTorch/serving/bigquery-bqml-import-model-onnx.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.svgrepo.com/download/217753/github.svg\" alt=\"GitHub logo\">\n",
    "      <br>View on<br>GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://colab.research.google.com/github/statmike/vertex-ai-mlops/blob/main/Framework%20Workflows/PyTorch/serving/bigquery-bqml-import-model-onnx.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.gstatic.com/pantheon/images/bigquery/welcome_page/colab-logo.svg\" alt=\"Google Colaboratory logo\">\n",
    "      <br>Run in<br>Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https%3A%2F%2Fraw.githubusercontent.com%2Fstatmike%2Fvertex-ai-mlops%2Fmain%2FFramework%2520Workflows%2FPyTorch%2Fserving%2Fbigquery-bqml-import-model-onnx.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\">\n",
    "      <br>Run in<br>Colab Enterprise\n",
    "    </a>\n",
    "  </td>      \n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/bigquery/import?url=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%20Workflows/PyTorch/serving/bigquery-bqml-import-model-onnx.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.gstatic.com/images/branding/gcpiconscolors/bigquery/v1/32px.svg\" alt=\"BigQuery logo\">\n",
    "      <br>Open in<br>BigQuery Studio\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/statmike/vertex-ai-mlops/main/Framework%20Workflows/PyTorch/serving/bigquery-bqml-import-model-onnx.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.gstatic.com/images/branding/gcpiconscolors/vertexai/v1/32px.svg\" alt=\"Vertex AI logo\">\n",
    "      <br>Open in<br>Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td colspan=\"5\" style=\"text-align: right\">\n",
    "    <b>Share This On: </b> \n",
    "    <a href=\"https://www.linkedin.com/sharing/share-offsite/?url=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%2520Workflows/PyTorch/serving/bigquery-bqml-import-model-onnx.ipynb\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"Linkedin Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://reddit.com/submit?url=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%2520Workflows/PyTorch/serving/bigquery-bqml-import-model-onnx.ipynb\"><img src=\"https://redditinc.com/hubfs/Reddit%20Inc/Brand/Reddit_Logo.png\" alt=\"Reddit Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://bsky.app/intent/compose?text=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%2520Workflows/PyTorch/serving/bigquery-bqml-import-model-onnx.ipynb\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"BlueSky Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://twitter.com/intent/tweet?url=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%2520Workflows/PyTorch/serving/bigquery-bqml-import-model-onnx.ipynb\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/5/5a/X_icon_2.svg\" alt=\"X (Twitter) Logo\" width=\"20px\"></a> \n",
    "  </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td colspan=\"5\" style=\"text-align: right\">\n",
    "    <b>Connect With Author On: </b> \n",
    "    <a href=\"https://www.linkedin.com/in/statmike\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"Linkedin Logo\" width=\"20px\"></a>\n",
    "    <a href=\"https://www.github.com/statmike\"><img src=\"https://www.svgrepo.com/download/217753/github.svg\" alt=\"GitHub Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://www.youtube.com/@statmike-channel\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/f/fd/YouTube_full-color_icon_%282024%29.svg\" alt=\"YouTube Logo\" width=\"20px\"></a>\n",
    "    <a href=\"https://bsky.app/profile/statmike.bsky.social\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"BlueSky Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://x.com/statmike\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/5/5a/X_icon_2.svg\" alt=\"X (Twitter) Logo\" width=\"20px\"></a>\n",
    "  </td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overview",
   "metadata": {},
   "source": [
    "# Import PyTorch Model to BigQuery ML via ONNX\n",
    "\n",
    "This notebook demonstrates how to convert a PyTorch model to ONNX format and import it directly into BigQuery ML for SQL-based inference. Unlike remote models that call external endpoints, imported ONNX models run natively within BigQuery, providing lower latency and eliminating endpoint management overhead.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "This workflow covers:\n",
    "\n",
    "1. **Extract Model from MAR Archive**: Unzip TorchServe .mar file to access PyTorch model\n",
    "2. **Convert to ONNX Format**: Export PyTorch model to ONNX for BigQuery compatibility\n",
    "3. **Validate Conversion**: Compare ONNX predictions against original PyTorch model\n",
    "4. **Upload to Cloud Storage**: Move ONNX model to GCS for BigQuery access\n",
    "5. **Import to BigQuery ML**: Create BigQuery ML model from ONNX file\n",
    "6. **SQL-Based Inference**: Use ML.PREDICT() for predictions entirely in SQL\n",
    "7. **Batch Scoring**: Create enriched tables with predictions and business logic\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "This notebook requires the PyTorch autoencoder model created in:\n",
    "- **[pytorch-autoencoder.ipynb](../pytorch-autoencoder.ipynb)** - Creates the .mar file we'll convert\n",
    "\n",
    "The .mar file should be located at: `../files/pytorch-autoencoder/pytorch_autoencoder.mar`\n",
    "\n",
    "## Core Concepts\n",
    "\n",
    "### ONNX (Open Neural Network Exchange)\n",
    "\n",
    "ONNX is an open format for representing machine learning models, enabling interoperability across frameworks:\n",
    "\n",
    "- **Framework agnostic**: Export from PyTorch, import to BigQuery, TensorFlow, etc.\n",
    "- **Optimized inference**: ONNX Runtime provides efficient execution\n",
    "- **Broad support**: Supported by BigQuery ML, Azure ML, AWS SageMaker, and more\n",
    "\n",
    "### BigQuery ML ONNX Import\n",
    "\n",
    "BigQuery ML can import ONNX models directly, running inference natively within BigQuery:\n",
    "\n",
    "**Benefits:**\n",
    "- ‚úÖ **No endpoint management**: No servers to provision or scale\n",
    "- ‚úÖ **Lower latency**: No network calls to external services\n",
    "- ‚úÖ **Cost-effective**: No Vertex AI endpoint charges (only BigQuery slot usage)\n",
    "- ‚úÖ **Simplified architecture**: Single system for data and predictions\n",
    "- ‚úÖ **SQL-native**: All inference logic in SQL queries\n",
    "\n",
    "**When to use:**\n",
    "- Models that fit within BigQuery's size limits (typically < 250 MB)\n",
    "- Batch inference workloads on BigQuery data\n",
    "- Simplified deployment without endpoint infrastructure\n",
    "- Cost-sensitive applications (avoid endpoint uptime charges)\n",
    "\n",
    "## ONNX Import vs. Remote Models: Key Differences\n",
    "\n",
    "This notebook demonstrates **ONNX import**, which differs from **remote models** ([bigquery-bqml-remote-model-vertex.ipynb](./bigquery-bqml-remote-model-vertex.ipynb)) in important ways:\n",
    "\n",
    "### 1. **No Endpoint Required**\n",
    "\n",
    "**ONNX Import:**\n",
    "- Model runs natively inside BigQuery\n",
    "- No Vertex AI endpoint to deploy or manage\n",
    "- No server provisioning or autoscaling configuration\n",
    "\n",
    "**Remote Model:**\n",
    "- Requires deployed Vertex AI endpoint\n",
    "- Must manage endpoint replicas, machine types, autoscaling\n",
    "- Endpoint runs 24/7 (or requires cold start delays)\n",
    "\n",
    "### 2. **Lower Latency**\n",
    "\n",
    "**ONNX Import:**\n",
    "- In-process execution within BigQuery workers\n",
    "- No network calls to external services\n",
    "- Typically **10-100x faster** for small batch sizes\n",
    "\n",
    "**Remote Model:**\n",
    "- Network round-trip to Vertex AI endpoint\n",
    "- Latency from endpoint processing + network overhead\n",
    "- Better for very large batches (efficient batching at endpoint)\n",
    "\n",
    "### 3. **Cost Comparison**\n",
    "\n",
    "**ONNX Import:**\n",
    "- **BigQuery costs only**: Slot usage for query processing\n",
    "- No endpoint uptime charges\n",
    "- Pay only when running queries\n",
    "- **Example**: $0.04/GB scanned + slot time\n",
    "\n",
    "**Remote Model:**\n",
    "- **BigQuery costs**: Query processing + ML.PREDICT API calls\n",
    "- **Vertex AI costs**: Endpoint uptime (24/7) + prediction requests\n",
    "- **Example**: $0.04/GB scanned + ~$0.20/hour endpoint + $0.10/1000 predictions\n",
    "\n",
    "üí∞ **Cost savings**: ONNX import can be **5-10x cheaper** for periodic batch scoring\n",
    "\n",
    "### 4. **Model Size Limits**\n",
    "\n",
    "**ONNX Import:**\n",
    "- ‚ö†Ô∏è **Size limit**: ~250 MB ONNX file size\n",
    "- Suitable for small to medium models\n",
    "- Our autoencoder (~30 KB) fits easily\n",
    "\n",
    "**Remote Model:**\n",
    "- ‚úÖ **No practical limit**: Endpoint can host multi-GB models\n",
    "- Better for large transformers, CNNs, etc.\n",
    "\n",
    "### 5. **Feature Support**\n",
    "\n",
    "**ONNX Import:**\n",
    "- Supports standard ONNX operations\n",
    "- May not support all custom PyTorch operations\n",
    "- Requires ONNX-compatible model architecture\n",
    "\n",
    "**Remote Model:**\n",
    "- ‚úÖ Full PyTorch support (runs original .pt or .mar file)\n",
    "- Custom operations, preprocessing, postprocessing all work\n",
    "- More flexible for complex models\n",
    "\n",
    "### Comparison Table\n",
    "\n",
    "| Aspect | ONNX Import (This Notebook) | Remote Model (Vertex Endpoint) |\n",
    "|--------|---------------------------|-------------------------------|\n",
    "| **Deployment** | Upload ONNX to GCS | Deploy endpoint with .mar file |\n",
    "| **Infrastructure** | None (runs in BigQuery) | Vertex AI endpoint (managed servers) |\n",
    "| **Latency** | Very low (in-process) | Higher (network calls) |\n",
    "| **Cost** | BigQuery slots only | BigQuery + Vertex AI endpoint |\n",
    "| **Model Size** | < 250 MB | No practical limit |\n",
    "| **Scaling** | Automatic (BigQuery slots) | Configure min/max replicas |\n",
    "| **Best For** | Periodic batch scoring | Always-on predictions, large models |\n",
    "\n",
    "### When to Choose ONNX Import:\n",
    "\n",
    "‚úÖ Model is < 250 MB  \n",
    "‚úÖ Batch inference workloads (not real-time serving)  \n",
    "‚úÖ Cost-sensitive applications  \n",
    "‚úÖ Simplified deployment (no endpoint management)  \n",
    "‚úÖ Data already in BigQuery  \n",
    "\n",
    "### When to Choose Remote Model:\n",
    "\n",
    "‚úÖ Model is > 250 MB  \n",
    "‚úÖ Need real-time/online predictions  \n",
    "‚úÖ Complex custom preprocessing/postprocessing  \n",
    "‚úÖ Reuse existing Vertex AI endpoints  \n",
    "‚úÖ Very large batch sizes (endpoint batching more efficient)  \n",
    "\n",
    "## Architecture\n",
    "\n",
    "```\n",
    "PyTorch .mar file ‚Üí Extract .pt ‚Üí Convert to ONNX ‚Üí Upload to GCS ‚Üí Import to BigQuery ML ‚Üí SQL Predictions\n",
    "```\n",
    "\n",
    "**Key steps:**\n",
    "1. Extract TorchScript model from TorchServe .mar archive\n",
    "2. Convert PyTorch model to ONNX format\n",
    "3. Validate ONNX model produces same predictions\n",
    "4. Upload ONNX model to Cloud Storage\n",
    "5. Import ONNX as BigQuery ML model\n",
    "6. Use ML.PREDICT() for SQL-based inference\n",
    "\n",
    "## Model Output Customization\n",
    "\n",
    "This notebook uses the **full model output** (13 fields) matching the prebuilt container endpoint:\n",
    "- `denormalized_MAE`, `denormalized_RMSE`, `denormalized_MSE`, `denormalized_MSLE`\n",
    "- `normalized_MAE`, `normalized_RMSE`, `normalized_MSE`, `normalized_MSLE`\n",
    "- `encoded` (latent representation)\n",
    "- `normalized_reconstruction`, `normalized_reconstruction_errors`\n",
    "- `denormalized_reconstruction`, `denormalized_reconstruction_errors`\n",
    "\n",
    "**Why full output?**\n",
    "- Consistent with Vertex AI endpoint deployments\n",
    "- Provides flexibility for different use cases (anomaly detection, reconstruction analysis, etc.)\n",
    "- Allows post-hoc selection of metrics in SQL\n",
    "\n",
    "**How to customize:**\n",
    "You can modify the ONNX export to return only specific outputs (e.g., just `anomaly_score` and `encoded`) by:\n",
    "1. Changing the model's forward method to return fewer fields\n",
    "2. Updating the ONNX export `output_names` parameter\n",
    "3. Adjusting the BigQuery ML model OUTPUT schema\n",
    "\n",
    "Example customization is shown in markdown throughout the conversion section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "j4psmoocu6i",
   "metadata": {},
   "source": [
    "---\n",
    "## Environment Setup\n",
    "\n",
    "This section will authenticate your session, enable required Google Cloud APIs, and install necessary Python packages.\n",
    "\n",
    "**Package Installation Options (`REQ_TYPE`):**\n",
    "- `PRIMARY`: Installs only the main packages. Faster, but pip resolves sub-dependencies which may result in different versions than development.\n",
    "- `ALL` (Default): Installs exact versions of all packages and dependencies. Best for perfectly reproducing the development environment.\n",
    "- `COLAB`: Installs a Colab-optimized list that excludes pre-installed packages like `ipython` and `ipykernel`.\n",
    "\n",
    "**Installation Tool Options (`INSTALL_TOOL`):**\n",
    "- `pip` (Default): Uses pip for package installation. Standard Python package installer.\n",
    "- `uv`: Modern, fast Python package installer. Must be installed separately. See: https://github.com/astral-sh/uv\n",
    "- `poetry`: Dependency management tool. Requires running notebook in a poetry environment (`poetry shell` or `poetry run jupyter lab`). Uses `pyproject.toml` instead of requirements.txt.\n",
    "\n",
    "> **Note:** If running in Google Colab, the script will automatically detect this and set `REQ_TYPE = 'COLAB'` to prevent package conflicts, overriding any manual setting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "set_project",
   "metadata": {},
   "source": [
    "### Set Your Project ID\n",
    "\n",
    "‚ö†Ô∏è **Action Required:** Replace the `PROJECT_ID` value below with your Google Cloud project ID before running this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "project_config",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = 'statmike-mlops-349915' # replace with GCP project ID\n",
    "REQ_TYPE = 'ALL' # Specify PRIMARY or ALL or COLAB\n",
    "INSTALL_TOOL = 'poetry' # Specify pip, uv, or poetry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config_header",
   "metadata": {},
   "source": [
    "### Configuration\n",
    "\n",
    "This cell defines the requirements files and Google Cloud APIs needed for this notebook. Run as-is without modification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": [
    "REQUIREMENTS_URL = 'https://raw.githubusercontent.com/statmike/vertex-ai-mlops/refs/heads/main/Framework%20Workflows/PyTorch/requirements.txt'\n",
    "\n",
    "REQUIRED_APIS = [\n",
    "    \"bigquery.googleapis.com\",\n",
    "    \"storage.googleapis.com\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "run_setup_header",
   "metadata": {},
   "source": [
    "### Run Setup\n",
    "\n",
    "This cell downloads the centralized setup code and configures your environment. It will:\n",
    "- Authenticate your session with Google Cloud\n",
    "- Enable required APIs for this notebook\n",
    "- Install necessary Python packages\n",
    "- Display a setup summary with your project information\n",
    "\n",
    "> **Note:** In Colab, if packages are installed, the kernel will automatically restart. After restart, continue from the next cell without re-running earlier cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "run_setup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "PYTHON GCP ENVIRONMENT SETUP\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "AUTHENTICATION\n",
      "==================================================\n",
      "Checking for existing ADC...\n",
      "‚úÖ Existing ADC found.\n",
      "‚úÖ Project is correctly set to 'statmike-mlops-349915'.\n",
      "\n",
      "==================================================\n",
      "API CHECK & ENABLE\n",
      "==================================================\n",
      "‚úÖ bigquery.googleapis.com is already enabled.\n",
      "‚úÖ storage.googleapis.com is already enabled.\n",
      "\n",
      "==================================================\n",
      "PACKAGE MANAGEMENT\n",
      "==================================================\n",
      "Installation Tool: poetry\n",
      "‚úÖ Found poetry at: /usr/local/google/home/statmike/.local/bin/poetry\n",
      "‚úÖ Running in poetry environment: /usr/local/google/home/statmike/.cache/pypoetry/virtualenvs/frameworks-pytorch-0KVJlKeQ-py3.13\n",
      "‚ÑπÔ∏è  Poetry mode: Installing from pyproject.toml (REQUIREMENTS_URL ignored)\n",
      "‚úÖ Found pyproject.toml at: /usr/local/google/home/statmike/Git/vertex-ai-mlops/Framework Workflows/PyTorch/pyproject.toml\n",
      "   Changed working directory to: /usr/local/google/home/statmike/Git/vertex-ai-mlops/Framework Workflows/PyTorch\n",
      "Running poetry install...\n",
      "   Restored working directory to: /usr/local/google/home/statmike/Git/vertex-ai-mlops/Framework Workflows/PyTorch/serving\n",
      "‚úÖ All packages are already installed and up to date.\n",
      "\n",
      "==================================================\n",
      "Google Cloud Project Information\n",
      "==================================================\n",
      "PROJECT_ID     = statmike-mlops-349915\n",
      "PROJECT_NUMBER = 1026793852137\n",
      "==================================================\n",
      "\n",
      "\n",
      "==================================================\n",
      "SETUP SUMMARY\n",
      "==================================================\n",
      "‚úÖ Authentication:    Success\n",
      "‚úÖ API Configuration: Success\n",
      "‚úÖ Package Install:   Already up to date\n",
      "‚úÖ Installation Tool: poetry\n",
      "‚úÖ Project ID:        statmike-mlops-349915\n",
      "‚úÖ Project Number:    1026793852137\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os, urllib.request\n",
    "\n",
    "# Download and import setup code\n",
    "url = 'https://raw.githubusercontent.com/statmike/vertex-ai-mlops/refs/heads/main/core/notebook-template/python_setup.py'\n",
    "urllib.request.urlretrieve(url, 'python_setup_local.py')\n",
    "import python_setup_local as python_setup\n",
    "os.remove('python_setup_local.py')\n",
    "\n",
    "# Run setup\n",
    "setup_info = python_setup.setup_environment(PROJECT_ID, REQ_TYPE, REQUIREMENTS_URL, REQUIRED_APIS, INSTALL_TOOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "python_setup",
   "metadata": {},
   "source": [
    "---\n",
    "## Python Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imports_header",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "import zipfile\n",
    "import shutil\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import onnx\n",
    "import onnxruntime as ort\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import storage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vars_user",
   "metadata": {},
   "source": [
    "### Variables - User Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "vars_user_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION = 'us-central1'\n",
    "SERIES = 'frameworks'\n",
    "EXPERIMENT = 'pytorch-bqml-import-onnx'\n",
    "\n",
    "# Source MAR file from pytorch-autoencoder notebook\n",
    "SOURCE_MAR = '../files/pytorch-autoencoder/pytorch_autoencoder.mar'\n",
    "\n",
    "# BigQuery configuration\n",
    "BQ_REGION = 'US'  # BigQuery dataset region\n",
    "BQ_DATASET = SERIES.replace('-', '_')  # 'frameworks'\n",
    "BQ_SOURCE_TABLE = SERIES  # Source data table\n",
    "BQ_MODEL_NAME = f\"{SERIES.replace('-', '_')}_{EXPERIMENT.replace('-', '_')}\"\n",
    "BQ_PREDICTIONS_TABLE = f\"{EXPERIMENT.replace('-', '_')}_predictions\"  # Will be created"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vars_auto",
   "metadata": {},
   "source": [
    "### Variables - Auto Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "vars_auto_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Google Cloud Project Information\n",
      "==================================================\n",
      "PROJECT_ID     = statmike-mlops-349915\n",
      "PROJECT_NUMBER = 1026793852137\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "PROJECT_ID = subprocess.run(['gcloud', 'config', 'get-value', 'project'], capture_output=True, text=True, check=True).stdout.strip()\n",
    "PROJECT_NUMBER = subprocess.run(['gcloud', 'projects', 'describe', PROJECT_ID, '--format=value(projectNumber)'], capture_output=True, text=True, check=True).stdout.strip()\n",
    "\n",
    "print(f\"\\n{'='*50}\\nGoogle Cloud Project Information\\n{'='*50}\\nPROJECT_ID     = {PROJECT_ID}\\nPROJECT_NUMBER = {PROJECT_NUMBER}\\n{'='*50}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "configs",
   "metadata": {},
   "source": [
    "### Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "configs_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local files directory: ./files/pytorch-bqml-import-onnx\n",
      "Local ONNX path: ./files/pytorch-bqml-import-onnx/model.onnx\n",
      "\n",
      "GCS bucket: gs://statmike-mlops-349915\n",
      "GCS ONNX directory: gs://statmike-mlops-349915/frameworks/pytorch-autoencoder/pytorch-bqml-import-onnx\n",
      "GCS ONNX path: gs://statmike-mlops-349915/frameworks/pytorch-autoencoder/pytorch-bqml-import-onnx/model.onnx\n"
     ]
    }
   ],
   "source": [
    "# Local file paths\n",
    "LOCAL_FILES_DIR = f'./files/{EXPERIMENT}'\n",
    "LOCAL_ONNX_PATH = f\"{LOCAL_FILES_DIR}/model.onnx\"\n",
    "\n",
    "# GCS configuration\n",
    "BUCKET_NAME = PROJECT_ID\n",
    "BUCKET_URI = f\"gs://{BUCKET_NAME}\"\n",
    "GCS_ONNX_DIR = f\"{BUCKET_URI}/frameworks/pytorch-autoencoder/{EXPERIMENT}\"\n",
    "GCS_ONNX_PATH = f\"{GCS_ONNX_DIR}/model.onnx\"\n",
    "\n",
    "print(f\"Local files directory: {LOCAL_FILES_DIR}\")\n",
    "print(f\"Local ONNX path: {LOCAL_ONNX_PATH}\")\n",
    "print(f\"\\nGCS bucket: {BUCKET_URI}\")\n",
    "print(f\"GCS ONNX directory: {GCS_ONNX_DIR}\")\n",
    "print(f\"GCS ONNX path: {GCS_ONNX_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "init_clients",
   "metadata": {},
   "source": [
    "### Initialize Clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "init_clients_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Clients initialized:\n",
      "   BigQuery: statmike-mlops-349915\n",
      "   Cloud Storage: statmike-mlops-349915\n"
     ]
    }
   ],
   "source": [
    "# Initialize BigQuery client\n",
    "bq = bigquery.Client(project=PROJECT_ID)\n",
    "\n",
    "# Initialize GCS client\n",
    "storage_client = storage.Client(project=PROJECT_ID)\n",
    "\n",
    "print(f\"‚úÖ Clients initialized:\")\n",
    "print(f\"   BigQuery: {PROJECT_ID}\")\n",
    "print(f\"   Cloud Storage: {PROJECT_ID}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extract_mar",
   "metadata": {},
   "source": [
    "---\n",
    "## Extract PyTorch Model from MAR Archive\n",
    "\n",
    "The TorchServe .mar file is a ZIP archive containing:\n",
    "- Serialized model file (`.pt` TorchScript)\n",
    "- Handler code for preprocessing/postprocessing\n",
    "- Manifest with metadata\n",
    "\n",
    "We'll extract the TorchScript model file for ONNX conversion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "check_mar",
   "metadata": {},
   "source": [
    "### Check for MAR File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "check_mar_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Found .mar file: ../files/pytorch-autoencoder/pytorch_autoencoder.mar\n",
      "   Size: 30,141 bytes (29.43 KB)\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(SOURCE_MAR):\n",
    "    file_size = os.path.getsize(SOURCE_MAR)\n",
    "    print(f\"‚úÖ Found .mar file: {SOURCE_MAR}\")\n",
    "    print(f\"   Size: {file_size:,} bytes ({file_size / 1024:.2f} KB)\")\n",
    "else:\n",
    "    print(f\"‚ùå .mar file not found at: {SOURCE_MAR}\")\n",
    "    print(f\"   Please run the pytorch-autoencoder.ipynb notebook first to create the .mar file\")\n",
    "    raise FileNotFoundError(f\"MAR file not found: {SOURCE_MAR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "create_local_dir",
   "metadata": {},
   "source": [
    "### Create Local Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "create_local_dir_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Created/verified local directory: ./files/pytorch-bqml-import-onnx\n"
     ]
    }
   ],
   "source": [
    "# Create directory for ONNX files\n",
    "os.makedirs(LOCAL_FILES_DIR, exist_ok=True)\n",
    "print(f\"‚úÖ Created/verified local directory: {LOCAL_FILES_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extract_pt",
   "metadata": {},
   "source": [
    "### Extract TorchScript Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "extract_pt_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MAR file to: ./files/pytorch-bqml-import-onnx/mar_extracted\n",
      "\n",
      "Extracted files:\n",
      "  - final_model_traced.pt (47,408 bytes)\n",
      "  - handler.py (6,191 bytes)\n",
      "  - MAR-INF (4,096 bytes)\n",
      "\n",
      "‚úÖ Found TorchScript model: final_model_traced.pt\n",
      "   Path: ./files/pytorch-bqml-import-onnx/mar_extracted/final_model_traced.pt\n"
     ]
    }
   ],
   "source": [
    "# Extract .mar file (it's a ZIP archive)\n",
    "extract_dir = f\"{LOCAL_FILES_DIR}/mar_extracted\"\n",
    "os.makedirs(extract_dir, exist_ok=True)\n",
    "\n",
    "print(f\"Extracting MAR file to: {extract_dir}\")\n",
    "with zipfile.ZipFile(SOURCE_MAR, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_dir)\n",
    "\n",
    "# List extracted files\n",
    "extracted_files = os.listdir(extract_dir)\n",
    "print(f\"\\nExtracted files:\")\n",
    "for f in extracted_files:\n",
    "    fpath = os.path.join(extract_dir, f)\n",
    "    fsize = os.path.getsize(fpath)\n",
    "    print(f\"  - {f} ({fsize:,} bytes)\")\n",
    "\n",
    "# Find the .pt file\n",
    "pt_files = [f for f in extracted_files if f.endswith('.pt')]\n",
    "if not pt_files:\n",
    "    raise FileNotFoundError(\"No .pt file found in MAR archive\")\n",
    "\n",
    "pt_file = pt_files[0]\n",
    "pt_path = os.path.join(extract_dir, pt_file)\n",
    "print(f\"\\n‚úÖ Found TorchScript model: {pt_file}\")\n",
    "print(f\"   Path: {pt_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load_pytorch",
   "metadata": {},
   "source": [
    "### Extract Postprocessing Submodule\n",
    "\n",
    "The full model wraps outputs in a dictionary, but the `postprocessing` submodule returns a tuple (ONNX-compatible)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "load_pytorch_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading TorchScript model...\n",
      "‚úÖ Full model loaded successfully\n",
      "\n",
      "Full model type: <class 'torch.jit._script.RecursiveScriptModule'>\n",
      "\n",
      "‚úÖ Extracted postprocessing submodule\n",
      "Postprocessing type: <class 'torch.jit._script.RecursiveScriptModule'>\n",
      "\n",
      "üìä Comparing outputs:\n",
      "  Full model output type: <class 'dict'>\n",
      "  Number of keys: 13\n",
      "  Postprocessing output type: <class 'tuple'>\n",
      "  Number of outputs: 13\n",
      "\n",
      "üí° We'll export the postprocessing submodule (tuple) to ONNX, not the full model (dict)\n"
     ]
    }
   ],
   "source": [
    "# Load the TorchScript model\n",
    "print(f\"Loading TorchScript model...\")\n",
    "pytorch_model = torch.jit.load(pt_path)\n",
    "pytorch_model.eval()  # Set to evaluation mode\n",
    "\n",
    "print(f\"‚úÖ Full model loaded successfully\")\n",
    "print(f\"\\nFull model type: {type(pytorch_model)}\")\n",
    "\n",
    "# Extract the postprocessing submodule (returns tuple, not dict!)\n",
    "postprocessing_module = pytorch_model.postprocessing\n",
    "print(f\"\\n‚úÖ Extracted postprocessing submodule\")\n",
    "print(f\"Postprocessing type: {type(postprocessing_module)}\")\n",
    "\n",
    "# Test with dummy input to verify output types\n",
    "test_tensor = torch.randn(1, 30)\n",
    "\n",
    "print(f\"\\nüìä Comparing outputs:\")\n",
    "\n",
    "# Full model returns dict\n",
    "dict_output = pytorch_model(test_tensor)\n",
    "print(f\"  Full model output type: {type(dict_output)}\")\n",
    "print(f\"  Number of keys: {len(dict_output)}\")\n",
    "\n",
    "# Postprocessing returns tuple\n",
    "tuple_output = postprocessing_module(test_tensor)\n",
    "print(f\"  Postprocessing output type: {type(tuple_output)}\")\n",
    "print(f\"  Number of outputs: {len(tuple_output)}\")\n",
    "\n",
    "print(f\"\\nüí° We'll export the postprocessing submodule (tuple) to ONNX, not the full model (dict)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convert_onnx",
   "metadata": {},
   "source": [
    "---\n",
    "## Convert PyTorch Model to ONNX\n",
    "\n",
    "ONNX (Open Neural Network Exchange) is a portable format for representing ML models.\n",
    "\n",
    "**Challenge: TorchScript Model Returns Dictionary (ONNX Incompatible)**\n",
    "\n",
    "The TorchScript model in the .mar file returns a **dictionary** with 13 named outputs, but ONNX only supports tensor/tuple returns. The `prim::DictConstruct` operation has no ONNX equivalent.\n",
    "\n",
    "**Solution: Extract the Tuple-Returning Submodule**\n",
    "\n",
    "After inspecting the TorchScript model structure, we discovered:\n",
    "\n",
    "```python\n",
    "# Full model's forward method (simplified)\n",
    "def forward(self, x):\n",
    "    # Postprocessing returns a TUPLE of 13 tensors\n",
    "    out0, out1, ..., out12 = self.postprocessing(x)\n",
    "    \n",
    "    # Dictionary construction happens AFTER tensor ops (incompatible with ONNX)\n",
    "    return {\"normalized_reconstruction\": out0, \"normalized_MAE\": out2, ...}\n",
    "```\n",
    "\n",
    "**The key insight:**\n",
    "- ‚úÖ The `postprocessing` submodule returns a **tuple of 13 tensors** (ONNX-compatible!)\n",
    "- ‚ùå The full model wraps this tuple in a dictionary (ONNX-incompatible)\n",
    "- ‚úÖ We can export just the `postprocessing` submodule to ONNX\n",
    "\n",
    "**What we'll do:**\n",
    "1. Load the TorchScript model from the MAR file\n",
    "2. Extract the `postprocessing` submodule\n",
    "3. Export this submodule to ONNX (it returns tuple, not dict)\n",
    "4. Map tuple outputs to named ONNX outputs in correct order"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "get_test_data",
   "metadata": {},
   "source": [
    "### Get Test Data for Dummy Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "get_test_data_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 1 test instance\n",
      "\n",
      "Features (30): ['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount']\n",
      "\n",
      "Test tensor shape: torch.Size([1, 30])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2709834/346852644.py:15: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)\n",
      "  test_tensor = torch.tensor([test_instance], dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "# Get a sample from BigQuery to understand input shape\n",
    "query = f\"\"\"\n",
    "SELECT * EXCEPT(splits, transaction_id, Class)\n",
    "FROM `{PROJECT_ID}.{BQ_DATASET}.{BQ_SOURCE_TABLE}`\n",
    "WHERE splits='TEST'\n",
    "LIMIT 1\n",
    "\"\"\"\n",
    "\n",
    "test_df = bq.query(query).to_dataframe()\n",
    "print(f\"Retrieved {len(test_df)} test instance\")\n",
    "print(f\"\\nFeatures ({len(test_df.columns)}): {list(test_df.columns)}\")\n",
    "\n",
    "# Convert to tensor for testing\n",
    "test_instance = test_df.values[0]\n",
    "test_tensor = torch.tensor([test_instance], dtype=torch.float32)\n",
    "print(f\"\\nTest tensor shape: {test_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "export_onnx",
   "metadata": {},
   "source": [
    "### Export to ONNX Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "export_onnx_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting PyTorch postprocessing module to ONNX...\n",
      "  Output path: ./files/pytorch-bqml-import-onnx/model.onnx\n",
      "\n",
      "ONNX output names (13 total):\n",
      "  0: normalized_reconstruction\n",
      "  1: normalized_reconstruction_errors\n",
      "  2: normalized_MAE\n",
      "  3: normalized_RMSE\n",
      "  4: normalized_MSE\n",
      "  5: normalized_MSLE\n",
      "  6: denormalized_reconstruction\n",
      "  7: denormalized_reconstruction_errors\n",
      "  8: denormalized_MAE\n",
      "  9: denormalized_RMSE\n",
      "  10: denormalized_MSE\n",
      "  11: denormalized_MSLE\n",
      "  12: encoded\n",
      "\n",
      "üîÑ Exporting postprocessing submodule to ONNX...\n",
      "\n",
      "‚úÖ ONNX model exported successfully!\n",
      "   Path: ./files/pytorch-bqml-import-onnx/model.onnx\n",
      "   Size: 12,387 bytes (12.10 KB)\n",
      "\n",
      "üí° Exported the postprocessing submodule (tuple output) instead of full model (dict output)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2709834/3807088549.py:28: DeprecationWarning: You are using the legacy TorchScript-based ONNX export. Starting in PyTorch 2.9, the new torch.export-based ONNX exporter will be the default. To switch now, set dynamo=True in torch.onnx.export. This new exporter supports features like exporting LLMs with DynamicCache. We encourage you to try it and share feedback to help improve the experience. Learn more about the new export logic: https://pytorch.org/docs/stable/onnx_dynamo.html. For exporting control flow: https://pytorch.org/tutorials/beginner/onnx/export_control_flow_model_to_onnx_tutorial.html.\n",
      "  torch.onnx.export(\n",
      "/usr/local/google/home/statmike/.cache/pypoetry/virtualenvs/frameworks-pytorch-0KVJlKeQ-py3.13/lib/python3.13/site-packages/torch/onnx/_internal/torchscript_exporter/utils.py:834: UserWarning: no signature found for builtin <built-in method __call__ of pybind11_builtins.pybind11_detail_function_record_v1_system_libstdcpp_gxx_abi_1xxx_use_cxx11_abi_1 object at 0x7f27664ef410>, skipping _decide_input_format\n",
      "  warnings.warn(f\"{e}, skipping _decide_input_format\")\n"
     ]
    }
   ],
   "source": [
    "print(f\"Converting PyTorch postprocessing module to ONNX...\")\n",
    "print(f\"  Output path: {LOCAL_ONNX_PATH}\")\n",
    "\n",
    "# Output names must match the order returned by postprocessing module\n",
    "# Based on the TorchScript forward method, the tuple order is:\n",
    "output_names = [\n",
    "    'normalized_reconstruction',      # Output 0\n",
    "    'normalized_reconstruction_errors',  # Output 1\n",
    "    'normalized_MAE',                 # Output 2\n",
    "    'normalized_RMSE',                # Output 3\n",
    "    'normalized_MSE',                 # Output 4\n",
    "    'normalized_MSLE',                # Output 5\n",
    "    'denormalized_reconstruction',    # Output 6\n",
    "    'denormalized_reconstruction_errors',  # Output 7\n",
    "    'denormalized_MAE',               # Output 8 (anomaly score)\n",
    "    'denormalized_RMSE',              # Output 9\n",
    "    'denormalized_MSE',               # Output 10\n",
    "    'denormalized_MSLE',              # Output 11\n",
    "    'encoded'                         # Output 12 (latent representation)\n",
    "]\n",
    "\n",
    "print(f\"\\nONNX output names ({len(output_names)} total):\")\n",
    "for i, name in enumerate(output_names):\n",
    "    print(f\"  {i}: {name}\")\n",
    "\n",
    "# Export the postprocessing submodule (not the full model!)\n",
    "print(f\"\\nüîÑ Exporting postprocessing submodule to ONNX...\")\n",
    "torch.onnx.export(\n",
    "    postprocessing_module,  # Export submodule that returns tuple!\n",
    "    test_tensor,\n",
    "    LOCAL_ONNX_PATH,\n",
    "    input_names=['input'],\n",
    "    output_names=output_names,\n",
    "    opset_version=13,\n",
    "    do_constant_folding=True,\n",
    "    export_params=True,\n",
    "    dynamo=False  # Use legacy exporter (works with TorchScript)\n",
    ")\n",
    "\n",
    "# Check file was created\n",
    "if os.path.exists(LOCAL_ONNX_PATH):\n",
    "    file_size = os.path.getsize(LOCAL_ONNX_PATH)\n",
    "    print(f\"\\n‚úÖ ONNX model exported successfully!\")\n",
    "    print(f\"   Path: {LOCAL_ONNX_PATH}\")\n",
    "    print(f\"   Size: {file_size:,} bytes ({file_size / 1024:.2f} KB)\")\n",
    "    print(f\"\\nüí° Exported the postprocessing submodule (tuple output) instead of full model (dict output)\")\n",
    "else:\n",
    "    raise FileNotFoundError(f\"ONNX export failed - file not found: {LOCAL_ONNX_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "validate_onnx",
   "metadata": {},
   "source": [
    "### Validate ONNX Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "validate_onnx_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating ONNX model...\n",
      "\n",
      "‚úÖ ONNX model is valid\n",
      "\n",
      "Model info:\n",
      "  IR version: 7\n",
      "  Opset version: 13\n",
      "  Producer: pytorch\n",
      "\n",
      "Inputs:\n",
      "  - input: tensor_type {\n",
      "  elem_type: 1\n",
      "  shape {\n",
      "    dim {\n",
      "      dim_value: 1\n",
      "    }\n",
      "    dim {\n",
      "      dim_value: 30\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "Outputs (13):\n",
      "  - normalized_reconstruction\n",
      "  - normalized_reconstruction_errors\n",
      "  - normalized_MAE\n",
      "  - normalized_RMSE\n",
      "  - normalized_MSE\n",
      "  - normalized_MSLE\n",
      "  - denormalized_reconstruction\n",
      "  - denormalized_reconstruction_errors\n",
      "  - denormalized_MAE\n",
      "  - denormalized_RMSE\n",
      "  - denormalized_MSE\n",
      "  - denormalized_MSLE\n",
      "  - encoded\n"
     ]
    }
   ],
   "source": [
    "print(f\"Validating ONNX model...\\n\")\n",
    "\n",
    "# Load and check ONNX model\n",
    "onnx_model = onnx.load(LOCAL_ONNX_PATH)\n",
    "onnx.checker.check_model(onnx_model)\n",
    "\n",
    "print(f\"‚úÖ ONNX model is valid\")\n",
    "print(f\"\\nModel info:\")\n",
    "print(f\"  IR version: {onnx_model.ir_version}\")\n",
    "print(f\"  Opset version: {onnx_model.opset_import[0].version}\")\n",
    "print(f\"  Producer: {onnx_model.producer_name}\")\n",
    "\n",
    "print(f\"\\nInputs:\")\n",
    "for inp in onnx_model.graph.input:\n",
    "    print(f\"  - {inp.name}: {inp.type}\")\n",
    "\n",
    "print(f\"\\nOutputs ({len(onnx_model.graph.output)}):\")\n",
    "for out in onnx_model.graph.output:\n",
    "    print(f\"  - {out.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compare_predictions",
   "metadata": {},
   "source": [
    "---\n",
    "## Validate ONNX Conversion\n",
    "\n",
    "Compare predictions from the original PyTorch model and the ONNX model to ensure conversion correctness.\n",
    "\n",
    "**Why validation matters:**\n",
    "- ONNX conversion can introduce numerical differences\n",
    "- Verify outputs match within acceptable tolerance\n",
    "- Catch conversion errors before deploying to BigQuery"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pytorch_predict",
   "metadata": {},
   "source": [
    "### PyTorch Model Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "pytorch_predict_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running PyTorch postprocessing module prediction...\n",
      "\n",
      "‚úÖ PyTorch prediction completed\n",
      "\n",
      "PyTorch outputs (tuple with 13 elements):\n",
      "  normalized_MAE: 0.7289\n",
      "  normalized_RMSE: 1.0277\n",
      "  normalized_MSE: 1.0562\n",
      "  normalized_MSLE: 0.1155\n",
      "  denormalized_MAE (anomaly score): 150.2066\n",
      "  denormalized_RMSE: 816.0109\n",
      "  denormalized_MSE: 665873.8125\n",
      "  denormalized_MSLE: 0.4062\n",
      "\n",
      "Array outputs:\n",
      "  normalized_reconstruction (index 0): shape torch.Size([1, 30])\n",
      "  normalized_reconstruction_errors (index 1): shape torch.Size([1, 30])\n",
      "  denormalized_reconstruction (index 6): shape torch.Size([1, 30])\n",
      "  denormalized_reconstruction_errors (index 7): shape torch.Size([1, 30])\n",
      "  encoded (index 12): shape torch.Size([1, 4]), values: [0.0, 0.0, 0.2779858708381653, 0.0]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Running PyTorch postprocessing module prediction...\\n\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Use postprocessing module (returns tuple, same as ONNX)\n",
    "    pytorch_output = postprocessing_module(test_tensor)\n",
    "\n",
    "print(f\"‚úÖ PyTorch prediction completed\")\n",
    "print(f\"\\nPyTorch outputs (tuple with {len(pytorch_output)} elements):\")\n",
    "\n",
    "# Show scalar output values (indices 2-11 are scalars)\n",
    "scalar_indices = {\n",
    "    2: 'normalized_MAE',\n",
    "    3: 'normalized_RMSE',\n",
    "    4: 'normalized_MSE',\n",
    "    5: 'normalized_MSLE',\n",
    "    8: 'denormalized_MAE (anomaly score)',\n",
    "    9: 'denormalized_RMSE',\n",
    "    10: 'denormalized_MSE',\n",
    "    11: 'denormalized_MSLE'\n",
    "}\n",
    "\n",
    "for idx, name in scalar_indices.items():\n",
    "    value = pytorch_output[idx]\n",
    "    if value.numel() == 1:\n",
    "        print(f\"  {name}: {value.item():.4f}\")\n",
    "\n",
    "# Show array outputs\n",
    "print(f\"\\nArray outputs:\")\n",
    "print(f\"  normalized_reconstruction (index 0): shape {pytorch_output[0].shape}\")\n",
    "print(f\"  normalized_reconstruction_errors (index 1): shape {pytorch_output[1].shape}\")\n",
    "print(f\"  denormalized_reconstruction (index 6): shape {pytorch_output[6].shape}\")\n",
    "print(f\"  denormalized_reconstruction_errors (index 7): shape {pytorch_output[7].shape}\")\n",
    "print(f\"  encoded (index 12): shape {pytorch_output[12].shape}, values: {pytorch_output[12].squeeze().tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "onnx_predict",
   "metadata": {},
   "source": [
    "### ONNX Model Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "onnx_predict_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running ONNX model prediction...\n",
      "\n",
      "‚úÖ ONNX prediction completed\n",
      "\n",
      "ONNX outputs (13 outputs):\n",
      "  normalized_reconstruction: array with shape (1, 30)\n",
      "  normalized_reconstruction_errors: array with shape (1, 30)\n",
      "  normalized_MAE: 0.7289\n",
      "  normalized_RMSE: 1.0277\n",
      "  normalized_MSE: 1.0562\n",
      "  normalized_MSLE: 0.1155\n",
      "  denormalized_reconstruction: array with shape (1, 30)\n",
      "  denormalized_reconstruction_errors: array with shape (1, 30)\n",
      "  denormalized_MAE: 150.2066\n",
      "  denormalized_RMSE: 816.0109\n",
      "  denormalized_MSE: 665873.8750\n",
      "  denormalized_MSLE: 0.4062\n",
      "  encoded: [0.0, 0.0, 0.2779858708381653, 0.0]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Running ONNX model prediction...\\n\")\n",
    "\n",
    "# Create ONNX Runtime session\n",
    "ort_session = ort.InferenceSession(LOCAL_ONNX_PATH)\n",
    "\n",
    "# Prepare input\n",
    "ort_inputs = {ort_session.get_inputs()[0].name: test_tensor.numpy()}\n",
    "\n",
    "# Run inference\n",
    "ort_outputs = ort_session.run(None, ort_inputs)\n",
    "\n",
    "print(f\"‚úÖ ONNX prediction completed\")\n",
    "print(f\"\\nONNX outputs ({len(ort_outputs)} outputs):\")\n",
    "for i, (name, value) in enumerate(zip(output_names, ort_outputs)):\n",
    "    if value.size == 1:\n",
    "        print(f\"  {name}: {value.item():.4f}\")\n",
    "    elif value.size <= 5:\n",
    "        print(f\"  {name}: {value.squeeze().tolist()}\")\n",
    "    else:\n",
    "        print(f\"  {name}: array with shape {value.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compare_outputs",
   "metadata": {},
   "source": [
    "### Compare Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "compare_outputs_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing PyTorch vs ONNX outputs...\n",
      "\n",
      "Scalar outputs comparison:\n",
      "Index    Output                         PyTorch         ONNX            Abs Diff        Rel Error %    \n",
      "--------------------------------------------------------------------------------------------------\n",
      "2        normalized_MAE                 0.7289          0.7289          0.000000        0.000008       \n",
      "3        normalized_RMSE                1.0277          1.0277          0.000000        0.000000       \n",
      "4        normalized_MSE                 1.0562          1.0562          0.000000        0.000011       \n",
      "5        normalized_MSLE                0.1155          0.1155          0.000000        0.000000       \n",
      "8        denormalized_MAE               150.2066        150.2066        0.000000        0.000000       \n",
      "9        denormalized_RMSE              816.0109        816.0109        0.000000        0.000000       \n",
      "10       denormalized_MSE               665873.8125     665873.8750     0.062500        0.000009       \n",
      "11       denormalized_MSLE              0.4062          0.4062          0.000000        0.000007       \n",
      "\n",
      "Maximum absolute difference: 0.062500\n",
      "Maximum relative error: 0.000011%\n",
      "‚úÖ Outputs match within tolerance (relative error < 0.01%)\n",
      "\n",
      "Array outputs comparison:\n",
      "  normalized_reconstruction: PyTorch torch.Size([1, 30]), ONNX (1, 30), max abs diff: 0.000000, max rel error: 0.000006%\n",
      "  normalized_reconstruction_errors: PyTorch torch.Size([1, 30]), ONNX (1, 30), max abs diff: 0.000000, max rel error: 0.000000%\n",
      "  denormalized_reconstruction: PyTorch torch.Size([1, 30]), ONNX (1, 30), max abs diff: 0.000000, max rel error: 0.000000%\n",
      "  denormalized_reconstruction_errors: PyTorch torch.Size([1, 30]), ONNX (1, 30), max abs diff: 0.000000, max rel error: 0.000000%\n",
      "  encoded: PyTorch torch.Size([1, 4]), ONNX (1, 4), max abs diff: 0.000000, max rel error: 0.000000%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Comparing PyTorch vs ONNX outputs...\\n\")\n",
    "\n",
    "# Define scalar output names and indices\n",
    "scalar_outputs = [\n",
    "    (2, 'normalized_MAE'),\n",
    "    (3, 'normalized_RMSE'),\n",
    "    (4, 'normalized_MSE'),\n",
    "    (5, 'normalized_MSLE'),\n",
    "    (8, 'denormalized_MAE'),\n",
    "    (9, 'denormalized_RMSE'),\n",
    "    (10, 'denormalized_MSE'),\n",
    "    (11, 'denormalized_MSLE')\n",
    "]\n",
    "\n",
    "print(f\"Scalar outputs comparison:\")\n",
    "print(f\"{'Index':<8} {'Output':<30} {'PyTorch':<15} {'ONNX':<15} {'Abs Diff':<15} {'Rel Error %':<15}\")\n",
    "print(f\"{'-'*98}\")\n",
    "\n",
    "max_abs_diff = 0.0\n",
    "max_rel_error = 0.0\n",
    "for idx, output_name in scalar_outputs:\n",
    "    # PyTorch output is a tuple\n",
    "    pytorch_val = pytorch_output[idx].item()\n",
    "    # ONNX output is a list of arrays\n",
    "    onnx_val = ort_outputs[idx].item()\n",
    "    abs_diff = abs(pytorch_val - onnx_val)\n",
    "    # Calculate relative error (avoid division by zero)\n",
    "    rel_error = abs_diff / max(abs(pytorch_val), 1e-8)\n",
    "    \n",
    "    max_abs_diff = max(max_abs_diff, abs_diff)\n",
    "    max_rel_error = max(max_rel_error, rel_error)\n",
    "    \n",
    "    print(f\"{idx:<8} {output_name:<30} {pytorch_val:<15.4f} {onnx_val:<15.4f} {abs_diff:<15.6f} {rel_error*100:<15.6f}\")\n",
    "\n",
    "print(f\"\\nMaximum absolute difference: {max_abs_diff:.6f}\")\n",
    "print(f\"Maximum relative error: {max_rel_error:.6%}\")\n",
    "\n",
    "# Use relative error for validation (more appropriate for numerical comparison)\n",
    "if max_rel_error < 0.0001:  # 0.01%\n",
    "    print(f\"‚úÖ Outputs match within tolerance (relative error < 0.01%)\")\n",
    "elif max_rel_error < 0.01:  # 1%\n",
    "    print(f\"‚ö†Ô∏è  Small differences detected (relative error < 1%) - acceptable for most use cases\")\n",
    "else:\n",
    "    print(f\"‚ùå Large differences detected (relative error > 1%) - review conversion\")\n",
    "\n",
    "# Also compare array outputs\n",
    "print(f\"\\nArray outputs comparison:\")\n",
    "array_outputs = [\n",
    "    (0, 'normalized_reconstruction'),\n",
    "    (1, 'normalized_reconstruction_errors'),\n",
    "    (6, 'denormalized_reconstruction'),\n",
    "    (7, 'denormalized_reconstruction_errors'),\n",
    "    (12, 'encoded')\n",
    "]\n",
    "\n",
    "for idx, output_name in array_outputs:\n",
    "    pt_shape = pytorch_output[idx].shape\n",
    "    onnx_shape = ort_outputs[idx].shape\n",
    "    pt_array = pytorch_output[idx].detach().cpu().numpy()\n",
    "    onnx_array = ort_outputs[idx]\n",
    "    abs_diff = np.abs(pt_array - onnx_array).max()\n",
    "    # Calculate relative error for arrays\n",
    "    rel_error = abs_diff / max(np.abs(pt_array).max(), 1e-8)\n",
    "    print(f\"  {output_name}: PyTorch {pt_shape}, ONNX {onnx_shape}, max abs diff: {abs_diff:.6f}, max rel error: {rel_error:.6%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "upload_gcs",
   "metadata": {},
   "source": [
    "---\n",
    "## Upload ONNX Model to Cloud Storage\n",
    "\n",
    "BigQuery ML imports ONNX models from Cloud Storage. Upload the validated model to GCS."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "create_bucket",
   "metadata": {},
   "source": [
    "### Create GCS Bucket (if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "create_bucket_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Bucket already exists: gs://statmike-mlops-349915\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    bucket = storage_client.get_bucket(BUCKET_NAME)\n",
    "    print(f\"‚úÖ Bucket already exists: {BUCKET_URI}\")\n",
    "except:\n",
    "    bucket = storage_client.create_bucket(BUCKET_NAME, location=REGION)\n",
    "    print(f\"‚úÖ Created new bucket: {BUCKET_URI}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "upload_onnx",
   "metadata": {},
   "source": [
    "### Upload ONNX Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "upload_onnx_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading ONNX model to GCS...\n",
      "  Source: ./files/pytorch-bqml-import-onnx/model.onnx\n",
      "  Destination: gs://statmike-mlops-349915/frameworks/pytorch-autoencoder/pytorch-bqml-import-onnx/model.onnx\n",
      "\n",
      "‚úÖ ONNX model uploaded successfully!\n",
      "   GCS path: gs://statmike-mlops-349915/frameworks/pytorch-autoencoder/pytorch-bqml-import-onnx/model.onnx\n",
      "   Size: 12,387 bytes (12.10 KB)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Uploading ONNX model to GCS...\")\n",
    "print(f\"  Source: {LOCAL_ONNX_PATH}\")\n",
    "print(f\"  Destination: {GCS_ONNX_PATH}\\n\")\n",
    "\n",
    "# Upload file\n",
    "blob_path = f\"frameworks/pytorch-autoencoder/{EXPERIMENT}/model.onnx\"\n",
    "blob = bucket.blob(blob_path)\n",
    "blob.upload_from_filename(LOCAL_ONNX_PATH)\n",
    "\n",
    "print(f\"‚úÖ ONNX model uploaded successfully!\")\n",
    "print(f\"   GCS path: {GCS_ONNX_PATH}\")\n",
    "print(f\"   Size: {blob.size:,} bytes ({blob.size / 1024:.2f} KB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "import_bqml",
   "metadata": {},
   "source": [
    "---\n",
    "## Import ONNX Model to BigQuery ML\n",
    "\n",
    "Create a BigQuery ML model by importing the ONNX file from Cloud Storage.\n",
    "\n",
    "**Model configuration:**\n",
    "- **Model type**: ONNX\n",
    "- **Input**: 30 features (transaction data)\n",
    "- **Output**: 13 fields (full model diagnostics)\n",
    "- **Location**: Model runs natively in BigQuery"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "create_bqml_model",
   "metadata": {},
   "source": [
    "### Create BigQuery ML Model from ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "create_bqml_model_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating BigQuery ML model from ONNX...\n",
      "  Model: statmike-mlops-349915.frameworks.frameworks_pytorch_bqml_import_onnx\n",
      "  ONNX file: gs://statmike-mlops-349915/frameworks/pytorch-autoencoder/pytorch-bqml-import-onnx/model.onnx\n",
      "\n",
      "SQL:\n",
      "\n",
      "CREATE OR REPLACE MODEL `statmike-mlops-349915.frameworks.frameworks_pytorch_bqml_import_onnx`\n",
      "OPTIONS(\n",
      "  model_type='ONNX',\n",
      "  model_path='gs://statmike-mlops-349915/frameworks/pytorch-autoencoder/pytorch-bqml-import-onnx/model.onnx'\n",
      ")\n",
      "\n",
      "\n",
      "‚úÖ BigQuery ML model created successfully!\n",
      "\n",
      "üìä Model details:\n",
      "   Full name: statmike-mlops-349915.frameworks.frameworks_pytorch_bqml_import_onnx\n",
      "   Type: ONNX\n",
      "   Source: gs://statmike-mlops-349915/frameworks/pytorch-autoencoder/pytorch-bqml-import-onnx/model.onnx\n",
      "\n",
      "üí° View in BigQuery Console:\n",
      "   https://console.cloud.google.com/bigquery?project=statmike-mlops-349915&ws=!1m5!1m4!4m3!1sstatmike-mlops-349915!2sframeworks!3sframeworks_pytorch_bqml_import_onnx\n"
     ]
    }
   ],
   "source": [
    "print(f\"Creating BigQuery ML model from ONNX...\")\n",
    "print(f\"  Model: {PROJECT_ID}.{BQ_DATASET}.{BQ_MODEL_NAME}\")\n",
    "print(f\"  ONNX file: {GCS_ONNX_PATH}\\n\")\n",
    "\n",
    "# Create model SQL\n",
    "create_model_sql = f\"\"\"\n",
    "CREATE OR REPLACE MODEL `{PROJECT_ID}.{BQ_DATASET}.{BQ_MODEL_NAME}`\n",
    "OPTIONS(\n",
    "  model_type='ONNX',\n",
    "  model_path='{GCS_ONNX_PATH}'\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "print(f\"SQL:\")\n",
    "print(create_model_sql)\n",
    "\n",
    "# Execute query\n",
    "job = bq.query(create_model_sql)\n",
    "job.result()\n",
    "\n",
    "if job.state == 'DONE':\n",
    "    print(f\"\\n‚úÖ BigQuery ML model created successfully!\")\n",
    "    print(f\"\\nüìä Model details:\")\n",
    "    print(f\"   Full name: {PROJECT_ID}.{BQ_DATASET}.{BQ_MODEL_NAME}\")\n",
    "    print(f\"   Type: ONNX\")\n",
    "    print(f\"   Source: {GCS_ONNX_PATH}\")\n",
    "    \n",
    "    print(f\"\\nüí° View in BigQuery Console:\")\n",
    "    print(f\"   https://console.cloud.google.com/bigquery?project={PROJECT_ID}&ws=!1m5!1m4!4m3!1s{PROJECT_ID}!2s{BQ_DATASET}!3s{BQ_MODEL_NAME}\")\n",
    "else:\n",
    "    print(f\"‚ùå Model creation failed\")\n",
    "    print(f\"   State: {job.state}\")\n",
    "    if job.errors:\n",
    "        print(f\"   Errors: {job.errors}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "usage_examples",
   "metadata": {},
   "source": [
    "---\n",
    "## SQL-Based Inference with ML.PREDICT\n",
    "\n",
    "Now that the ONNX model is imported, use it for predictions entirely in SQL.\n",
    "\n",
    "We'll demonstrate three progressively complex examples:\n",
    "1. **Simple prediction**: Get full prediction output\n",
    "2. **Extract specific fields**: Work with output to get individual values\n",
    "3. **Apply business logic**: Use predictions for decision-making\n",
    "4. **Batch scoring**: Create enriched tables with predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "example1_simple",
   "metadata": {},
   "source": [
    "### Example 1: Simple Prediction\n",
    "\n",
    "Get predictions for a few test records with full output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "example1_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running simple ML.PREDICT query...\n",
      "\n",
      "SQL Query:\n",
      "\n",
      "SELECT *\n",
      "FROM ML.PREDICT(\n",
      "    MODEL `statmike-mlops-349915.frameworks.frameworks_pytorch_bqml_import_onnx`,\n",
      "    (\n",
      "        SELECT ARRAY[Time, V1, V2, V3, V4, V5, V6, V7, V8, V9, V10, V11, V12, V13, V14, V15, V16, V17, V18, V19, V20, V21, V22, V23, V24, V25, V26, V27, V28, Amount] AS input\n",
      "        FROM `statmike-mlops-349915.frameworks.frameworks`\n",
      "        WHERE splits = 'TEST'\n",
      "        LIMIT 3\n",
      "    )\n",
      ")\n",
      "\n",
      "\n",
      "‚úÖ Prediction query executed!\n",
      "\n",
      "üìä Result columns (14 total):\n",
      "   ['normalized_reconstruction', 'normalized_reconstruction_errors', 'normalized_MAE', 'normalized_RMSE', 'normalized_MSE', 'normalized_MSLE', 'denormalized_reconstruction', 'denormalized_reconstruction_errors', 'denormalized_MAE', 'denormalized_RMSE', 'denormalized_MSE', 'denormalized_MSLE', 'encoded', 'input']\n",
      "\n",
      "First 3 predictions:\n",
      "   denormalized_MAE  denormalized_RMSE  denormalized_MSE\n",
      "0        150.206635         816.010925      6.658739e+05\n",
      "1         78.624168         423.066681      1.789854e+05\n",
      "2        553.304749        3023.146484      9.139414e+06\n",
      "\n",
      "‚úÖ ONNX Model Output:\n",
      "   Anomaly Score (MAE): 150.21\n",
      "   RMSE: 816.01\n",
      "   MSE: 665873.88\n",
      "\n",
      "üí° Model runs natively in BigQuery - no external endpoint calls!\n"
     ]
    }
   ],
   "source": [
    "print(f\"Running simple ML.PREDICT query...\\n\")\n",
    "\n",
    "query = f\"\"\"\n",
    "SELECT *\n",
    "FROM ML.PREDICT(\n",
    "    MODEL `{PROJECT_ID}.{BQ_DATASET}.{BQ_MODEL_NAME}`,\n",
    "    (\n",
    "        SELECT ARRAY[Time, V1, V2, V3, V4, V5, V6, V7, V8, V9, V10, V11, V12, V13, V14, V15, V16, V17, V18, V19, V20, V21, V22, V23, V24, V25, V26, V27, V28, Amount] AS input\n",
    "        FROM `{PROJECT_ID}.{BQ_DATASET}.{BQ_SOURCE_TABLE}`\n",
    "        WHERE splits = 'TEST'\n",
    "        LIMIT 3\n",
    "    )\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "print(f\"SQL Query:\")\n",
    "print(query)\n",
    "print()\n",
    "\n",
    "result_df = bq.query(query).to_dataframe()\n",
    "\n",
    "print(f\"‚úÖ Prediction query executed!\")\n",
    "print(f\"\\nüìä Result columns ({len(result_df.columns)} total):\")\n",
    "print(f\"   {list(result_df.columns)}\\n\")\n",
    "\n",
    "# Show key predictions\n",
    "if 'denormalized_MAE' in result_df.columns:\n",
    "    print(f\"First 3 predictions:\")\n",
    "    print(result_df[['denormalized_MAE', 'denormalized_RMSE', 'denormalized_MSE']].to_string())\n",
    "    \n",
    "    print(f\"\\n‚úÖ ONNX Model Output:\")\n",
    "    print(f\"   Anomaly Score (MAE): {result_df['denormalized_MAE'].iloc[0]:.2f}\")\n",
    "    print(f\"   RMSE: {result_df['denormalized_RMSE'].iloc[0]:.2f}\")\n",
    "    print(f\"   MSE: {result_df['denormalized_MSE'].iloc[0]:.2f}\")\n",
    "    \n",
    "print(f\"\\nüí° Model runs natively in BigQuery - no external endpoint calls!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "example2_extract",
   "metadata": {},
   "source": [
    "### Example 2: Extract Specific Fields\n",
    "\n",
    "In practice, you often only need specific prediction fields. Show how to extract just the anomaly score and a few key metrics.\n",
    "\n",
    "**Why this matters:**\n",
    "- Cleaner query results (fewer columns)\n",
    "- Better performance (less data transferred)\n",
    "- Easier to join with other tables\n",
    "- Simpler downstream processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "example2_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running ML.PREDICT with field extraction...\n",
      "\n",
      "‚úÖ Prediction successful!\n",
      "\n",
      "üìä Extracted fields (4 columns):\n",
      "   ['anomaly_score', 'rmse', 'mse', 'latent_encoding']\n",
      "\n",
      "Results:\n",
      "   anomaly_score         rmse           mse                       latent_encoding\n",
      "0     150.206635   816.010925  6.658739e+05   [0.0, 0.0, 0.2779858708381653, 0.0]\n",
      "1      78.624168   423.066681  1.789854e+05   [0.0, 0.0, 0.2594718337059021, 0.0]\n",
      "2     553.304749  3023.146484  9.139414e+06  [0.0, 0.0, 0.15171857178211212, 0.0]\n",
      "3     700.802002  3830.523193  1.467291e+07   [0.0, 0.0, 0.1383998692035675, 0.0]\n",
      "4     818.971497  4477.672363  2.004955e+07   [0.0, 0.0, 13.524330139160156, 0.0]\n",
      "\n",
      "üí° Benefits of field extraction:\n",
      "   ‚Ä¢ Only get data you need\n",
      "   ‚Ä¢ Rename fields for clarity (denormalized_MAE ‚Üí anomaly_score)\n",
      "   ‚Ä¢ Easier to use in downstream queries\n"
     ]
    }
   ],
   "source": [
    "print(f\"Running ML.PREDICT with field extraction...\\n\")\n",
    "\n",
    "query = f\"\"\"\n",
    "SELECT\n",
    "    -- Extracted prediction fields\n",
    "    denormalized_MAE as anomaly_score,\n",
    "    denormalized_RMSE as rmse,\n",
    "    denormalized_MSE as mse,\n",
    "    encoded as latent_encoding\n",
    "FROM ML.PREDICT(\n",
    "    MODEL `{PROJECT_ID}.{BQ_DATASET}.{BQ_MODEL_NAME}`,\n",
    "    (\n",
    "        SELECT ARRAY[Time, V1, V2, V3, V4, V5, V6, V7, V8, V9, V10, V11, V12, V13, V14, V15, V16, V17, V18, V19, V20, V21, V22, V23, V24, V25, V26, V27, V28, Amount] AS input\n",
    "        FROM `{PROJECT_ID}.{BQ_DATASET}.{BQ_SOURCE_TABLE}`\n",
    "        WHERE splits = 'TEST'\n",
    "        LIMIT 5\n",
    "    )\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "result_df = bq.query(query).to_dataframe()\n",
    "\n",
    "print(f\"‚úÖ Prediction successful!\")\n",
    "print(f\"\\nüìä Extracted fields ({len(result_df.columns)} columns):\")\n",
    "print(f\"   {list(result_df.columns)}\\n\")\n",
    "\n",
    "print(f\"Results:\")\n",
    "print(result_df.to_string())\n",
    "\n",
    "print(f\"\\nüí° Benefits of field extraction:\")\n",
    "print(f\"   ‚Ä¢ Only get data you need\")\n",
    "print(f\"   ‚Ä¢ Rename fields for clarity (denormalized_MAE ‚Üí anomaly_score)\")\n",
    "print(f\"   ‚Ä¢ Easier to use in downstream queries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "example3_logic",
   "metadata": {},
   "source": [
    "### Example 3: Apply Business Logic\n",
    "\n",
    "Use predictions to make decisions in SQL. This example:\n",
    "- Classifies transactions as normal or anomaly based on threshold\n",
    "- Adds risk levels (low, medium, high)\n",
    "- Shows how to use predictions for filtering and grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "example3_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running ML.PREDICT with business logic...\n",
      "\n",
      "Anomaly thresholds:\n",
      "  Medium risk: > 100.0\n",
      "  High risk: > 200.0\n",
      "\n",
      "‚úÖ Prediction with business logic successful!\n",
      "\n",
      "üìä Top 10 highest risk transactions:\n",
      "\n",
      "   anomaly_score        risk_category  is_anomaly  risk_score\n",
      "0    3096.734619  ANOMALY - HIGH RISK           1         100\n",
      "1    2524.846436  ANOMALY - HIGH RISK           1         100\n",
      "2    2359.387695  ANOMALY - HIGH RISK           1         100\n",
      "3    2286.601807  ANOMALY - HIGH RISK           1         100\n",
      "4    2014.056885  ANOMALY - HIGH RISK           1         100\n",
      "5    1982.533691  ANOMALY - HIGH RISK           1         100\n",
      "6    1919.844116  ANOMALY - HIGH RISK           1         100\n",
      "7    1831.535767  ANOMALY - HIGH RISK           1         100\n",
      "8    1798.430298  ANOMALY - HIGH RISK           1         100\n",
      "9    1792.919434  ANOMALY - HIGH RISK           1         100\n",
      "\n",
      "üí° Use cases for business logic:\n",
      "   ‚Ä¢ Filter: WHERE is_anomaly = 1 (only anomalies)\n",
      "   ‚Ä¢ Route: Send high-risk transactions to manual review\n",
      "   ‚Ä¢ Alert: Trigger notifications for ANOMALY - HIGH RISK\n"
     ]
    }
   ],
   "source": [
    "print(f\"Running ML.PREDICT with business logic...\\n\")\n",
    "\n",
    "# Define anomaly thresholds\n",
    "THRESHOLD_MEDIUM = 100.0\n",
    "THRESHOLD_HIGH = 200.0\n",
    "\n",
    "print(f\"Anomaly thresholds:\")\n",
    "print(f\"  Medium risk: > {THRESHOLD_MEDIUM}\")\n",
    "print(f\"  High risk: > {THRESHOLD_HIGH}\\n\")\n",
    "\n",
    "query = f\"\"\"\n",
    "SELECT\n",
    "    denormalized_MAE as anomaly_score,\n",
    "    \n",
    "    -- Business logic: Classify transaction\n",
    "    CASE\n",
    "        WHEN denormalized_MAE > {THRESHOLD_HIGH} THEN 'ANOMALY - HIGH RISK'\n",
    "        WHEN denormalized_MAE > {THRESHOLD_MEDIUM} THEN 'ANOMALY - MEDIUM RISK'\n",
    "        ELSE 'NORMAL'\n",
    "    END as risk_category,\n",
    "    \n",
    "    -- Binary flag for filtering\n",
    "    CASE WHEN denormalized_MAE > {THRESHOLD_MEDIUM} THEN 1 ELSE 0 END as is_anomaly,\n",
    "    \n",
    "    -- Risk score (0-100)\n",
    "    LEAST(100, CAST(denormalized_MAE / {THRESHOLD_HIGH} * 100 AS INT64)) as risk_score\n",
    "\n",
    "FROM ML.PREDICT(\n",
    "    MODEL `{PROJECT_ID}.{BQ_DATASET}.{BQ_MODEL_NAME}`,\n",
    "    (\n",
    "        SELECT ARRAY[Time, V1, V2, V3, V4, V5, V6, V7, V8, V9, V10, V11, V12, V13, V14, V15, V16, V17, V18, V19, V20, V21, V22, V23, V24, V25, V26, V27, V28, Amount] AS input\n",
    "        FROM `{PROJECT_ID}.{BQ_DATASET}.{BQ_SOURCE_TABLE}`\n",
    "        WHERE splits = 'TEST'\n",
    "        LIMIT 100\n",
    "    )\n",
    ")\n",
    "ORDER BY anomaly_score DESC\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "\n",
    "result_df = bq.query(query).to_dataframe()\n",
    "\n",
    "print(f\"‚úÖ Prediction with business logic successful!\")\n",
    "print(f\"\\nüìä Top 10 highest risk transactions:\\n\")\n",
    "print(result_df.to_string())\n",
    "\n",
    "print(f\"\\nüí° Use cases for business logic:\")\n",
    "print(f\"   ‚Ä¢ Filter: WHERE is_anomaly = 1 (only anomalies)\")\n",
    "print(f\"   ‚Ä¢ Route: Send high-risk transactions to manual review\")\n",
    "print(f\"   ‚Ä¢ Alert: Trigger notifications for ANOMALY - HIGH RISK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "batch_scoring",
   "metadata": {},
   "source": [
    "### Example 4: Batch Scoring\n",
    "\n",
    "Create an enriched table with predictions for all test transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "batch_scoring_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating predictions table with batch scoring...\n",
      "  Source: statmike-mlops-349915.frameworks.frameworks (TEST split)\n",
      "  Target: statmike-mlops-349915.frameworks.pytorch_bqml_import_onnx_predictions\n",
      "\n",
      "Executing CREATE TABLE query...\n",
      "\n",
      "‚úÖ Predictions table created successfully!\n",
      "\n",
      "üìä Table details:\n",
      "   Table: statmike-mlops-349915.frameworks.pytorch_bqml_import_onnx_predictions\n",
      "   Rows: 28,588\n",
      "   Size: 4.58 MB\n"
     ]
    }
   ],
   "source": [
    "print(f\"Creating predictions table with batch scoring...\")\n",
    "print(f\"  Source: {PROJECT_ID}.{BQ_DATASET}.{BQ_SOURCE_TABLE} (TEST split)\")\n",
    "print(f\"  Target: {PROJECT_ID}.{BQ_DATASET}.{BQ_PREDICTIONS_TABLE}\\n\")\n",
    "\n",
    "create_table_query = f\"\"\"\n",
    "CREATE OR REPLACE TABLE `{PROJECT_ID}.{BQ_DATASET}.{BQ_PREDICTIONS_TABLE}` AS\n",
    "SELECT\n",
    "    -- Original data\n",
    "    orig.transaction_id,\n",
    "    orig.Class as actual_label,\n",
    "    orig.Time,\n",
    "    orig.Amount,\n",
    "    \n",
    "    -- Predictions\n",
    "    pred.denormalized_MAE as anomaly_score,\n",
    "    pred.denormalized_RMSE as rmse,\n",
    "    pred.denormalized_MSE as mse,\n",
    "    pred.encoded as latent_encoding,\n",
    "    \n",
    "    -- Business logic\n",
    "    CASE\n",
    "        WHEN pred.denormalized_MAE > {THRESHOLD_HIGH} THEN 'ANOMALY - HIGH RISK'\n",
    "        WHEN pred.denormalized_MAE > {THRESHOLD_MEDIUM} THEN 'ANOMALY - MEDIUM RISK'\n",
    "        ELSE 'NORMAL'\n",
    "    END as risk_category,\n",
    "    \n",
    "    CASE WHEN pred.denormalized_MAE > {THRESHOLD_MEDIUM} THEN 1 ELSE 0 END as is_anomaly,\n",
    "    LEAST(100, CAST(pred.denormalized_MAE / {THRESHOLD_HIGH} * 100 AS INT64)) as risk_score,\n",
    "    \n",
    "    -- Metadata\n",
    "    CURRENT_TIMESTAMP() as prediction_timestamp,\n",
    "    'ONNX' as model_type\n",
    "\n",
    "FROM (\n",
    "    SELECT *\n",
    "    FROM ML.PREDICT(\n",
    "        MODEL `{PROJECT_ID}.{BQ_DATASET}.{BQ_MODEL_NAME}`,\n",
    "        (\n",
    "            SELECT \n",
    "                ARRAY[Time, V1, V2, V3, V4, V5, V6, V7, V8, V9, V10, V11, V12, V13, V14, V15, V16, V17, V18, V19, V20, V21, V22, V23, V24, V25, V26, V27, V28, Amount] AS input,\n",
    "                Time,\n",
    "                Amount\n",
    "            FROM `{PROJECT_ID}.{BQ_DATASET}.{BQ_SOURCE_TABLE}`\n",
    "            WHERE splits = 'TEST'\n",
    "        )\n",
    "    )\n",
    ") pred\n",
    "JOIN (\n",
    "    SELECT *\n",
    "    FROM `{PROJECT_ID}.{BQ_DATASET}.{BQ_SOURCE_TABLE}`\n",
    "    WHERE splits = 'TEST'\n",
    ") orig\n",
    "ON pred.Time = orig.Time AND pred.Amount = orig.Amount\n",
    "\"\"\"\n",
    "\n",
    "print(f\"Executing CREATE TABLE query...\\n\")\n",
    "job = bq.query(create_table_query)\n",
    "job.result()\n",
    "\n",
    "if job.state == 'DONE':\n",
    "    print(f\"‚úÖ Predictions table created successfully!\")\n",
    "    \n",
    "    table = bq.get_table(f\"{PROJECT_ID}.{BQ_DATASET}.{BQ_PREDICTIONS_TABLE}\")\n",
    "    print(f\"\\nüìä Table details:\")\n",
    "    print(f\"   Table: {PROJECT_ID}.{BQ_DATASET}.{BQ_PREDICTIONS_TABLE}\")\n",
    "    print(f\"   Rows: {table.num_rows:,}\")\n",
    "    print(f\"   Size: {table.num_bytes / 1024 / 1024:.2f} MB\")\n",
    "else:\n",
    "    print(f\"‚ùå Table creation failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "query_predictions",
   "metadata": {},
   "source": [
    "### Query Predictions Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "query_predictions_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Querying predictions table...\n",
      "\n",
      "üìä Top 10 Anomalies:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transaction_id</th>\n",
       "      <th>actual_label</th>\n",
       "      <th>anomaly_score</th>\n",
       "      <th>risk_category</th>\n",
       "      <th>risk_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>02c93fd4-e981-4e92-80cf-e3fa9a8a8681</td>\n",
       "      <td>0</td>\n",
       "      <td>4079.226807</td>\n",
       "      <td>ANOMALY - HIGH RISK</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>40679e68-f1d6-4120-a3ee-3b388fbedfa4</td>\n",
       "      <td>0</td>\n",
       "      <td>3836.285645</td>\n",
       "      <td>ANOMALY - HIGH RISK</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>f78e4a10-4e09-4c64-aff1-c582d3168660</td>\n",
       "      <td>0</td>\n",
       "      <td>3602.211670</td>\n",
       "      <td>ANOMALY - HIGH RISK</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>345a37d5-1601-4e5d-a742-ff83e4ea8603</td>\n",
       "      <td>0</td>\n",
       "      <td>3594.519287</td>\n",
       "      <td>ANOMALY - HIGH RISK</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>69da5434-65a8-4213-a491-7683f19a3ca0</td>\n",
       "      <td>0</td>\n",
       "      <td>3563.286621</td>\n",
       "      <td>ANOMALY - HIGH RISK</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>f4a60a12-7058-4e87-9756-c4feddacee40</td>\n",
       "      <td>0</td>\n",
       "      <td>3523.909912</td>\n",
       "      <td>ANOMALY - HIGH RISK</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>953dcad0-e85c-44c0-82c2-65da23a71603</td>\n",
       "      <td>0</td>\n",
       "      <td>3501.584717</td>\n",
       "      <td>ANOMALY - HIGH RISK</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0090a7df-9c1a-471c-8dc5-edba9cf7c772</td>\n",
       "      <td>0</td>\n",
       "      <td>3457.641357</td>\n",
       "      <td>ANOMALY - HIGH RISK</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>d9b30262-cc54-488f-83a6-4214522ad8df</td>\n",
       "      <td>0</td>\n",
       "      <td>3456.861816</td>\n",
       "      <td>ANOMALY - HIGH RISK</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>a7531840-0efe-4356-898d-dbfb0b39567d</td>\n",
       "      <td>0</td>\n",
       "      <td>3430.818604</td>\n",
       "      <td>ANOMALY - HIGH RISK</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         transaction_id  actual_label  anomaly_score  \\\n",
       "0  02c93fd4-e981-4e92-80cf-e3fa9a8a8681             0    4079.226807   \n",
       "1  40679e68-f1d6-4120-a3ee-3b388fbedfa4             0    3836.285645   \n",
       "2  f78e4a10-4e09-4c64-aff1-c582d3168660             0    3602.211670   \n",
       "3  345a37d5-1601-4e5d-a742-ff83e4ea8603             0    3594.519287   \n",
       "4  69da5434-65a8-4213-a491-7683f19a3ca0             0    3563.286621   \n",
       "5  f4a60a12-7058-4e87-9756-c4feddacee40             0    3523.909912   \n",
       "6  953dcad0-e85c-44c0-82c2-65da23a71603             0    3501.584717   \n",
       "7  0090a7df-9c1a-471c-8dc5-edba9cf7c772             0    3457.641357   \n",
       "8  d9b30262-cc54-488f-83a6-4214522ad8df             0    3456.861816   \n",
       "9  a7531840-0efe-4356-898d-dbfb0b39567d             0    3430.818604   \n",
       "\n",
       "         risk_category  risk_score  \n",
       "0  ANOMALY - HIGH RISK         100  \n",
       "1  ANOMALY - HIGH RISK         100  \n",
       "2  ANOMALY - HIGH RISK         100  \n",
       "3  ANOMALY - HIGH RISK         100  \n",
       "4  ANOMALY - HIGH RISK         100  \n",
       "5  ANOMALY - HIGH RISK         100  \n",
       "6  ANOMALY - HIGH RISK         100  \n",
       "7  ANOMALY - HIGH RISK         100  \n",
       "8  ANOMALY - HIGH RISK         100  \n",
       "9  ANOMALY - HIGH RISK         100  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìà Risk Distribution:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>risk_category</th>\n",
       "      <th>count</th>\n",
       "      <th>avg_anomaly_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ANOMALY - HIGH RISK</td>\n",
       "      <td>23440</td>\n",
       "      <td>891.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ANOMALY - MEDIUM RISK</td>\n",
       "      <td>2591</td>\n",
       "      <td>151.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NORMAL</td>\n",
       "      <td>2557</td>\n",
       "      <td>51.17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           risk_category  count  avg_anomaly_score\n",
       "0    ANOMALY - HIGH RISK  23440             891.65\n",
       "1  ANOMALY - MEDIUM RISK   2591             151.25\n",
       "2                 NORMAL   2557              51.17"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üí° Benefits:\n",
      "   ‚ö° No endpoint calls - instant queries\n",
      "   üí∞ No Vertex AI costs - BigQuery slots only\n",
      "   üîÑ Joinable with other BigQuery tables\n"
     ]
    }
   ],
   "source": [
    "print(f\"Querying predictions table...\\n\")\n",
    "\n",
    "# Top anomalies\n",
    "query = f\"\"\"\n",
    "SELECT \n",
    "    transaction_id,\n",
    "    actual_label,\n",
    "    anomaly_score,\n",
    "    risk_category,\n",
    "    risk_score\n",
    "FROM `{PROJECT_ID}.{BQ_DATASET}.{BQ_PREDICTIONS_TABLE}`\n",
    "ORDER BY anomaly_score DESC\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "\n",
    "top_anomalies = bq.query(query).to_dataframe()\n",
    "print(f\"üìä Top 10 Anomalies:\\n\")\n",
    "display(top_anomalies)\n",
    "\n",
    "# Risk distribution\n",
    "query = f\"\"\"\n",
    "SELECT \n",
    "    risk_category,\n",
    "    COUNT(*) as count,\n",
    "    ROUND(AVG(anomaly_score), 2) as avg_anomaly_score\n",
    "FROM `{PROJECT_ID}.{BQ_DATASET}.{BQ_PREDICTIONS_TABLE}`\n",
    "GROUP BY risk_category\n",
    "ORDER BY avg_anomaly_score DESC\n",
    "\"\"\"\n",
    "\n",
    "print(f\"\\nüìà Risk Distribution:\\n\")\n",
    "risk_dist = bq.query(query).to_dataframe()\n",
    "display(risk_dist)\n",
    "\n",
    "print(f\"\\nüí° Benefits:\")\n",
    "print(f\"   ‚ö° No endpoint calls - instant queries\")\n",
    "print(f\"   üí∞ No Vertex AI costs - BigQuery slots only\")\n",
    "print(f\"   üîÑ Joinable with other BigQuery tables\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advanced_topics",
   "metadata": {},
   "source": [
    "---\n",
    "## Advanced Topics\n",
    "\n",
    "This section discusses advanced capabilities and optimizations for BigQuery ML models. These topics apply to both ONNX import and remote models.\n",
    "\n",
    "### Scheduled Queries for Automated Predictions\n",
    "\n",
    "**BigQuery Scheduled Queries** enable automatic, recurring batch predictions:\n",
    "\n",
    "**Setup steps:**\n",
    "1. Write `CREATE OR REPLACE TABLE` query with `ML.PREDICT()`\n",
    "2. Click \"Schedule\" in BigQuery Console\n",
    "3. Set frequency: hourly, daily, weekly, monthly, or custom cron\n",
    "4. Configure notifications for failures\n",
    "\n",
    "**Example: Daily fraud scoring**\n",
    "```sql\n",
    "CREATE OR REPLACE TABLE `fraud.daily_scores`\n",
    "PARTITION BY DATE(transaction_date) AS\n",
    "SELECT \n",
    "    transaction_date,\n",
    "    anomaly_score,\n",
    "    risk_category\n",
    "FROM ML.PREDICT(\n",
    "    MODEL `models.fraud_detector`,\n",
    "    (SELECT * FROM `transactions.daily` WHERE date = CURRENT_DATE())\n",
    ")\n",
    "```\n",
    "\n",
    "### Continuous Queries for Real-Time Predictions\n",
    "\n",
    "BigQuery Continuous Queries enable near-real-time ML inference:\n",
    "\n",
    "```sql\n",
    "CREATE OR REPLACE TABLE `fraud.continuous_scores` AS (\n",
    "  SELECT\n",
    "    transaction_id,\n",
    "    anomaly_score,\n",
    "    risk_category\n",
    "  FROM ML.PREDICT(\n",
    "    MODEL `models.fraud_detector`,\n",
    "    TABLE `transactions.streaming_input`\n",
    "  )\n",
    ")\n",
    "OPTIONS (\n",
    "  continuous = TRUE,\n",
    "  max_staleness = INTERVAL 30 SECOND\n",
    ");\n",
    "```\n",
    "\n",
    "### Performance Optimization\n",
    "\n",
    "**Best practices for ONNX models:**\n",
    "\n",
    "**1. Query Optimization**\n",
    "- **Filter early**: Apply WHERE before ML.PREDICT()\n",
    "- **Project only needed columns**: Reduce data transfer\n",
    "- **Use partitioning**: Scan less data\n",
    "\n",
    "**2. Model Size**\n",
    "- ONNX models must be < 250 MB\n",
    "- Larger models won't import to BigQuery ML\n",
    "- Consider model compression techniques\n",
    "\n",
    "**3. Cost Management**\n",
    "- **No endpoint costs**: Only pay for BigQuery slot usage\n",
    "- **Cache results**: CREATE TABLE instead of repeated predictions\n",
    "- **Monitoring**: Track query costs in BigQuery console\n",
    "\n",
    "**Example: Optimized query**\n",
    "```sql\n",
    "-- Efficient: Filters first, then predicts\n",
    "SELECT * FROM ML.PREDICT(\n",
    "    MODEL ...,\n",
    "    (SELECT * FROM table WHERE date = CURRENT_DATE())  -- Filter BEFORE\n",
    ")\n",
    "```\n",
    "\n",
    "### ONNX vs. Remote Model Decision Guide\n",
    "\n",
    "**Choose ONNX Import when:**\n",
    "- ‚úÖ Model < 250 MB\n",
    "- ‚úÖ Batch inference only (not real-time serving)\n",
    "- ‚úÖ Cost optimization priority\n",
    "- ‚úÖ Simplified deployment preferred\n",
    "\n",
    "**Choose Remote Model when:**\n",
    "- ‚úÖ Model > 250 MB\n",
    "- ‚úÖ Real-time predictions needed\n",
    "- ‚úÖ Reusing existing Vertex AI endpoint\n",
    "- ‚úÖ Complex preprocessing/postprocessing\n",
    "\n",
    "**Hybrid Approach:**\n",
    "Use both! Deploy large models as remote models, small models as ONNX imports."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cleanup",
   "metadata": {},
   "source": [
    "---\n",
    "## Cleanup\n",
    "\n",
    "To avoid incurring unnecessary charges, clean up resources created in this notebook.\n",
    "\n",
    "**‚ö†Ô∏è Warning:** The commands below are commented out to prevent accidental deletion. Uncomment only what you want to remove.\n",
    "\n",
    "**What gets deleted:**\n",
    "- BigQuery ML ONNX model\n",
    "- BigQuery predictions table\n",
    "- Local files (extracted MAR, ONNX model)\n",
    "- GCS ONNX model file\n",
    "\n",
    "**What is NOT deleted:**\n",
    "- Source data table (`frameworks` dataset)\n",
    "- Original .mar file (in pytorch-autoencoder notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cleanup_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to delete BigQuery ML model\n",
    "# print(f\"Deleting BQML model: {PROJECT_ID}.{BQ_DATASET}.{BQ_MODEL_NAME}\")\n",
    "# bq.delete_model(f\"{PROJECT_ID}.{BQ_DATASET}.{BQ_MODEL_NAME}\", not_found_ok=True)\n",
    "# print(f\"‚úÖ Model deleted\")\n",
    "\n",
    "# Uncomment to delete predictions table\n",
    "# print(f\"Deleting predictions table: {PROJECT_ID}.{BQ_DATASET}.{BQ_PREDICTIONS_TABLE}\")\n",
    "# bq.delete_table(f\"{PROJECT_ID}.{BQ_DATASET}.{BQ_PREDICTIONS_TABLE}\", not_found_ok=True)\n",
    "# print(f\"‚úÖ Predictions table deleted\")\n",
    "\n",
    "# Uncomment to delete GCS ONNX file\n",
    "# print(f\"Deleting GCS ONNX file: {GCS_ONNX_PATH}\")\n",
    "# blob = bucket.blob(f\"frameworks/pytorch-autoencoder/{EXPERIMENT}/model.onnx\")\n",
    "# blob.delete()\n",
    "# print(f\"‚úÖ GCS file deleted\")\n",
    "\n",
    "# Uncomment to delete local files\n",
    "# print(f\"Deleting local files: {LOCAL_FILES_DIR}\")\n",
    "# shutil.rmtree(LOCAL_FILES_DIR, ignore_errors=True)\n",
    "# print(f\"‚úÖ Local files deleted\")\n",
    "\n",
    "print(f\"\\nüí° To clean up resources, uncomment the commands above\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "Congratulations! You've successfully converted a PyTorch model to ONNX and imported it into BigQuery ML for SQL-based inference.\n",
    "\n",
    "### What You Accomplished\n",
    "\n",
    "‚úÖ **Model Extraction & Conversion**\n",
    "- Extracted TorchScript model from .mar archive\n",
    "- Converted PyTorch model to ONNX format\n",
    "- Validated conversion correctness\n",
    "\n",
    "‚úÖ **Cloud Deployment**\n",
    "- Uploaded ONNX model to Cloud Storage\n",
    "- Imported ONNX as BigQuery ML model\n",
    "- Model runs natively in BigQuery (no endpoints!)\n",
    "\n",
    "‚úÖ **SQL-Based Inference**\n",
    "- Made simple predictions with `ML.PREDICT()`\n",
    "- Extracted specific fields from outputs\n",
    "- Applied business logic (risk categorization)\n",
    "- Created enriched tables with batch predictions\n",
    "\n",
    "‚úÖ **Production Patterns**\n",
    "- Batch scored large datasets\n",
    "- Persisted predictions to queryable tables\n",
    "- Demonstrated joining predictions with source data\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "**Benefits of ONNX Import:**\n",
    "- üöÄ **No endpoint management**: Simplified deployment\n",
    "- ‚ö° **Lower latency**: In-process execution\n",
    "- üí∞ **Cost-effective**: No Vertex AI endpoint charges\n",
    "- üéØ **SQL-native**: All inference in SQL\n",
    "\n",
    "**When to Use ONNX Import:**\n",
    "- ‚úÖ Model < 250 MB\n",
    "- ‚úÖ Batch inference workloads\n",
    "- ‚úÖ Data already in BigQuery\n",
    "- ‚úÖ Cost-sensitive applications\n",
    "\n",
    "**Comparison with Remote Models:**\n",
    "\n",
    "| Aspect | ONNX Import | Remote Model |\n",
    "|--------|-------------|-------------|\n",
    "| **Deployment** | Upload ONNX to GCS | Deploy Vertex endpoint |\n",
    "| **Infrastructure** | None | Managed servers |\n",
    "| **Latency** | Very low | Higher (network) |\n",
    "| **Cost** | BigQuery only | BigQuery + Vertex AI |\n",
    "| **Model Size** | < 250 MB | No limit |\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "**Explore Related Notebooks:**\n",
    "- [bigquery-bqml-remote-model-vertex.ipynb](./bigquery-bqml-remote-model-vertex.ipynb) - Compare with remote model approach\n",
    "- [vertex-ai-endpoint-prebuilt-container.ipynb](./vertex-ai-endpoint-prebuilt-container.ipynb) - Deploy as endpoint instead\n",
    "- [pytorch-autoencoder.ipynb](../pytorch-autoencoder.ipynb) - Original model training\n",
    "\n",
    "**Production Enhancements:**\n",
    "1. **Set up scheduled queries** for daily/hourly predictions\n",
    "2. **Create materialized views** for fast dashboard queries\n",
    "3. **Add monitoring** for prediction drift\n",
    "4. **Implement alerting** for high-risk predictions\n",
    "\n",
    "**Advanced Topics:**\n",
    "- Continuous queries for real-time scoring\n",
    "- Model versioning and A/B testing\n",
    "- Integration with BI tools\n",
    "\n",
    "### Resources\n",
    "\n",
    "**Documentation:**\n",
    "- [BigQuery ML ONNX Models](https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-syntax-create-onnx)\n",
    "- [ML.PREDICT() Function](https://cloud.google.com/bigquery/docs/reference/standard-sql/bigqueryml-syntax-predict)\n",
    "- [ONNX Documentation](https://onnx.ai/)\n",
    "\n",
    "**Tutorials:**\n",
    "- [Import ONNX models to BigQuery ML](https://cloud.google.com/bigquery-ml/docs/create-onnx-model)\n",
    "- [PyTorch to ONNX Export](https://pytorch.org/tutorials/advanced/super_resolution_with_onnxruntime.html)\n",
    "\n",
    "---\n",
    "\n",
    "**Questions or feedback?**\n",
    "- Open an issue: [GitHub Issues](https://github.com/statmike/vertex-ai-mlops/issues)\n",
    "- Connect: [LinkedIn](https://www.linkedin.com/in/statmike) | [Twitter/X](https://x.com/statmike) | [BlueSky](https://bsky.app/profile/statmike.bsky.social)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (frameworks-pytorch)",
   "language": "python",
   "name": "frameworks-pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
