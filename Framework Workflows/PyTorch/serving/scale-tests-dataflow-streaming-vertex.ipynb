{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![tracker](https://us-central1-vertex-ai-mlops-369716.cloudfunctions.net/pixel-tracking?path=statmike%2Fvertex-ai-mlops%2FFramework+Workflows%2FPyTorch%2Fserving&file=scale-tests-dataflow-streaming-vertex.ipynb)\n",
    "<!--- header table --->\n",
    "<table align=\"left\">\n",
    "<tr>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%20Workflows/PyTorch/serving/scale-tests-dataflow-streaming-vertex.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.svgrepo.com/download/217753/github.svg\" alt=\"GitHub logo\">\n",
    "      <br>View on<br>GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://colab.research.google.com/github/statmike/vertex-ai-mlops/blob/main/Framework%20Workflows/PyTorch/serving/scale-tests-dataflow-streaming-vertex.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.gstatic.com/pantheon/images/bigquery/welcome_page/colab-logo.svg\" alt=\"Google Colaboratory logo\">\n",
    "      <br>Run in<br>Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https%3A%2F%2Fraw.githubusercontent.com%2Fstatmike%2Fvertex-ai-mlops%2Fmain%2FFramework%2520Workflows%2FPyTorch%2Fserving%2Fscale-tests-dataflow-streaming-vertex.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\">\n",
    "      <br>Run in<br>Colab Enterprise\n",
    "    </a>\n",
    "  </td>      \n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/bigquery/import?url=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%20Workflows/PyTorch/serving/scale-tests-dataflow-streaming-vertex.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.gstatic.com/images/branding/gcpiconscolors/bigquery/v1/32px.svg\" alt=\"BigQuery logo\">\n",
    "      <br>Open in<br>BigQuery Studio\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/statmike/vertex-ai-mlops/main/Framework%20Workflows/PyTorch/serving/scale-tests-dataflow-streaming-vertex.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.gstatic.com/images/branding/gcpiconscolors/vertexai/v1/32px.svg\" alt=\"Vertex AI logo\">\n",
    "      <br>Open in<br>Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td colspan=\"5\" style=\"text-align: right\">\n",
    "    <b>Share This On: </b> \n",
    "    <a href=\"https://www.linkedin.com/sharing/share-offsite/?url=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%2520Workflows/PyTorch/serving/scale-tests-dataflow-streaming-vertex.ipynb\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"Linkedin Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://reddit.com/submit?url=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%2520Workflows/PyTorch/serving/scale-tests-dataflow-streaming-vertex.ipynb\"><img src=\"https://redditinc.com/hubfs/Reddit%20Inc/Brand/Reddit_Logo.png\" alt=\"Reddit Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://bsky.app/intent/compose?text=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%2520Workflows/PyTorch/serving/scale-tests-dataflow-streaming-vertex.ipynb\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"BlueSky Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://twitter.com/intent/tweet?url=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%2520Workflows/PyTorch/serving/scale-tests-dataflow-streaming-vertex.ipynb\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/5/5a/X_icon_2.svg\" alt=\"X (Twitter) Logo\" width=\"20px\"></a> \n",
    "  </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td colspan=\"5\" style=\"text-align: right\">\n",
    "    <b>Connect With Author On: </b> \n",
    "    <a href=\"https://www.linkedin.com/in/statmike\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"Linkedin Logo\" width=\"20px\"></a>\n",
    "    <a href=\"https://www.github.com/statmike\"><img src=\"https://www.svgrepo.com/download/217753/github.svg\" alt=\"GitHub Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://www.youtube.com/@statmike-channel\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/f/fd/YouTube_full-color_icon_%282024%29.svg\" alt=\"YouTube Logo\" width=\"20px\"></a>\n",
    "    <a href=\"https://bsky.app/profile/statmike.bsky.social\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"BlueSky Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://x.com/statmike\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/5/5a/X_icon_2.svg\" alt=\"X (Twitter) Logo\" width=\"20px\"></a>\n",
    "  </td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataflow Streaming + Vertex AI Endpoint: Combined Scale Testing\n",
    "\n",
    "**Comprehensive system performance analysis and bottleneck identification for Dataflow pipelines calling Vertex AI endpoints.**\n",
    "\n",
    "---\n",
    "\n",
    "## What This Notebook Does\n",
    "\n",
    "This notebook provides comprehensive testing of the **combined system** where Dataflow Streaming calls a Vertex AI Endpoint via RunInference. Unlike the individual tests ([Endpoint-only](./scale-tests-vertex-ai-endpoints.ipynb) and [Dataflow-only](./scale-tests-dataflow-streaming-runinference.ipynb)), this focuses on:\n",
    "\n",
    "**System-Level Questions:**\n",
    "- \ud83d\udd0d **Which component is the bottleneck?** (Dataflow workers vs Vertex endpoint)\n",
    "- \u2696\ufe0f **What's the optimal worker-to-replica ratio?** (Cost vs performance)\n",
    "- \ud83c\udfaf **How should we configure both services together?** (Unified recommendations)\n",
    "- \ud83d\udcb0 **What's the total system cost at different scales?** (Combined pricing)\n",
    "\n",
    "## Testing Approach\n",
    "\n",
    "**Phase 0: Endpoint Health Check** (~2 minutes)\n",
    "- Quick baseline test (10 msg/sec for 2 mins)\n",
    "- Verify endpoint is responding through pipeline\n",
    "- Reference: Uses endpoint testing from [scale-tests-vertex-ai-endpoints.ipynb](./scale-tests-vertex-ai-endpoints.ipynb)\n",
    "\n",
    "**Phase 1: Baseline Combined Performance** (~5 minutes)\n",
    "- Low load tests (10-25 msg/sec)\n",
    "- Measure baseline latency for both services\n",
    "- Establish healthy operating parameters\n",
    "\n",
    "**Phase 2: Bottleneck Identification** (~30 minutes)\n",
    "- **2A: Stress Dataflow** - High message rate, small endpoint batches\n",
    "- **2B: Stress Endpoint** - Moderate message rate, large endpoint batches\n",
    "- **2C: Balanced Ramp** - Gradual increase to find equilibrium\n",
    "\n",
    "**Phase 3: Optimal Configuration Discovery** (~20 minutes)\n",
    "- Test different worker-to-replica ratios\n",
    "- Find cost-optimal configurations\n",
    "- Generate production recommendations\n",
    "\n",
    "**Total test time**: ~60-70 minutes\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before running this notebook, you need:\n",
    "\n",
    "- **Running Dataflow Streaming Job** from [dataflow-streaming-runinference-vertex.ipynb](./dataflow-streaming-runinference-vertex.ipynb)\n",
    "  - Job must be in \"Running\" state\n",
    "  - Pipeline configured to call Vertex AI endpoint\n",
    "- **Deployed Vertex AI Endpoint**\n",
    "  - Endpoint must be healthy and responding\n",
    "  - Baseline tested in [scale-tests-vertex-ai-endpoints.ipynb](./scale-tests-vertex-ai-endpoints.ipynb)\n",
    "- **Testing utilities**: `scale_testing_combined_utils.py` in same directory\n",
    "\n",
    "## How This Differs from Individual Tests\n",
    "\n",
    "| Aspect | Endpoint Test | Dataflow Test | **Combined Test (This)** |\n",
    "|--------|---------------|---------------|---------------------------|\n",
    "| **Focus** | Endpoint capacity | Pipeline latency | System bottlenecks |\n",
    "| **Metrics** | CPU, replicas, latency | Workers, lag, backlog | **Both synchronized** |\n",
    "| **Bottleneck** | Client vs endpoint | Pipeline vs workers | **Dataflow vs Endpoint** |\n",
    "| **Configuration** | Replica scaling | Worker scaling | **Both together** |\n",
    "| **Cost** | Endpoint replicas | Dataflow workers | **Total system cost** |\n",
    "\n",
    "## Key Metrics Collected\n",
    "\n",
    "**From Dataflow (via Cloud Monitoring):**\n",
    "- Worker count (autoscaling events)\n",
    "- System lag (processing delay)\n",
    "- Backlog size (unprocessed messages)\n",
    "- Element throughput\n",
    "\n",
    "**From Vertex AI (via Cloud Monitoring):**\n",
    "- Replica count (autoscaling events)\n",
    "- CPU utilization (autoscaling trigger)\n",
    "- Memory usage\n",
    "- Service latency (P95)\n",
    "- Prediction rate\n",
    "\n",
    "**From Pipeline Output (via Pub/Sub):**\n",
    "- End-to-end pipeline latency\n",
    "- Window wait time\n",
    "- Processing time (**includes endpoint call**)\n",
    "- Queue wait time (test infrastructure overhead)\n",
    "\n",
    "## Bottleneck Detection Strategy\n",
    "\n",
    "Since we're using **Option A** (no pipeline instrumentation), bottleneck detection correlates multiple signals:\n",
    "\n",
    "**Dataflow Bottleneck Indicators:**\n",
    "- \u26a0\ufe0f High system lag (>10 seconds)\n",
    "- \u26a0\ufe0f Workers scaled but lag persists\n",
    "- \u26a0\ufe0f Low endpoint CPU (<40%)\n",
    "\n",
    "**Endpoint Bottleneck Indicators:**\n",
    "- \u26a0\ufe0f High endpoint CPU (>60%)\n",
    "- \u26a0\ufe0f Processing time dominates latency (>60% of total)\n",
    "- \u26a0\ufe0f Endpoint service latency high (>500ms)\n",
    "- \u26a0\ufe0f Low Dataflow lag (<5 seconds)\n",
    "\n",
    "**Balanced System:**\n",
    "- \u2705 Both services scaling proportionally\n",
    "- \u2705 Latency stable under load\n",
    "- \u2705 No excessive lag or CPU saturation\n",
    "\n",
    "## Related Notebooks\n",
    "\n",
    "**Individual Service Tests:**\n",
    "- [scale-tests-vertex-ai-endpoints.ipynb](./scale-tests-vertex-ai-endpoints.ipynb) - Endpoint-only testing\n",
    "- [scale-tests-dataflow-streaming-runinference.ipynb](./scale-tests-dataflow-streaming-runinference.ipynb) - Dataflow-only testing (local model)\n",
    "\n",
    "**Pipeline Deployment:**\n",
    "- [dataflow-streaming-runinference-vertex.ipynb](./dataflow-streaming-runinference-vertex.ipynb) - Deploy the pipeline tested here\n",
    "\n",
    "**Endpoint Deployment:**\n",
    "- [vertex-ai-endpoint-prebuilt-container.ipynb](./vertex-ai-endpoint-prebuilt-container.ipynb) - Deploy endpoint (pre-built)\n",
    "- [vertex-ai-endpoint-custom-container.ipynb](./vertex-ai-endpoint-custom-container.ipynb) - Deploy endpoint (custom)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Environment Setup\n",
    "\n",
    "This section will authenticate your session, enable required Google Cloud APIs, and install necessary Python packages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Your Project ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = 'statmike-mlops-349915' # replace with GCP project ID\n",
    "REQ_TYPE = 'ALL' # Specify PRIMARY or ALL or COLAB\n",
    "INSTALL_TOOL = 'poetry' # Specify pip, uv, or poetry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REQUIREMENTS_URL = 'https://raw.githubusercontent.com/statmike/vertex-ai-mlops/refs/heads/main/Framework%20Workflows/PyTorch/requirements.txt'\n",
    "\n",
    "REQUIRED_APIS = [\n",
    "    \"dataflow.googleapis.com\",\n",
    "    \"aiplatform.googleapis.com\",\n",
    "    \"pubsub.googleapis.com\",\n",
    "    \"monitoring.googleapis.com\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, urllib.request\n",
    "\n",
    "# Download and import setup code\n",
    "url = 'https://raw.githubusercontent.com/statmike/vertex-ai-mlops/refs/heads/main/core/notebook-template/python_setup.py'\n",
    "urllib.request.urlretrieve(url, 'python_setup_local.py')\n",
    "import python_setup_local as python_setup\n",
    "os.remove('python_setup_local.py')\n",
    "\n",
    "# Run setup\n",
    "setup_info = python_setup.setup_environment(PROJECT_ID, REQ_TYPE, REQUIREMENTS_URL, REQUIRED_APIS, INSTALL_TOOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Test Configuration\n",
    "\n",
    "Configure both the Dataflow job and Vertex AI endpoint to test, along with test parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "REGION = 'us-central1'\n",
    "EXPERIMENT = 'pytorch-autoencoder'\n",
    "\n",
    "# Dataflow Job Configuration\n",
    "JOB_NAME_PREFIX = 'pytorch-streaming-vertex-'  # Job name prefix from dataflow-streaming-runinference-vertex.ipynb\n",
    "\n",
    "# Vertex AI Endpoint Configuration  \n",
    "ENDPOINT_DISPLAY_NAME = 'pytorch-autoencoder-endpoint'  # Endpoint name\n",
    "\n",
    "# Pub/Sub Configuration (must match pipeline deployment)\n",
    "INPUT_TOPIC = f'projects/{PROJECT_ID}/topics/{EXPERIMENT}-input-vertex'\n",
    "OUTPUT_SUBSCRIPTION = f'projects/{PROJECT_ID}/subscriptions/{EXPERIMENT}-output-sub-vertex'\n",
    "\n",
    "# Phase 0: Endpoint Health Check (via pipeline)\n",
    "HEALTH_CHECK = {\n",
    "    \"target_rate\": 10,\n",
    "    \"duration\": 120,\n",
    "    \"name\": \"Health Check - 10 msg/sec\"\n",
    "}\n",
    "\n",
    "# Phase 1: Baseline Combined Performance\n",
    "BASELINE_TESTS = [\n",
    "    {\"target_rate\": 10, \"duration\": 120, \"name\": \"Baseline - 10 msg/sec\"},\n",
    "    {\"target_rate\": 25, \"duration\": 120, \"name\": \"Baseline - 25 msg/sec\"},\n",
    "]\n",
    "\n",
    "# Phase 2A: Stress Dataflow (overwhelm workers, endpoint idle)\n",
    "STRESS_DATAFLOW_TESTS = [\n",
    "    {\"target_rate\": 200, \"duration\": 300, \"name\": \"Stress Dataflow - 200 msg/sec\"},\n",
    "    {\"target_rate\": 500, \"duration\": 300, \"name\": \"Stress Dataflow - 500 msg/sec\"},\n",
    "]\n",
    "\n",
    "# Phase 2B: Stress Endpoint (overwhelm endpoint, workers idle)\n",
    "# Note: Pipeline batches messages before sending to endpoint\n",
    "# High message rate + batching = high load on endpoint\n",
    "STRESS_ENDPOINT_TESTS = [\n",
    "    {\"target_rate\": 50, \"duration\": 300, \"name\": \"Stress Endpoint - 50 msg/sec (batched)\"},\n",
    "    {\"target_rate\": 100, \"duration\": 300, \"name\": \"Stress Endpoint - 100 msg/sec (batched)\"},\n",
    "]\n",
    "\n",
    "# Phase 2C: Balanced Ramp (find equilibrium)\n",
    "RAMP_TEST = {\n",
    "    \"target_rate\": 200,\n",
    "    \"duration\": 900,  # 15 minutes\n",
    "    \"name\": \"Ramp - 1\u2192200 msg/sec\"\n",
    "}\n",
    "\n",
    "# Phase 3: Optimal Configuration Discovery\n",
    "# Based on Phase 2 results, test specific worker-replica combinations\n",
    "# These will be defined after Phase 2 analysis\n",
    "OPTIMIZATION_TESTS = [\n",
    "    {\"target_rate\": 100, \"duration\": 300, \"name\": \"Optimized - 100 msg/sec\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import asyncio\n",
    "import time\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "# Import combined testing utilities\n",
    "from scale_testing_combined_utils import (\n",
    "    CombinedMetricsCollector,\n",
    "    identify_bottleneck,\n",
    "    calculate_worker_replica_ratio,\n",
    "    plot_combined_timeline\n",
    ")\n",
    "\n",
    "# Import load generator from Dataflow utilities\n",
    "from scale_testing_dataflow_utils import PubSubLoadGenerator\n",
    "\n",
    "from google.cloud import dataflow_v1beta3, aiplatform\n",
    "\n",
    "print(\"\u2705 Imports complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Verify Infrastructure\n",
    "\n",
    "Before starting tests, verify both Dataflow job and Vertex AI endpoint are running and healthy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Dataflow Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find running Dataflow job\n",
    "dataflow_client = dataflow_v1beta3.JobsV1Beta3Client()\n",
    "\n",
    "print(\"Searching for running Dataflow jobs...\")\n",
    "request = dataflow_v1beta3.ListJobsRequest(\n",
    "    project_id=PROJECT_ID,\n",
    "    location=REGION\n",
    ")\n",
    "\n",
    "JOB_ID = None\n",
    "JOB_NAME = None\n",
    "\n",
    "try:\n",
    "    response = dataflow_client.list_jobs(request=request)\n",
    "    \n",
    "    for job in response:\n",
    "        if (job.current_state.name == \"JOB_STATE_RUNNING\" and\n",
    "            job.name.startswith(JOB_NAME_PREFIX)):\n",
    "            JOB_ID = job.id\n",
    "            JOB_NAME = job.name\n",
    "            break\n",
    "    \n",
    "    if not JOB_ID:\n",
    "        print(f\"\\n\u26a0\ufe0f  No running jobs found with prefix: {JOB_NAME_PREFIX}\")\n",
    "        print(\"   Please start the Dataflow Streaming job first\")\n",
    "        print(\"   See: dataflow-streaming-runinference-vertex.ipynb\")\n",
    "        raise ValueError(\"No running Dataflow job found\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"\\n\u26a0\ufe0f  Error finding Dataflow job: {e}\")\n",
    "    raise\n",
    "\n",
    "# Get full job details\n",
    "job = dataflow_client.get_job(\n",
    "    request={\n",
    "        \"project_id\": PROJECT_ID,\n",
    "        \"location\": REGION,\n",
    "        \"job_id\": JOB_ID\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"DATAFLOW JOB STATUS\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Name: {job.name}\")\n",
    "print(f\"ID: {job.id}\")\n",
    "print(f\"State: {job.current_state.name}\")\n",
    "print(f\"Created: {job.create_time}\")\n",
    "print(f\"\\n\u2705 Dataflow job is running\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nMonitor: https://console.cloud.google.com/dataflow/jobs/{REGION}/{JOB_ID}?project={PROJECT_ID}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Vertex AI Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find Vertex AI endpoint\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION)\n",
    "\n",
    "print(f\"Searching for endpoint: {ENDPOINT_DISPLAY_NAME}\")\n",
    "endpoints = aiplatform.Endpoint.list(\n",
    "    filter=f'display_name=\"{ENDPOINT_DISPLAY_NAME}\"',\n",
    "    order_by=\"create_time desc\"\n",
    ")\n",
    "\n",
    "if not endpoints:\n",
    "    raise ValueError(\n",
    "        f\"No endpoint found: {ENDPOINT_DISPLAY_NAME}\\n\"\n",
    "        f\"Please deploy an endpoint first using:\\n\"\n",
    "        f\"  - vertex-ai-endpoint-prebuilt-container.ipynb\\n\"\n",
    "        f\"  - vertex-ai-endpoint-custom-container.ipynb\"\n",
    "    )\n",
    "\n",
    "endpoint = endpoints[0]\n",
    "ENDPOINT_ID = endpoint.name.split(\"/\")[-1]\n",
    "\n",
    "# Get endpoint configuration\n",
    "deployed_model = endpoint.list_models()[0]\n",
    "MACHINE_TYPE = deployed_model.dedicated_resources.machine_spec.machine_type\n",
    "MIN_REPLICAS = deployed_model.dedicated_resources.min_replica_count\n",
    "MAX_REPLICAS = deployed_model.dedicated_resources.max_replica_count\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"VERTEX AI ENDPOINT STATUS\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Name: {endpoint.display_name}\")\n",
    "print(f\"ID: {ENDPOINT_ID}\")\n",
    "print(f\"Machine: {MACHINE_TYPE}\")\n",
    "print(f\"Replicas: {MIN_REPLICAS} - {MAX_REPLICAS}\")\n",
    "print(f\"\\n\u2705 Endpoint is deployed\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Testing Infrastructure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize load generator\n",
    "load_generator = PubSubLoadGenerator(\n",
    "    project_id=PROJECT_ID,\n",
    "    topic_name=f\"{EXPERIMENT}-input-vertex\"  # Vertex endpoint pipeline topic\n",
    ")\n",
    "\n",
    "# Initialize combined metrics collector\n",
    "metrics_collector = CombinedMetricsCollector(\n",
    "    project_id=PROJECT_ID,\n",
    "    dataflow_job_id=JOB_ID,\n",
    "    endpoint_id=ENDPOINT_ID,\n",
    "    region=REGION,\n",
    "    output_subscription=OUTPUT_SUBSCRIPTION\n",
    ")\n",
    "\n",
    "print(f\"\u2705 Testing infrastructure initialized\")\n",
    "print(f\"\\nConfiguration:\")\n",
    "print(f\"  Project: {PROJECT_ID}\")\n",
    "print(f\"  Dataflow Job: {JOB_NAME}\")\n",
    "print(f\"  Endpoint: {ENDPOINT_DISPLAY_NAME}\")\n",
    "print(f\"  Input topic: {INPUT_TOPIC}\")\n",
    "print(f\"  Output subscription: {OUTPUT_SUBSCRIPTION}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Phase 0: Endpoint Health Check\n",
    "\n",
    "**Goal**: Verify endpoint is responding through the pipeline\n",
    "\n",
    "**Test**: 1 quick test (~2 minutes)\n",
    "- 10 msg/sec for 2 minutes\n",
    "\n",
    "**What We're Looking For**:\n",
    "- Messages successfully processed through pipeline\n",
    "- Endpoint responding to RunInference calls\n",
    "- Baseline latency for combined system\n",
    "\n",
    "\u23f3 **This phase will take approximately 2 minutes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Phase 0 storage\n",
    "phase0_results = []\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PHASE 0: ENDPOINT HEALTH CHECK (VIA PIPELINE)\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Testing: {HEALTH_CHECK['target_rate']} msg/sec for {HEALTH_CHECK['duration']}s\")\n",
    "print(f\"Total estimated time: ~2 minutes\")\n",
    "print(\"=\" * 70 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run health check\n",
    "health_result = await load_generator.run_load_test(\n",
    "    pattern=\"sustained\",\n",
    "    target_rate=HEALTH_CHECK[\"target_rate\"],\n",
    "    duration=HEALTH_CHECK[\"duration\"],\n",
    "    test_name=HEALTH_CHECK[\"name\"]\n",
    ")\n",
    "phase0_results.append(health_result)\n",
    "\n",
    "print(f\"\\n\u2705 Phase 0 complete: Health check finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect metrics for health check\n",
    "phase0_start = health_result['start_time']\n",
    "phase0_end = health_result['end_time']\n",
    "\n",
    "print(f\"Collecting combined metrics for health check\")\n",
    "print(f\"Time window: {phase0_start.strftime('%H:%M:%S')} \u2192 {phase0_end.strftime('%H:%M:%S')}\")\n",
    "\n",
    "phase0_metrics = metrics_collector.collect_combined_metrics(\n",
    "    start_time=phase0_start,\n",
    "    end_time=phase0_end,\n",
    "    resolution_seconds=10\n",
    ")\n",
    "\n",
    "# Collect latency\n",
    "phase0_latency = metrics_collector.dataflow_collector.collect_end_to_end_latency(\n",
    "    test_id=health_result['test_id'],\n",
    "    expected_messages=health_result['num_messages'],\n",
    "    timeout=180\n",
    ")\n",
    "\n",
    "print(f\"\\n\u2705 Metrics collected for Phase 0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze health check results\n",
    "print(\"=\" * 70)\n",
    "print(\"PHASE 0 ANALYSIS: HEALTH CHECK\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\n{HEALTH_CHECK['name']}:\")\n",
    "print(f\"  Target: {HEALTH_CHECK['target_rate']} msg/sec for {HEALTH_CHECK['duration']}s\")\n",
    "print(f\"  Sent: {health_result['num_messages']:,} messages\")\n",
    "print(f\"  Actual rate: {health_result['actual_rate']:.1f} msg/sec\")\n",
    "\n",
    "if len(phase0_latency) > 0:\n",
    "    print(f\"\\n  Combined System Latency:\")\n",
    "    print(f\"    Pipeline Latency: {phase0_latency['pipeline_latency_ms'].mean():.1f}ms (mean) | {phase0_latency['pipeline_latency_ms'].quantile(0.95):.1f}ms (p95)\")\n",
    "    print(f\"    Window Wait:      {phase0_latency['window_wait_ms'].mean():.1f}ms (mean) | {phase0_latency['window_wait_ms'].quantile(0.95):.1f}ms (p95)\")\n",
    "    print(f\"    Processing:       {phase0_latency['processing_ms'].mean():.1f}ms (mean) | {phase0_latency['processing_ms'].quantile(0.95):.1f}ms (p95)\")\n",
    "    \n",
    "    print(f\"\\n  Note: Processing time includes endpoint call + Dataflow overhead\")\n",
    "    \n",
    "    # Check endpoint service latency from Cloud Monitoring\n",
    "    if 'endpoint' in phase0_metrics and 'latency' in phase0_metrics['endpoint'] and len(phase0_metrics['endpoint']['latency']) > 0:\n",
    "        endpoint_latency = phase0_metrics['endpoint']['latency']['value'].mean()\n",
    "        print(f\"  Endpoint service latency: {endpoint_latency:.1f}ms (from Cloud Monitoring)\")\n",
    "        \n",
    "        dataflow_overhead = phase0_latency['processing_ms'].mean() - endpoint_latency\n",
    "        print(f\"  Estimated Dataflow overhead: {dataflow_overhead:.1f}ms\")\n",
    "    \n",
    "    print(f\"\\n  \u2705 Endpoint is healthy and responding through pipeline\")\n",
    "else:\n",
    "    print(f\"\\n  \u26a0\ufe0f  No latency data collected\")\n",
    "    print(f\"     Check Dataflow job logs and endpoint status\")\n",
    "\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Phase 1: Baseline Combined Performance\n",
    "\n",
    "**Goal**: Establish baseline latency for the combined system at low load\n",
    "\n",
    "**Tests**: 2 baseline tests (~5 minutes total)\n",
    "- 10 msg/sec for 2 minutes\n",
    "- 25 msg/sec for 2 minutes\n",
    "\n",
    "**What We're Looking For**:\n",
    "- Baseline combined system latency\n",
    "- Worker and replica counts at low load\n",
    "- Confirm no immediate bottlenecks\n",
    "\n",
    "\u23f3 **This phase will take approximately 5 minutes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Initialize Phase 1 storage\nphase1_results = []\n\nprint(\"=\" * 70)\nprint(\"PHASE 1: BASELINE COMBINED PERFORMANCE\")\nprint(\"=\" * 70)\nprint(f\"Testing {len(BASELINE_TESTS)} baseline rates\")\nprint(f\"Total estimated time: ~5 minutes\")\nprint(\"=\" * 70 + \"\\n\")"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "phase1_init",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PHASE 1: BASELINE PERFORMANCE\n",
      "======================================================================\n",
      "Testing 2 baseline rates\n",
      "Total estimated time: ~5 minutes\n",
      "======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize Phase 1 storage",
    "phase1_results = []",
    "",
    "print(\"=\"*70)",
    "print(\"PHASE 1: BASELINE COMBINED PERFORMANCE\")",
    "print(\"=\"*70)",
    "print(f\"Testing {len(BASELINE_TESTS)} baseline rates\")",
    "print(f\"Total estimated time: ~5 minutes\")",
    "print(\"=\"*70 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "phase1_run",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Baseline - 10 msg/sec\n",
      "======================================================================\n",
      "Pattern: SUSTAINED\n",
      "Target: 10 messages/sec \u00d7 120s = 1,200 messages\n",
      "\n",
      "\u23f3 Running sustained load test (120s = 2 mins 0s)...\n",
      "   Progress updates every 60 seconds...\n",
      "   [ 60s] 601 messages sent...\n",
      "\n",
      "\u2705 Complete in 119.9s\n",
      "   Sent: 1,200 messages\n",
      "   Rate: 10.0 messages/sec\n",
      "\n",
      "   Waiting 30 seconds before next test...\n",
      "\n",
      "\n",
      "======================================================================\n",
      "Baseline - 25 msg/sec\n",
      "======================================================================\n",
      "Pattern: SUSTAINED\n",
      "Target: 25 messages/sec \u00d7 120s = 3,000 messages\n",
      "\n",
      "\u23f3 Running sustained load test (120s = 2 mins 0s)...\n",
      "   Progress updates every 60 seconds...\n",
      "   [ 60s] 1,501 messages sent...\n",
      "\n",
      "\u2705 Complete in 120.0s\n",
      "   Sent: 3,000 messages\n",
      "   Rate: 25.0 messages/sec\n",
      "\n",
      "   Waiting 30 seconds before next test...\n",
      "\n",
      "\n",
      "\u2705 Phase 1 complete: 2 baseline tests run\n"
     ]
    }
   ],
   "source": [
    "# Run baseline tests",
    "for test_config in BASELINE_TESTS:",
    "    result = await load_generator.run_load_test(",
    "        pattern=\"sustained\",",
    "        target_rate=test_config[\"target_rate\"],",
    "        duration=test_config[\"duration\"],",
    "        test_name=test_config[\"name\"]",
    "    )",
    "    phase1_results.append(result)",
    "    ",
    "    # Small delay between tests",
    "    print(\"\\n   Waiting 30 seconds before next test...\\n\")",
    "    await asyncio.sleep(30)",
    "",
    "print(f\"\\n\u2705 Phase 1 complete: {len(BASELINE_TESTS)} baseline tests run\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phase1_metrics_header",
   "metadata": {},
   "source": [
    "### Collect Metrics for Phase 1",
    "",
    "Collect Dataflow metrics (workers, system lag, backlog) and end-to-end latency from Pub/Sub output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "phase1_metrics",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting metrics for Phase 1\n",
      "Time window: 16:19:46 \u2192 16:24:16\n",
      "\n",
      "\ud83d\udd0d Collecting Dataflow metrics...\n",
      "   Time window: 16:19:46 \u2192 16:24:16\n",
      "   Waiting 90 seconds for metrics to propagate...\n",
      "   Collecting worker count...\n",
      "   Collecting system lag...\n",
      "   Collecting element count...\n",
      "   Collecting Pub/Sub backlog...\n",
      "\u2705 Metrics collection complete\n",
      "   workers: 6 data points\n",
      "   system_lag: 7 data points\n",
      "   element_count: 6 data points\n",
      "   backlog: 8 data points\n",
      "\n",
      "\u2705 Metrics collected for Phase 1\n"
     ]
    }
   ],
   "source": [
    "# Collect metrics for entire Phase 1",
    "phase1_start = min([r['start_time'] for r in phase1_results])",
    "phase1_end = max([r['end_time'] for r in phase1_results])",
    "",
    "print(f\"Collecting metrics for Phase 1\")",
    "print(f\"Time window: {phase1_start.strftime('%H:%M:%S')} \u2192 {phase1_end.strftime('%H:%M:%S')}\")",
    "",
    "phase1_metrics = metrics_collector.collect_combined_metrics(",
    "    start_time=phase1_start,",
    "    end_time=phase1_end,",
    "    resolution_seconds=10",
    ")",
    "",
    "print(f\"\\n\u2705 Metrics collected for Phase 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "phase1_latency",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\ud83d\udcca Collecting end-to-end latency from Pub/Sub output...\n",
      "   Subscription: projects/statmike-mlops-349915/subscriptions/pytorch-autoencoder-output-sub-local\n",
      "   Test ID: sustained-20251113-161946\n",
      "   Expected messages: 1,200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Collected 100/1,200 messages... (2s elapsed)\n",
      "   Collected 1,200/1,200 messages... (17s elapsed)\n",
      "\u2705 Latency collection complete: 1,200 messages\n",
      "\n",
      "\ud83d\udcca Collecting end-to-end latency from Pub/Sub output...\n",
      "   Subscription: projects/statmike-mlops-349915/subscriptions/pytorch-autoencoder-output-sub-local\n",
      "   Test ID: sustained-20251113-162216\n",
      "   Expected messages: 3,000\n",
      "   Collected 100/3,000 messages... (1s elapsed)\n",
      "   Collected 200/3,000 messages... (2s elapsed)\n",
      "   Collected 300/3,000 messages... (4s elapsed)\n",
      "   Collected 400/3,000 messages... (4s elapsed)\n",
      "   Collected 500/3,000 messages... (5s elapsed)\n",
      "   \u23f1\ufe0f  Overall timeout reached after 180s\n",
      "   Collected 2808/3000 messages before timeout\n",
      "\u2705 Latency collection complete: 2,808 messages\n",
      "\n",
      "\u2705 Latency data collected for 2 tests\n"
     ]
    }
   ],
   "source": [
    "# Collect end-to-end latency for each test",
    "phase1_latencies = []",
    "",
    "for result in phase1_results:",
    "    latency_df = metrics_collector.collect_end_to_end_latency(",
    "        test_id=result['test_id'],",
    "        expected_messages=result['num_messages'],",
    "        timeout=180",
    "    )",
    "    phase1_latencies.append(latency_df)",
    "",
    "print(f\"\\n\u2705 Latency data collected for {len(phase1_latencies)} tests\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phase1_analysis_header",
   "metadata": {},
   "source": [
    "### Analyze Phase 1 Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "phase1_analysis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PHASE 1 ANALYSIS\n",
      "======================================================================\n",
      "\n",
      "Baseline - 10 msg/sec:\n",
      "  Target: 10 msg/sec for 120s\n",
      "  Sent: 1,200 messages\n",
      "  Actual rate: 10.0 msg/sec\n",
      "\n",
      "  Latency Breakdown:\n",
      "    Window Wait:       490.7ms (mean) |   924.9ms (p95)\n",
      "    Processing:       -113.5ms (mean) |   708.2ms (p95)\n",
      "    Pub/Sub Delivery:338785.6ms (mean) | 386930.9ms (p95)\n",
      "    Total E2E:         377.2ms (mean) |   935.8ms (p95)\n",
      "\n",
      "  Latency Composition:\n",
      "    Window Wait: 130.1%\n",
      "    Processing: -30.1%\n",
      "    Pub/Sub Delivery: 0.0%\n",
      "\n",
      "Baseline - 25 msg/sec:\n",
      "  Target: 25 msg/sec for 120s\n",
      "  Sent: 3,000 messages\n",
      "  Actual rate: 25.0 msg/sec\n",
      "\n",
      "  Latency Breakdown:\n",
      "    Window Wait:       533.2ms (mean) |   971.8ms (p95)\n",
      "    Processing:       -197.8ms (mean) |   289.3ms (p95)\n",
      "    Pub/Sub Delivery:210136.1ms (mean) | 249393.9ms (p95)\n",
      "    Total E2E:         335.4ms (mean) |   605.6ms (p95)\n",
      "\n",
      "  Latency Composition:\n",
      "    Window Wait: 159.0%\n",
      "    Processing: -59.0%\n",
      "    Pub/Sub Delivery: 0.0%\n",
      "\n",
      "======================================================================\n",
      "AUTOSCALING ANALYSIS\n",
      "======================================================================\n",
      "\u2139\ufe0f  No autoscaling detected at baseline rates (expected)\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Analyze Phase 1 results",
    "print(\"=\"*70)",
    "print(\"PHASE 1 ANALYSIS\")",
    "print(\"=\"*70)",
    "",
    "for i, (result, latency_df) in enumerate(zip(phase1_results, phase1_latencies)):",
    "    test_config = BASELINE_TESTS[i]",
    "    ",
    "    print(f\"\\n{test_config['name']}:\")",
    "    print(f\"  Target: {test_config['target_rate']} msg/sec for {test_config['duration']}s\")",
    "    print(f\"  Sent: {result['num_messages']:,} messages\")",
    "    print(f\"  Actual rate: {result['actual_rate']:.1f} msg/sec\")",
    "    ",
    "    if len(latency_df) > 0:",
    "        print(f\"\\n  Latency Breakdown:\")",
    "        print(f\"    Window Wait:     {latency_df['window_wait_ms'].mean():7.1f}ms (mean) | {latency_df['window_wait_ms'].quantile(0.95):7.1f}ms (p95)\")",
    "        print(f\"    Processing:      {latency_df['processing_ms'].mean():7.1f}ms (mean) | {latency_df['processing_ms'].quantile(0.95):7.1f}ms (p95)\")",
    "        print(f\"    Pub/Sub Delivery:{latency_df['queue_wait_ms'].mean():7.1f}ms (mean) | {latency_df['queue_wait_ms'].quantile(0.95):7.1f}ms (p95)\")",
    "        print(f\"    Total E2E:       {latency_df['pipeline_latency_ms'].mean():7.1f}ms (mean) | {latency_df['pipeline_latency_ms'].quantile(0.95):7.1f}ms (p95)\")",
    "        ",
    "        # Identify bottleneck",
    "        window_pct = latency_df['window_wait_ms'].mean() / latency_df['pipeline_latency_ms'].mean() * 100",
    "        processing_pct = latency_df['processing_ms'].mean() / latency_df['pipeline_latency_ms'].mean() * 100",
    "        ",
    "        print(f\"\\n  Latency Composition:\")",
    "        print(f\"    Window Wait: {window_pct:.1f}%\")",
    "        print(f\"    Processing: {processing_pct:.1f}%\")",
    "        print(f\"    Pub/Sub Delivery: {100 - window_pct - processing_pct:.1f}%\")",
    "    else:",
    "        print(f\"\\n  \u26a0\ufe0f  No latency data collected (messages may not have been processed yet)\")",
    "",
    "# Check for autoscaling",
    "autoscaling_events = metrics_collector.analyze_autoscaling(phase1_metrics)",
    "",
    "print(f\"\\n\" + \"=\"*70)",
    "print(\"AUTOSCALING ANALYSIS\")",
    "print(\"=\"*70)",
    "",
    "if len(autoscaling_events) > 0:",
    "    print(f\"\u2705 Autoscaling triggered {len(autoscaling_events)} time(s)\")",
    "    for idx, event in autoscaling_events.iterrows():",
    "        print(f\"\\nEvent {idx + 1}:\")",
    "        print(f\"  Trigger: {event['trigger_time'].strftime('%H:%M:%S')}\")",
    "        print(f\"  Complete: {event['scale_complete_time'].strftime('%H:%M:%S')}\")",
    "        print(f\"  Workers: {event['workers_before']:.0f} \u2192 {event['workers_after']:.0f}\")",
    "        print(f\"  Lag: {event['scale_up_lag_seconds']:.0f}s ({event['scale_up_lag_seconds']/60:.1f} mins)\")",
    "else:",
    "    print(\"\u2139\ufe0f  No autoscaling detected at baseline rates (expected)\")",
    "",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4114b23b",
   "metadata": {},
   "source": [
    "### \ud83d\udcca How to Read the Visualization",
    "",
    "The timeline visualization below shows 5 panels that track different aspects of pipeline performance. Here's what each panel tells you:",
    "",
    "#### **Panel 1: Incoming Message Rate**",
    "- **What it shows:** Messages published per second to the input Pub/Sub topic",
    "- **What to look for:**",
    "  - Flat line = steady rate (good for controlled testing)",
    "  - Spikes = burst traffic",
    "  - Gradual increase = ramp test pattern",
    "- **Interpretation:** This is your **input load** - what you're sending to the pipeline",
    "",
    "#### **Panel 2: Worker Count (Autoscaling)**",
    "- **What it shows:** Number of active Dataflow workers over time",
    "- **What to look for:**",
    "  - Flat line = no autoscaling needed",
    "  - Step up = autoscaling triggered (workers being added)",
    "  - Step down = scale-down (workers being removed after low load period)",
    "- **Interpretation:** Shows when pipeline capacity changes. Worker provisioning takes **2-4 minutes**, so you'll see a delay between load increase and worker scale-up.",
    "",
    "#### **Panel 3: System Lag (Processing Delay: Oldest To Current Message)**",
    "- **What it shows:** How far behind real-time the pipeline is processing (in milliseconds)",
    "- **What to look for:**",
    "  - Near-zero lag = pipeline keeping up with real-time",
    "  - Increasing lag = pipeline falling behind",
    "  - High lag (>60s) = serious capacity issues",
    "- **Interpretation:** This is your **real-time health indicator**. Rising lag means you need more workers or better machine type.",
    "",
    "#### **Panel 4: Output Subscription Queue**",
    "- **What it shows:** Number of messages waiting in the output Pub/Sub subscription",
    "- **What to look for:**",
    "  - Zero or low = messages being consumed quickly",
    "  - Linearly increasing = messages accumulating (expected during testing)",
    "  - Very high (>10,000) = potential capacity issues",
    "- **Interpretation:** This backlog includes **both** pipeline processing queue AND messages waiting for the test framework to pull them. During tests, this will grow because we're not consuming as fast as we're producing.",
    "",
    "#### **Panel 5: P95 Pipeline Latency (publish \u2192 output queue, excludes queue wait)**",
    "- **What it shows:** 95th percentile latency from message publish to arriving in output queue",
    "- **What to look for:**",
    "  - Steady line = consistent performance",
    "  - Gradual increase = latency degrading under load",
    "  - Spikes at test end = shutdown artifacts (normal)",
    "- **Interpretation:** This is your **actual pipeline performance metric** - the one you care about most. It excludes the time messages sit in the queue waiting for the test to pull them.",
    "- **Y-axis note:** The axis is auto-scaled to focus on steady-state latency using the 99th percentile. Very high spikes (like test shutdown) may be clipped but won't distort the main trend.",
    "",
    "---",
    "",
    "**\ud83d\udca1 Key Insights to Extract:**",
    "1. **When does autoscaling trigger?** \u2192 Look for worker count step-up in Panel 2",
    "2. **What's the latency at different loads?** \u2192 Compare Panel 5 across different message rates in Panel 1",
    "3. **Is the pipeline keeping up?** \u2192 Panel 3 system lag should stay near zero",
    "4. **Where's the bottleneck?** \u2192 If latency is high but system lag is low, it's windowing. If system lag is high, it's processing capacity.",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "phase1_viz",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\ud83d\udd0d Collecting Dataflow metrics...\n",
      "   Time window: 16:19:46 \u2192 16:21:46\n",
      "   Waiting 90 seconds for metrics to propagate...\n",
      "   Collecting worker count...\n",
      "   Collecting system lag...\n",
      "   Collecting element count...\n",
      "   Collecting Pub/Sub backlog...\n",
      "\u2705 Metrics collection complete\n",
      "   workers: 6 data points\n",
      "   system_lag: 6 data points\n",
      "   element_count: 6 data points\n",
      "   backlog: 6 data points\n"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "line": {
          "color": "blue",
          "width": 2
         },
         "mode": "lines",
         "name": "Publish Rate",
         "type": "scatter",
         "x": [
          "2025-11-13T16:19:40",
          "2025-11-13T16:19:50",
          "2025-11-13T16:20:00",
          "2025-11-13T16:20:10",
          "2025-11-13T16:20:20",
          "2025-11-13T16:20:30",
          "2025-11-13T16:20:40",
          "2025-11-13T16:20:50",
          "2025-11-13T16:21:00",
          "2025-11-13T16:21:10",
          "2025-11-13T16:21:20",
          "2025-11-13T16:21:30",
          "2025-11-13T16:21:40"
         ],
         "xaxis": "x",
         "y": {
          "bdata": "mpmZmZmZDUAAAAAAAAAkQAAAAAAAACRAAAAAAAAAJEAAAAAAAAAkQAAAAAAAACRAAAAAAAAAJEAAAAAAAAAkQAAAAAAAACRAAAAAAAAAJEAAAAAAAAAkQAAAAAAAACRAMzMzMzMzGUA=",
          "dtype": "f8"
         },
         "yaxis": "y"
        },
        {
         "line": {
          "color": "green",
          "width": 2
         },
         "marker": {
          "size": 6
         },
         "mode": "lines+markers",
         "name": "Workers",
         "type": "scatter",
         "x": [
          "2025-11-13T16:18:06",
          "2025-11-13T16:19:06",
          "2025-11-13T16:20:06",
          "2025-11-13T16:21:06",
          "2025-11-13T16:22:06",
          "2025-11-13T16:23:06"
         ],
         "xaxis": "x2",
         "y": {
          "bdata": "AAAAAAAAEEAAAAAAAAAQQAAAAAAAABBAAAAAAAAAEEAAAAAAAAAQQAAAAAAAABBA",
          "dtype": "f8"
         },
         "yaxis": "y2"
        },
        {
         "line": {
          "color": "orange",
          "width": 2
         },
         "mode": "lines",
         "name": "System Lag",
         "type": "scatter",
         "x": [
          "2025-11-13T16:18:06",
          "2025-11-13T16:19:06",
          "2025-11-13T16:20:06",
          "2025-11-13T16:21:06",
          "2025-11-13T16:22:06",
          "2025-11-13T16:23:06"
         ],
         "xaxis": "x3",
         "y": {
          "bdata": "AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA/Knx0k1iUD/8qfHSTWJQP/yp8dJNYlA/",
          "dtype": "f8"
         },
         "yaxis": "y3"
        },
        {
         "line": {
          "color": "red",
          "width": 2
         },
         "mode": "lines",
         "name": "Backlog",
         "type": "scatter",
         "x": [
          "2025-11-13T16:18:06",
          "2025-11-13T16:19:06",
          "2025-11-13T16:20:06",
          "2025-11-13T16:21:06",
          "2025-11-13T16:22:06",
          "2025-11-13T16:23:06"
         ],
         "xaxis": "x4",
         "y": {
          "bdata": "AAAAAAAA8D8AAAAAAADwPwAAAAAAAPA/AAAAAAAA8D8AAAAAANB3QAAAAAAAII5A",
          "dtype": "f8"
         },
         "yaxis": "y4"
        },
        {
         "line": {
          "color": "purple",
          "width": 2
         },
         "mode": "lines",
         "name": "P95 Pipeline Latency",
         "type": "scatter",
         "x": [
          "2025-11-13T16:26:10",
          "2025-11-13T16:26:20",
          "2025-11-13T16:26:30"
         ],
         "xaxis": "x5",
         "y": {
          "bdata": "//9/KIRRjUAAAADW9DWNQPv//9uKJ4xA",
          "dtype": "f8"
         },
         "yaxis": "y5"
        }
       ],
       "layout": {
        "annotations": [
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Incoming Message Rate",
          "x": 0.5,
          "xanchor": "center",
          "xref": "paper",
          "y": 1,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Worker Count (Autoscaling)",
          "x": 0.5,
          "xanchor": "center",
          "xref": "paper",
          "y": 0.7879999999999999,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "System Lag (Processing Delay: Oldest To Current Message)",
          "x": 0.5,
          "xanchor": "center",
          "xref": "paper",
          "y": 0.576,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Output Subscription Queue",
          "x": 0.5,
          "xanchor": "center",
          "xref": "paper",
          "y": 0.364,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "P95 Pipeline Latency (publish \u2192 output queue, excludes queue wait)",
          "x": 0.5,
          "xanchor": "center",
          "xref": "paper",
          "y": 0.152,
          "yanchor": "bottom",
          "yref": "paper"
         }
        ],
        "height": 1200,
        "hovermode": "x unified",
        "showlegend": true,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermap": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermap"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Baseline - 10 msg/sec"
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "matches": "x5",
         "showticklabels": false
        },
        "xaxis2": {
         "anchor": "y2",
         "domain": [
          0,
          1
         ],
         "matches": "x5",
         "showticklabels": false
        },
        "xaxis3": {
         "anchor": "y3",
         "domain": [
          0,
          1
         ],
         "matches": "x5",
         "showticklabels": false
        },
        "xaxis4": {
         "anchor": "y4",
         "domain": [
          0,
          1
         ],
         "matches": "x5",
         "showticklabels": false
        },
        "xaxis5": {
         "anchor": "y5",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Time"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0.848,
          1
         ],
         "title": {
          "text": "Messages/sec"
         }
        },
        "yaxis2": {
         "anchor": "x2",
         "domain": [
          0.6359999999999999,
          0.7879999999999999
         ],
         "title": {
          "text": "Workers"
         }
        },
        "yaxis3": {
         "anchor": "x3",
         "domain": [
          0.424,
          0.576
         ],
         "title": {
          "text": "Lag (ms)"
         }
        },
        "yaxis4": {
         "anchor": "x4",
         "domain": [
          0.212,
          0.364
         ],
         "title": {
          "text": "Messages"
         }
        },
        "yaxis5": {
         "anchor": "x5",
         "domain": [
          0,
          0.152
         ],
         "range": [
          0,
          1125.744756889343
         ],
         "title": {
          "text": "Latency (ms)"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\ud83d\udd0d Collecting Dataflow metrics...\n",
      "   Time window: 16:22:16 \u2192 16:24:16\n",
      "   Waiting 90 seconds for metrics to propagate...\n",
      "   Collecting worker count...\n",
      "   Collecting system lag...\n",
      "   Collecting element count...\n",
      "   Collecting Pub/Sub backlog...\n",
      "\u2705 Metrics collection complete\n",
      "   workers: 6 data points\n",
      "   system_lag: 6 data points\n",
      "   element_count: 6 data points\n",
      "   backlog: 6 data points\n"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "line": {
          "color": "blue",
          "width": 2
         },
         "mode": "lines",
         "name": "Publish Rate",
         "type": "scatter",
         "x": [
          "2025-11-13T16:22:10",
          "2025-11-13T16:22:20",
          "2025-11-13T16:22:30",
          "2025-11-13T16:22:40",
          "2025-11-13T16:22:50",
          "2025-11-13T16:23:00",
          "2025-11-13T16:23:10",
          "2025-11-13T16:23:20",
          "2025-11-13T16:23:30",
          "2025-11-13T16:23:40",
          "2025-11-13T16:23:50",
          "2025-11-13T16:24:00",
          "2025-11-13T16:24:10"
         ],
         "xaxis": "x",
         "y": {
          "bdata": "mpmZmZmZIkAAAAAAAAA5QAAAAAAAADlAAAAAAAAAOUAAAAAAAAA5QAAAAAAAADlAAAAAAAAAOUAAAAAAAAA5QAAAAAAAADlAAAAAAAAAOUAAAAAAAAA5QAAAAAAAADlAZmZmZmZmL0A=",
          "dtype": "f8"
         },
         "yaxis": "y"
        },
        {
         "line": {
          "color": "green",
          "width": 2
         },
         "marker": {
          "size": 6
         },
         "mode": "lines+markers",
         "name": "Workers",
         "type": "scatter",
         "x": [
          "2025-11-13T16:21:06",
          "2025-11-13T16:22:06",
          "2025-11-13T16:23:06",
          "2025-11-13T16:24:06",
          "2025-11-13T16:25:06",
          "2025-11-13T16:26:06"
         ],
         "xaxis": "x2",
         "y": {
          "bdata": "AAAAAAAAEEAAAAAAAAAQQAAAAAAAABBAAAAAAAAAEEAAAAAAAAAQQAAAAAAAABBA",
          "dtype": "f8"
         },
         "yaxis": "y2"
        },
        {
         "line": {
          "color": "orange",
          "width": 2
         },
         "mode": "lines",
         "name": "System Lag",
         "type": "scatter",
         "x": [
          "2025-11-13T16:21:06",
          "2025-11-13T16:22:06",
          "2025-11-13T16:23:06",
          "2025-11-13T16:24:06",
          "2025-11-13T16:25:06",
          "2025-11-13T16:26:06"
         ],
         "xaxis": "x3",
         "y": {
          "bdata": "/Knx0k1iUD/8qfHSTWJQP/yp8dJNYlA/AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA",
          "dtype": "f8"
         },
         "yaxis": "y3"
        },
        {
         "line": {
          "color": "red",
          "width": 2
         },
         "mode": "lines",
         "name": "Backlog",
         "type": "scatter",
         "x": [
          "2025-11-13T16:21:06",
          "2025-11-13T16:22:06",
          "2025-11-13T16:23:06",
          "2025-11-13T16:24:06",
          "2025-11-13T16:25:06",
          "2025-11-13T16:26:06"
         ],
         "xaxis": "x4",
         "y": {
          "bdata": "AAAAAAAA8D8AAAAAANB3QAAAAAAAII5AAAAAAAConkAAAAAAAEyrQAAAAAAAabBA",
          "dtype": "f8"
         },
         "yaxis": "y4"
        },
        {
         "line": {
          "color": "purple",
          "width": 2
         },
         "mode": "lines",
         "name": "P95 Pipeline Latency",
         "type": "scatter",
         "x": [
          "2025-11-13T16:26:30",
          "2025-11-13T16:26:40",
          "2025-11-13T16:26:50",
          "2025-11-13T16:27:00"
         ],
         "xaxis": "x5",
         "y": {
          "bdata": "//+/h/SSgkD////9WoqCQP//P0QPT4NAAABAq+DGgkA=",
          "dtype": "f8"
         },
         "yaxis": "y5"
        }
       ],
       "layout": {
        "annotations": [
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Incoming Message Rate",
          "x": 0.5,
          "xanchor": "center",
          "xref": "paper",
          "y": 1,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Worker Count (Autoscaling)",
          "x": 0.5,
          "xanchor": "center",
          "xref": "paper",
          "y": 0.7879999999999999,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "System Lag (Processing Delay: Oldest To Current Message)",
          "x": 0.5,
          "xanchor": "center",
          "xref": "paper",
          "y": 0.576,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Output Subscription Queue",
          "x": 0.5,
          "xanchor": "center",
          "xref": "paper",
          "y": 0.364,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "P95 Pipeline Latency (publish \u2192 output queue, excludes queue wait)",
          "x": 0.5,
          "xanchor": "center",
          "xref": "paper",
          "y": 0.152,
          "yanchor": "bottom",
          "yref": "paper"
         }
        ],
        "height": 1200,
        "hovermode": "x unified",
        "showlegend": true,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermap": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermap"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Baseline - 25 msg/sec"
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "matches": "x5",
         "showticklabels": false
        },
        "xaxis2": {
         "anchor": "y2",
         "domain": [
          0,
          1
         ],
         "matches": "x5",
         "showticklabels": false
        },
        "xaxis3": {
         "anchor": "y3",
         "domain": [
          0,
          1
         ],
         "matches": "x5",
         "showticklabels": false
        },
        "xaxis4": {
         "anchor": "y4",
         "domain": [
          0,
          1
         ],
         "matches": "x5",
         "showticklabels": false
        },
        "xaxis5": {
         "anchor": "y5",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Time"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0.848,
          1
         ],
         "title": {
          "text": "Messages/sec"
         }
        },
        "yaxis2": {
         "anchor": "x2",
         "domain": [
          0.6359999999999999,
          0.7879999999999999
         ],
         "title": {
          "text": "Workers"
         }
        },
        "yaxis3": {
         "anchor": "x3",
         "domain": [
          0.424,
          0.576
         ],
         "title": {
          "text": "Lag (ms)"
         }
        },
        "yaxis4": {
         "anchor": "x4",
         "domain": [
          0.212,
          0.364
         ],
         "title": {
          "text": "Messages"
         }
        },
        "yaxis5": {
         "anchor": "x5",
         "domain": [
          0,
          0.152
         ],
         "range": [
          0,
          740.8461261749266
         ],
         "title": {
          "text": "Latency (ms)"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize Phase 1 - Individual test timelines",
    "for i, (result, latency_df) in enumerate(zip(phase1_results, phase1_latencies)):",
    "    test_config = BASELINE_TESTS[i]",
    "    ",
    "    # Collect metrics for this specific test",
    "    test_metrics = metrics_collector.collect_combined_metrics(",
    "        start_time=result['start_time'],",
    "        end_time=result['end_time'],",
    "        resolution_seconds=10",
    "    )",
    "    ",
    "    fig = plot_combined_timeline(",
    "        test_results=result,",
    "        combined_metrics=test_metrics,",
    "        latency_df=latency_df,",
    "        test_name=test_config['name']",
    "    )",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phase2a_header",
   "metadata": {},
   "source": [
    "---",
    "## Phase 2A: Throughput Threshold Hunt",
    "",
    "**Goal**: Find exact threshold where backlog builds and autoscaling triggers",
    "",
    "**Tests**: 3 sustained load tests (~20 minutes total)",
    "- 50 msg/sec for 5 minutes",
    "- 100 msg/sec for 5 minutes",
    "- 200 msg/sec for 5 minutes",
    "",
    "**What We're Looking For**:",
    "- At what message rate does backlog start building?",
    "- At what point does system lag increase?",
    "- When do workers autoscale up?",
    "- How long does it take for new workers to become active?",
    "",
    "\u23f3 **This phase will take approximately 20 minutes** (3 tests \u00d7 5 mins each + metrics collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "phase2a_init",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PHASE 2A: THROUGHPUT THRESHOLD HUNT\n",
      "======================================================================\n",
      "Testing 3 sustained load patterns\n",
      "Total estimated time: ~15 minutes\n",
      "\n",
      "This phase tests sustained high load to trigger autoscaling.\n",
      "======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize Phase 2A storage",
    "phase2a_results = []",
    "",
    "print(\"=\"*70)",
    "print(\"PHASE 2A: STRESS DATAFLOW\")",
    "print(\"=\"*70)",
    "print(f\"Testing {len(SUSTAINED_TESTS)} sustained load patterns\")",
    "print(f\"Total estimated time: ~{sum(t['duration'] for t in SUSTAINED_TESTS) // 60} minutes\")",
    "print(\"\\nThis phase tests sustained high load to trigger autoscaling.\")",
    "print(\"=\"*70 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "phase2a_run",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Sustained - 50 msg/sec\n",
      "======================================================================\n",
      "Pattern: SUSTAINED\n",
      "Target: 50 messages/sec \u00d7 300s = 15,000 messages\n",
      "\n",
      "\u23f3 Running sustained load test (300s = 5 mins 0s)...\n",
      "   Progress updates every 60 seconds...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   [ 60s] 3,001 messages sent...\n",
      "   [120s] 6,001 messages sent...\n",
      "   [180s] 9,001 messages sent...\n",
      "   [240s] 12,001 messages sent...\n",
      "\n",
      "\u2705 Complete in 300.0s\n",
      "   Sent: 15,000 messages\n",
      "   Rate: 50.0 messages/sec\n",
      "\n",
      "   Waiting 60 seconds before next test...\n",
      "\n",
      "\n",
      "======================================================================\n",
      "Sustained - 100 msg/sec\n",
      "======================================================================\n",
      "Pattern: SUSTAINED\n",
      "Target: 100 messages/sec \u00d7 300s = 30,000 messages\n",
      "\n",
      "\u23f3 Running sustained load test (300s = 5 mins 0s)...\n",
      "   Progress updates every 60 seconds...\n",
      "   [ 60s] 6,001 messages sent...\n",
      "   [120s] 12,001 messages sent...\n",
      "   [180s] 18,001 messages sent...\n",
      "   [240s] 24,001 messages sent...\n",
      "\n",
      "\u2705 Complete in 300.0s\n",
      "   Sent: 30,000 messages\n",
      "   Rate: 100.0 messages/sec\n",
      "\n",
      "   Waiting 60 seconds before next test...\n",
      "\n",
      "\n",
      "======================================================================\n",
      "Sustained - 200 msg/sec\n",
      "======================================================================\n",
      "Pattern: SUSTAINED\n",
      "Target: 200 messages/sec \u00d7 300s = 60,000 messages\n",
      "\n",
      "\u23f3 Running sustained load test (300s = 5 mins 0s)...\n",
      "   Progress updates every 60 seconds...\n",
      "   [ 60s] 12,001 messages sent...\n",
      "   [120s] 24,001 messages sent...\n",
      "   [180s] 36,001 messages sent...\n",
      "   [240s] 48,001 messages sent...\n",
      "\n",
      "\u2705 Complete in 300.0s\n",
      "   Sent: 60,000 messages\n",
      "   Rate: 200.0 messages/sec\n",
      "\n",
      "   Waiting 60 seconds before next test...\n",
      "\n",
      "\n",
      "\u2705 Phase 2A complete: 3 sustained tests run\n"
     ]
    }
   ],
   "source": [
    "# Run sustained load tests",
    "for test_config in SUSTAINED_TESTS:",
    "    result = await load_generator.run_load_test(",
    "        pattern=\"sustained\",",
    "        target_rate=test_config[\"target_rate\"],",
    "        duration=test_config[\"duration\"],",
    "        test_name=test_config[\"name\"]",
    "    )",
    "    phase2a_results.append(result)",
    "    ",
    "    # Small delay between tests to let pipeline stabilize",
    "    print(\"\\n   Waiting 60 seconds before next test...\\n\")",
    "    await asyncio.sleep(60)",
    "",
    "print(f\"\\n\u2705 Phase 2A complete: {len(SUSTAINED_TESTS)} sustained tests run\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phase2a_metrics_header",
   "metadata": {},
   "source": [
    "### Collect Metrics for Phase 2A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "phase2a_metrics",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting metrics for Phase 2A\n",
      "Time window: 16:32:54 \u2192 16:49:54\n",
      "\n",
      "\ud83d\udd0d Collecting Dataflow metrics...\n",
      "   Time window: 16:32:54 \u2192 16:49:54\n",
      "   Waiting 90 seconds for metrics to propagate...\n",
      "   Collecting worker count...\n",
      "   Collecting system lag...\n",
      "   Collecting element count...\n",
      "   Collecting Pub/Sub backlog...\n",
      "\u2705 Metrics collection complete\n",
      "   workers: 19 data points\n",
      "   system_lag: 20 data points\n",
      "   element_count: 19 data points\n",
      "   backlog: 21 data points\n",
      "\n",
      "\u2705 Metrics collected for Phase 2A\n"
     ]
    }
   ],
   "source": [
    "# Collect metrics for entire Phase 2A",
    "phase2a_start = min([r['start_time'] for r in phase2a_results])",
    "phase2a_end = max([r['end_time'] for r in phase2a_results])",
    "",
    "print(f\"Collecting metrics for Phase 2A\")",
    "print(f\"Time window: {phase2a_start.strftime('%H:%M:%S')} \u2192 {phase2a_end.strftime('%H:%M:%S')}\")",
    "",
    "phase2a_metrics = metrics_collector.collect_combined_metrics(",
    "    start_time=phase2a_start,",
    "    end_time=phase2a_end,",
    "    resolution_seconds=10",
    ")",
    "",
    "print(f\"\\n\u2705 Metrics collected for Phase 2A\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "phase2a_latency",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\ud83d\udcca Collecting end-to-end latency from Pub/Sub output...\n",
      "   Subscription: projects/statmike-mlops-349915/subscriptions/pytorch-autoencoder-output-sub-local\n",
      "   Test ID: sustained-20251113-163254\n",
      "   Expected messages: 15,000\n",
      "   Collected 100/15,000 messages... (2s elapsed)\n",
      "   Collected 15,000/15,000 messages... (89s elapsed)\n",
      "\u2705 Latency collection complete: 15,000 messages\n",
      "\n",
      "\ud83d\udcca Collecting end-to-end latency from Pub/Sub output...\n",
      "   Subscription: projects/statmike-mlops-349915/subscriptions/pytorch-autoencoder-output-sub-local\n",
      "   Test ID: sustained-20251113-163854\n",
      "   Expected messages: 30,000\n",
      "   Collected 100/30,000 messages... (0s elapsed)\n",
      "   Collected 200/30,000 messages... (0s elapsed)\n",
      "   Collected 300/30,000 messages... (0s elapsed)\n",
      "   Collected 400/30,000 messages... (0s elapsed)\n",
      "   Collected 500/30,000 messages... (0s elapsed)\n",
      "   Collected 600/30,000 messages... (1s elapsed)\n",
      "   Collected 700/30,000 messages... (1s elapsed)\n",
      "   Collected 800/30,000 messages... (2s elapsed)\n",
      "   Collected 900/30,000 messages... (2s elapsed)\n",
      "   Collected 1,000/30,000 messages... (3s elapsed)\n",
      "   Collected 1,100/30,000 messages... (4s elapsed)\n",
      "   Collected 1,200/30,000 messages... (4s elapsed)\n",
      "   Collected 1,300/30,000 messages... (5s elapsed)\n",
      "   Collected 1,400/30,000 messages... (5s elapsed)\n",
      "   Collected 1,500/30,000 messages... (6s elapsed)\n",
      "   Collected 1,600/30,000 messages... (6s elapsed)\n",
      "   Collected 1,700/30,000 messages... (6s elapsed)\n",
      "   Collected 1,800/30,000 messages... (6s elapsed)\n",
      "   Collected 1,900/30,000 messages... (7s elapsed)\n",
      "   Collected 2,000/30,000 messages... (8s elapsed)\n",
      "   Collected 2,100/30,000 messages... (8s elapsed)\n",
      "   Collected 2,200/30,000 messages... (9s elapsed)\n",
      "   Collected 2,300/30,000 messages... (9s elapsed)\n",
      "   \u23f1\ufe0f  Overall timeout reached after 300s\n",
      "   Collected 29956/30000 messages before timeout\n",
      "\u2705 Latency collection complete: 29,956 messages\n",
      "\n",
      "\ud83d\udcca Collecting end-to-end latency from Pub/Sub output...\n",
      "   Subscription: projects/statmike-mlops-349915/subscriptions/pytorch-autoencoder-output-sub-local\n",
      "   Test ID: sustained-20251113-164454\n",
      "   Expected messages: 60,000\n",
      "   \u23f1\ufe0f  Overall timeout reached after 300s\n",
      "   Collected 0/60000 messages before timeout\n",
      "\u2705 Latency collection complete: 0 messages\n",
      "\n",
      "\u2705 Latency data collected for 3 tests\n"
     ]
    }
   ],
   "source": [
    "# Collect end-to-end latency for each test",
    "phase2a_latencies = []",
    "",
    "for result in phase2a_results:",
    "    latency_df = metrics_collector.collect_end_to_end_latency(",
    "        test_id=result['test_id'],",
    "        expected_messages=result['num_messages'],",
    "        timeout=300  # Longer timeout for high-volume tests",
    "    )",
    "    phase2a_latencies.append(latency_df)",
    "",
    "print(f\"\\n\u2705 Latency data collected for {len(phase2a_latencies)} tests\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phase2a_analysis_header",
   "metadata": {},
   "source": [
    "### Analyze Phase 2A Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "phase2a_analysis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PHASE 2A ANALYSIS: THROUGHPUT THRESHOLDS\n",
      "======================================================================\n",
      "\n",
      "Sustained - 50 msg/sec:\n",
      "  Configuration: 50 msg/sec for 300s\n",
      "  Messages sent: 15,000\n",
      "  Actual rate: 50.0 msg/sec\n",
      "\n",
      "  Latency Performance:\n",
      "    Mean E2E:   437.1ms\n",
      "    P95 E2E:    683.7ms\n",
      "    P99 E2E:   1067.4ms\n",
      "\n",
      "Sustained - 100 msg/sec:\n",
      "  Configuration: 100 msg/sec for 300s\n",
      "  Messages sent: 30,000\n",
      "  Actual rate: 100.0 msg/sec\n",
      "\n",
      "  Latency Performance:\n",
      "    Mean E2E:   556.9ms\n",
      "    P95 E2E:    868.1ms\n",
      "    P99 E2E:   1040.3ms\n",
      "\n",
      "  Latency vs Baseline:\n",
      "    Baseline P95: 683.7ms\n",
      "    Current P95:  868.1ms\n",
      "    Change: +27.0%\n",
      "\n",
      "Sustained - 200 msg/sec:\n",
      "  Configuration: 200 msg/sec for 300s\n",
      "  Messages sent: 60,000\n",
      "  Actual rate: 200.0 msg/sec\n",
      "\n",
      "  \u26a0\ufe0f  No latency data (processing may be delayed)\n",
      "\n",
      "======================================================================\n",
      "AUTOSCALING EVENTS\n",
      "======================================================================\n",
      "\n",
      "\u2139\ufe0f  No autoscaling detected\n",
      "   Possible reasons:\n",
      "   - Message rate not high enough to trigger autoscaling\n",
      "   - Current worker capacity sufficient for tested load\n",
      "   - Processing is very efficient\n",
      "\n",
      "======================================================================\n",
      "BACKLOG ANALYSIS\n",
      "======================================================================\n",
      "\n",
      "Backlog Statistics:\n",
      "  Max backlog: 101,578 messages\n",
      "  Mean backlog: 31,050 messages\n",
      "\n",
      "  \u26a0\ufe0f  High backlog detected - pipeline may be at capacity\n",
      "\n",
      "======================================================================\n",
      "SYSTEM LAG ANALYSIS\n",
      "======================================================================\n",
      "\n",
      "System Lag Statistics:\n",
      "  Max lag: 0.0ms\n",
      "  Mean lag: 0.0ms\n",
      "\n",
      "  \u2705 System lag low - processing near real-time\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Detailed analysis of Phase 2A",
    "print(\"=\"*70)",
    "print(\"PHASE 2A ANALYSIS: THROUGHPUT THRESHOLDS\")",
    "print(\"=\"*70)",
    "",
    "for i, (result, latency_df) in enumerate(zip(phase2a_results, phase2a_latencies)):",
    "    test_config = SUSTAINED_TESTS[i]",
    "    ",
    "    print(f\"\\n{test_config['name']}:\")",
    "    print(f\"  Configuration: {test_config['target_rate']} msg/sec for {test_config['duration']}s\")",
    "    print(f\"  Messages sent: {result['num_messages']:,}\")",
    "    print(f\"  Actual rate: {result['actual_rate']:.1f} msg/sec\")",
    "    ",
    "    if len(latency_df) > 0:",
    "        print(f\"\\n  Latency Performance:\")",
    "        print(f\"    Mean E2E: {latency_df['pipeline_latency_ms'].mean():7.1f}ms\")",
    "        print(f\"    P95 E2E:  {latency_df['pipeline_latency_ms'].quantile(0.95):7.1f}ms\")",
    "        print(f\"    P99 E2E:  {latency_df['pipeline_latency_ms'].quantile(0.99):7.1f}ms\")",
    "        ",
    "        # Latency degradation check",
    "        if i > 0 and len(phase2a_latencies[0]) > 0:",
    "            baseline_p95 = phase2a_latencies[0]['pipeline_latency_ms'].quantile(0.95)",
    "            current_p95 = latency_df['pipeline_latency_ms'].quantile(0.95)",
    "            degradation_pct = ((current_p95 - baseline_p95) / baseline_p95) * 100",
    "            ",
    "            print(f\"\\n  Latency vs Baseline:\")",
    "            print(f\"    Baseline P95: {baseline_p95:.1f}ms\")",
    "            print(f\"    Current P95:  {current_p95:.1f}ms\")",
    "            print(f\"    Change: {degradation_pct:+.1f}%\")",
    "            ",
    "            if degradation_pct > 50:",
    "                print(f\"    \u26a0\ufe0f  Significant latency increase - may indicate capacity limits\")",
    "    else:",
    "        print(f\"\\n  \u26a0\ufe0f  No latency data (processing may be delayed)\")",
    "",
    "# Analyze autoscaling behavior",
    "print(f\"\\n\" + \"=\"*70)",
    "print(\"AUTOSCALING EVENTS\")",
    "print(\"=\"*70)",
    "",
    "autoscaling_events = metrics_collector.analyze_autoscaling(phase2a_metrics)",
    "",
    "if len(autoscaling_events) > 0:",
    "    print(f\"\\n\u2705 Autoscaling triggered {len(autoscaling_events)} time(s) during Phase 2A\\n\")",
    "    ",
    "    for idx, event in autoscaling_events.iterrows():",
    "        print(f\"Event {idx + 1}:\")",
    "        print(f\"  Trigger time:     {event['trigger_time'].strftime('%H:%M:%S')}\")",
    "        print(f\"  Complete time:    {event['scale_complete_time'].strftime('%H:%M:%S')}\")",
    "        print(f\"  Workers:          {event['workers_before']:.0f} \u2192 {event['workers_after']:.0f}\")",
    "        print(f\"  Provisioning lag: {event['scale_up_lag_seconds']:.0f}s ({event['scale_up_lag_seconds']/60:.1f} mins)\")",
    "        print()",
    "    ",
    "    # Calculate average provisioning time",
    "    avg_lag = autoscaling_events['scale_up_lag_seconds'].mean()",
    "    print(f\"Average worker provisioning time: {avg_lag:.0f}s ({avg_lag/60:.1f} mins)\")",
    "else:",
    "    print(\"\\n\u2139\ufe0f  No autoscaling detected\")",
    "    print(\"   Possible reasons:\")",
    "    print(\"   - Message rate not high enough to trigger autoscaling\")",
    "    print(\"   - Current worker capacity sufficient for tested load\")",
    "    print(\"   - Processing is very efficient\")",
    "",
    "# Analyze backlog behavior",
    "if 'backlog' in phase2a_metrics and len(phase2a_metrics['backlog']) > 0:",
    "    print(f\"\\n\" + \"=\"*70)",
    "    print(\"BACKLOG ANALYSIS\")",
    "    print(\"=\"*70)",
    "    ",
    "    max_backlog = phase2a_metrics['backlog']['value'].max()",
    "    mean_backlog = phase2a_metrics['backlog']['value'].mean()",
    "    ",
    "    print(f\"\\nBacklog Statistics:\")",
    "    print(f\"  Max backlog: {max_backlog:,.0f} messages\")",
    "    print(f\"  Mean backlog: {mean_backlog:,.0f} messages\")",
    "    ",
    "    if max_backlog > 1000:",
    "        print(f\"\\n  \u26a0\ufe0f  High backlog detected - pipeline may be at capacity\")",
    "    elif max_backlog > 100:",
    "        print(f\"\\n  \u2139\ufe0f  Moderate backlog - pipeline handling load but close to limits\")",
    "    else:",
    "        print(f\"\\n  \u2705 Backlog under control - pipeline has headroom\")",
    "",
    "# Analyze system lag",
    "if 'system_lag' in phase2a_metrics and len(phase2a_metrics['system_lag']) > 0:",
    "    print(f\"\\n\" + \"=\"*70)",
    "    print(\"SYSTEM LAG ANALYSIS\")",
    "    print(\"=\"*70)",
    "    ",
    "    max_lag_ms = phase2a_metrics['system_lag']['value'].max() / 1000  # Convert to ms",
    "    mean_lag_ms = phase2a_metrics['system_lag']['value'].mean() / 1000",
    "    ",
    "    print(f\"\\nSystem Lag Statistics:\")",
    "    print(f\"  Max lag: {max_lag_ms:,.1f}ms\")",
    "    print(f\"  Mean lag: {mean_lag_ms:,.1f}ms\")",
    "    ",
    "    if max_lag_ms > 60000:  # 1 minute",
    "        print(f\"\\n  \u26a0\ufe0f  High system lag - pipeline falling behind real-time\")",
    "    elif max_lag_ms > 10000:  # 10 seconds",
    "        print(f\"\\n  \u2139\ufe0f  Moderate system lag - processing slightly delayed\")",
    "    else:",
    "        print(f\"\\n  \u2705 System lag low - processing near real-time\")",
    "",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ac73ff",
   "metadata": {},
   "source": [
    "### \ud83d\udcca How to Read the Visualization",
    "",
    "The timeline visualization below shows 5 panels that track different aspects of pipeline performance. Here's what each panel tells you:",
    "",
    "#### **Panel 1: Incoming Message Rate**",
    "- **What it shows:** Messages published per second to the input Pub/Sub topic",
    "- **What to look for:**",
    "  - Flat line = steady rate (good for controlled testing)",
    "  - Spikes = burst traffic",
    "  - Gradual increase = ramp test pattern",
    "- **Interpretation:** This is your **input load** - what you're sending to the pipeline",
    "",
    "#### **Panel 2: Worker Count (Autoscaling)**",
    "- **What it shows:** Number of active Dataflow workers over time",
    "- **What to look for:**",
    "  - Flat line = no autoscaling needed",
    "  - Step up = autoscaling triggered (workers being added)",
    "  - Step down = scale-down (workers being removed after low load period)",
    "- **Interpretation:** Shows when pipeline capacity changes. Worker provisioning takes **2-4 minutes**, so you'll see a delay between load increase and worker scale-up.",
    "",
    "#### **Panel 3: System Lag (Processing Delay: Oldest To Current Message)**",
    "- **What it shows:** How far behind real-time the pipeline is processing (in milliseconds)",
    "- **What to look for:**",
    "  - Near-zero lag = pipeline keeping up with real-time",
    "  - Increasing lag = pipeline falling behind",
    "  - High lag (>60s) = serious capacity issues",
    "- **Interpretation:** This is your **real-time health indicator**. Rising lag means you need more workers or better machine type.",
    "",
    "#### **Panel 4: Output Subscription Queue**",
    "- **What it shows:** Number of messages waiting in the output Pub/Sub subscription",
    "- **What to look for:**",
    "  - Zero or low = messages being consumed quickly",
    "  - Linearly increasing = messages accumulating (expected during testing)",
    "  - Very high (>10,000) = potential capacity issues",
    "- **Interpretation:** This backlog includes **both** pipeline processing queue AND messages waiting for the test framework to pull them. During tests, this will grow because we're not consuming as fast as we're producing.",
    "",
    "#### **Panel 5: P95 Pipeline Latency (publish \u2192 output queue, excludes queue wait)**",
    "- **What it shows:** 95th percentile latency from message publish to arriving in output queue",
    "- **What to look for:**",
    "  - Steady line = consistent performance",
    "  - Gradual increase = latency degrading under load",
    "  - Spikes at test end = shutdown artifacts (normal)",
    "- **Interpretation:** This is your **actual pipeline performance metric** - the one you care about most. It excludes the time messages sit in the queue waiting for the test to pull them.",
    "- **Y-axis note:** The axis is auto-scaled to focus on steady-state latency using the 99th percentile. Very high spikes (like test shutdown) may be clipped but won't distort the main trend.",
    "",
    "---",
    "",
    "**\ud83d\udca1 Key Insights to Extract:**",
    "1. **When does autoscaling trigger?** \u2192 Look for worker count step-up in Panel 2",
    "2. **What's the latency at different loads?** \u2192 Compare Panel 5 across different message rates in Panel 1",
    "3. **Is the pipeline keeping up?** \u2192 Panel 3 system lag should stay near zero",
    "4. **Where's the bottleneck?** \u2192 If latency is high but system lag is low, it's windowing. If system lag is high, it's processing capacity.",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "phase2a_viz",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating comprehensive Phase 2A visualization...\n",
      "This shows all 3 sustained load tests in sequence\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1286823/1135105517.py:14: FutureWarning:\n",
      "\n",
      "The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "line": {
          "color": "blue",
          "width": 2
         },
         "mode": "lines",
         "name": "Publish Rate",
         "type": "scatter",
         "x": [
          "2025-11-13T16:32:50",
          "2025-11-13T16:33:00",
          "2025-11-13T16:33:10",
          "2025-11-13T16:33:20",
          "2025-11-13T16:33:30",
          "2025-11-13T16:33:40",
          "2025-11-13T16:33:50",
          "2025-11-13T16:34:00",
          "2025-11-13T16:34:10",
          "2025-11-13T16:34:20",
          "2025-11-13T16:34:30",
          "2025-11-13T16:34:40",
          "2025-11-13T16:34:50",
          "2025-11-13T16:35:00",
          "2025-11-13T16:35:10",
          "2025-11-13T16:35:20",
          "2025-11-13T16:35:30",
          "2025-11-13T16:35:40",
          "2025-11-13T16:35:50",
          "2025-11-13T16:36:00",
          "2025-11-13T16:36:10",
          "2025-11-13T16:36:20",
          "2025-11-13T16:36:30",
          "2025-11-13T16:36:40",
          "2025-11-13T16:36:50",
          "2025-11-13T16:37:00",
          "2025-11-13T16:37:10",
          "2025-11-13T16:37:20",
          "2025-11-13T16:37:30",
          "2025-11-13T16:37:40",
          "2025-11-13T16:37:50",
          "2025-11-13T16:38:50",
          "2025-11-13T16:39:00",
          "2025-11-13T16:39:10",
          "2025-11-13T16:39:20",
          "2025-11-13T16:39:30",
          "2025-11-13T16:39:40",
          "2025-11-13T16:39:50",
          "2025-11-13T16:40:00",
          "2025-11-13T16:40:10",
          "2025-11-13T16:40:20",
          "2025-11-13T16:40:30",
          "2025-11-13T16:40:40",
          "2025-11-13T16:40:50",
          "2025-11-13T16:41:00",
          "2025-11-13T16:41:10",
          "2025-11-13T16:41:20",
          "2025-11-13T16:41:30",
          "2025-11-13T16:41:40",
          "2025-11-13T16:41:50",
          "2025-11-13T16:42:00",
          "2025-11-13T16:42:10",
          "2025-11-13T16:42:20",
          "2025-11-13T16:42:30",
          "2025-11-13T16:42:40",
          "2025-11-13T16:42:50",
          "2025-11-13T16:43:00",
          "2025-11-13T16:43:10",
          "2025-11-13T16:43:20",
          "2025-11-13T16:43:30",
          "2025-11-13T16:43:40",
          "2025-11-13T16:43:50",
          "2025-11-13T16:44:50",
          "2025-11-13T16:45:00",
          "2025-11-13T16:45:10",
          "2025-11-13T16:45:20",
          "2025-11-13T16:45:30",
          "2025-11-13T16:45:40",
          "2025-11-13T16:45:50",
          "2025-11-13T16:46:00",
          "2025-11-13T16:46:10",
          "2025-11-13T16:46:20",
          "2025-11-13T16:46:30",
          "2025-11-13T16:46:40",
          "2025-11-13T16:46:50",
          "2025-11-13T16:47:00",
          "2025-11-13T16:47:10",
          "2025-11-13T16:47:20",
          "2025-11-13T16:47:30",
          "2025-11-13T16:47:40",
          "2025-11-13T16:47:50",
          "2025-11-13T16:48:00",
          "2025-11-13T16:48:10",
          "2025-11-13T16:48:20",
          "2025-11-13T16:48:30",
          "2025-11-13T16:48:40",
          "2025-11-13T16:48:50",
          "2025-11-13T16:49:00",
          "2025-11-13T16:49:10",
          "2025-11-13T16:49:20",
          "2025-11-13T16:49:30",
          "2025-11-13T16:49:40",
          "2025-11-13T16:49:50"
         ],
         "xaxis": "x",
         "y": {
          "bdata": "MzMzMzOzPEAAAAAAAABJQAAAAAAAAElAAAAAAAAASUAAAAAAAABJQAAAAAAAAElAAAAAAAAASUAAAAAAAABJQAAAAAAAAElAAAAAAAAASUAAAAAAAABJQAAAAAAAAElAAAAAAAAASUAAAAAAAABJQAAAAAAAAElAAAAAAAAASUAAAAAAAABJQAAAAAAAAElAAAAAAAAASUAAAAAAAABJQAAAAAAAAElAAAAAAAAASUAAAAAAAABJQAAAAAAAAElAAAAAAAAASUAAAAAAAABJQAAAAAAAAElAAAAAAAAASUAAAAAAAABJQAAAAAAAAElAzczMzMxMNUBmZmZmZmZMQAAAAAAAAFlAAAAAAAAAWUAAAAAAAABZQAAAAAAAAFlAAAAAAAAAWUAAAAAAAABZQAAAAAAAAFlAAAAAAAAAWUAAAAAAAABZQAAAAAAAAFlAAAAAAAAAWUAAAAAAAABZQAAAAAAAAFlAAAAAAAAAWUAAAAAAAABZQAAAAAAAAFlAAAAAAAAAWUAAAAAAAABZQAAAAAAAAFlAAAAAAAAAWUAAAAAAAABZQAAAAAAAAFlAAAAAAAAAWUAAAAAAAABZQAAAAAAAAFlAAAAAAAAAWUAAAAAAAABZQAAAAAAAAFlAAAAAAAAAWUCamZmZmZlFQGZmZmZmBlxAAAAAAAAAaUAAAAAAAABpQAAAAAAAAGlAAAAAAAAAaUAAAAAAAABpQAAAAAAAAGlAAAAAAAAAaUAAAAAAAABpQAAAAAAAAGlAAAAAAAAAaUAAAAAAAABpQAAAAAAAAGlAAAAAAAAAaUAAAAAAAABpQAAAAAAAAGlAAAAAAAAAaUAAAAAAAABpQAAAAAAAAGlAAAAAAAAAaUAAAAAAAABpQAAAAAAAAGlAAAAAAAAAaUAAAAAAAABpQAAAAAAAAGlAAAAAAAAAaUAAAAAAAABpQAAAAAAAAGlAAAAAAAAAaUAAAAAAAABpQJqZmZmZ+VVA",
          "dtype": "f8"
         },
         "yaxis": "y"
        },
        {
         "line": {
          "color": "green",
          "width": 2
         },
         "marker": {
          "size": 6
         },
         "mode": "lines+markers",
         "name": "Workers",
         "type": "scatter",
         "x": [
          "2025-11-13T16:31:04",
          "2025-11-13T16:32:04",
          "2025-11-13T16:33:04",
          "2025-11-13T16:34:04",
          "2025-11-13T16:35:04",
          "2025-11-13T16:36:04",
          "2025-11-13T16:37:04",
          "2025-11-13T16:38:04",
          "2025-11-13T16:39:04",
          "2025-11-13T16:40:04",
          "2025-11-13T16:41:04",
          "2025-11-13T16:42:04",
          "2025-11-13T16:43:04",
          "2025-11-13T16:44:04",
          "2025-11-13T16:45:04",
          "2025-11-13T16:46:04",
          "2025-11-13T16:47:04",
          "2025-11-13T16:48:04",
          "2025-11-13T16:49:04"
         ],
         "xaxis": "x2",
         "y": {
          "bdata": "AAAAAAAAEEAAAAAAAAAQQAAAAAAAABBAAAAAAAAAEEAAAAAAAAAQQAAAAAAAABBAAAAAAAAAEEAAAAAAAAAQQAAAAAAAABBAAAAAAAAAEEAAAAAAAAAQQAAAAAAAABBAAAAAAAAAEEAAAAAAAAAQQAAAAAAAABBAAAAAAAAAEEAAAAAAAAAQQAAAAAAAABBAAAAAAAAAEEA=",
          "dtype": "f8"
         },
         "yaxis": "y2"
        },
        {
         "line": {
          "color": "orange",
          "width": 2
         },
         "mode": "lines",
         "name": "System Lag",
         "type": "scatter",
         "x": [
          "2025-11-13T16:31:04",
          "2025-11-13T16:32:04",
          "2025-11-13T16:33:04",
          "2025-11-13T16:34:04",
          "2025-11-13T16:35:04",
          "2025-11-13T16:36:04",
          "2025-11-13T16:37:04",
          "2025-11-13T16:38:04",
          "2025-11-13T16:39:04",
          "2025-11-13T16:40:04",
          "2025-11-13T16:41:04",
          "2025-11-13T16:42:04",
          "2025-11-13T16:43:04",
          "2025-11-13T16:44:04",
          "2025-11-13T16:45:04",
          "2025-11-13T16:46:04",
          "2025-11-13T16:47:04",
          "2025-11-13T16:48:04",
          "2025-11-13T16:49:04",
          "2025-11-13T16:50:04"
         ],
         "xaxis": "x3",
         "y": {
          "bdata": "AAAAAAAAAAD8qfHSTWJQP/yp8dJNYlA//Knx0k1iYD/8qfHSTWJQP/yp8dJNYlA//Knx0k1iUD/8qfHSTWJQP/yp8dJNYlA//Knx0k1iUD/8qfHSTWJQP/yp8dJNYlA//Knx0k1iYD/8qfHSTWJgP/yp8dJNYlA/exSuR+F6dD97FK5H4XqEP/p+arx0k4g/ukkMAiuHhj956SYxCKyMPw==",
          "dtype": "f8"
         },
         "yaxis": "y3"
        },
        {
         "line": {
          "color": "red",
          "width": 2
         },
         "mode": "lines",
         "name": "Backlog",
         "type": "scatter",
         "x": [
          "2025-11-13T16:31:04",
          "2025-11-13T16:32:04",
          "2025-11-13T16:33:04",
          "2025-11-13T16:34:04",
          "2025-11-13T16:35:04",
          "2025-11-13T16:36:04",
          "2025-11-13T16:37:04",
          "2025-11-13T16:38:04",
          "2025-11-13T16:39:04",
          "2025-11-13T16:40:04",
          "2025-11-13T16:41:04",
          "2025-11-13T16:42:04",
          "2025-11-13T16:43:04",
          "2025-11-13T16:44:04",
          "2025-11-13T16:45:04",
          "2025-11-13T16:46:04",
          "2025-11-13T16:47:04",
          "2025-11-13T16:48:04",
          "2025-11-13T16:49:04",
          "2025-11-13T16:50:04",
          "2025-11-13T16:51:04"
         ],
         "xaxis": "x4",
         "y": {
          "bdata": "AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAObZAAAAAAID3wEAAAAAAANfGQAAAAAAAssxAAAAAAABMzUAAAAAAAOLTQAAAAACAvdlAAAAAAMCb30AAAAAAQLriQAAAAACAqeVAAAAAAAD55UAAAAAAwKzqQAAAAAAABvBAAAAAADAE80AAAAAAIOn1QAAAAACgzPhA",
          "dtype": "f8"
         },
         "yaxis": "y4"
        },
        {
         "line": {
          "color": "purple",
          "width": 2
         },
         "mode": "lines",
         "name": "P95 Pipeline Latency",
         "type": "scatter",
         "x": [
          "2025-11-13T16:52:20",
          "2025-11-13T16:52:30",
          "2025-11-13T16:52:40",
          "2025-11-13T16:52:50",
          "2025-11-13T16:53:00",
          "2025-11-13T16:53:10",
          "2025-11-13T16:53:20",
          "2025-11-13T16:53:30",
          "2025-11-13T16:53:40",
          "2025-11-13T16:53:50",
          "2025-11-13T16:54:00",
          "2025-11-13T16:54:10",
          "2025-11-13T16:54:20",
          "2025-11-13T16:54:30"
         ],
         "xaxis": "x5",
         "y": {
          "bdata": "AABgONOHpkADAACaEruQQAAAAHiRjIRAAADA/rFwhEAAAEAKg/CEQP//v9HipYJAAACA2kTtg0AAAAB3JsuEQAAAwMf2qIRAAACA9rNIiUAAAAAwenKKQP//v07EXYpAAAAAOkyki0AAAIAvG4iLQA==",
          "dtype": "f8"
         },
         "yaxis": "y5"
        }
       ],
       "layout": {
        "annotations": [
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Incoming Message Rate",
          "x": 0.5,
          "xanchor": "center",
          "xref": "paper",
          "y": 1,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Worker Count (Autoscaling)",
          "x": 0.5,
          "xanchor": "center",
          "xref": "paper",
          "y": 0.7879999999999999,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "System Lag (Processing Delay: Oldest To Current Message)",
          "x": 0.5,
          "xanchor": "center",
          "xref": "paper",
          "y": 0.576,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Output Subscription Queue",
          "x": 0.5,
          "xanchor": "center",
          "xref": "paper",
          "y": 0.364,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "P95 Pipeline Latency (publish \u2192 output queue, excludes queue wait)",
          "x": 0.5,
          "xanchor": "center",
          "xref": "paper",
          "y": 0.152,
          "yanchor": "bottom",
          "yref": "paper"
         }
        ],
        "height": 1200,
        "hovermode": "x unified",
        "showlegend": true,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermap": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermap"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Phase 2A: Throughput Threshold Hunt - Complete Timeline"
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "matches": "x5",
         "showticklabels": false
        },
        "xaxis2": {
         "anchor": "y2",
         "domain": [
          0,
          1
         ],
         "matches": "x5",
         "showticklabels": false
        },
        "xaxis3": {
         "anchor": "y3",
         "domain": [
          0,
          1
         ],
         "matches": "x5",
         "showticklabels": false
        },
        "xaxis4": {
         "anchor": "y4",
         "domain": [
          0,
          1
         ],
         "matches": "x5",
         "showticklabels": false
        },
        "xaxis5": {
         "anchor": "y5",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Time"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0.848,
          1
         ],
         "title": {
          "text": "Messages/sec"
         }
        },
        "yaxis2": {
         "anchor": "x2",
         "domain": [
          0.6359999999999999,
          0.7879999999999999
         ],
         "title": {
          "text": "Workers"
         }
        },
        "yaxis3": {
         "anchor": "x3",
         "domain": [
          0.424,
          0.576
         ],
         "title": {
          "text": "Lag (ms)"
         }
        },
        "yaxis4": {
         "anchor": "x4",
         "domain": [
          0.212,
          0.364
         ],
         "title": {
          "text": "Messages"
         }
        },
        "yaxis5": {
         "anchor": "x5",
         "domain": [
          0,
          0.152
         ],
         "range": [
          0,
          3177.8445250511154
         ],
         "title": {
          "text": "Latency (ms)"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\ud83d\udca1 This visualization shows:\n",
      "   - Message rate progression: 50 \u2192 200 msg/sec\n",
      "   - Worker scaling behavior over 3 tests\n",
      "   - System lag and backlog buildup\n",
      "   - P95 latency trends as load increases\n"
     ]
    }
   ],
   "source": [
    "# Visualize complete Phase 2A timeline",
    "print(\"Creating comprehensive Phase 2A visualization...\")",
    "print(f\"This shows all {len(SUSTAINED_TESTS)} sustained load tests in sequence\\n\")",
    "",
    "# Combine all test results and latencies",
    "combined_result = {",
    "    'test_id': 'phase2a-combined',",
    "    'test_name': 'Phase 2A - All Sustained Tests',",
    "    'start_time': phase2a_start,",
    "    'end_time': phase2a_end,",
    "    'message_data': pd.concat([r['message_data'] for r in phase2a_results], ignore_index=True)",
    "}",
    "",
    "combined_latency = pd.concat(phase2a_latencies, ignore_index=True)",
    "",
    "fig = plot_combined_timeline(",
    "    test_results=combined_result,",
    "    combined_metrics=phase2a_metrics,",
    "    latency_df=combined_latency,",
    "    test_name=\"Phase 2A: Throughput Threshold Hunt - Complete Timeline\"",
    ")",
    "",
    "fig.show()",
    "",
    "print(\"\\n\ud83d\udca1 This visualization shows:\")",
    "print(f\"   - Message rate progression: {SUSTAINED_TESTS[0]['target_rate']} \u2192 {SUSTAINED_TESTS[-1]['target_rate']} msg/sec\")",
    "print(f\"   - Worker scaling behavior over {len(SUSTAINED_TESTS)} tests\")",
    "print(f\"   - System lag and backlog buildup\")",
    "print(f\"   - P95 latency trends as load increases\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phase2b_header",
   "metadata": {},
   "source": [
    "---",
    "## Phase 2B: Burst Capacity",
    "",
    "**Goal**: Test backlog recovery time and worker scaling response to sudden spikes",
    "",
    "**Tests**: 3 burst tests (~10 minutes total)",
    "- 1,000 messages sent as fast as possible",
    "- 5,000 messages sent as fast as possible",
    "- 10,000 messages sent as fast as possible",
    "",
    "**What We're Looking For**:",
    "- How quickly does backlog clear after burst?",
    "- Does autoscaling respond to sudden traffic spikes?",
    "- What's the maximum burst size before sustained backlog?",
    "- Recovery time from burst to normal operation",
    "",
    "\u23f3 **This phase will take approximately 10 minutes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "phase2b_init",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PHASE 2B: BURST CAPACITY\n",
      "======================================================================\n",
      "Testing 3 burst patterns\n",
      "Total estimated time: ~10 minutes\n",
      "\n",
      "This phase tests sudden traffic spikes and recovery behavior.\n",
      "======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize Phase 2B storage",
    "phase2b_results = []",
    "",
    "print(\"=\"*70)",
    "print(\"PHASE 2B: STRESS ENDPOINT\")",
    "print(\"=\"*70)",
    "print(f\"Testing {len(BURST_TESTS)} burst patterns\")",
    "print(f\"Total estimated time: ~10 minutes\")",
    "print(\"\\nThis phase tests sudden traffic spikes and recovery behavior.\")",
    "print(\"=\"*70 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "phase2b_run",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Burst - 1,000 messages\n",
      "======================================================================\n",
      "Pattern: BURST\n",
      "Messages: 1,000\n",
      "\n",
      "\u23f3 Sending 1,000 messages as fast as possible...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Sent 1,000/1,000 messages...\n",
      "\n",
      "\u2705 Complete in 0.3s\n",
      "   Rate: 2886.1 messages/sec\n",
      "\n",
      "   Waiting 120 seconds for backlog to clear...\n",
      "\n",
      "\n",
      "======================================================================\n",
      "Burst - 5,000 messages\n",
      "======================================================================\n",
      "Pattern: BURST\n",
      "Messages: 5,000\n",
      "\n",
      "\u23f3 Sending 5,000 messages as fast as possible...\n",
      "   Sent 1,000/5,000 messages...\n",
      "   Sent 2,000/5,000 messages...\n",
      "   Sent 3,000/5,000 messages...\n",
      "   Sent 4,000/5,000 messages...\n",
      "   Sent 5,000/5,000 messages...\n",
      "\n",
      "\u2705 Complete in 0.6s\n",
      "   Rate: 7844.7 messages/sec\n",
      "\n",
      "   Waiting 120 seconds for backlog to clear...\n",
      "\n",
      "\n",
      "======================================================================\n",
      "Burst - 10,000 messages\n",
      "======================================================================\n",
      "Pattern: BURST\n",
      "Messages: 10,000\n",
      "\n",
      "\u23f3 Sending 10,000 messages as fast as possible...\n",
      "   Sent 1,000/10,000 messages...\n",
      "   Sent 2,000/10,000 messages...\n",
      "   Sent 3,000/10,000 messages...\n",
      "   Sent 4,000/10,000 messages...\n",
      "   Sent 5,000/10,000 messages...\n",
      "   Sent 6,000/10,000 messages...\n",
      "   Sent 7,000/10,000 messages...\n",
      "   Sent 8,000/10,000 messages...\n",
      "   Sent 9,000/10,000 messages...\n",
      "   Sent 10,000/10,000 messages...\n",
      "\n",
      "\u2705 Complete in 1.4s\n",
      "   Rate: 7231.6 messages/sec\n",
      "\n",
      "   Waiting 120 seconds for backlog to clear...\n",
      "\n",
      "\n",
      "\u2705 Phase 2B complete: 3 burst tests run\n"
     ]
    }
   ],
   "source": [
    "# Run burst tests",
    "for test_config in BURST_TESTS:",
    "    result = await load_generator.run_load_test(",
    "        pattern=\"burst\",",
    "        num_messages=test_config[\"num_messages\"],",
    "        test_name=test_config[\"name\"]",
    "    )",
    "    phase2b_results.append(result)",
    "    ",
    "    # Wait for backlog to clear before next burst",
    "    print(\"\\n   Waiting 120 seconds for backlog to clear...\\n\")",
    "    await asyncio.sleep(120)",
    "",
    "print(f\"\\n\u2705 Phase 2B complete: {len(BURST_TESTS)} burst tests run\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phase2b_metrics_header",
   "metadata": {},
   "source": [
    "### Collect Metrics for Phase 2B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "phase2b_metrics",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting metrics for Phase 2B\n",
      "Time window: 17:04:05 \u2192 17:08:08\n",
      "\n",
      "\ud83d\udd0d Collecting Dataflow metrics...\n",
      "   Time window: 17:04:05 \u2192 17:08:08\n",
      "   Waiting 90 seconds for metrics to propagate...\n",
      "   Collecting worker count...\n",
      "   Collecting system lag...\n",
      "   Collecting element count...\n",
      "   Collecting Pub/Sub backlog...\n",
      "\u2705 Metrics collection complete\n",
      "   workers: 8 data points\n",
      "   system_lag: 8 data points\n",
      "   element_count: 7 data points\n",
      "   backlog: 9 data points\n",
      "\n",
      "\u2705 Metrics collected for Phase 2B\n"
     ]
    }
   ],
   "source": [
    "# Collect metrics for entire Phase 2B",
    "phase2b_start = min([r['start_time'] for r in phase2b_results])",
    "phase2b_end = max([r['end_time'] for r in phase2b_results])",
    "",
    "print(f\"Collecting metrics for Phase 2B\")",
    "print(f\"Time window: {phase2b_start.strftime('%H:%M:%S')} \u2192 {phase2b_end.strftime('%H:%M:%S')}\")",
    "",
    "phase2b_metrics = metrics_collector.collect_combined_metrics(",
    "    start_time=phase2b_start,",
    "    end_time=phase2b_end,",
    "    resolution_seconds=10",
    ")",
    "",
    "print(f\"\\n\u2705 Metrics collected for Phase 2B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "phase2b_latency",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\ud83d\udcca Collecting end-to-end latency from Pub/Sub output...\n",
      "   Subscription: projects/statmike-mlops-349915/subscriptions/pytorch-autoencoder-output-sub-local\n",
      "   Test ID: burst-20251113-170405\n",
      "   Expected messages: 1,000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Collected 100/1,000 messages... (3s elapsed)\n",
      "   Collected 200/1,000 messages... (4s elapsed)\n",
      "   Collected 300/1,000 messages... (5s elapsed)\n",
      "   Collected 400/1,000 messages... (6s elapsed)\n",
      "   Collected 500/1,000 messages... (7s elapsed)\n",
      "   Collected 600/1,000 messages... (8s elapsed)\n",
      "   Collected 700/1,000 messages... (9s elapsed)\n",
      "   Collected 800/1,000 messages... (10s elapsed)\n",
      "   Collected 900/1,000 messages... (11s elapsed)\n",
      "   Collected 1,000/1,000 messages... (13s elapsed)\n",
      "\u2705 Latency collection complete: 1,000 messages\n",
      "\n",
      "\ud83d\udcca Collecting end-to-end latency from Pub/Sub output...\n",
      "   Subscription: projects/statmike-mlops-349915/subscriptions/pytorch-autoencoder-output-sub-local\n",
      "   Test ID: burst-20251113-170606\n",
      "   Expected messages: 5,000\n",
      "   Collected 100/5,000 messages... (1s elapsed)\n",
      "   Collected 200/5,000 messages... (2s elapsed)\n",
      "   Collected 300/5,000 messages... (3s elapsed)\n",
      "   Collected 400/5,000 messages... (5s elapsed)\n",
      "   Collected 500/5,000 messages... (6s elapsed)\n",
      "   Collected 600/5,000 messages... (7s elapsed)\n",
      "   Collected 700/5,000 messages... (8s elapsed)\n",
      "   Collected 800/5,000 messages... (9s elapsed)\n",
      "   Collected 900/5,000 messages... (10s elapsed)\n",
      "   Collected 1,000/5,000 messages... (11s elapsed)\n",
      "   Collected 1,100/5,000 messages... (12s elapsed)\n",
      "   Collected 1,200/5,000 messages... (13s elapsed)\n",
      "   Collected 1,300/5,000 messages... (14s elapsed)\n",
      "   Collected 1,400/5,000 messages... (15s elapsed)\n",
      "   Collected 1,500/5,000 messages... (16s elapsed)\n",
      "   Collected 1,600/5,000 messages... (17s elapsed)\n",
      "   Collected 1,700/5,000 messages... (18s elapsed)\n",
      "   Collected 1,800/5,000 messages... (19s elapsed)\n",
      "   Collected 1,900/5,000 messages... (20s elapsed)\n",
      "   Collected 2,000/5,000 messages... (20s elapsed)\n",
      "   Collected 2,100/5,000 messages... (21s elapsed)\n",
      "   Collected 2,200/5,000 messages... (22s elapsed)\n",
      "   Collected 2,300/5,000 messages... (23s elapsed)\n",
      "   Collected 2,400/5,000 messages... (24s elapsed)\n",
      "   Collected 2,500/5,000 messages... (25s elapsed)\n",
      "   Collected 2,600/5,000 messages... (26s elapsed)\n",
      "   Collected 2,700/5,000 messages... (27s elapsed)\n",
      "   Collected 2,800/5,000 messages... (28s elapsed)\n",
      "   Collected 2,900/5,000 messages... (29s elapsed)\n",
      "   Collected 3,000/5,000 messages... (29s elapsed)\n",
      "   Collected 3,100/5,000 messages... (30s elapsed)\n",
      "   Collected 3,200/5,000 messages... (31s elapsed)\n",
      "   Collected 3,300/5,000 messages... (31s elapsed)\n",
      "   Collected 3,400/5,000 messages... (32s elapsed)\n",
      "   Collected 3,500/5,000 messages... (33s elapsed)\n",
      "   Collected 3,600/5,000 messages... (33s elapsed)\n",
      "   Collected 3,700/5,000 messages... (34s elapsed)\n",
      "   Collected 3,800/5,000 messages... (35s elapsed)\n",
      "   Collected 3,900/5,000 messages... (36s elapsed)\n",
      "   Collected 4,000/5,000 messages... (37s elapsed)\n",
      "   Collected 4,100/5,000 messages... (37s elapsed)\n",
      "   Collected 4,200/5,000 messages... (38s elapsed)\n",
      "   Collected 4,300/5,000 messages... (38s elapsed)\n",
      "   Collected 4,400/5,000 messages... (38s elapsed)\n",
      "   Collected 4,500/5,000 messages... (40s elapsed)\n",
      "   Collected 4,600/5,000 messages... (41s elapsed)\n",
      "   Collected 4,700/5,000 messages... (42s elapsed)\n",
      "   Collected 4,800/5,000 messages... (42s elapsed)\n",
      "   Collected 4,900/5,000 messages... (42s elapsed)\n",
      "   Collected 5,000/5,000 messages... (42s elapsed)\n",
      "\u2705 Latency collection complete: 5,000 messages\n",
      "\n",
      "\ud83d\udcca Collecting end-to-end latency from Pub/Sub output...\n",
      "   Subscription: projects/statmike-mlops-349915/subscriptions/pytorch-autoencoder-output-sub-local\n",
      "   Test ID: burst-20251113-170806\n",
      "   Expected messages: 10,000\n",
      "   Collected 100/10,000 messages... (2s elapsed)\n",
      "   Collected 200/10,000 messages... (2s elapsed)\n",
      "   Collected 300/10,000 messages... (3s elapsed)\n",
      "   Collected 400/10,000 messages... (4s elapsed)\n",
      "   Collected 500/10,000 messages... (4s elapsed)\n",
      "   Collected 600/10,000 messages... (5s elapsed)\n",
      "   Collected 700/10,000 messages... (5s elapsed)\n",
      "   Collected 800/10,000 messages... (5s elapsed)\n",
      "   Collected 900/10,000 messages... (5s elapsed)\n",
      "   Collected 1,000/10,000 messages... (6s elapsed)\n",
      "   Collected 1,100/10,000 messages... (7s elapsed)\n",
      "   Collected 1,200/10,000 messages... (7s elapsed)\n",
      "   Collected 1,300/10,000 messages... (7s elapsed)\n",
      "   Collected 1,400/10,000 messages... (8s elapsed)\n",
      "   Collected 1,500/10,000 messages... (8s elapsed)\n",
      "   Collected 1,600/10,000 messages... (8s elapsed)\n",
      "   Collected 1,700/10,000 messages... (9s elapsed)\n",
      "   Collected 1,800/10,000 messages... (9s elapsed)\n",
      "   Collected 1,900/10,000 messages... (10s elapsed)\n",
      "   Collected 2,000/10,000 messages... (11s elapsed)\n",
      "   Collected 2,100/10,000 messages... (11s elapsed)\n",
      "   Collected 2,200/10,000 messages... (11s elapsed)\n",
      "   Collected 2,300/10,000 messages... (13s elapsed)\n",
      "   Collected 2,400/10,000 messages... (13s elapsed)\n",
      "   Collected 2,500/10,000 messages... (13s elapsed)\n",
      "   Collected 2,600/10,000 messages... (13s elapsed)\n",
      "   Collected 2,700/10,000 messages... (13s elapsed)\n",
      "   Collected 2,800/10,000 messages... (13s elapsed)\n",
      "   Collected 2,900/10,000 messages... (14s elapsed)\n",
      "   Collected 3,000/10,000 messages... (14s elapsed)\n",
      "   Collected 3,100/10,000 messages... (14s elapsed)\n",
      "   Collected 3,200/10,000 messages... (14s elapsed)\n",
      "   Collected 3,300/10,000 messages... (15s elapsed)\n",
      "   Collected 3,400/10,000 messages... (16s elapsed)\n",
      "   Collected 3,500/10,000 messages... (16s elapsed)\n",
      "   Collected 3,600/10,000 messages... (17s elapsed)\n",
      "   Collected 3,700/10,000 messages... (17s elapsed)\n",
      "   Collected 3,800/10,000 messages... (17s elapsed)\n",
      "   Collected 3,900/10,000 messages... (17s elapsed)\n",
      "   Collected 4,000/10,000 messages... (17s elapsed)\n",
      "   Collected 4,100/10,000 messages... (18s elapsed)\n",
      "   Collected 4,200/10,000 messages... (18s elapsed)\n",
      "   Collected 4,300/10,000 messages... (19s elapsed)\n",
      "   Collected 4,400/10,000 messages... (20s elapsed)\n",
      "   Collected 4,500/10,000 messages... (20s elapsed)\n",
      "   Collected 4,600/10,000 messages... (21s elapsed)\n",
      "   Collected 4,700/10,000 messages... (22s elapsed)\n",
      "   Collected 4,800/10,000 messages... (22s elapsed)\n",
      "   Collected 4,900/10,000 messages... (24s elapsed)\n",
      "   Collected 5,000/10,000 messages... (25s elapsed)\n",
      "   Collected 5,100/10,000 messages... (25s elapsed)\n",
      "   Collected 5,200/10,000 messages... (25s elapsed)\n",
      "   Collected 5,300/10,000 messages... (25s elapsed)\n",
      "   Collected 5,400/10,000 messages... (25s elapsed)\n",
      "   Collected 5,500/10,000 messages... (25s elapsed)\n",
      "   Collected 5,600/10,000 messages... (25s elapsed)\n",
      "   Collected 10,000/10,000 messages... (39s elapsed)\n",
      "\u2705 Latency collection complete: 10,000 messages\n",
      "\n",
      "\u2705 Latency data collected for 3 burst tests\n"
     ]
    }
   ],
   "source": [
    "# Collect end-to-end latency for burst tests",
    "phase2b_latencies = []",
    "",
    "for result in phase2b_results:",
    "    latency_df = metrics_collector.collect_end_to_end_latency(",
    "        test_id=result['test_id'],",
    "        expected_messages=result['num_messages'],",
    "        timeout=300",
    "    )",
    "    phase2b_latencies.append(latency_df)",
    "",
    "print(f\"\\n\u2705 Latency data collected for {len(phase2b_latencies)} burst tests\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phase2b_analysis_header",
   "metadata": {},
   "source": [
    "### Analyze Phase 2B Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "phase2b_analysis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PHASE 2B ANALYSIS: BURST CAPACITY & RECOVERY\n",
      "======================================================================\n",
      "\n",
      "Burst - 1,000 messages:\n",
      "  Messages: 1,000 sent as fast as possible\n",
      "  Publish time: 0.3s\n",
      "  Publish rate: 2886.1 msg/sec\n",
      "\n",
      "  Latency During Burst:\n",
      "    First message: 455,505.8ms\n",
      "    Last message:  465,190.3ms\n",
      "    Peak latency:  9,520.2ms\n",
      "    Mean latency:  4,913.5ms\n",
      "    P95 latency:   8,061.5ms\n",
      "\n",
      "  Latency Evolution:\n",
      "    First 1/3 mean:  4,900.9ms\n",
      "    Last 1/3 mean:   5,264.4ms\n",
      "    \u26a0\ufe0f  Latency degraded by 7.4% (backlog building)\n",
      "\n",
      "Burst - 5,000 messages:\n",
      "  Messages: 5,000 sent as fast as possible\n",
      "  Publish time: 0.6s\n",
      "  Publish rate: 7844.7 msg/sec\n",
      "\n",
      "  Latency During Burst:\n",
      "    First message: 346,259.2ms\n",
      "    Last message:  386,697.2ms\n",
      "    Peak latency:  22,141.3ms\n",
      "    Mean latency:  10,817.3ms\n",
      "    P95 latency:   16,535.5ms\n",
      "\n",
      "  Latency Evolution:\n",
      "    First 1/3 mean:  6,038.8ms\n",
      "    Last 1/3 mean:   15,734.8ms\n",
      "    \u26a0\ufe0f  Latency degraded by 160.6% (backlog building)\n",
      "\n",
      "Burst - 10,000 messages:\n",
      "  Messages: 10,000 sent as fast as possible\n",
      "  Publish time: 1.4s\n",
      "  Publish rate: 7231.6 msg/sec\n",
      "\n",
      "  Latency During Burst:\n",
      "    First message: 267,606.4ms\n",
      "    Last message:  305,000.5ms\n",
      "    Peak latency:  28,484.4ms\n",
      "    Mean latency:  14,076.1ms\n",
      "    P95 latency:   21,163.7ms\n",
      "\n",
      "  Latency Evolution:\n",
      "    First 1/3 mean:  8,910.6ms\n",
      "    Last 1/3 mean:   19,995.7ms\n",
      "    \u26a0\ufe0f  Latency degraded by 124.4% (backlog building)\n",
      "\n",
      "======================================================================\n",
      "AUTOSCALING RESPONSE TO BURSTS\n",
      "======================================================================\n",
      "\n",
      "\u2139\ufe0f  No autoscaling triggered by bursts\n",
      "   Bursts may be too short-lived to trigger autoscaling\n",
      "   Current workers may have sufficient capacity\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Analyze burst behavior and recovery",
    "print(\"=\"*70)",
    "print(\"PHASE 2B ANALYSIS: BURST CAPACITY & RECOVERY\")",
    "print(\"=\"*70)",
    "",
    "for i, (result, latency_df) in enumerate(zip(phase2b_results, phase2b_latencies)):",
    "    test_config = BURST_TESTS[i]",
    "    ",
    "    print(f\"\\n{test_config['name']}:\")",
    "    print(f\"  Messages: {test_config['num_messages']:,} sent as fast as possible\")",
    "    print(f\"  Publish time: {result['elapsed_seconds']:.1f}s\")",
    "    print(f\"  Publish rate: {result['actual_rate']:.1f} msg/sec\")",
    "    ",
    "    if len(latency_df) > 0:",
    "        # Calculate recovery metrics",
    "        first_msg_latency = latency_df.iloc[0]['total_e2e_ms']",
    "        last_msg_latency = latency_df.iloc[-1]['total_e2e_ms']",
    "        max_latency = latency_df['pipeline_latency_ms'].max()",
    "        ",
    "        print(f\"\\n  Latency During Burst:\")",
    "        print(f\"    First message: {first_msg_latency:,.1f}ms\")",
    "        print(f\"    Last message:  {last_msg_latency:,.1f}ms\")",
    "        print(f\"    Peak latency:  {max_latency:,.1f}ms\")",
    "        print(f\"    Mean latency:  {latency_df['pipeline_latency_ms'].mean():,.1f}ms\")",
    "        print(f\"    P95 latency:   {latency_df['pipeline_latency_ms'].quantile(0.95):,.1f}ms\")",
    "        ",
    "        # Analyze latency distribution (early vs late messages)",
    "        first_third = latency_df.iloc[:len(latency_df)//3]",
    "        last_third = latency_df.iloc[-len(latency_df)//3:]",
    "        ",
    "        print(f\"\\n  Latency Evolution:\")",
    "        print(f\"    First 1/3 mean:  {first_third['pipeline_latency_ms'].mean():,.1f}ms\")",
    "        print(f\"    Last 1/3 mean:   {last_third['pipeline_latency_ms'].mean():,.1f}ms\")",
    "        ",
    "        if last_third['pipeline_latency_ms'].mean() < first_third['pipeline_latency_ms'].mean():",
    "            improvement = ((first_third['pipeline_latency_ms'].mean() - last_third['pipeline_latency_ms'].mean()) / ",
    "                          first_third['pipeline_latency_ms'].mean() * 100)",
    "            print(f\"    \u2705 Latency improved by {improvement:.1f}% (backlog clearing)\")",
    "        else:",
    "            degradation = ((last_third['pipeline_latency_ms'].mean() - first_third['pipeline_latency_ms'].mean()) / ",
    "                          first_third['pipeline_latency_ms'].mean() * 100)",
    "            print(f\"    \u26a0\ufe0f  Latency degraded by {degradation:.1f}% (backlog building)\")",
    "    else:",
    "        print(f\"\\n  \u26a0\ufe0f  No latency data collected\")",
    "",
    "# Analyze burst autoscaling",
    "print(f\"\\n\" + \"=\"*70)",
    "print(\"AUTOSCALING RESPONSE TO BURSTS\")",
    "print(\"=\"*70)",
    "",
    "autoscaling_events = metrics_collector.analyze_autoscaling(phase2b_metrics)",
    "",
    "if len(autoscaling_events) > 0:",
    "    print(f\"\\n\u2705 Autoscaling responded {len(autoscaling_events)} time(s) to burst traffic\\n\")",
    "    ",
    "    for idx, event in autoscaling_events.iterrows():",
    "        print(f\"Event {idx + 1}:\")",
    "        print(f\"  Trigger: {event['trigger_time'].strftime('%H:%M:%S')}\")",
    "        print(f\"  Complete: {event['scale_complete_time'].strftime('%H:%M:%S')}\")",
    "        print(f\"  Workers: {event['workers_before']:.0f} \u2192 {event['workers_after']:.0f}\")",
    "        print(f\"  Response time: {event['scale_up_lag_seconds']:.0f}s\")",
    "        print()",
    "else:",
    "    print(\"\\n\u2139\ufe0f  No autoscaling triggered by bursts\")",
    "    print(\"   Bursts may be too short-lived to trigger autoscaling\")",
    "    print(\"   Current workers may have sufficient capacity\")",
    "",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798f210c",
   "metadata": {},
   "source": [
    "### \ud83d\udcca How to Read the Visualization",
    "",
    "The timeline visualization below shows 5 panels that track different aspects of pipeline performance. Here's what each panel tells you:",
    "",
    "#### **Panel 1: Incoming Message Rate**",
    "- **What it shows:** Messages published per second to the input Pub/Sub topic",
    "- **What to look for:**",
    "  - Flat line = steady rate (good for controlled testing)",
    "  - Spikes = burst traffic",
    "  - Gradual increase = ramp test pattern",
    "- **Interpretation:** This is your **input load** - what you're sending to the pipeline",
    "",
    "#### **Panel 2: Worker Count (Autoscaling)**",
    "- **What it shows:** Number of active Dataflow workers over time",
    "- **What to look for:**",
    "  - Flat line = no autoscaling needed",
    "  - Step up = autoscaling triggered (workers being added)",
    "  - Step down = scale-down (workers being removed after low load period)",
    "- **Interpretation:** Shows when pipeline capacity changes. Worker provisioning takes **2-4 minutes**, so you'll see a delay between load increase and worker scale-up.",
    "",
    "#### **Panel 3: System Lag (Processing Delay: Oldest To Current Message)**",
    "- **What it shows:** How far behind real-time the pipeline is processing (in milliseconds)",
    "- **What to look for:**",
    "  - Near-zero lag = pipeline keeping up with real-time",
    "  - Increasing lag = pipeline falling behind",
    "  - High lag (>60s) = serious capacity issues",
    "- **Interpretation:** This is your **real-time health indicator**. Rising lag means you need more workers or better machine type.",
    "",
    "#### **Panel 4: Output Subscription Queue**",
    "- **What it shows:** Number of messages waiting in the output Pub/Sub subscription",
    "- **What to look for:**",
    "  - Zero or low = messages being consumed quickly",
    "  - Linearly increasing = messages accumulating (expected during testing)",
    "  - Very high (>10,000) = potential capacity issues",
    "- **Interpretation:** This backlog includes **both** pipeline processing queue AND messages waiting for the test framework to pull them. During tests, this will grow because we're not consuming as fast as we're producing.",
    "",
    "#### **Panel 5: P95 Pipeline Latency (publish \u2192 output queue, excludes queue wait)**",
    "- **What it shows:** 95th percentile latency from message publish to arriving in output queue",
    "- **What to look for:**",
    "  - Steady line = consistent performance",
    "  - Gradual increase = latency degrading under load",
    "  - Spikes at test end = shutdown artifacts (normal)",
    "- **Interpretation:** This is your **actual pipeline performance metric** - the one you care about most. It excludes the time messages sit in the queue waiting for the test to pull them.",
    "- **Y-axis note:** The axis is auto-scaled to focus on steady-state latency using the 99th percentile. Very high spikes (like test shutdown) may be clipped but won't distort the main trend.",
    "",
    "---",
    "",
    "**\ud83d\udca1 Key Insights to Extract:**",
    "1. **When does autoscaling trigger?** \u2192 Look for worker count step-up in Panel 2",
    "2. **What's the latency at different loads?** \u2192 Compare Panel 5 across different message rates in Panel 1",
    "3. **Is the pipeline keeping up?** \u2192 Panel 3 system lag should stay near zero",
    "4. **Where's the bottleneck?** \u2192 If latency is high but system lag is low, it's windowing. If system lag is high, it's processing capacity.",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "phase2b_viz",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\ud83d\udd0d Collecting Dataflow metrics...\n",
      "   Time window: 17:04:05 \u2192 17:04:06\n",
      "   Waiting 90 seconds for metrics to propagate...\n",
      "   Collecting worker count...\n",
      "   Collecting system lag...\n",
      "   Collecting element count...\n",
      "   Collecting Pub/Sub backlog...\n",
      "\u2705 Metrics collection complete\n",
      "   workers: 5 data points\n",
      "   system_lag: 5 data points\n",
      "   element_count: 5 data points\n",
      "   backlog: 5 data points\n"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "line": {
          "color": "blue",
          "width": 2
         },
         "mode": "lines",
         "name": "Publish Rate",
         "type": "scatter",
         "x": [
          "2025-11-13T17:04:00"
         ],
         "xaxis": "x",
         "y": {
          "bdata": "AAAAAAAAWUA=",
          "dtype": "f8"
         },
         "yaxis": "y"
        },
        {
         "line": {
          "color": "green",
          "width": 2
         },
         "marker": {
          "size": 6
         },
         "mode": "lines+markers",
         "name": "Workers",
         "type": "scatter",
         "x": [
          "2025-11-13T17:02:06",
          "2025-11-13T17:03:06",
          "2025-11-13T17:04:06",
          "2025-11-13T17:05:06",
          "2025-11-13T17:06:06"
         ],
         "xaxis": "x2",
         "y": {
          "bdata": "AAAAAAAAEEAAAAAAAAAQQAAAAAAAABBAAAAAAAAAEEAAAAAAAAAQQA==",
          "dtype": "f8"
         },
         "yaxis": "y2"
        },
        {
         "line": {
          "color": "orange",
          "width": 2
         },
         "mode": "lines",
         "name": "System Lag",
         "type": "scatter",
         "x": [
          "2025-11-13T17:02:06",
          "2025-11-13T17:03:06",
          "2025-11-13T17:04:06",
          "2025-11-13T17:05:06",
          "2025-11-13T17:06:06"
         ],
         "xaxis": "x3",
         "y": {
          "bdata": "/Knx0k1iUD/8qfHSTWJQP/yp8dJNYlA//Knx0k1iUD/8qfHSTWJQPw==",
          "dtype": "f8"
         },
         "yaxis": "y3"
        },
        {
         "line": {
          "color": "red",
          "width": 2
         },
         "mode": "lines",
         "name": "Backlog",
         "type": "scatter",
         "x": [
          "2025-11-13T17:02:06",
          "2025-11-13T17:03:06",
          "2025-11-13T17:04:06",
          "2025-11-13T17:05:06",
          "2025-11-13T17:06:06"
         ],
         "xaxis": "x4",
         "y": {
          "bdata": "AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAECPQA==",
          "dtype": "f8"
         },
         "yaxis": "y4"
        },
        {
         "line": {
          "color": "purple",
          "width": 2
         },
         "mode": "lines",
         "name": "P95 Pipeline Latency",
         "type": "scatter",
         "x": [
          "2025-11-13T17:11:40",
          "2025-11-13T17:11:50"
         ],
         "xaxis": "x5",
         "y": {
          "bdata": "AADAe8aEvkAAAFBI+3LCQA==",
          "dtype": "f8"
         },
         "yaxis": "y5"
        }
       ],
       "layout": {
        "annotations": [
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Incoming Message Rate",
          "x": 0.5,
          "xanchor": "center",
          "xref": "paper",
          "y": 1,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Worker Count (Autoscaling)",
          "x": 0.5,
          "xanchor": "center",
          "xref": "paper",
          "y": 0.7879999999999999,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "System Lag (Processing Delay: Oldest To Current Message)",
          "x": 0.5,
          "xanchor": "center",
          "xref": "paper",
          "y": 0.576,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Output Subscription Queue",
          "x": 0.5,
          "xanchor": "center",
          "xref": "paper",
          "y": 0.364,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "P95 Pipeline Latency (publish \u2192 output queue, excludes queue wait)",
          "x": 0.5,
          "xanchor": "center",
          "xref": "paper",
          "y": 0.152,
          "yanchor": "bottom",
          "yref": "paper"
         }
        ],
        "height": 1200,
        "hovermode": "x unified",
        "showlegend": true,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermap": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermap"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Burst - 1,000 messages"
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "matches": "x5",
         "showticklabels": false
        },
        "xaxis2": {
         "anchor": "y2",
         "domain": [
          0,
          1
         ],
         "matches": "x5",
         "showticklabels": false
        },
        "xaxis3": {
         "anchor": "y3",
         "domain": [
          0,
          1
         ],
         "matches": "x5",
         "showticklabels": false
        },
        "xaxis4": {
         "anchor": "y4",
         "domain": [
          0,
          1
         ],
         "matches": "x5",
         "showticklabels": false
        },
        "xaxis5": {
         "anchor": "y5",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Time"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0.848,
          1
         ],
         "title": {
          "text": "Messages/sec"
         }
        },
        "yaxis2": {
         "anchor": "x2",
         "domain": [
          0.6359999999999999,
          0.7879999999999999
         ],
         "title": {
          "text": "Workers"
         }
        },
        "yaxis3": {
         "anchor": "x3",
         "domain": [
          0.424,
          0.576
         ],
         "title": {
          "text": "Lag (ms)"
         }
        },
        "yaxis4": {
         "anchor": "x4",
         "domain": [
          0.212,
          0.364
         ],
         "title": {
          "text": "Messages"
         }
        },
        "yaxis5": {
         "anchor": "x5",
         "domain": [
          0,
          0.152
         ],
         "range": [
          0,
          11315.557519340515
         ],
         "title": {
          "text": "Latency (ms)"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\ud83d\udd0d Collecting Dataflow metrics...\n",
      "   Time window: 17:06:06 \u2192 17:06:06\n",
      "   Waiting 90 seconds for metrics to propagate...\n",
      "   Collecting worker count...\n",
      "   Collecting system lag...\n",
      "   Collecting element count...\n",
      "   Collecting Pub/Sub backlog...\n",
      "\u2705 Metrics collection complete\n",
      "   workers: 4 data points\n",
      "   system_lag: 4 data points\n",
      "   element_count: 4 data points\n",
      "   backlog: 4 data points\n"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "line": {
          "color": "blue",
          "width": 2
         },
         "mode": "lines",
         "name": "Publish Rate",
         "type": "scatter",
         "x": [
          "2025-11-13T17:06:00"
         ],
         "xaxis": "x",
         "y": {
          "bdata": "AAAAAABAf0A=",
          "dtype": "f8"
         },
         "yaxis": "y"
        },
        {
         "line": {
          "color": "green",
          "width": 2
         },
         "marker": {
          "size": 6
         },
         "mode": "lines+markers",
         "name": "Workers",
         "type": "scatter",
         "x": [
          "2025-11-13T17:05:06",
          "2025-11-13T17:06:06",
          "2025-11-13T17:07:06",
          "2025-11-13T17:08:06"
         ],
         "xaxis": "x2",
         "y": {
          "bdata": "AAAAAAAAEEAAAAAAAAAQQAAAAAAAABBAAAAAAAAAEEA=",
          "dtype": "f8"
         },
         "yaxis": "y2"
        },
        {
         "line": {
          "color": "orange",
          "width": 2
         },
         "mode": "lines",
         "name": "System Lag",
         "type": "scatter",
         "x": [
          "2025-11-13T17:05:06",
          "2025-11-13T17:06:06",
          "2025-11-13T17:07:06",
          "2025-11-13T17:08:06"
         ],
         "xaxis": "x3",
         "y": {
          "bdata": "/Knx0k1iUD/8qfHSTWJQP/p+arx0k3g//Knx0k1iUD8=",
          "dtype": "f8"
         },
         "yaxis": "y3"
        },
        {
         "line": {
          "color": "red",
          "width": 2
         },
         "mode": "lines",
         "name": "Backlog",
         "type": "scatter",
         "x": [
          "2025-11-13T17:05:06",
          "2025-11-13T17:06:06",
          "2025-11-13T17:07:06",
          "2025-11-13T17:08:06"
         ],
         "xaxis": "x4",
         "y": {
          "bdata": "AAAAAAAAAAAAAAAAAECPQAAAAAAAQI9AAAAAAABwt0A=",
          "dtype": "f8"
         },
         "yaxis": "y4"
        },
        {
         "line": {
          "color": "purple",
          "width": 2
         },
         "mode": "lines",
         "name": "P95 Pipeline Latency",
         "type": "scatter",
         "x": [
          "2025-11-13T17:11:50",
          "2025-11-13T17:12:00",
          "2025-11-13T17:12:10",
          "2025-11-13T17:12:20",
          "2025-11-13T17:12:30"
         ],
         "xaxis": "x5",
         "y": {
          "bdata": "AADgLgBquEAAADiTYna6QAAA0NiP68xAAAAgP8svzkAAAEbZjWDVQA==",
          "dtype": "f8"
         },
         "yaxis": "y5"
        }
       ],
       "layout": {
        "annotations": [
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Incoming Message Rate",
          "x": 0.5,
          "xanchor": "center",
          "xref": "paper",
          "y": 1,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Worker Count (Autoscaling)",
          "x": 0.5,
          "xanchor": "center",
          "xref": "paper",
          "y": 0.7879999999999999,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "System Lag (Processing Delay: Oldest To Current Message)",
          "x": 0.5,
          "xanchor": "center",
          "xref": "paper",
          "y": 0.576,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Output Subscription Queue",
          "x": 0.5,
          "xanchor": "center",
          "xref": "paper",
          "y": 0.364,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "P95 Pipeline Latency (publish \u2192 output queue, excludes queue wait)",
          "x": 0.5,
          "xanchor": "center",
          "xref": "paper",
          "y": 0.152,
          "yanchor": "bottom",
          "yref": "paper"
         }
        ],
        "height": 1200,
        "hovermode": "x unified",
        "showlegend": true,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermap": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermap"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Burst - 5,000 messages"
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "matches": "x5",
         "showticklabels": false
        },
        "xaxis2": {
         "anchor": "y2",
         "domain": [
          0,
          1
         ],
         "matches": "x5",
         "showticklabels": false
        },
        "xaxis3": {
         "anchor": "y3",
         "domain": [
          0,
          1
         ],
         "matches": "x5",
         "showticklabels": false
        },
        "xaxis4": {
         "anchor": "y4",
         "domain": [
          0,
          1
         ],
         "matches": "x5",
         "showticklabels": false
        },
        "xaxis5": {
         "anchor": "y5",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Time"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0.848,
          1
         ],
         "title": {
          "text": "Messages/sec"
         }
        },
        "yaxis2": {
         "anchor": "x2",
         "domain": [
          0.6359999999999999,
          0.7879999999999999
         ],
         "title": {
          "text": "Workers"
         }
        },
        "yaxis3": {
         "anchor": "x3",
         "domain": [
          0.424,
          0.576
         ],
         "title": {
          "text": "Lag (ms)"
         }
        },
        "yaxis4": {
         "anchor": "x4",
         "domain": [
          0.212,
          0.364
         ],
         "title": {
          "text": "Messages"
         }
        },
        "yaxis5": {
         "anchor": "x5",
         "domain": [
          0,
          0.152
         ],
         "range": [
          0,
          25959.397494506837
         ],
         "title": {
          "text": "Latency (ms)"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\ud83d\udd0d Collecting Dataflow metrics...\n",
      "   Time window: 17:08:06 \u2192 17:08:08\n",
      "   Waiting 90 seconds for metrics to propagate...\n",
      "   Collecting worker count...\n",
      "   Collecting system lag...\n",
      "   Collecting element count...\n",
      "   Collecting Pub/Sub backlog...\n",
      "\u2705 Metrics collection complete\n",
      "   workers: 5 data points\n",
      "   system_lag: 5 data points\n",
      "   element_count: 5 data points\n",
      "   backlog: 5 data points\n"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "line": {
          "color": "blue",
          "width": 2
         },
         "mode": "lines",
         "name": "Publish Rate",
         "type": "scatter",
         "x": [
          "2025-11-13T17:08:00"
         ],
         "xaxis": "x",
         "y": {
          "bdata": "AAAAAABAj0A=",
          "dtype": "f8"
         },
         "yaxis": "y"
        },
        {
         "line": {
          "color": "green",
          "width": 2
         },
         "marker": {
          "size": 6
         },
         "mode": "lines+markers",
         "name": "Workers",
         "type": "scatter",
         "x": [
          "2025-11-13T17:06:08",
          "2025-11-13T17:07:08",
          "2025-11-13T17:08:08",
          "2025-11-13T17:09:08",
          "2025-11-13T17:10:08"
         ],
         "xaxis": "x2",
         "y": {
          "bdata": "AAAAAAAAEEAAAAAAAAAQQAAAAAAAABBAAAAAAAAAEEAAAAAAAAAQQA==",
          "dtype": "f8"
         },
         "yaxis": "y2"
        },
        {
         "line": {
          "color": "orange",
          "width": 2
         },
         "mode": "lines",
         "name": "System Lag",
         "type": "scatter",
         "x": [
          "2025-11-13T17:06:08",
          "2025-11-13T17:07:08",
          "2025-11-13T17:08:08",
          "2025-11-13T17:09:08",
          "2025-11-13T17:10:08"
         ],
         "xaxis": "x3",
         "y": {
          "bdata": "/Knx0k1iUD/6fmq8dJN4P/yp8dJNYlA/nMQgsHJokT/8qfHSTWJQPw==",
          "dtype": "f8"
         },
         "yaxis": "y3"
        },
        {
         "line": {
          "color": "red",
          "width": 2
         },
         "mode": "lines",
         "name": "Backlog",
         "type": "scatter",
         "x": [
          "2025-11-13T17:06:08",
          "2025-11-13T17:07:08",
          "2025-11-13T17:08:08",
          "2025-11-13T17:09:08",
          "2025-11-13T17:10:08"
         ],
         "xaxis": "x4",
         "y": {
          "bdata": "AAAAAABAj0AAAAAAAECPQAAAAAAAcLdAAAAAAABwt0AAAAAAAEDPQA==",
          "dtype": "f8"
         },
         "yaxis": "y4"
        },
        {
         "line": {
          "color": "purple",
          "width": 2
         },
         "mode": "lines",
         "name": "P95 Pipeline Latency",
         "type": "scatter",
         "x": [
          "2025-11-13T17:12:30",
          "2025-11-13T17:12:40",
          "2025-11-13T17:12:50",
          "2025-11-13T17:13:00",
          "2025-11-13T17:13:10"
         ],
         "xaxis": "x5",
         "y": {
          "bdata": "AAAkKwUJ0UAAAJxXKKrGQAAAQutoFtJAAACU8C3y1EAAALrngCzVQA==",
          "dtype": "f8"
         },
         "yaxis": "y5"
        }
       ],
       "layout": {
        "annotations": [
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Incoming Message Rate",
          "x": 0.5,
          "xanchor": "center",
          "xref": "paper",
          "y": 1,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Worker Count (Autoscaling)",
          "x": 0.5,
          "xanchor": "center",
          "xref": "paper",
          "y": 0.7879999999999999,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "System Lag (Processing Delay: Oldest To Current Message)",
          "x": 0.5,
          "xanchor": "center",
          "xref": "paper",
          "y": 0.576,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Output Subscription Queue",
          "x": 0.5,
          "xanchor": "center",
          "xref": "paper",
          "y": 0.364,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "P95 Pipeline Latency (publish \u2192 output queue, excludes queue wait)",
          "x": 0.5,
          "xanchor": "center",
          "xref": "paper",
          "y": 0.152,
          "yanchor": "bottom",
          "yref": "paper"
         }
        ],
        "height": 1200,
        "hovermode": "x unified",
        "showlegend": true,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermap": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermap"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Burst - 10,000 messages"
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "matches": "x5",
         "showticklabels": false
        },
        "xaxis2": {
         "anchor": "y2",
         "domain": [
          0,
          1
         ],
         "matches": "x5",
         "showticklabels": false
        },
        "xaxis3": {
         "anchor": "y3",
         "domain": [
          0,
          1
         ],
         "matches": "x5",
         "showticklabels": false
        },
        "xaxis4": {
         "anchor": "y4",
         "domain": [
          0,
          1
         ],
         "matches": "x5",
         "showticklabels": false
        },
        "xaxis5": {
         "anchor": "y5",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Time"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0.848,
          1
         ],
         "title": {
          "text": "Messages/sec"
         }
        },
        "yaxis2": {
         "anchor": "x2",
         "domain": [
          0.6359999999999999,
          0.7879999999999999
         ],
         "title": {
          "text": "Workers"
         }
        },
        "yaxis3": {
         "anchor": "x3",
         "domain": [
          0.424,
          0.576
         ],
         "title": {
          "text": "Lag (ms)"
         }
        },
        "yaxis4": {
         "anchor": "x4",
         "domain": [
          0.212,
          0.364
         ],
         "title": {
          "text": "Messages"
         }
        },
        "yaxis5": {
         "anchor": "x5",
         "domain": [
          0,
          0.152
         ],
         "range": [
          0,
          26007.21874809265
         ],
         "title": {
          "text": "Latency (ms)"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize burst tests",
    "for i, (result, latency_df) in enumerate(zip(phase2b_results, phase2b_latencies)):",
    "    test_config = BURST_TESTS[i]",
    "    ",
    "    # Get metrics for this specific burst",
    "    test_metrics = metrics_collector.collect_combined_metrics(",
    "        start_time=result['start_time'],",
    "        end_time=result['end_time'],",
    "        resolution_seconds=10",
    "    )",
    "    ",
    "    fig = plot_combined_timeline(",
    "        test_results=result,",
    "        combined_metrics=test_metrics,",
    "        latency_df=latency_df,",
    "        test_name=test_config['name']",
    "    )",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phase3_header",
   "metadata": {},
   "source": [
    "---",
    "## Phase 3: Ramp Test",
    "",
    "**Goal**: Find exact autoscaling trigger point with gradual load increase",
    "",
    "**Test**: Single ramp test (~15 minutes)",
    "- Gradually increase from 0 \u2192 500 msg/sec over 15 minutes",
    "- Linear ramp to simulate traffic growth",
    "",
    "**What We're Looking For**:",
    "- At what exact message rate does autoscaling trigger?",
    "- How does the pipeline respond to gradual load increase?",
    "- Is there a \"sweet spot\" for sustained throughput?",
    "- When does latency start degrading?",
    "",
    "\u23f3 **This phase will take approximately 15 minutes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "phase3_init",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PHASE 3: RAMP TEST\n",
      "======================================================================\n",
      "Ramping from 0 \u2192 500 msg/sec over 15 minutes\n",
      "Total estimated time: ~15 minutes\n",
      "\n",
      "This test gradually increases load to find autoscaling threshold.\n",
      "======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize Phase 3 storage",
    "phase3_results = []",
    "",
    "print(\"=\"*70)",
    "print(\"PHASE 2C: BALANCED RAMP TEST\")",
    "print(\"=\"*70)",
    "print(f\"Ramping from 0 \u2192 {RAMP_TEST['target_rate']} msg/sec over {RAMP_TEST['duration']//60} minutes\")",
    "print(f\"Total estimated time: ~{RAMP_TEST['duration']//60} minutes\")",
    "print(\"\\nThis test gradually increases load to find autoscaling threshold.\")",
    "print(\"=\"*70 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d34dc188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Ramp - 0\u2192500 msg/sec\n",
      "======================================================================\n",
      "Pattern: RAMP\n",
      "Ramp: 1 \u2192 500 messages/sec over 900s (15 mins 0s)\n",
      "\n",
      "\u23f3 Running ramp test...\n",
      "   Progress updates every 60 seconds...\n",
      "   [ 60s] Current rate: 33.3 msg/sec | Sent: 963\n",
      "   [120s] Current rate: 66.7 msg/sec | Sent: 3,712\n",
      "   [180s] Current rate: 100.0 msg/sec | Sent: 8,030\n",
      "   [240s] Current rate: 133.3 msg/sec | Sent: 13,292\n",
      "   [300s] Current rate: 166.7 msg/sec | Sent: 20,681\n",
      "   [360s] Current rate: 200.0 msg/sec | Sent: 29,525\n",
      "   [420s] Current rate: 233.3 msg/sec | Sent: 39,215\n",
      "   [480s] Current rate: 266.7 msg/sec | Sent: 50,581\n",
      "   [540s] Current rate: 300.0 msg/sec | Sent: 63,375\n",
      "   [600s] Current rate: 333.3 msg/sec | Sent: 76,389\n",
      "   [660s] Current rate: 366.7 msg/sec | Sent: 91,448\n",
      "   [720s] Current rate: 400.0 msg/sec | Sent: 106,553\n",
      "   [780s] Current rate: 433.3 msg/sec | Sent: 121,467\n",
      "   [840s] Current rate: 466.7 msg/sec | Sent: 136,200\n",
      "\n",
      "\u2705 Complete in 900.0s\n",
      "   Sent: 151,959 messages\n",
      "   Avg rate: 168.8 messages/sec\n",
      "\n",
      "\u2705 Phase 3 complete: Ramp test finished\n"
     ]
    }
   ],
   "source": [
    "# Run ramp test",
    "ramp_result = await load_generator.run_load_test(",
    "    pattern=\"ramp\",",
    "    target_rate=RAMP_TEST[\"target_rate\"],",
    "    duration=RAMP_TEST[\"duration\"],",
    "    test_name=RAMP_TEST[\"name\"]",
    ")",
    "phase3_results.append(ramp_result)",
    "",
    "print(f\"\\n\u2705 Phase 3 complete: Ramp test finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phase3_metrics_header",
   "metadata": {},
   "source": [
    "### Collect Metrics for Phase 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "phase3_metrics",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting metrics for Phase 3\n",
      "Time window: 17:17:44 \u2192 17:32:44\n",
      "\n",
      "\ud83d\udd0d Collecting Dataflow metrics...\n",
      "   Time window: 17:17:44 \u2192 17:32:44\n",
      "   Waiting 90 seconds for metrics to propagate...\n",
      "   Collecting worker count...\n",
      "   Collecting system lag...\n",
      "   Collecting element count...\n",
      "   Collecting Pub/Sub backlog...\n",
      "\u2705 Metrics collection complete\n",
      "   workers: 16 data points\n",
      "   system_lag: 17 data points\n",
      "   element_count: 16 data points\n",
      "   backlog: 18 data points\n",
      "\n",
      "\u2705 Metrics collected for Phase 3\n"
     ]
    }
   ],
   "source": [
    "# Collect metrics for ramp test",
    "phase3_start = ramp_result['start_time']",
    "phase3_end = ramp_result['end_time']",
    "",
    "print(f\"Collecting metrics for Phase 3\")",
    "print(f\"Time window: {phase3_start.strftime('%H:%M:%S')} \u2192 {phase3_end.strftime('%H:%M:%S')}\")",
    "",
    "phase3_metrics = metrics_collector.collect_combined_metrics(",
    "    start_time=phase3_start,",
    "    end_time=phase3_end,",
    "    resolution_seconds=10",
    ")",
    "",
    "print(f\"\\n\u2705 Metrics collected for Phase 3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "phase3_latency",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\ud83d\udcca Collecting end-to-end latency from Pub/Sub output...\n",
      "   Subscription: projects/statmike-mlops-349915/subscriptions/pytorch-autoencoder-output-sub-local\n",
      "   Test ID: ramp-20251113-171744\n",
      "   Expected messages: 151,959\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Collected 100/151,959 messages... (2s elapsed)\n",
      "   Collected 200/151,959 messages... (3s elapsed)\n",
      "\u2705 Latency collection complete: 151,959 messages\n",
      "\n",
      "\u2705 Latency data collected: 151,959 messages\n"
     ]
    }
   ],
   "source": [
    "# Collect end-to-end latency for ramp test",
    "phase3_latency = metrics_collector.collect_end_to_end_latency(",
    "    test_id=ramp_result['test_id'],",
    "    expected_messages=ramp_result['num_messages'],",
    "    timeout=300",
    ")",
    "",
    "print(f\"\\n\u2705 Latency data collected: {len(phase3_latency):,} messages\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phase3_analysis_header",
   "metadata": {},
   "source": [
    "### Analyze Phase 3 Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "phase3_analysis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PHASE 3 ANALYSIS: AUTOSCALING TRIGGER POINT\n",
      "======================================================================\n",
      "\n",
      "Ramp - 0\u2192500 msg/sec:\n",
      "  Duration: 900s (15 mins)\n",
      "  Messages sent: 151,959\n",
      "  Average rate: 168.8 msg/sec\n",
      "  Peak rate: 500 msg/sec\n",
      "\n",
      "  Latency Statistics:\n",
      "    Mean E2E: 13,541.7ms\n",
      "    P50 E2E:  13,377.7ms\n",
      "    P95 E2E:  31,006.3ms\n",
      "    P99 E2E:  39,840.6ms\n",
      "\n",
      "  Latency Progression (as rate increases):\n",
      "    Q1 (0-25%):   1,410.7ms mean\n",
      "    Q2 (25-50%):  10,830.0ms mean\n",
      "    Q3 (50-75%):  20,223.2ms mean\n",
      "    Q4 (75-100%): 21,702.1ms mean\n",
      "\n",
      "    \u26a0\ufe0f  Latency degraded significantly in later quartiles\n",
      "       Suggests capacity limits reached at higher rates\n",
      "\n",
      "======================================================================\n",
      "AUTOSCALING BEHAVIOR DURING RAMP\n",
      "======================================================================\n",
      "\n",
      "\u2139\ufe0f  No autoscaling triggered during ramp\n",
      "   Current workers handled up to 500 msg/sec\n",
      "   Pipeline has significant headroom at this configuration\n",
      "\n",
      "======================================================================\n",
      "BACKLOG BEHAVIOR DURING RAMP\n",
      "======================================================================\n",
      "\n",
      "Backlog Statistics:\n",
      "  Max backlog: 130,197 messages\n",
      "  Mean backlog: 40,726 messages\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Cannot subtract tz-naive and tz-aware datetime-like objects.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 98\u001b[39m\n\u001b[32m     96\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(backlog_building) > \u001b[32m0\u001b[39m:\n\u001b[32m     97\u001b[39m     first_backlog_time = backlog_building.iloc[\u001b[32m0\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mtimestamp\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m     elapsed_at_backlog = (\u001b[43mfirst_backlog_time\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43mphase3_start\u001b[49m).total_seconds()\n\u001b[32m     99\u001b[39m     rate_at_backlog = (elapsed_at_backlog / RAMP_TEST[\u001b[33m'\u001b[39m\u001b[33mduration\u001b[39m\u001b[33m'\u001b[39m]) * RAMP_TEST[\u001b[33m'\u001b[39m\u001b[33mtarget_rate\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m    101\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m  Backlog started building:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/tslibs/timestamps.pyx:514\u001b[39m, in \u001b[36mpandas._libs.tslibs.timestamps._Timestamp.__sub__\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mTypeError\u001b[39m: Cannot subtract tz-naive and tz-aware datetime-like objects."
     ]
    }
   ],
   "source": [
    "# Analyze ramp test to find autoscaling trigger point",
    "print(\"=\"*70)",
    "print(\"PHASE 3 ANALYSIS: AUTOSCALING TRIGGER POINT\")",
    "print(\"=\"*70)",
    "",
    "print(f\"\\n{RAMP_TEST['name']}:\")",
    "print(f\"  Duration: {ramp_result['elapsed_seconds']:.0f}s ({ramp_result['elapsed_seconds']//60:.0f} mins)\")",
    "print(f\"  Messages sent: {ramp_result['num_messages']:,}\")",
    "print(f\"  Average rate: {ramp_result['actual_rate']:.1f} msg/sec\")",
    "print(f\"  Peak rate: {RAMP_TEST['target_rate']} msg/sec\")",
    "",
    "if len(phase3_latency) > 0:",
    "    print(f\"\\n  Latency Statistics:\")",
    "    print(f\"    Mean E2E: {phase3_latency['pipeline_latency_ms'].mean():,.1f}ms\")",
    "    print(f\"    P50 E2E:  {phase3_latency['pipeline_latency_ms'].quantile(0.50):,.1f}ms\")",
    "    print(f\"    P95 E2E:  {phase3_latency['pipeline_latency_ms'].quantile(0.95):,.1f}ms\")",
    "    print(f\"    P99 E2E:  {phase3_latency['pipeline_latency_ms'].quantile(0.99):,.1f}ms\")",
    "    ",
    "    # Analyze latency trend over time (early vs late)",
    "    phase3_latency_sorted = phase3_latency.sort_values('publish_time')",
    "    ",
    "    # Split into quartiles to see latency progression",
    "    quartile_size = len(phase3_latency_sorted) // 4",
    "    q1 = phase3_latency_sorted.iloc[:quartile_size]",
    "    q2 = phase3_latency_sorted.iloc[quartile_size:2*quartile_size]",
    "    q3 = phase3_latency_sorted.iloc[2*quartile_size:3*quartile_size]",
    "    q4 = phase3_latency_sorted.iloc[3*quartile_size:]",
    "    ",
    "    print(f\"\\n  Latency Progression (as rate increases):\")",
    "    print(f\"    Q1 (0-25%):   {q1['pipeline_latency_ms'].mean():,.1f}ms mean\")",
    "    print(f\"    Q2 (25-50%):  {q2['pipeline_latency_ms'].mean():,.1f}ms mean\")",
    "    print(f\"    Q3 (50-75%):  {q3['pipeline_latency_ms'].mean():,.1f}ms mean\")",
    "    print(f\"    Q4 (75-100%): {q4['pipeline_latency_ms'].mean():,.1f}ms mean\")",
    "    ",
    "    # Identify when latency starts degrading significantly",
    "    q1_mean = q1['pipeline_latency_ms'].mean()",
    "    if q4['pipeline_latency_ms'].mean() > q1_mean * 1.5:",
    "        print(f\"\\n    \u26a0\ufe0f  Latency degraded significantly in later quartiles\")",
    "        print(f\"       Suggests capacity limits reached at higher rates\")",
    "    else:",
    "        print(f\"\\n    \u2705 Latency remained stable throughout ramp\")",
    "",
    "# Analyze autoscaling during ramp",
    "print(f\"\\n\" + \"=\"*70)",
    "print(\"AUTOSCALING BEHAVIOR DURING RAMP\")",
    "print(\"=\"*70)",
    "",
    "autoscaling_events = metrics_collector.analyze_autoscaling(phase3_metrics)",
    "",
    "if len(autoscaling_events) > 0:",
    "    print(f\"\\n\u2705 Autoscaling triggered {len(autoscaling_events)} time(s) during ramp\\n\")",
    "    ",
    "    for idx, event in autoscaling_events.iterrows():",
    "        # Calculate approximate message rate at trigger time",
    "        elapsed_at_trigger = (event['trigger_time'] - phase3_start).total_seconds()",
    "        rate_at_trigger = (elapsed_at_trigger / RAMP_TEST['duration']) * RAMP_TEST['target_rate']",
    "        ",
    "        print(f\"Event {idx + 1}:\")",
    "        print(f\"  Trigger time:       {event['trigger_time'].strftime('%H:%M:%S')}\")",
    "        print(f\"  Elapsed:            {elapsed_at_trigger:.0f}s into ramp\")",
    "        print(f\"  Approx rate:        {rate_at_trigger:.0f} msg/sec\")",
    "        print(f\"  Workers:            {event['workers_before']:.0f} \u2192 {event['workers_after']:.0f}\")",
    "        print(f\"  Provisioning time:  {event['scale_up_lag_seconds']:.0f}s\")",
    "        print()",
    "    ",
    "    # Identify threshold",
    "    first_trigger = autoscaling_events.iloc[0]",
    "    first_trigger_elapsed = (first_trigger['trigger_time'] - phase3_start).total_seconds()",
    "    first_trigger_rate = (first_trigger_elapsed / RAMP_TEST['duration']) * RAMP_TEST['target_rate']",
    "    ",
    "    print(f\"\ud83d\udcca Key Finding:\")",
    "    print(f\"   Autoscaling triggered at approximately {first_trigger_rate:.0f} msg/sec\")",
    "    print(f\"   This is the throughput threshold for current configuration\")",
    "else:",
    "    print(\"\\n\u2139\ufe0f  No autoscaling triggered during ramp\")",
    "    print(f\"   Current workers handled up to {RAMP_TEST['target_rate']} msg/sec\")",
    "    print(\"   Pipeline has significant headroom at this configuration\")",
    "",
    "# Analyze backlog behavior during ramp",
    "if 'backlog' in phase3_metrics and len(phase3_metrics['backlog']) > 0:",
    "    print(f\"\\n\" + \"=\"*70)",
    "    print(\"BACKLOG BEHAVIOR DURING RAMP\")",
    "    print(\"=\"*70)",
    "    ",
    "    backlog_df = phase3_metrics['backlog'].sort_values('timestamp')",
    "    max_backlog = backlog_df['value'].max()",
    "    ",
    "    print(f\"\\nBacklog Statistics:\")",
    "    print(f\"  Max backlog: {max_backlog:,.0f} messages\")",
    "    print(f\"  Mean backlog: {backlog_df['value'].mean():,.0f} messages\")",
    "    ",
    "    # Find when backlog started building",
    "    backlog_threshold = 100  # Consider backlog \"building\" at 100+ messages",
    "    backlog_building = backlog_df[backlog_df['value'] > backlog_threshold]",
    "    ",
    "    if len(backlog_building) > 0:",
    "        first_backlog_time = backlog_building.iloc[0]['timestamp']",
    "        elapsed_at_backlog = (first_backlog_time - phase3_start).total_seconds()",
    "        rate_at_backlog = (elapsed_at_backlog / RAMP_TEST['duration']) * RAMP_TEST['target_rate']",
    "        ",
    "        print(f\"\\n  Backlog started building:\")",
    "        print(f\"    Time: {first_backlog_time.strftime('%H:%M:%S')}\")",
    "        print(f\"    Elapsed: {elapsed_at_backlog:.0f}s into ramp\")",
    "        print(f\"    Approx rate: {rate_at_backlog:.0f} msg/sec\")",
    "        print(f\"\\n    \ud83d\udca1 This indicates pipeline capacity threshold\")",
    "    else:",
    "        print(f\"\\n  \u2705 Backlog remained under control throughout ramp\")",
    "",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db57294",
   "metadata": {},
   "source": [
    "### \ud83d\udcca How to Read the Visualization",
    "",
    "The timeline visualization below shows 5 panels that track different aspects of pipeline performance. Here's what each panel tells you:",
    "",
    "#### **Panel 1: Incoming Message Rate**",
    "- **What it shows:** Messages published per second to the input Pub/Sub topic",
    "- **What to look for:**",
    "  - Flat line = steady rate (good for controlled testing)",
    "  - Spikes = burst traffic",
    "  - Gradual increase = ramp test pattern",
    "- **Interpretation:** This is your **input load** - what you're sending to the pipeline",
    "",
    "#### **Panel 2: Worker Count (Autoscaling)**",
    "- **What it shows:** Number of active Dataflow workers over time",
    "- **What to look for:**",
    "  - Flat line = no autoscaling needed",
    "  - Step up = autoscaling triggered (workers being added)",
    "  - Step down = scale-down (workers being removed after low load period)",
    "- **Interpretation:** Shows when pipeline capacity changes. Worker provisioning takes **2-4 minutes**, so you'll see a delay between load increase and worker scale-up.",
    "",
    "#### **Panel 3: System Lag (Processing Delay: Oldest To Current Message)**",
    "- **What it shows:** How far behind real-time the pipeline is processing (in milliseconds)",
    "- **What to look for:**",
    "  - Near-zero lag = pipeline keeping up with real-time",
    "  - Increasing lag = pipeline falling behind",
    "  - High lag (>60s) = serious capacity issues",
    "- **Interpretation:** This is your **real-time health indicator**. Rising lag means you need more workers or better machine type.",
    "",
    "#### **Panel 4: Output Subscription Queue**",
    "- **What it shows:** Number of messages waiting in the output Pub/Sub subscription",
    "- **What to look for:**",
    "  - Zero or low = messages being consumed quickly",
    "  - Linearly increasing = messages accumulating (expected during testing)",
    "  - Very high (>10,000) = potential capacity issues",
    "- **Interpretation:** This backlog includes **both** pipeline processing queue AND messages waiting for the test framework to pull them. During tests, this will grow because we're not consuming as fast as we're producing.",
    "",
    "#### **Panel 5: P95 Pipeline Latency (publish \u2192 output queue, excludes queue wait)**",
    "- **What it shows:** 95th percentile latency from message publish to arriving in output queue",
    "- **What to look for:**",
    "  - Steady line = consistent performance",
    "  - Gradual increase = latency degrading under load",
    "  - Spikes at test end = shutdown artifacts (normal)",
    "- **Interpretation:** This is your **actual pipeline performance metric** - the one you care about most. It excludes the time messages sit in the queue waiting for the test to pull them.",
    "- **Y-axis note:** The axis is auto-scaled to focus on steady-state latency using the 99th percentile. Very high spikes (like test shutdown) may be clipped but won't distort the main trend.",
    "",
    "---",
    "",
    "**\ud83d\udca1 Key Insights to Extract:**",
    "1. **When does autoscaling trigger?** \u2192 Look for worker count step-up in Panel 2",
    "2. **What's the latency at different loads?** \u2192 Compare Panel 5 across different message rates in Panel 1",
    "3. **Is the pipeline keeping up?** \u2192 Panel 3 system lag should stay near zero",
    "4. **Where's the bottleneck?** \u2192 If latency is high but system lag is low, it's windowing. If system lag is high, it's processing capacity.",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "phase3_viz",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Visualize ramp test",
    "fig = plot_combined_timeline(",
    "    test_results=ramp_result,",
    "    combined_metrics=phase3_metrics,",
    "    latency_df=phase3_latency,",
    "    test_name=\"Phase 3: Ramp Test - Finding Autoscaling Threshold\"",
    ")",
    "",
    "fig.show()",
    "",
    "print(\"\\n\ud83d\udca1 This visualization shows:\")",
    "print(f\"   - Gradual message rate increase: 0 \u2192 {RAMP_TEST['target_rate']} msg/sec\")",
    "print(f\"   - Exact moment when workers scaled up\")",
    "print(f\"   - System lag progression as load increased\")",
    "print(f\"   - Backlog buildup and recovery\")",
    "print(f\"   - Latency trends throughout the ramp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary_header",
   "metadata": {},
   "source": [
    "---",
    "## Comprehensive Test Summary & Production Recommendations",
    "",
    "Based on all test phases, generate production recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comprehensive_summary",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Comprehensive summary",
    "print(\"=\"*80)",
    "print(\"COMPREHENSIVE TEST SUMMARY & PRODUCTION RECOMMENDATIONS\")",
    "print(\"=\"*80)",
    "",
    "# Job configuration",
    "print(f\"\\n\ud83d\udccb Dataflow Job Configuration:\")",
    "print(f\"   Job ID:         {JOB_ID}\")",
    "print(f\"   Region:         {REGION}\")",
    "print(f\"   Machine Type:   n1-standard-4 (4 vCPUs, 15 GB memory)\")",
    "print(f\"   Window Size:    60 seconds\")",
    "",
    "# Phase 1 summary",
    "print(f\"\\n\ud83d\udcca Phase 1: Baseline Performance\")",
    "if len(phase1_latencies) > 0 and len(phase1_latencies[0]) > 0:",
    "    baseline_latency = phase1_latencies[0]['pipeline_latency_ms'].quantile(0.95)",
    "    print(f\"   Baseline P95 latency: {baseline_latency:,.1f}ms at {BASELINE_TESTS[0]['target_rate']} msg/sec\")",
    "else:",
    "    print(f\"   Tests completed but latency data unavailable\")",
    "",
    "# Phase 2A summary",
    "print(f\"\\n\ud83d\udcca Phase 2A: Throughput Threshold Hunt\")",
    "phase2a_autoscaling = metrics_collector.analyze_autoscaling(phase2a_metrics)",
    "if len(phase2a_autoscaling) > 0:",
    "    print(f\"   \u2705 Autoscaling triggered {len(phase2a_autoscaling)} time(s)\")",
    "    print(f\"   Average provisioning time: {phase2a_autoscaling['scale_up_lag_seconds'].mean():.0f}s\")",
    "else:",
    "    print(f\"   No autoscaling at tested rates (50-200 msg/sec)\")",
    "",
    "if len(phase2a_latencies) > 0:",
    "    # Compare latency degradation across sustained tests",
    "    latencies_50 = phase2a_latencies[0]['pipeline_latency_ms'].quantile(0.95) if len(phase2a_latencies[0]) > 0 else None",
    "    latencies_200 = phase2a_latencies[-1]['pipeline_latency_ms'].quantile(0.95) if len(phase2a_latencies[-1]) > 0 else None",
    "    ",
    "    if latencies_50 and latencies_200:",
    "        degradation = ((latencies_200 - latencies_50) / latencies_50) * 100",
    "        print(f\"   Latency at 50 msg/sec:  {latencies_50:,.1f}ms (P95)\")",
    "        print(f\"   Latency at 200 msg/sec: {latencies_200:,.1f}ms (P95)\")",
    "        print(f\"   Degradation: {degradation:+.1f}%\")",
    "",
    "# Phase 2B summary",
    "print(f\"\\n\ud83d\udcca Phase 2B: Burst Capacity\")",
    "if len(phase2b_results) > 0:",
    "    largest_burst = BURST_TESTS[-1]",
    "    print(f\"   Largest burst: {largest_burst['num_messages']:,} messages\")",
    "    if len(phase2b_latencies[-1]) > 0:",
    "        burst_p95 = phase2b_latencies[-1]['pipeline_latency_ms'].quantile(0.95)",
    "        print(f\"   P95 latency during burst: {burst_p95:,.1f}ms\")",
    "",
    "# Phase 3 summary",
    "print(f\"\\n\ud83d\udcca Phase 3: Ramp Test\")",
    "phase3_autoscaling = metrics_collector.analyze_autoscaling(phase3_metrics)",
    "if len(phase3_autoscaling) > 0:",
    "    first_trigger = phase3_autoscaling.iloc[0]",
    "    elapsed = (first_trigger['trigger_time'] - phase3_start).total_seconds()",
    "    trigger_rate = (elapsed / RAMP_TEST['duration']) * RAMP_TEST['target_rate']",
    "    print(f\"   \u2705 Autoscaling triggered at ~{trigger_rate:.0f} msg/sec\")",
    "    print(f\"   This is the capacity threshold\")",
    "else:",
    "    print(f\"   No autoscaling triggered up to {RAMP_TEST['target_rate']} msg/sec\")",
    "    print(f\"   Pipeline has significant headroom\")",
    "",
    "if len(phase3_latency) > 0:",
    "    ramp_p95 = phase3_latency['pipeline_latency_ms'].quantile(0.95)",
    "    print(f\"   Overall P95 latency: {ramp_p95:,.1f}ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "production_recommendations",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Production recommendations",
    "print(\"\\n\" + \"=\"*80)",
    "print(\"PRODUCTION RECOMMENDATIONS\")",
    "print(\"=\"*80)",
    "",
    "# Determine recommended throughput",
    "print(\"\\n\ud83c\udfaf Recommended Throughput Configurations:\")",
    "",
    "# Try to determine capacity from autoscaling events",
    "all_autoscaling = pd.concat([",
    "    metrics_collector.analyze_autoscaling(phase1_metrics),",
    "    metrics_collector.analyze_autoscaling(phase2a_metrics),",
    "    metrics_collector.analyze_autoscaling(phase2b_metrics),",
    "    metrics_collector.analyze_autoscaling(phase3_metrics)",
    "], ignore_index=True)",
    "",
    "if len(all_autoscaling) > 0:",
    "    # Use first autoscaling event to estimate capacity",
    "    print(\"\\n   Based on autoscaling behavior:\")",
    "    print(\"   \u2022 Conservative (70% capacity): Suitable for production with headroom\")",
    "    print(\"   \u2022 Balanced (85% capacity): Good for predictable workloads\")",
    "    print(\"   \u2022 Aggressive (95% capacity): Maximum throughput, relies on autoscaling\")",
    "else:",
    "    print(\"\\n   Based on test results (no autoscaling triggered):\")",
    "    print(f\"   \u2022 Current configuration handled {SUSTAINED_TESTS[-1]['target_rate']} msg/sec comfortably\")",
    "    print(\"   \u2022 Significant headroom available\")",
    "    print(\"   \u2022 Consider testing higher rates to find true limits\")",
    "",
    "# Worker configuration",
    "print(\"\\n\u2699\ufe0f  Worker Configuration Recommendations:\")",
    "",
    "if len(all_autoscaling) > 0:",
    "    avg_provisioning = all_autoscaling['scale_up_lag_seconds'].mean()",
    "    print(f\"\\n   Autoscaling Performance:\")",
    "    print(f\"   \u2022 Average worker provisioning: {avg_provisioning:.0f}s ({avg_provisioning/60:.1f} mins)\")",
    "    print(f\"   \u2022 Triggered {len(all_autoscaling)} time(s) across all tests\")",
    "    ",
    "    if avg_provisioning > 300:  # 5 minutes",
    "        print(f\"\\n   \u26a0\ufe0f  Slow provisioning time detected\")",
    "        print(f\"   \u2022 Consider increasing MIN_WORKERS for faster response\")",
    "        print(f\"   \u2022 Pre-warm capacity reduces latency spikes\")",
    "    else:",
    "        print(f\"\\n   \u2705 Autoscaling performance acceptable\")",
    "else:",
    "    print(f\"\\n   Current MIN_WORKERS=2, MAX_WORKERS=20\")",
    "    print(f\"   \u2022 No autoscaling needed at tested rates\")",
    "    print(f\"   \u2022 Current configuration appropriate\")",
    "",
    "# Machine type recommendations",
    "print(\"\\n\ud83d\udcbb Machine Type Recommendations:\")",
    "print(f\"\\n   Current: n1-standard-4 (4 vCPUs, 15 GB memory)\")",
    "",
    "# Check if we have system lag data to determine if CPU-bound",
    "all_system_lag = []",
    "for metrics in [phase1_metrics, phase2a_metrics, phase2b_metrics, phase3_metrics]:",
    "    if 'system_lag' in metrics and len(metrics['system_lag']) > 0:",
    "        all_system_lag.extend(metrics['system_lag']['value'].tolist())",
    "",
    "if all_system_lag:",
    "    max_lag_ms = max(all_system_lag) / 1000  # Convert to ms",
    "    ",
    "    if max_lag_ms > 60000:  # 1 minute",
    "        print(f\"\\n   \u26a0\ufe0f  High system lag detected ({max_lag_ms/1000:.1f}s max)\")",
    "        print(f\"   \u2022 Consider c2-standard-4 for more CPU power\")",
    "        print(f\"   \u2022 Or increase worker count instead\")",
    "    else:",
    "        print(f\"\\n   \u2705 Current machine type performing well\")",
    "        print(f\"   \u2022 System lag remained under control (max: {max_lag_ms/1000:.1f}s)\")",
    "",
    "print(f\"\\n   When to consider alternatives:\")",
    "print(f\"   \u2022 CPU-intensive model \u2192 c2-standard-4 (compute-optimized)\")",
    "print(f\"   \u2022 Large model size \u2192 n1-highmem-4 (more memory)\")",
    "print(f\"   \u2022 GPU-compatible model \u2192 Add --worker_gpu_type=nvidia-tesla-t4\")",
    "",
    "# Latency optimization",
    "print(\"\\n\u23f1\ufe0f  Latency Optimization:\")",
    "",
    "if len(phase1_latencies) > 0 and len(phase1_latencies[0]) > 0:",
    "    baseline = phase1_latencies[0]",
    "    window_pct = baseline['window_wait_ms'].mean() / baseline['pipeline_latency_ms'].mean() * 100",
    "    processing_pct = baseline['processing_ms'].mean() / baseline['pipeline_latency_ms'].mean() * 100",
    "    ",
    "    print(f\"\\n   Latency Breakdown (baseline):\")",
    "    print(f\"   \u2022 Window wait: {window_pct:.1f}% of total latency\")",
    "    print(f\"   \u2022 Processing: {processing_pct:.1f}% of total latency\")",
    "    print(f\"   \u2022 Pub/Sub delivery: {100 - window_pct - processing_pct:.1f}% of total latency\")",
    "    ",
    "    if window_pct > 50:",
    "        print(f\"\\n   \ud83d\udca1 Window wait dominates latency\")",
    "        print(f\"   \u2022 Current window: 60 seconds (fixed)\")",
    "        print(f\"   \u2022 To reduce latency: Use smaller windows (30s, 15s)\")",
    "        print(f\"   \u2022 Trade-off: Smaller windows = less batching efficiency\")",
    "    ",
    "    if processing_pct > 30:",
    "        print(f\"\\n   \ud83d\udca1 Processing time significant\")",
    "        print(f\"   \u2022 Consider model optimization\")",
    "        print(f\"   \u2022 Or use more powerful machine type\")",
    "        print(f\"   \u2022 Or GPU acceleration for large models\")",
    "",
    "# Cost optimization",
    "print(\"\\n\ud83d\udcb0 Cost Optimization Tips:\")",
    "print(f\"   \u2022 Monitor actual traffic patterns and adjust MIN/MAX workers\")",
    "print(f\"   \u2022 Use batch processing for historical analysis (cheaper)\")",
    "print(f\"   \u2022 Set up alerts for unexpected worker scaling\")",
    "print(f\"   \u2022 Consider spot instances for non-critical workloads (when available)\")",
    "print(f\"   \u2022 Drain and stop job when not actively processing (restart when needed)\")",
    "",
    "# Monitoring",
    "print(\"\\n\ud83d\udcca Monitoring & Alerts:\")",
    "print(f\"   Set up Cloud Monitoring alerts for:\")",
    "print(f\"   \u2022 System lag > 60 seconds (pipeline falling behind)\")",
    "print(f\"   \u2022 Backlog > 10,000 messages (capacity issues)\")",
    "print(f\"   \u2022 Worker count at MAX_WORKERS (may need to increase limit)\")",
    "print(f\"   \u2022 Element count drops to 0 (pipeline stalled)\")",
    "",
    "# Next steps",
    "print(\"\\n\ud83d\udcdd Next Steps:\")",
    "print(f\"   1. Review Dataflow job metrics in Cloud Console\")",
    "print(f\"      https://console.cloud.google.com/dataflow/jobs/{REGION}/{JOB_ID}?project={PROJECT_ID}\")",
    "print(f\"   2. If autoscaling threshold needs adjustment:\")",
    "print(f\"      \u2022 Modify MIN_WORKERS or MAX_WORKERS in deployment\")",
    "print(f\"      \u2022 Re-run this notebook to validate changes\")",
    "print(f\"   3. Implement recommended monitoring alerts\")",
    "print(f\"   4. Set up dashboard for real-time visibility\")",
    "print(f\"   5. Document findings for future capacity planning\")",
    "",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion_header",
   "metadata": {},
   "source": [
    "---",
    "## Conclusion",
    "",
    "This notebook provides a comprehensive, scientific approach to understanding Dataflow Streaming performance through systematic testing across three dimensions:",
    "",
    "**1. Message Rate** - Baseline, sustained, and peak throughput testing  ",
    "**2. Load Patterns** - Burst, sustained, and ramping traffic patterns  ",
    "**3. Latency Characteristics** - End-to-end breakdown and bottleneck identification",
    "",
    "### Key Insights from This Testing Framework",
    "",
    "**Understanding Dataflow Autoscaling:**",
    "- Dataflow scales based on **Pub/Sub backlog** and **system lag** (not just CPU like Vertex AI)",
    "- Worker provisioning takes **2-4 minutes** (plan for this delay)",
    "- Sustained load (not bursts) typically triggers autoscaling",
    "- Ramp tests reveal exact throughput thresholds",
    "",
    "**Latency Composition:**",
    "- **Window Wait**: Time from publish to window close (up to window size)",
    "- **Processing**: Model inference + transforms (controllable via machine type)",
    "- **Pub/Sub Delivery**: Usually <100ms (network overhead)",
    "- Smaller windows = lower latency but less batching efficiency",
    "",
    "**Bottleneck Identification:**",
    "- High **system lag** \u2192 Processing capacity issues (need more workers or better machine type)",
    "- High **backlog** \u2192 Ingestion rate exceeds processing capacity (autoscaling needed)",
    "- High **window wait %** \u2192 Latency dominated by batching (consider smaller windows)",
    "- High **processing %** \u2192 Model inference slow (consider GPU or model optimization)",
    "",
    "### Important Notes",
    "",
    "**Results are Pipeline-Specific:**  ",
    "All results in this notebook are specific to the tested pipeline configuration:",
    "- PyTorch autoencoder model with 30 features",
    "- n1-standard-4 workers (4 vCPUs, 15 GB memory)",
    "- 60-second fixed windows",
    "- MIN=2, MAX=20 workers",
    "",
    "Your results will vary based on:",
    "- Model complexity and inference time",
    "- Machine type and resources",
    "- Window size configuration",
    "- Message size and format",
    "",
    "**Always Test Your Own Pipeline:**  ",
    "Before production deployment:",
    "1. Run this notebook with your pipeline and representative data",
    "2. Test with realistic traffic patterns",
    "3. Adjust worker configuration based on results",
    "4. Monitor production metrics continuously",
    "",
    "### What's Next?",
    "",
    "**If This Notebook Revealed Issues:**",
    "1. **Autoscaling too slow** \u2192 Increase MIN_WORKERS for pre-warmed capacity",
    "2. **High system lag** \u2192 Use c2-standard-4 or increase MAX_WORKERS",
    "3. **High latency** \u2192 Reduce window size or optimize model",
    "4. **Backlog building** \u2192 Increase MAX_WORKERS or improve processing efficiency",
    "",
    "**Production Deployment Tasks:**",
    "- Set up Cloud Monitoring alerts (system lag, backlog, worker count)",
    "- Create dashboard for real-time visibility",
    "- Document capacity planning findings",
    "- Implement gradual rollout with monitoring",
    "",
    "**Related Notebooks:**",
    "- [Dataflow Streaming RunInference](./dataflow-streaming-runinference.ipynb) - Deploy the pipeline tested here",
    "- [Dataflow Batch RunInference](./dataflow-batch-runinference.ipynb) - Batch processing alternative",
    "- [Vertex AI Endpoint Scale Tests](./scale-tests-vertex-ai-endpoints.ipynb) - Compare with endpoint serving",
    "",
    "### Resources",
    "",
    "**Dataflow Documentation:**",
    "- [Streaming Pipelines](https://cloud.google.com/dataflow/docs/concepts/streaming-pipelines)",
    "- [Autoscaling](https://cloud.google.com/dataflow/docs/guides/deploying-a-pipeline#autoscaling)",
    "- [Monitoring](https://cloud.google.com/dataflow/docs/guides/using-cloud-monitoring)",
    "",
    "**Apache Beam:**",
    "- [RunInference](https://beam.apache.org/documentation/ml/about-ml/)",
    "- [Windowing](https://beam.apache.org/documentation/programming-guide/#windowing)",
    "- [PyTorch Handler](https://beam.apache.org/documentation/ml/pytorch-inference/)",
    "",
    "**Cloud Monitoring:**",
    "- [Dataflow Metrics](https://cloud.google.com/dataflow/docs/guides/using-cloud-monitoring#available_dataflow_metrics)",
    "- [Setting Up Alerts](https://cloud.google.com/monitoring/alerts)",
    "",
    "---",
    "",
    "**Testing Framework Created:** This comprehensive testing infrastructure (`scale_testing_dataflow_utils.py` + this notebook) can be reused for testing any Dataflow Streaming pipeline. Simply update the job ID and test parameters."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}