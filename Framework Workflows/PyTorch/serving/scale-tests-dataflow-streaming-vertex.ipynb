{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![tracker](https://us-central1-vertex-ai-mlops-369716.cloudfunctions.net/pixel-tracking?path=statmike%2Fvertex-ai-mlops%2FFramework+Workflows%2FPyTorch%2Fserving&file=scale-tests-dataflow-streaming-vertex.ipynb)\n",
    "<!--- header table --->\n",
    "<table align=\"left\">\n",
    "<tr>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%20Workflows/PyTorch/serving/scale-tests-dataflow-streaming-vertex.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.svgrepo.com/download/217753/github.svg\" alt=\"GitHub logo\">\n",
    "      <br>View on<br>GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://colab.research.google.com/github/statmike/vertex-ai-mlops/blob/main/Framework%20Workflows/PyTorch/serving/scale-tests-dataflow-streaming-vertex.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.gstatic.com/pantheon/images/bigquery/welcome_page/colab-logo.svg\" alt=\"Google Colaboratory logo\">\n",
    "      <br>Run in<br>Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https%3A%2F%2Fraw.githubusercontent.com%2Fstatmike%2Fvertex-ai-mlops%2Fmain%2FFramework%2520Workflows%2FPyTorch%2Fserving%2Fscale-tests-dataflow-streaming-vertex.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\">\n",
    "      <br>Run in<br>Colab Enterprise\n",
    "    </a>\n",
    "  </td>      \n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/bigquery/import?url=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%20Workflows/PyTorch/serving/scale-tests-dataflow-streaming-vertex.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.gstatic.com/images/branding/gcpiconscolors/bigquery/v1/32px.svg\" alt=\"BigQuery logo\">\n",
    "      <br>Open in<br>BigQuery Studio\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/statmike/vertex-ai-mlops/main/Framework%20Workflows/PyTorch/serving/scale-tests-dataflow-streaming-vertex.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.gstatic.com/images/branding/gcpiconscolors/vertexai/v1/32px.svg\" alt=\"Vertex AI logo\">\n",
    "      <br>Open in<br>Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td colspan=\"5\" style=\"text-align: right\">\n",
    "    <b>Share This On: </b> \n",
    "    <a href=\"https://www.linkedin.com/sharing/share-offsite/?url=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%2520Workflows/PyTorch/serving/scale-tests-dataflow-streaming-vertex.ipynb\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"Linkedin Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://reddit.com/submit?url=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%2520Workflows/PyTorch/serving/scale-tests-dataflow-streaming-vertex.ipynb\"><img src=\"https://redditinc.com/hubfs/Reddit%20Inc/Brand/Reddit_Logo.png\" alt=\"Reddit Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://bsky.app/intent/compose?text=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%2520Workflows/PyTorch/serving/scale-tests-dataflow-streaming-vertex.ipynb\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"BlueSky Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://twitter.com/intent/tweet?url=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%2520Workflows/PyTorch/serving/scale-tests-dataflow-streaming-vertex.ipynb\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/5/5a/X_icon_2.svg\" alt=\"X (Twitter) Logo\" width=\"20px\"></a> \n",
    "  </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td colspan=\"5\" style=\"text-align: right\">\n",
    "    <b>Connect With Author On: </b> \n",
    "    <a href=\"https://www.linkedin.com/in/statmike\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"Linkedin Logo\" width=\"20px\"></a>\n",
    "    <a href=\"https://www.github.com/statmike\"><img src=\"https://www.svgrepo.com/download/217753/github.svg\" alt=\"GitHub Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://www.youtube.com/@statmike-channel\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/f/fd/YouTube_full-color_icon_%282024%29.svg\" alt=\"YouTube Logo\" width=\"20px\"></a>\n",
    "    <a href=\"https://bsky.app/profile/statmike.bsky.social\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"BlueSky Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://x.com/statmike\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/5/5a/X_icon_2.svg\" alt=\"X (Twitter) Logo\" width=\"20px\"></a>\n",
    "  </td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataflow Streaming + Vertex AI Endpoint: Combined Scale Testing\n",
    "\n",
    "**Comprehensive system performance analysis and bottleneck identification for Dataflow pipelines calling Vertex AI endpoints.**\n",
    "\n",
    "---\n",
    "\n",
    "## What This Notebook Does\n",
    "\n",
    "This notebook provides comprehensive testing of the **combined system** where Dataflow Streaming calls a Vertex AI Endpoint via RunInference. Unlike the individual tests ([Endpoint-only](./scale-tests-vertex-ai-endpoints.ipynb) and [Dataflow-only](./scale-tests-dataflow-streaming-runinference.ipynb)), this focuses on:\n",
    "\n",
    "**System-Level Questions:**\n",
    "- üîç **Which component is the bottleneck?** (Dataflow workers vs Vertex endpoint)\n",
    "- ‚öñÔ∏è **What's the optimal worker-to-replica ratio?** (Cost vs performance)\n",
    "- üéØ **How should we configure both services together?** (Unified recommendations)\n",
    "- üí∞ **What's the total system cost at different scales?** (Combined pricing)\n",
    "\n",
    "## Testing Approach\n",
    "\n",
    "**Phase 0: Endpoint Health Check** (~2 minutes)\n",
    "- Quick baseline test (10 msg/sec for 2 mins)\n",
    "- Verify endpoint is responding through pipeline\n",
    "- Reference: Uses endpoint testing from [scale-tests-vertex-ai-endpoints.ipynb](./scale-tests-vertex-ai-endpoints.ipynb)\n",
    "\n",
    "**Phase 1: Baseline Combined Performance** (~5 minutes)\n",
    "- Low load tests (10-25 msg/sec)\n",
    "- Measure baseline latency for both services\n",
    "- Establish healthy operating parameters\n",
    "\n",
    "**Phase 2: Bottleneck Identification** (~30 minutes)\n",
    "- **2A: Stress Dataflow** - High message rate, small endpoint batches\n",
    "- **2B: Stress Endpoint** - Moderate message rate, large endpoint batches\n",
    "- **2C: Balanced Ramp** - Gradual increase to find equilibrium\n",
    "\n",
    "**Phase 3: Optimal Configuration Discovery** (~20 minutes)\n",
    "- Test different worker-to-replica ratios\n",
    "- Find cost-optimal configurations\n",
    "- Generate production recommendations\n",
    "\n",
    "**Total test time**: ~60-70 minutes\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before running this notebook, you need:\n",
    "\n",
    "- **Running Dataflow Streaming Job** from [dataflow-streaming-runinference-vertex.ipynb](./dataflow-streaming-runinference-vertex.ipynb)\n",
    "  - Job must be in \"Running\" state\n",
    "  - Pipeline configured to call Vertex AI endpoint\n",
    "- **Deployed Vertex AI Endpoint**\n",
    "  - Endpoint must be healthy and responding\n",
    "  - Baseline tested in [scale-tests-vertex-ai-endpoints.ipynb](./scale-tests-vertex-ai-endpoints.ipynb)\n",
    "- **Testing utilities**: `scale_testing_combined_utils.py` in same directory\n",
    "\n",
    "## How This Differs from Individual Tests\n",
    "\n",
    "| Aspect | Endpoint Test | Dataflow Test | **Combined Test (This)** |\n",
    "|--------|---------------|---------------|---------------------------|\n",
    "| **Focus** | Endpoint capacity | Pipeline latency | System bottlenecks |\n",
    "| **Metrics** | CPU, replicas, latency | Workers, lag, backlog | **Both synchronized** |\n",
    "| **Bottleneck** | Client vs endpoint | Pipeline vs workers | **Dataflow vs Endpoint** |\n",
    "| **Configuration** | Replica scaling | Worker scaling | **Both together** |\n",
    "| **Cost** | Endpoint replicas | Dataflow workers | **Total system cost** |\n",
    "\n",
    "## Key Metrics Collected\n",
    "\n",
    "**From Dataflow (via Cloud Monitoring):**\n",
    "- Worker count (autoscaling events)\n",
    "- System lag (processing delay)\n",
    "- Backlog size (unprocessed messages)\n",
    "- Element throughput\n",
    "\n",
    "**From Vertex AI (via Cloud Monitoring):**\n",
    "- Replica count (autoscaling events)\n",
    "- CPU utilization (autoscaling trigger)\n",
    "- Memory usage\n",
    "- Service latency (P95)\n",
    "- Prediction rate\n",
    "\n",
    "**From Pipeline Output (via Pub/Sub):**\n",
    "- End-to-end pipeline latency\n",
    "- Window wait time\n",
    "- Processing time (**includes endpoint call**)\n",
    "- Queue wait time (test infrastructure overhead)\n",
    "\n",
    "## Bottleneck Detection Strategy\n",
    "\n",
    "Since we're using **Option A** (no pipeline instrumentation), bottleneck detection correlates multiple signals:\n",
    "\n",
    "**Dataflow Bottleneck Indicators:**\n",
    "- ‚ö†Ô∏è High system lag (>10 seconds)\n",
    "- ‚ö†Ô∏è Workers scaled but lag persists\n",
    "- ‚ö†Ô∏è Low endpoint CPU (<40%)\n",
    "\n",
    "**Endpoint Bottleneck Indicators:**\n",
    "- ‚ö†Ô∏è High endpoint CPU (>60%)\n",
    "- ‚ö†Ô∏è Processing time dominates latency (>60% of total)\n",
    "- ‚ö†Ô∏è Endpoint service latency high (>500ms)\n",
    "- ‚ö†Ô∏è Low Dataflow lag (<5 seconds)\n",
    "\n",
    "**Balanced System:**\n",
    "- ‚úÖ Both services scaling proportionally\n",
    "- ‚úÖ Latency stable under load\n",
    "- ‚úÖ No excessive lag or CPU saturation\n",
    "\n",
    "## Related Notebooks\n",
    "\n",
    "**Individual Service Tests:**\n",
    "- [scale-tests-vertex-ai-endpoints.ipynb](./scale-tests-vertex-ai-endpoints.ipynb) - Endpoint-only testing\n",
    "- [scale-tests-dataflow-streaming-runinference.ipynb](./scale-tests-dataflow-streaming-runinference.ipynb) - Dataflow-only testing (local model)\n",
    "\n",
    "**Pipeline Deployment:**\n",
    "- [dataflow-streaming-runinference-vertex.ipynb](./dataflow-streaming-runinference-vertex.ipynb) - Deploy the pipeline tested here\n",
    "\n",
    "**Endpoint Deployment:**\n",
    "- [vertex-ai-endpoint-prebuilt-container.ipynb](./vertex-ai-endpoint-prebuilt-container.ipynb) - Deploy endpoint (pre-built)\n",
    "- [vertex-ai-endpoint-custom-container.ipynb](./vertex-ai-endpoint-custom-container.ipynb) - Deploy endpoint (custom)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Environment Setup\n",
    "\n",
    "This section will authenticate your session, enable required Google Cloud APIs, and install necessary Python packages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Your Project ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = 'statmike-mlops-349915' # replace with GCP project ID\n",
    "REQ_TYPE = 'ALL' # Specify PRIMARY or ALL or COLAB\n",
    "INSTALL_TOOL = 'poetry' # Specify pip, uv, or poetry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REQUIREMENTS_URL = 'https://raw.githubusercontent.com/statmike/vertex-ai-mlops/refs/heads/main/Framework%20Workflows/PyTorch/requirements.txt'\n",
    "\n",
    "REQUIRED_APIS = [\n",
    "    \"dataflow.googleapis.com\",\n",
    "    \"aiplatform.googleapis.com\",\n",
    "    \"pubsub.googleapis.com\",\n",
    "    \"monitoring.googleapis.com\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, urllib.request\n",
    "\n",
    "# Download and import setup code\n",
    "url = 'https://raw.githubusercontent.com/statmike/vertex-ai-mlops/refs/heads/main/core/notebook-template/python_setup.py'\n",
    "urllib.request.urlretrieve(url, 'python_setup_local.py')\n",
    "import python_setup_local as python_setup\n",
    "os.remove('python_setup_local.py')\n",
    "\n",
    "# Run setup\n",
    "setup_info = python_setup.setup_environment(PROJECT_ID, REQ_TYPE, REQUIREMENTS_URL, REQUIRED_APIS, INSTALL_TOOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Test Configuration\n",
    "\n",
    "Configure both the Dataflow job and Vertex AI endpoint to test, along with test parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "REGION = 'us-central1'\n",
    "EXPERIMENT = 'pytorch-autoencoder'\n",
    "\n",
    "# Dataflow Job Configuration\n",
    "JOB_NAME_PREFIX = 'pytorch-streaming-vertex-'  # Job name prefix from dataflow-streaming-runinference-vertex.ipynb\n",
    "\n",
    "# Vertex AI Endpoint Configuration  \n",
    "ENDPOINT_DISPLAY_NAME = 'pytorch-autoencoder-endpoint'  # Endpoint name\n",
    "\n",
    "# Pub/Sub Configuration (must match pipeline deployment)\n",
    "INPUT_TOPIC = f'projects/{PROJECT_ID}/topics/{EXPERIMENT}-input-vertex'\n",
    "OUTPUT_SUBSCRIPTION = f'projects/{PROJECT_ID}/subscriptions/{EXPERIMENT}-output-sub-vertex'\n",
    "\n",
    "# Phase 0: Endpoint Health Check (via pipeline)\n",
    "HEALTH_CHECK = {\n",
    "    \"target_rate\": 10,\n",
    "    \"duration\": 120,\n",
    "    \"name\": \"Health Check - 10 msg/sec\"\n",
    "}\n",
    "\n",
    "# Phase 1: Baseline Combined Performance\n",
    "BASELINE_TESTS = [\n",
    "    {\"target_rate\": 10, \"duration\": 120, \"name\": \"Baseline - 10 msg/sec\"},\n",
    "    {\"target_rate\": 25, \"duration\": 120, \"name\": \"Baseline - 25 msg/sec\"},\n",
    "]\n",
    "\n",
    "# Phase 2A: Stress Dataflow (overwhelm workers, endpoint idle)\n",
    "STRESS_DATAFLOW_TESTS = [\n",
    "    {\"target_rate\": 200, \"duration\": 300, \"name\": \"Stress Dataflow - 200 msg/sec\"},\n",
    "    {\"target_rate\": 500, \"duration\": 300, \"name\": \"Stress Dataflow - 500 msg/sec\"},\n",
    "]\n",
    "\n",
    "# Phase 2B: Stress Endpoint (overwhelm endpoint, workers idle)\n",
    "# Note: Pipeline batches messages before sending to endpoint\n",
    "# High message rate + batching = high load on endpoint\n",
    "STRESS_ENDPOINT_TESTS = [\n",
    "    {\"target_rate\": 50, \"duration\": 300, \"name\": \"Stress Endpoint - 50 msg/sec (batched)\"},\n",
    "    {\"target_rate\": 100, \"duration\": 300, \"name\": \"Stress Endpoint - 100 msg/sec (batched)\"},\n",
    "]\n",
    "\n",
    "# Phase 2C: Balanced Ramp (find equilibrium)\n",
    "RAMP_TEST = {\n",
    "    \"target_rate\": 200,\n",
    "    \"duration\": 900,  # 15 minutes\n",
    "    \"name\": \"Ramp - 1‚Üí200 msg/sec\"\n",
    "}\n",
    "\n",
    "# Phase 3: Optimal Configuration Discovery\n",
    "# Based on Phase 2 results, test specific worker-replica combinations\n",
    "# These will be defined after Phase 2 analysis\n",
    "OPTIMIZATION_TESTS = [\n",
    "    {\"target_rate\": 100, \"duration\": 300, \"name\": \"Optimized - 100 msg/sec\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import asyncio\n",
    "import time\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "# Import combined testing utilities\n",
    "from scale_testing_combined_utils import (\n",
    "    CombinedMetricsCollector,\n",
    "    identify_bottleneck,\n",
    "    calculate_worker_replica_ratio,\n",
    "    plot_combined_timeline\n",
    ")\n",
    "\n",
    "# Import load generator from Dataflow utilities\n",
    "from scale_testing_dataflow_utils import PubSubLoadGenerator\n",
    "\n",
    "from google.cloud import dataflow_v1beta3, aiplatform\n",
    "\n",
    "print(\"‚úÖ Imports complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Verify Infrastructure\n",
    "\n",
    "Before starting tests, verify both Dataflow job and Vertex AI endpoint are running and healthy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Dataflow Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find running Dataflow job\n",
    "dataflow_client = dataflow_v1beta3.JobsV1Beta3Client()\n",
    "\n",
    "print(\"Searching for running Dataflow jobs...\")\n",
    "request = dataflow_v1beta3.ListJobsRequest(\n",
    "    project_id=PROJECT_ID,\n",
    "    location=REGION\n",
    ")\n",
    "\n",
    "JOB_ID = None\n",
    "JOB_NAME = None\n",
    "\n",
    "try:\n",
    "    response = dataflow_client.list_jobs(request=request)\n",
    "    \n",
    "    for job in response:\n",
    "        if (job.current_state.name == \"JOB_STATE_RUNNING\" and\n",
    "            job.name.startswith(JOB_NAME_PREFIX)):\n",
    "            JOB_ID = job.id\n",
    "            JOB_NAME = job.name\n",
    "            break\n",
    "    \n",
    "    if not JOB_ID:\n",
    "        print(f\"\\n‚ö†Ô∏è  No running jobs found with prefix: {JOB_NAME_PREFIX}\")\n",
    "        print(\"   Please start the Dataflow Streaming job first\")\n",
    "        print(\"   See: dataflow-streaming-runinference-vertex.ipynb\")\n",
    "        raise ValueError(\"No running Dataflow job found\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ö†Ô∏è  Error finding Dataflow job: {e}\")\n",
    "    raise\n",
    "\n",
    "# Get full job details\n",
    "job = dataflow_client.get_job(\n",
    "    request={\n",
    "        \"project_id\": PROJECT_ID,\n",
    "        \"location\": REGION,\n",
    "        \"job_id\": JOB_ID\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"DATAFLOW JOB STATUS\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Name: {job.name}\")\n",
    "print(f\"ID: {job.id}\")\n",
    "print(f\"State: {job.current_state.name}\")\n",
    "print(f\"Created: {job.create_time}\")\n",
    "print(f\"\\n‚úÖ Dataflow job is running\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nMonitor: https://console.cloud.google.com/dataflow/jobs/{REGION}/{JOB_ID}?project={PROJECT_ID}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Vertex AI Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find Vertex AI endpoint\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION)\n",
    "\n",
    "print(f\"Searching for endpoint: {ENDPOINT_DISPLAY_NAME}\")\n",
    "endpoints = aiplatform.Endpoint.list(\n",
    "    filter=f'display_name=\"{ENDPOINT_DISPLAY_NAME}\"',\n",
    "    order_by=\"create_time desc\"\n",
    ")\n",
    "\n",
    "if not endpoints:\n",
    "    raise ValueError(\n",
    "        f\"No endpoint found: {ENDPOINT_DISPLAY_NAME}\\n\"\n",
    "        f\"Please deploy an endpoint first using:\\n\"\n",
    "        f\"  - vertex-ai-endpoint-prebuilt-container.ipynb\\n\"\n",
    "        f\"  - vertex-ai-endpoint-custom-container.ipynb\"\n",
    "    )\n",
    "\n",
    "endpoint = endpoints[0]\n",
    "ENDPOINT_ID = endpoint.name.split(\"/\")[-1]\n",
    "\n",
    "# Get endpoint configuration\n",
    "deployed_model = endpoint.list_models()[0]\n",
    "MACHINE_TYPE = deployed_model.dedicated_resources.machine_spec.machine_type\n",
    "MIN_REPLICAS = deployed_model.dedicated_resources.min_replica_count\n",
    "MAX_REPLICAS = deployed_model.dedicated_resources.max_replica_count\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"VERTEX AI ENDPOINT STATUS\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Name: {endpoint.display_name}\")\n",
    "print(f\"ID: {ENDPOINT_ID}\")\n",
    "print(f\"Machine: {MACHINE_TYPE}\")\n",
    "print(f\"Replicas: {MIN_REPLICAS} - {MAX_REPLICAS}\")\n",
    "print(f\"\\n‚úÖ Endpoint is deployed\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Testing Infrastructure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize load generator\n",
    "load_generator = PubSubLoadGenerator(\n",
    "    project_id=PROJECT_ID,\n",
    "    topic_name=f\"{EXPERIMENT}-input-vertex\"  # Vertex endpoint pipeline topic\n",
    ")\n",
    "\n",
    "# Initialize combined metrics collector\n",
    "metrics_collector = CombinedMetricsCollector(\n",
    "    project_id=PROJECT_ID,\n",
    "    dataflow_job_id=JOB_ID,\n",
    "    endpoint_id=ENDPOINT_ID,\n",
    "    region=REGION,\n",
    "    output_subscription=OUTPUT_SUBSCRIPTION\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Testing infrastructure initialized\")\n",
    "print(f\"\\nConfiguration:\")\n",
    "print(f\"  Project: {PROJECT_ID}\")\n",
    "print(f\"  Dataflow Job: {JOB_NAME}\")\n",
    "print(f\"  Endpoint: {ENDPOINT_DISPLAY_NAME}\")\n",
    "print(f\"  Input topic: {INPUT_TOPIC}\")\n",
    "print(f\"  Output subscription: {OUTPUT_SUBSCRIPTION}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Phase 0: Endpoint Health Check\n",
    "\n",
    "**Goal**: Verify endpoint is responding through the pipeline\n",
    "\n",
    "**Test**: 1 quick test (~2 minutes)\n",
    "- 10 msg/sec for 2 minutes\n",
    "\n",
    "**What We're Looking For**:\n",
    "- Messages successfully processed through pipeline\n",
    "- Endpoint responding to RunInference calls\n",
    "- Baseline latency for combined system\n",
    "\n",
    "‚è≥ **This phase will take approximately 2 minutes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Phase 0 storage\n",
    "phase0_results = []\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PHASE 0: ENDPOINT HEALTH CHECK (VIA PIPELINE)\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Testing: {HEALTH_CHECK['target_rate']} msg/sec for {HEALTH_CHECK['duration']}s\")\n",
    "print(f\"Total estimated time: ~2 minutes\")\n",
    "print(\"=\" * 70 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run health check\n",
    "health_result = await load_generator.run_load_test(\n",
    "    pattern=\"sustained\",\n",
    "    target_rate=HEALTH_CHECK[\"target_rate\"],\n",
    "    duration=HEALTH_CHECK[\"duration\"],\n",
    "    test_name=HEALTH_CHECK[\"name\"]\n",
    ")\n",
    "phase0_results.append(health_result)\n",
    "\n",
    "print(f\"\\n‚úÖ Phase 0 complete: Health check finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect metrics for health check\n",
    "phase0_start = health_result['start_time']\n",
    "phase0_end = health_result['end_time']\n",
    "\n",
    "print(f\"Collecting combined metrics for health check\")\n",
    "print(f\"Time window: {phase0_start.strftime('%H:%M:%S')} ‚Üí {phase0_end.strftime('%H:%M:%S')}\")\n",
    "\n",
    "phase0_metrics = metrics_collector.collect_combined_metrics(\n",
    "    start_time=phase0_start,\n",
    "    end_time=phase0_end,\n",
    "    resolution_seconds=10\n",
    ")\n",
    "\n",
    "# Collect latency\n",
    "phase0_latency = metrics_collector.dataflow_collector.collect_end_to_end_latency(\n",
    "    test_id=health_result['test_id'],\n",
    "    expected_messages=health_result['num_messages'],\n",
    "    timeout=180\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Metrics collected for Phase 0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze health check results\n",
    "print(\"=\" * 70)\n",
    "print(\"PHASE 0 ANALYSIS: HEALTH CHECK\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\n{HEALTH_CHECK['name']}:\")\n",
    "print(f\"  Target: {HEALTH_CHECK['target_rate']} msg/sec for {HEALTH_CHECK['duration']}s\")\n",
    "print(f\"  Sent: {health_result['num_messages']:,} messages\")\n",
    "print(f\"  Actual rate: {health_result['actual_rate']:.1f} msg/sec\")\n",
    "\n",
    "if len(phase0_latency) > 0:\n",
    "    print(f\"\\n  Combined System Latency:\")\n",
    "    print(f\"    Pipeline (P50):  {phase0_latency['pipeline_latency_ms'].quantile(0.50):7.1f}ms\")\n",
    "    print(f\"    Pipeline (P95):  {phase0_latency['pipeline_latency_ms'].quantile(0.95):7.1f}ms\")\n",
    "    print(f\"    Pipeline (P99):  {phase0_latency['pipeline_latency_ms'].quantile(0.99):7.1f}ms\")\n",
    "    print(f\"    Pipeline (mean): {phase0_latency['pipeline_latency_ms'].mean():7.1f}ms\")\n",
    "    \n",
    "    print(f\"\\n  Note: Pipeline latency includes all processing (endpoint + Dataflow + windowing)\")\n",
    "    \n",
    "    # Check endpoint service latency from Cloud Monitoring\n",
    "    if 'endpoint' in phase0_metrics and 'latency' in phase0_metrics['endpoint'] and len(phase0_metrics['endpoint']['latency']) > 0:\n",
    "        endpoint_latency = phase0_metrics['endpoint']['latency']['value'].mean()\n",
    "        print(f\"\\n  Endpoint service latency: {endpoint_latency:.1f}ms (from Cloud Monitoring)\")\n",
    "        \n",
    "        # Estimate endpoint contribution to pipeline latency\n",
    "        endpoint_ratio = endpoint_latency / phase0_latency['pipeline_latency_ms'].mean() * 100\n",
    "        print(f\"  Endpoint contributes ~{endpoint_ratio:.1f}% of pipeline latency\")\n",
    "    \n",
    "    print(f\"\\n  ‚úÖ Endpoint is healthy and responding through pipeline\")\n",
    "else:\n",
    "    print(f\"\\n  ‚ö†Ô∏è  No latency data collected\")\n",
    "    print(f\"     Check Dataflow job logs and endpoint status\")\n",
    "\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Phase 1: Baseline Combined Performance\n",
    "\n",
    "**Goal**: Establish baseline latency for the combined system at low load\n",
    "\n",
    "**Tests**: 2 baseline tests (~5 minutes total)\n",
    "- 10 msg/sec for 2 minutes\n",
    "- 25 msg/sec for 2 minutes\n",
    "\n",
    "**What We're Looking For**:\n",
    "- Baseline combined system latency\n",
    "- Worker and replica counts at low load\n",
    "- Confirm no immediate bottlenecks\n",
    "\n",
    "‚è≥ **This phase will take approximately 5 minutes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Phase 1 storage\n",
    "phase1_results = []\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PHASE 1: BASELINE COMBINED PERFORMANCE\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Testing {len(BASELINE_TESTS)} baseline rates\")\n",
    "print(f\"Total estimated time: ~5 minutes\")\n",
    "print(\"=\" * 70 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "phase1_init",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Phase 1 storage\n",
    "phase1_results = []\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"PHASE 1: BASELINE COMBINED PERFORMANCE\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Testing {len(BASELINE_TESTS)} baseline rates\")\n",
    "print(f\"Total estimated time: ~5 minutes\")\n",
    "print(\"=\"*70 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "phase1_run",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run baseline tests\n",
    "for test_config in BASELINE_TESTS:\n",
    "    result = await load_generator.run_load_test(\n",
    "        pattern=\"sustained\",\n",
    "        target_rate=test_config[\"target_rate\"],\n",
    "        duration=test_config[\"duration\"],\n",
    "        test_name=test_config[\"name\"]\n",
    "    )\n",
    "    phase1_results.append(result)\n",
    "    \n",
    "    # Small delay between tests\n",
    "    print(\"\\n   Waiting 30 seconds before next test...\\n\")\n",
    "    await asyncio.sleep(30)\n",
    "\n",
    "print(f\"\\n‚úÖ Phase 1 complete: {len(BASELINE_TESTS)} baseline tests run\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phase1_metrics_header",
   "metadata": {},
   "source": [
    "### Collect Metrics for Phase 1\n",
    "\n",
    "Collect Dataflow metrics (workers, system lag, backlog) and end-to-end latency from Pub/Sub output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "phase1_metrics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect metrics for entire Phase 1\n",
    "phase1_start = min([r['start_time'] for r in phase1_results])\n",
    "phase1_end = max([r['end_time'] for r in phase1_results])\n",
    "\n",
    "print(f\"Collecting metrics for Phase 1\")\n",
    "print(f\"Time window: {phase1_start.strftime('%H:%M:%S')} ‚Üí {phase1_end.strftime('%H:%M:%S')}\")\n",
    "\n",
    "phase1_metrics = metrics_collector.collect_combined_metrics(\n",
    "    start_time=phase1_start,\n",
    "    end_time=phase1_end,\n",
    "    resolution_seconds=10\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Metrics collected for Phase 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "phase1_latency",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect end-to-end latency for each test\n",
    "phase1_latencies = []\n",
    "\n",
    "for result in phase1_results:\n",
    "    latency_df = metrics_collector.dataflow_collector.collect_end_to_end_latency(\n",
    "        test_id=result['test_id'],\n",
    "        expected_messages=result['num_messages'],\n",
    "        timeout=180\n",
    "    )\n",
    "    phase1_latencies.append(latency_df)\n",
    "\n",
    "print(f\"\\n‚úÖ Latency data collected for {len(phase1_latencies)} tests\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phase1_analysis_header",
   "metadata": {},
   "source": [
    "### Analyze Phase 1 Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "phase1_analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze Phase 1 results\n",
    "print(\"=\"*70)\n",
    "print(\"PHASE 1 ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for i, (result, latency_df) in enumerate(zip(phase1_results, phase1_latencies)):\n",
    "    test_config = BASELINE_TESTS[i]\n",
    "    \n",
    "    print(f\"\\n{test_config['name']}:\")\n",
    "    print(f\"  Target: {test_config['target_rate']} msg/sec for {test_config['duration']}s\")\n",
    "    print(f\"  Sent: {result['num_messages']:,} messages\")\n",
    "    print(f\"  Actual rate: {result['actual_rate']:.1f} msg/sec\")\n",
    "    \n",
    "    if len(latency_df) > 0:\n",
    "        print(f\"\\n  Latency Statistics:\")\n",
    "        print(f\"    Pipeline (P50):  {latency_df['pipeline_latency_ms'].quantile(0.50):7.1f}ms\")\n",
    "        print(f\"    Pipeline (P95):  {latency_df['pipeline_latency_ms'].quantile(0.95):7.1f}ms\")\n",
    "        print(f\"    Pipeline (P99):  {latency_df['pipeline_latency_ms'].quantile(0.99):7.1f}ms\")\n",
    "        print(f\"    Pipeline (mean): {latency_df['pipeline_latency_ms'].mean():7.1f}ms\")\n",
    "        \n",
    "        print(f\"\\n  Note: Pipeline latency includes all processing (endpoint + Dataflow + windowing)\")\n",
    "    else:\n",
    "        print(f\"\\n  ‚ö†Ô∏è  No latency data collected (messages may not have been processed yet)\")\n",
    "\n",
    "# Check for autoscaling\n",
    "autoscaling_events = metrics_collector.dataflow_collector.analyze_autoscaling(phase1_metrics['dataflow'])\n",
    "\n",
    "print(f\"\\n\" + \"=\"*70)\n",
    "print(\"AUTOSCALING ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "if len(autoscaling_events) > 0:\n",
    "    print(f\"‚úÖ Autoscaling triggered {len(autoscaling_events)} time(s)\")\n",
    "    for idx, event in autoscaling_events.iterrows():\n",
    "        print(f\"\\nEvent {idx + 1}:\")\n",
    "        print(f\"  Trigger: {event['trigger_time'].strftime('%H:%M:%S')}\")\n",
    "        print(f\"  Complete: {event['scale_complete_time'].strftime('%H:%M:%S')}\")\n",
    "        print(f\"  Workers: {event['workers_before']:.0f} ‚Üí {event['workers_after']:.0f}\")\n",
    "        print(f\"  Lag: {event['scale_up_lag_seconds']:.0f}s ({event['scale_up_lag_seconds']/60:.1f} mins)\")\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è  No autoscaling detected at baseline rates (expected)\")\n",
    "\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4114b23b",
   "metadata": {},
   "source": [
    "### üìä How to Read the Visualization\n",
    "\n",
    "The timeline visualization below shows 7 panels that track different aspects of pipeline performance. Here's what each panel tells you:\n",
    "\n",
    "#### **Panel 1: Incoming Message Rate**\n",
    "- **What it shows:** Messages published per second to the input Pub/Sub topic\n",
    "- **What to look for:**\n",
    "  - Flat line = steady rate (good for controlled testing)\n",
    "  - Spikes = burst traffic\n",
    "  - Gradual increase = ramp test pattern\n",
    "- **Interpretation:** This is your **input load** - what you're sending to the pipeline\n",
    "\n",
    "#### **Panel 2: Worker Count (Autoscaling)**\n",
    "- **What it shows:** Number of active Dataflow workers over time\n",
    "- **What to look for:**\n",
    "  - Flat line = no autoscaling needed\n",
    "  - Step up = autoscaling triggered (workers being added)\n",
    "  - Step down = scale-down (workers being removed after low load period)\n",
    "- **Interpretation:** Shows when pipeline capacity changes. Worker provisioning takes **2-4 minutes**, so you'll see a delay between load increase and worker scale-up.\n",
    "\n",
    "#### **Panel 3: System Lag (Processing Delay: Oldest To Current Message)**\n",
    "- **What it shows:** How far behind real-time the pipeline is processing (in milliseconds)\n",
    "- **What to look for:**\n",
    "  - Near-zero lag = pipeline keeping up with real-time\n",
    "  - Increasing lag = pipeline falling behind\n",
    "  - High lag (>60s) = serious capacity issues\n",
    "- **Interpretation:** This is your **real-time health indicator**. Rising lag means you need more workers or better machine type.\n",
    "\n",
    "#### **Panel 4: Output Subscription Backlog**\n",
    "- **What it shows:** Number of messages waiting in the output Pub/Sub subscription\n",
    "- **What to look for:**\n",
    "  - Zero or low = messages being consumed quickly\n",
    "  - Linearly increasing = messages accumulating (expected during testing)\n",
    "  - Very high (>10,000) = potential capacity issues\n",
    "- **Interpretation:** This backlog includes **both** pipeline processing queue AND messages waiting for the test framework to pull them. During tests, this will grow because we're not consuming as fast as we're producing.\n",
    "- **Why backlog may decline then increase:** Phase 1 runs 2 tests back-to-back but only collects latency AFTER both complete. If backlog starts high (residual from Phase 0), it declines as the pipeline clears old messages faster than new ones arrive at low rates, then increases as new test messages accumulate. This pattern shows the pipeline's backlog recovery capability‚Äîvaluable for understanding behavior when catching up after traffic spikes.\n",
    "\n",
    "#### **Panel 5: Vertex Endpoint Replicas (Autoscaling)**\n",
    "- **What it shows:** Number of active endpoint replicas over time\n",
    "- **What to look for:**\n",
    "  - Flat line = no autoscaling needed\n",
    "  - Step up = endpoint autoscaling triggered (CPU >60%)\n",
    "  - Step down = scale-down after low CPU period\n",
    "- **Interpretation:** Shows when endpoint capacity changes. Replica provisioning takes **3-5 minutes**.\n",
    "\n",
    "#### **Panel 6: Vertex Endpoint CPU Utilization**\n",
    "- **What it shows:** CPU usage percentage across all endpoint replicas\n",
    "- **What to look for:**\n",
    "  - Low CPU (<40%) = endpoint has headroom\n",
    "  - Medium CPU (40-60%) = healthy utilization\n",
    "  - High CPU (>60%) = autoscaling threshold, replicas will be added\n",
    "- **Interpretation:** Endpoint autoscaling triggers at 60% CPU. Values shown are now **actual CPU usage** (fix applied to prevent aggregation producing zeros).\n",
    "\n",
    "#### **Panel 7: P95 Pipeline Latency (publish ‚Üí output queue)**\n",
    "- **What it shows:** 95th percentile latency from message publish to arriving in output queue\n",
    "- **What to look for:**\n",
    "  - Steady line = consistent performance\n",
    "  - Gradual increase = latency degrading under load\n",
    "  - Spikes at test end = shutdown artifacts (normal)\n",
    "- **Interpretation:** This is your **actual pipeline performance metric** - the one you care about most. It excludes the time messages sit in the queue waiting for the test to pull them.\n",
    "- **Y-axis note:** The axis is auto-scaled to focus on steady-state latency using the 99th percentile. Very high spikes (like test shutdown) may be clipped but won't distort the main trend.\n",
    "- **Timeline fix:** Now uses **publish_time** instead of receive_time for better alignment with test execution.\n",
    "\n",
    "---\n",
    "\n",
    "**üí° Key Insights to Extract:**\n",
    "1. **When does autoscaling trigger?** ‚Üí Look for worker/replica count step-ups in Panels 2 and 5\n",
    "2. **What's the latency at different loads?** ‚Üí Compare Panel 7 across different message rates in Panel 1\n",
    "3. **Is the pipeline keeping up?** ‚Üí Panel 3 system lag should stay near zero\n",
    "4. **Is backlog building up?** ‚Üí Panel 4 shows message accumulation in output queue\n",
    "5. **Where's the bottleneck?** ‚Üí If latency is high but system lag is low, it's windowing/batching. If system lag is high, it's processing capacity. If endpoint CPU is high, endpoint is the bottleneck.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "phase1_viz",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Phase 1 - Individual test timelines\n",
    "for i, (result, latency_df) in enumerate(zip(phase1_results, phase1_latencies)):\n",
    "    test_config = BASELINE_TESTS[i]\n",
    "    \n",
    "    # Collect metrics for this specific test\n",
    "    test_metrics = metrics_collector.collect_combined_metrics(\n",
    "        start_time=result['start_time'],\n",
    "        end_time=result['end_time'],\n",
    "        resolution_seconds=10\n",
    "    )\n",
    "    \n",
    "    fig = plot_combined_timeline(\n",
    "        combined_metrics=test_metrics,\n",
    "        latency_df=latency_df,\n",
    "        load_pattern_data=result['message_data'],\n",
    "        test_name=test_config['name']\n",
    "    )\n",
    "    fig.show(renderer=\"png\", width=1400, height=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç Queuing Diagnostic: Where Does the Latency Come From?\n",
    "\n",
    "The visualization above shows WHAT happened. Now let's understand WHY.\n",
    "\n",
    "This diagnostic reveals:\n",
    "- **Latency Attribution**: What % is window assignment vs internal queuing\n",
    "- **Queue Growth**: How queue depth evolves over time\n",
    "- **Autoscaling Response**: Did workers/replicas scale in response to queuing?\n",
    "\n",
    "**How to Read the Diagnostic:**\n",
    "\n",
    "**Panel 1 (Pie Chart)**: Latency Attribution\n",
    "- üîµ **Blue = Window Assignment** (just metadata timestamps, 0-1 second)\n",
    "- üî¥ **Red = Internal Queuing** (ACTUAL bottleneck - messages waiting inside workers)\n",
    "- ‚úÖ Good: Red < 20% (window config is appropriate)\n",
    "- ‚ö†Ô∏è  Concern: Red > 50% (workers can't keep up)\n",
    "- ‚ùå Problem: Red > 80% (severe worker saturation)\n",
    "\n",
    "**Panel 2 (Stacked Area)**: Components Over Time\n",
    "- Shows how latency components change as load increases\n",
    "- Watch for red area growing ‚Üí queue building up\n",
    "\n",
    "**Panel 3 (Heatmap)**: Queue Depth Over Time\n",
    "- Dark red = Messages severely delayed\n",
    "- Shows WHEN queuing starts (correlate with load changes)\n",
    "\n",
    "**Panel 4 (Autoscaling)**: THE SMOKING GUN\n",
    "- üîµ Blue line = Worker count (Dataflow)\n",
    "- üü¢ Green dashed = Replica count (Vertex Endpoint)\n",
    "- üî¥ Red line = Queue depth\n",
    "- **Key insight**: If queue grows but workers stay flat ‚Üí autoscaling failed\n",
    "\n",
    "**Common Patterns:**\n",
    "\n",
    "1. **Healthy System**:\n",
    "   - Window assignment: 5-15%\n",
    "   - Internal queue: <20%\n",
    "   - Workers scale up as queue grows\n",
    "   - Queue depth stays low (<1 second)\n",
    "\n",
    "2. **Worker Autoscaling Failure** (most common):\n",
    "   - Internal queue: 80-95%\n",
    "   - Workers stay at 1 despite configuration\n",
    "   - Queue depth grows to 10+ seconds\n",
    "   - **Fix**: See recommended fixes below\n",
    "\n",
    "3. **Endpoint Bottleneck**:\n",
    "   - Internal queue: 30-50%\n",
    "   - Replicas stay at min\n",
    "   - Endpoint CPU at 100%\n",
    "   - **Fix**: Increase max replicas or machine type\n",
    "\n",
    "**If You See Workers NOT Scaling:**\n",
    "\n",
    "The autoscaling algorithm may not be triggering because:\n",
    "1. Pub/Sub backlog stays small (workers pull fast, process slow)\n",
    "2. System lag metric doesn't capture internal worker queuing\n",
    "3. Wrong autoscaling algorithm for the workload\n",
    "\n",
    "**Recommended Fixes:**\n",
    "\n",
    "```python\n",
    "# In pipeline options, add:\n",
    "\"--autoscaling_algorithm=THROUGHPUT_BASED\",  # Scale based on throughput, not backlog\n",
    "\"--target_worker_utilization=0.7\",            # Scale at 70% capacity (more aggressive)\n",
    "\"--num_workers=2\",                             # Ensure proper initial worker count\n",
    "```\n",
    "\n",
    "Additionally in the pipeline code:\n",
    "```python\n",
    "# Reduce buffering to expose backpressure faster\n",
    "| \"Batch elements\" >> beam.BatchElements(\n",
    "    min_batch_size=1,\n",
    "    max_batch_size=50,\n",
    "    max_batch_duration_secs=0.001  # 1ms instead of 10ms\n",
    ")\n",
    "```\n",
    "\n",
    "**Understanding the Metrics:**\n",
    "\n",
    "- **Window wait time**: Time from publish to window_end timestamp (metadata only, not actual waiting)\n",
    "- **Internal queue time**: Time from window_end to pipeline output (REAL delay - workers saturated)\n",
    "- **P95 queue depth**: 95th percentile worst-case delay (most sensitive metric)\n",
    "\n",
    "The key insight: Window configuration is almost NEVER the bottleneck. The real issue is always worker saturation or endpoint throttling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create queuing diagnostic\n",
    "from scale_testing_combined_utils import create_queuing_diagnostic, print_queuing_summary\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"QUEUING DIAGNOSTIC\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create visualization\n",
    "fig_diagnostic = create_queuing_diagnostic(\n",
    "    latency_df=pd.concat(phase1_latencies, ignore_index=True),\n",
    "    dataflow_metrics=phase1_metrics['dataflow'],\n",
    "    endpoint_metrics=phase1_metrics['endpoint'],\n",
    "    test_name=\"Phase 1: Baseline Performance\"\n",
    ")\n",
    "\n",
    "fig_diagnostic.show(renderer=\"png\", width=1400, height=1200)\n",
    "\n",
    "# Print summary\n",
    "print_queuing_summary(\n",
    "    latency_df=pd.concat(phase1_latencies, ignore_index=True),\n",
    "    dataflow_metrics=phase1_metrics['dataflow'],\n",
    "    endpoint_metrics=phase1_metrics['endpoint']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phase2a_header",
   "metadata": {},
   "source": [
    "---\n",
    "## Phase 2A: Throughput Threshold Hunt\n",
    "\n",
    "**Goal**: Find exact threshold where backlog builds and autoscaling triggers\n",
    "\n",
    "**Tests**: 3 sustained load tests (~20 minutes total)\n",
    "- 50 msg/sec for 5 minutes\n",
    "- 100 msg/sec for 5 minutes\n",
    "- 200 msg/sec for 5 minutes\n",
    "\n",
    "**What We're Looking For**:\n",
    "- At what message rate does backlog start building?\n",
    "- At what point does system lag increase?\n",
    "- When do workers autoscale up?\n",
    "- How long does it take for new workers to become active?\n",
    "\n",
    "‚è≥ **This phase will take approximately 20 minutes** (3 tests √ó 5 mins each + metrics collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "phase2a_init",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Phase 2A storage\n",
    "phase2a_results = []\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"PHASE 2A: STRESS DATAFLOW\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Testing {len(STRESS_DATAFLOW_TESTS)} sustained load patterns\")\n",
    "print(f\"Total estimated time: ~{sum(t['duration'] for t in STRESS_DATAFLOW_TESTS) // 60} minutes\")\n",
    "print(\"\\nThis phase tests sustained high load to trigger autoscaling.\")\n",
    "print(\"=\"*70 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "phase2a_run",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run sustained load tests\n",
    "for test_config in STRESS_DATAFLOW_TESTS:\n",
    "    result = await load_generator.run_load_test(\n",
    "        pattern=\"sustained\",\n",
    "        target_rate=test_config[\"target_rate\"],\n",
    "        duration=test_config[\"duration\"],\n",
    "        test_name=test_config[\"name\"]\n",
    "    )\n",
    "    phase2a_results.append(result)\n",
    "    \n",
    "    # Small delay between tests to let pipeline stabilize\n",
    "    print(\"\\n   Waiting 60 seconds before next test...\\n\")\n",
    "    await asyncio.sleep(60)\n",
    "\n",
    "print(f\"\\n‚úÖ Phase 2A complete: {len(STRESS_DATAFLOW_TESTS)} sustained tests run\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phase2a_metrics_header",
   "metadata": {},
   "source": [
    "### Collect Metrics for Phase 2A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "phase2a_metrics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect metrics for entire Phase 2A\n",
    "phase2a_start = min([r['start_time'] for r in phase2a_results])\n",
    "phase2a_end = max([r['end_time'] for r in phase2a_results])\n",
    "\n",
    "print(f\"Collecting metrics for Phase 2A\")\n",
    "print(f\"Time window: {phase2a_start.strftime('%H:%M:%S')} ‚Üí {phase2a_end.strftime('%H:%M:%S')}\")\n",
    "\n",
    "phase2a_metrics = metrics_collector.collect_combined_metrics(\n",
    "    start_time=phase2a_start,\n",
    "    end_time=phase2a_end,\n",
    "    resolution_seconds=10\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Metrics collected for Phase 2A\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "phase2a_latency",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect end-to-end latency for each test\n",
    "phase2a_latencies = []\n",
    "\n",
    "for result in phase2a_results:\n",
    "    latency_df = metrics_collector.dataflow_collector.collect_end_to_end_latency(\n",
    "        test_id=result['test_id'],\n",
    "        expected_messages=result['num_messages'],\n",
    "        timeout=300  # Longer timeout for high-volume tests\n",
    "    )\n",
    "    phase2a_latencies.append(latency_df)\n",
    "\n",
    "print(f\"\\n‚úÖ Latency data collected for {len(phase2a_latencies)} tests\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phase2a_analysis_header",
   "metadata": {},
   "source": [
    "### Analyze Phase 2A Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "phase2a_analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed analysis of Phase 2A\n",
    "print(\"=\"*70)\n",
    "print(\"PHASE 2A ANALYSIS: THROUGHPUT THRESHOLDS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for i, (result, latency_df) in enumerate(zip(phase2a_results, phase2a_latencies)):\n",
    "    test_config = STRESS_DATAFLOW_TESTS[i]\n",
    "    \n",
    "    print(f\"\\n{test_config['name']}:\")\n",
    "    print(f\"  Configuration: {test_config['target_rate']} msg/sec for {test_config['duration']}s\")\n",
    "    print(f\"  Messages sent: {result['num_messages']:,}\")\n",
    "    print(f\"  Actual rate: {result['actual_rate']:.1f} msg/sec\")\n",
    "    \n",
    "    if len(latency_df) > 0:\n",
    "        print(f\"\\n  Latency Performance:\")\n",
    "        print(f\"    Mean E2E: {latency_df['pipeline_latency_ms'].mean():7.1f}ms\")\n",
    "        print(f\"    P95 E2E:  {latency_df['pipeline_latency_ms'].quantile(0.95):7.1f}ms\")\n",
    "        print(f\"    P99 E2E:  {latency_df['pipeline_latency_ms'].quantile(0.99):7.1f}ms\")\n",
    "        \n",
    "        # Latency degradation check\n",
    "        if i > 0 and len(phase2a_latencies[0]) > 0:\n",
    "            baseline_p95 = phase2a_latencies[0]['pipeline_latency_ms'].quantile(0.95)\n",
    "            current_p95 = latency_df['pipeline_latency_ms'].quantile(0.95)\n",
    "            degradation_pct = ((current_p95 - baseline_p95) / baseline_p95) * 100\n",
    "            \n",
    "            print(f\"\\n  Latency vs Baseline:\")\n",
    "            print(f\"    Baseline P95: {baseline_p95:.1f}ms\")\n",
    "            print(f\"    Current P95:  {current_p95:.1f}ms\")\n",
    "            print(f\"    Change: {degradation_pct:+.1f}%\")\n",
    "            \n",
    "            if degradation_pct > 50:\n",
    "                print(f\"    ‚ö†Ô∏è  Significant latency increase - may indicate capacity limits\")\n",
    "    else:\n",
    "        print(f\"\\n  ‚ö†Ô∏è  No latency data (processing may be delayed)\")\n",
    "\n",
    "# Analyze autoscaling behavior\n",
    "print(f\"\\n\" + \"=\"*70)\n",
    "print(\"AUTOSCALING EVENTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "autoscaling_events = metrics_collector.dataflow_collector.analyze_autoscaling(phase2a_metrics['dataflow'])\n",
    "\n",
    "if len(autoscaling_events) > 0:\n",
    "    print(f\"\\n‚úÖ Autoscaling triggered {len(autoscaling_events)} time(s) during Phase 2A\\n\")\n",
    "    \n",
    "    for idx, event in autoscaling_events.iterrows():\n",
    "        print(f\"Event {idx + 1}:\")\n",
    "        print(f\"  Trigger time:     {event['trigger_time'].strftime('%H:%M:%S')}\")\n",
    "        print(f\"  Complete time:    {event['scale_complete_time'].strftime('%H:%M:%S')}\")\n",
    "        print(f\"  Workers:          {event['workers_before']:.0f} ‚Üí {event['workers_after']:.0f}\")\n",
    "        print(f\"  Provisioning lag: {event['scale_up_lag_seconds']:.0f}s ({event['scale_up_lag_seconds']/60:.1f} mins)\")\n",
    "        print()\n",
    "    \n",
    "    # Calculate average provisioning time\n",
    "    avg_lag = autoscaling_events['scale_up_lag_seconds'].mean()\n",
    "    print(f\"Average worker provisioning time: {avg_lag:.0f}s ({avg_lag/60:.1f} mins)\")\n",
    "else:\n",
    "    print(\"\\n‚ÑπÔ∏è  No autoscaling detected\")\n",
    "    print(\"   Possible reasons:\")\n",
    "    print(\"   - Message rate not high enough to trigger autoscaling\")\n",
    "    print(\"   - Current worker capacity sufficient for tested load\")\n",
    "    print(\"   - Processing is very efficient\")\n",
    "\n",
    "# Analyze backlog behavior\n",
    "if 'backlog' in phase2a_metrics['dataflow'] and len(phase2a_metrics['dataflow']['backlog']) > 0:\n",
    "    print(f\"\\n\" + \"=\"*70)\n",
    "    print(\"BACKLOG ANALYSIS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    max_backlog = phase2a_metrics['dataflow']['backlog']['value'].max()\n",
    "    mean_backlog = phase2a_metrics['dataflow']['backlog']['value'].mean()\n",
    "    \n",
    "    print(f\"\\nBacklog Statistics:\")\n",
    "    print(f\"  Max backlog: {max_backlog:,.0f} messages\")\n",
    "    print(f\"  Mean backlog: {mean_backlog:,.0f} messages\")\n",
    "    \n",
    "    if max_backlog > 1000:\n",
    "        print(f\"\\n  ‚ö†Ô∏è  High backlog detected - pipeline may be at capacity\")\n",
    "    elif max_backlog > 100:\n",
    "        print(f\"\\n  ‚ÑπÔ∏è  Moderate backlog - pipeline handling load but close to limits\")\n",
    "    else:\n",
    "        print(f\"\\n  ‚úÖ Backlog under control - pipeline has headroom\")\n",
    "\n",
    "# Analyze system lag\n",
    "if 'system_lag' in phase2a_metrics['dataflow'] and len(phase2a_metrics['dataflow']['system_lag']) > 0:\n",
    "    print(f\"\\n\" + \"=\"*70)\n",
    "    print(\"SYSTEM LAG ANALYSIS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    max_lag_ms = phase2a_metrics['dataflow']['system_lag']['value'].max() / 1000  # Convert to ms\n",
    "    mean_lag_ms = phase2a_metrics['dataflow']['system_lag']['value'].mean() / 1000\n",
    "    \n",
    "    print(f\"\\nSystem Lag Statistics:\")\n",
    "    print(f\"  Max lag: {max_lag_ms:,.1f}ms\")\n",
    "    print(f\"  Mean lag: {mean_lag_ms:,.1f}ms\")\n",
    "    \n",
    "    if max_lag_ms > 60000:  # 1 minute\n",
    "        print(f\"\\n  ‚ö†Ô∏è  High system lag - pipeline falling behind real-time\")\n",
    "    elif max_lag_ms > 10000:  # 10 seconds\n",
    "        print(f\"\\n  ‚ÑπÔ∏è  Moderate system lag - processing slightly delayed\")\n",
    "    else:\n",
    "        print(f\"\\n  ‚úÖ System lag low - processing near real-time\")\n",
    "\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ac73ff",
   "metadata": {},
   "source": [
    "### üìä How to Read the Visualization\n",
    "\n",
    "The timeline visualization below shows 5 panels that track different aspects of pipeline performance. Here's what each panel tells you:\n",
    "\n",
    "#### **Panel 1: Incoming Message Rate**\n",
    "- **What it shows:** Messages published per second to the input Pub/Sub topic\n",
    "- **What to look for:**\n",
    "  - Flat line = steady rate (good for controlled testing)\n",
    "  - Spikes = burst traffic\n",
    "  - Gradual increase = ramp test pattern\n",
    "- **Interpretation:** This is your **input load** - what you're sending to the pipeline\n",
    "\n",
    "#### **Panel 2: Worker Count (Autoscaling)**\n",
    "- **What it shows:** Number of active Dataflow workers over time\n",
    "- **What to look for:**\n",
    "  - Flat line = no autoscaling needed\n",
    "  - Step up = autoscaling triggered (workers being added)\n",
    "  - Step down = scale-down (workers being removed after low load period)\n",
    "- **Interpretation:** Shows when pipeline capacity changes. Worker provisioning takes **2-4 minutes**, so you'll see a delay between load increase and worker scale-up.\n",
    "\n",
    "#### **Panel 3: System Lag (Processing Delay: Oldest To Current Message)**\n",
    "- **What it shows:** How far behind real-time the pipeline is processing (in milliseconds)\n",
    "- **What to look for:**\n",
    "  - Near-zero lag = pipeline keeping up with real-time\n",
    "  - Increasing lag = pipeline falling behind\n",
    "  - High lag (>60s) = serious capacity issues\n",
    "- **Interpretation:** This is your **real-time health indicator**. Rising lag means you need more workers or better machine type.\n",
    "\n",
    "#### **Panel 4: Output Subscription Queue**\n",
    "- **What it shows:** Number of messages waiting in the output Pub/Sub subscription\n",
    "- **What to look for:**\n",
    "  - Zero or low = messages being consumed quickly\n",
    "  - Linearly increasing = messages accumulating (expected during testing)\n",
    "  - Very high (>10,000) = potential capacity issues\n",
    "- **Interpretation:** This backlog includes **both** pipeline processing queue AND messages waiting for the test framework to pull them. During tests, this will grow because we're not consuming as fast as we're producing.\n",
    "\n",
    "#### **Panel 5: P95 Pipeline Latency (publish ‚Üí output queue, excludes queue wait)**\n",
    "- **What it shows:** 95th percentile latency from message publish to arriving in output queue\n",
    "- **What to look for:**\n",
    "  - Steady line = consistent performance\n",
    "  - Gradual increase = latency degrading under load\n",
    "  - Spikes at test end = shutdown artifacts (normal)\n",
    "- **Interpretation:** This is your **actual pipeline performance metric** - the one you care about most. It excludes the time messages sit in the queue waiting for the test to pull them.\n",
    "- **Y-axis note:** The axis is auto-scaled to focus on steady-state latency using the 99th percentile. Very high spikes (like test shutdown) may be clipped but won't distort the main trend.\n",
    "\n",
    "---\n",
    "\n",
    "**üí° Key Insights to Extract:**\n",
    "1. **When does autoscaling trigger?** ‚Üí Look for worker count step-up in Panel 2\n",
    "2. **What's the latency at different loads?** ‚Üí Compare Panel 5 across different message rates in Panel 1\n",
    "3. **Is the pipeline keeping up?** ‚Üí Panel 3 system lag should stay near zero\n",
    "4. **Where's the bottleneck?** ‚Üí If latency is high but system lag is low, it's windowing. If system lag is high, it's processing capacity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "phase2a_viz",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize complete Phase 2A timeline\n",
    "print(\"Creating comprehensive Phase 2A visualization...\")\n",
    "print(f\"This shows all {len(STRESS_DATAFLOW_TESTS)} sustained load tests in sequence\\n\")\n",
    "\n",
    "# Combine all test results and latencies\n",
    "combined_result = {\n",
    "    'test_id': 'phase2a-combined',\n",
    "    'test_name': 'Phase 2A - All Sustained Tests',\n",
    "    'start_time': phase2a_start,\n",
    "    'end_time': phase2a_end,\n",
    "    'message_data': pd.concat([r['message_data'] for r in phase2a_results], ignore_index=True)\n",
    "}\n",
    "\n",
    "combined_latency = pd.concat(phase2a_latencies, ignore_index=True)\n",
    "\n",
    "fig = plot_combined_timeline(\n",
    "    combined_metrics=phase2a_metrics,\n",
    "    latency_df=combined_latency,\n",
    "    load_pattern_data=combined_result['message_data'],\n",
    "    test_name=\"Phase 2A: Throughput Threshold Hunt - Complete Timeline\"\n",
    ")\n",
    "\n",
    "fig.show(renderer=\"png\", width=1400, height=1000)\n",
    "\n",
    "print(\"\\nüí° This visualization shows:\")\n",
    "print(f\"   - Message rate progression: {STRESS_DATAFLOW_TESTS[0]['target_rate']} ‚Üí {STRESS_DATAFLOW_TESTS[-1]['target_rate']} msg/sec\")\n",
    "print(f\"   - Worker scaling behavior over {len(STRESS_DATAFLOW_TESTS)} tests\")\n",
    "print(f\"   - System lag and backlog buildup\")\n",
    "print(f\"   - P95 latency trends as load increases\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç Queuing Diagnostic: Where Does the Latency Come From?\n",
    "\n",
    "The visualization above shows WHAT happened. Now let's understand WHY.\n",
    "\n",
    "This diagnostic reveals:\n",
    "- **Latency Attribution**: What % is window assignment vs internal queuing\n",
    "- **Queue Growth**: How queue depth evolves over time\n",
    "- **Autoscaling Response**: Did workers/replicas scale in response to queuing?\n",
    "\n",
    "**How to Read the Diagnostic:**\n",
    "\n",
    "**Panel 1 (Pie Chart)**: Latency Attribution\n",
    "- üîµ **Blue = Window Assignment** (just metadata timestamps, 0-1 second)\n",
    "- üî¥ **Red = Internal Queuing** (ACTUAL bottleneck - messages waiting inside workers)\n",
    "- ‚úÖ Good: Red < 20% (window config is appropriate)\n",
    "- ‚ö†Ô∏è  Concern: Red > 50% (workers can't keep up)\n",
    "- ‚ùå Problem: Red > 80% (severe worker saturation)\n",
    "\n",
    "**Panel 2 (Stacked Area)**: Components Over Time\n",
    "- Shows how latency components change as load increases\n",
    "- Watch for red area growing ‚Üí queue building up\n",
    "\n",
    "**Panel 3 (Heatmap)**: Queue Depth Over Time\n",
    "- Dark red = Messages severely delayed\n",
    "- Shows WHEN queuing starts (correlate with load changes)\n",
    "\n",
    "**Panel 4 (Autoscaling)**: THE SMOKING GUN\n",
    "- üîµ Blue line = Worker count (Dataflow)\n",
    "- üü¢ Green dashed = Replica count (Vertex Endpoint)\n",
    "- üî¥ Red line = Queue depth\n",
    "- **Key insight**: If queue grows but workers stay flat ‚Üí autoscaling failed\n",
    "\n",
    "**Common Patterns:**\n",
    "\n",
    "1. **Healthy System**:\n",
    "   - Window assignment: 5-15%\n",
    "   - Internal queue: <20%\n",
    "   - Workers scale up as queue grows\n",
    "   - Queue depth stays low (<1 second)\n",
    "\n",
    "2. **Worker Autoscaling Failure** (most common):\n",
    "   - Internal queue: 80-95%\n",
    "   - Workers stay at 1 despite configuration\n",
    "   - Queue depth grows to 10+ seconds\n",
    "   - **Fix**: See recommended fixes below\n",
    "\n",
    "3. **Endpoint Bottleneck**:\n",
    "   - Internal queue: 30-50%\n",
    "   - Replicas stay at min\n",
    "   - Endpoint CPU at 100%\n",
    "   - **Fix**: Increase max replicas or machine type\n",
    "\n",
    "**If You See Workers NOT Scaling:**\n",
    "\n",
    "The autoscaling algorithm may not be triggering because:\n",
    "1. Pub/Sub backlog stays small (workers pull fast, process slow)\n",
    "2. System lag metric doesn't capture internal worker queuing\n",
    "3. Wrong autoscaling algorithm for the workload\n",
    "\n",
    "**Recommended Fixes:**\n",
    "\n",
    "```python\n",
    "# In pipeline options, add:\n",
    "\"--autoscaling_algorithm=THROUGHPUT_BASED\",  # Scale based on throughput, not backlog\n",
    "\"--target_worker_utilization=0.7\",            # Scale at 70% capacity (more aggressive)\n",
    "\"--num_workers=2\",                             # Ensure proper initial worker count\n",
    "```\n",
    "\n",
    "Additionally in the pipeline code:\n",
    "```python\n",
    "# Reduce buffering to expose backpressure faster\n",
    "| \"Batch elements\" >> beam.BatchElements(\n",
    "    min_batch_size=1,\n",
    "    max_batch_size=50,\n",
    "    max_batch_duration_secs=0.001  # 1ms instead of 10ms\n",
    ")\n",
    "```\n",
    "\n",
    "**Understanding the Metrics:**\n",
    "\n",
    "- **Window wait time**: Time from publish to window_end timestamp (metadata only, not actual waiting)\n",
    "- **Internal queue time**: Time from window_end to pipeline output (REAL delay - workers saturated)\n",
    "- **P95 queue depth**: 95th percentile worst-case delay (most sensitive metric)\n",
    "\n",
    "The key insight: Window configuration is almost NEVER the bottleneck. The real issue is always worker saturation or endpoint throttling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create queuing diagnostic\n",
    "from scale_testing_combined_utils import create_queuing_diagnostic, print_queuing_summary\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"QUEUING DIAGNOSTIC\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create visualization\n",
    "fig_diagnostic = create_queuing_diagnostic(\n",
    "    latency_df=combined_latency,\n",
    "    dataflow_metrics=phase2a_metrics['dataflow'],\n",
    "    endpoint_metrics=phase2a_metrics['endpoint'],\n",
    "    test_name=\"Phase 2A: Stress Dataflow Workers\"\n",
    ")\n",
    "\n",
    "fig_diagnostic.show(renderer=\"png\", width=1400, height=1200)\n",
    "\n",
    "# Print summary\n",
    "print_queuing_summary(\n",
    "    latency_df=combined_latency,\n",
    "    dataflow_metrics=phase2a_metrics['dataflow'],\n",
    "    endpoint_metrics=phase2a_metrics['endpoint']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phase2b_header",
   "metadata": {},
   "source": [
    "---\n",
    "## Phase 2B: Burst Capacity\n",
    "\n",
    "**Goal**: Test backlog recovery time and worker scaling response to sudden spikes\n",
    "\n",
    "**Tests**: 3 burst tests (~10 minutes total)\n",
    "- 1,000 messages sent as fast as possible\n",
    "- 5,000 messages sent as fast as possible\n",
    "- 10,000 messages sent as fast as possible\n",
    "\n",
    "**What We're Looking For**:\n",
    "- How quickly does backlog clear after burst?\n",
    "- Does autoscaling respond to sudden traffic spikes?\n",
    "- What's the maximum burst size before sustained backlog?\n",
    "- Recovery time from burst to normal operation\n",
    "\n",
    "‚è≥ **This phase will take approximately 10 minutes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "phase2b_init",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Phase 2B storage\n",
    "phase2b_results = []\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"PHASE 2B: STRESS ENDPOINT\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Testing {len(STRESS_ENDPOINT_TESTS)} sustained load patterns\")\n",
    "print(f\"Total estimated time: ~{sum(t['duration'] for t in STRESS_ENDPOINT_TESTS) // 60} minutes\")\n",
    "print(\"\\nThis phase tests sustained moderate load to stress the endpoint.\")\n",
    "print(\"=\"*70 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "phase2b_run",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run sustained load tests to stress endpoint\n",
    "for test_config in STRESS_ENDPOINT_TESTS:\n",
    "    result = await load_generator.run_load_test(\n",
    "        pattern=\"sustained\",\n",
    "        target_rate=test_config[\"target_rate\"],\n",
    "        duration=test_config[\"duration\"],\n",
    "        test_name=test_config[\"name\"]\n",
    "    )\n",
    "    phase2b_results.append(result)\n",
    "    \n",
    "    # Small delay between tests\n",
    "    print(\"\\n   Waiting 60 seconds before next test...\\n\")\n",
    "    await asyncio.sleep(60)\n",
    "\n",
    "print(f\"\\n‚úÖ Phase 2B complete: {len(STRESS_ENDPOINT_TESTS)} endpoint stress tests run\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phase2b_metrics_header",
   "metadata": {},
   "source": [
    "### Collect Metrics for Phase 2B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "phase2b_metrics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect metrics for entire Phase 2B\n",
    "phase2b_start = min([r['start_time'] for r in phase2b_results])\n",
    "phase2b_end = max([r['end_time'] for r in phase2b_results])\n",
    "\n",
    "print(f\"Collecting metrics for Phase 2B\")\n",
    "print(f\"Time window: {phase2b_start.strftime('%H:%M:%S')} ‚Üí {phase2b_end.strftime('%H:%M:%S')}\")\n",
    "\n",
    "phase2b_metrics = metrics_collector.collect_combined_metrics(\n",
    "    start_time=phase2b_start,\n",
    "    end_time=phase2b_end,\n",
    "    resolution_seconds=10\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Metrics collected for Phase 2B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "phase2b_latency",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect end-to-end latency for burst tests\n",
    "phase2b_latencies = []\n",
    "\n",
    "for result in phase2b_results:\n",
    "    latency_df = metrics_collector.dataflow_collector.collect_end_to_end_latency(\n",
    "        test_id=result['test_id'],\n",
    "        expected_messages=result['num_messages'],\n",
    "        timeout=300\n",
    "    )\n",
    "    phase2b_latencies.append(latency_df)\n",
    "\n",
    "print(f\"\\n‚úÖ Latency data collected for {len(phase2b_latencies)} burst tests\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phase2b_analysis_header",
   "metadata": {},
   "source": [
    "### Analyze Phase 2B Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "phase2b_analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze Phase 2B: Endpoint stress tests\n",
    "print(\"=\"*70)\n",
    "print(\"PHASE 2B ANALYSIS: ENDPOINT STRESS TESTING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for i, (result, latency_df) in enumerate(zip(phase2b_results, phase2b_latencies)):\n",
    "    test_config = STRESS_ENDPOINT_TESTS[i]\n",
    "    \n",
    "    print(f\"\\n{test_config['name']}:\")\n",
    "    print(f\"  Configuration: {test_config['target_rate']} msg/sec for {test_config['duration']}s\")\n",
    "    print(f\"  Messages sent: {result['num_messages']:,}\")\n",
    "    print(f\"  Actual rate: {result['actual_rate']:.1f} msg/sec\")\n",
    "    \n",
    "    if len(latency_df) > 0:\n",
    "        print(f\"\\n  Latency Performance:\")\n",
    "        print(f\"    Mean E2E: {latency_df['pipeline_latency_ms'].mean():7.1f}ms\")\n",
    "        print(f\"    P95 E2E:  {latency_df['pipeline_latency_ms'].quantile(0.95):7.1f}ms\")\n",
    "        print(f\"    P99 E2E:  {latency_df['pipeline_latency_ms'].quantile(0.99):7.1f}ms\")\n",
    "        \n",
    "        # Latency degradation check\n",
    "        if i > 0 and len(phase2b_latencies[0]) > 0:\n",
    "            baseline_p95 = phase2b_latencies[0]['pipeline_latency_ms'].quantile(0.95)\n",
    "            current_p95 = latency_df['pipeline_latency_ms'].quantile(0.95)\n",
    "            degradation_pct = ((current_p95 - baseline_p95) / baseline_p95) * 100\n",
    "            \n",
    "            print(f\"\\n  Latency vs Phase 2B Baseline:\")\n",
    "            print(f\"    Baseline P95: {baseline_p95:.1f}ms\")\n",
    "            print(f\"    Current P95:  {current_p95:.1f}ms\")\n",
    "            print(f\"    Change: {degradation_pct:+.1f}%\")\n",
    "            \n",
    "            if degradation_pct > 50:\n",
    "                print(f\"    ‚ö†Ô∏è  Significant latency increase - may indicate endpoint capacity limits\")\n",
    "    else:\n",
    "        print(f\"\\n  ‚ö†Ô∏è  No latency data (processing may be delayed)\")\n",
    "\n",
    "# Check endpoint CPU utilization during this phase\n",
    "if 'endpoint' in phase2b_metrics and 'cpu' in phase2b_metrics['endpoint'] and len(phase2b_metrics['endpoint']['cpu']) > 0:\n",
    "    print(f\"\\n\" + \"=\"*70)\n",
    "    print(\"ENDPOINT CPU ANALYSIS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    cpu_df = phase2b_metrics['endpoint']['cpu']\n",
    "    max_cpu = cpu_df['value'].max()\n",
    "    mean_cpu = cpu_df['value'].mean()\n",
    "    \n",
    "    print(f\"\\nEndpoint CPU Statistics:\")\n",
    "    print(f\"  Max CPU: {max_cpu:.1f}%\")\n",
    "    print(f\"  Mean CPU: {mean_cpu:.1f}%\")\n",
    "    \n",
    "    if max_cpu > 70:\n",
    "        print(f\"\\n  ‚ö†Ô∏è  High endpoint CPU - endpoint may be bottleneck\")\n",
    "        print(f\"     Consider increasing endpoint replicas or optimizing model\")\n",
    "    elif max_cpu > 50:\n",
    "        print(f\"\\n  ‚ÑπÔ∏è  Moderate endpoint CPU - endpoint handling load well\")\n",
    "    else:\n",
    "        print(f\"\\n  ‚úÖ Low endpoint CPU - endpoint has significant headroom\")\n",
    "\n",
    "# Analyze autoscaling behavior\n",
    "print(f\"\\n\" + \"=\"*70)\n",
    "print(\"AUTOSCALING EVENTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "autoscaling_events = metrics_collector.dataflow_collector.analyze_autoscaling(phase2b_metrics['dataflow'])\n",
    "\n",
    "if len(autoscaling_events) > 0:\n",
    "    print(f\"\\n‚úÖ Dataflow autoscaling triggered {len(autoscaling_events)} time(s) during Phase 2B\\n\")\n",
    "    \n",
    "    for idx, event in autoscaling_events.iterrows():\n",
    "        print(f\"Event {idx + 1}:\")\n",
    "        print(f\"  Trigger time:     {event['trigger_time'].strftime('%H:%M:%S')}\")\n",
    "        print(f\"  Complete time:    {event['scale_complete_time'].strftime('%H:%M:%S')}\")\n",
    "        print(f\"  Workers:          {event['workers_before']:.0f} ‚Üí {event['workers_after']:.0f}\")\n",
    "        print(f\"  Provisioning lag: {event['scale_up_lag_seconds']:.0f}s ({event['scale_up_lag_seconds']/60:.1f} mins)\")\n",
    "        print()\n",
    "else:\n",
    "    print(\"\\n‚ÑπÔ∏è  No Dataflow autoscaling detected\")\n",
    "    print(\"   Workers sufficient for tested endpoint load\")\n",
    "\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798f210c",
   "metadata": {},
   "source": [
    "### üìä How to Read the Visualization\n",
    "\n",
    "The timeline visualization below shows 5 panels that track different aspects of pipeline performance. Here's what each panel tells you:\n",
    "\n",
    "#### **Panel 1: Incoming Message Rate**\n",
    "- **What it shows:** Messages published per second to the input Pub/Sub topic\n",
    "- **What to look for:**\n",
    "  - Flat line = steady rate (good for controlled testing)\n",
    "  - Spikes = burst traffic\n",
    "  - Gradual increase = ramp test pattern\n",
    "- **Interpretation:** This is your **input load** - what you're sending to the pipeline\n",
    "\n",
    "#### **Panel 2: Worker Count (Autoscaling)**\n",
    "- **What it shows:** Number of active Dataflow workers over time\n",
    "- **What to look for:**\n",
    "  - Flat line = no autoscaling needed\n",
    "  - Step up = autoscaling triggered (workers being added)\n",
    "  - Step down = scale-down (workers being removed after low load period)\n",
    "- **Interpretation:** Shows when pipeline capacity changes. Worker provisioning takes **2-4 minutes**, so you'll see a delay between load increase and worker scale-up.\n",
    "\n",
    "#### **Panel 3: System Lag (Processing Delay: Oldest To Current Message)**\n",
    "- **What it shows:** How far behind real-time the pipeline is processing (in milliseconds)\n",
    "- **What to look for:**\n",
    "  - Near-zero lag = pipeline keeping up with real-time\n",
    "  - Increasing lag = pipeline falling behind\n",
    "  - High lag (>60s) = serious capacity issues\n",
    "- **Interpretation:** This is your **real-time health indicator**. Rising lag means you need more workers or better machine type.\n",
    "\n",
    "#### **Panel 4: Output Subscription Queue**\n",
    "- **What it shows:** Number of messages waiting in the output Pub/Sub subscription\n",
    "- **What to look for:**\n",
    "  - Zero or low = messages being consumed quickly\n",
    "  - Linearly increasing = messages accumulating (expected during testing)\n",
    "  - Very high (>10,000) = potential capacity issues\n",
    "- **Interpretation:** This backlog includes **both** pipeline processing queue AND messages waiting for the test framework to pull them. During tests, this will grow because we're not consuming as fast as we're producing.\n",
    "\n",
    "#### **Panel 5: P95 Pipeline Latency (publish ‚Üí output queue, excludes queue wait)**\n",
    "- **What it shows:** 95th percentile latency from message publish to arriving in output queue\n",
    "- **What to look for:**\n",
    "  - Steady line = consistent performance\n",
    "  - Gradual increase = latency degrading under load\n",
    "  - Spikes at test end = shutdown artifacts (normal)\n",
    "- **Interpretation:** This is your **actual pipeline performance metric** - the one you care about most. It excludes the time messages sit in the queue waiting for the test to pull them.\n",
    "- **Y-axis note:** The axis is auto-scaled to focus on steady-state latency using the 99th percentile. Very high spikes (like test shutdown) may be clipped but won't distort the main trend.\n",
    "\n",
    "---\n",
    "\n",
    "**üí° Key Insights to Extract:**\n",
    "1. **When does autoscaling trigger?** ‚Üí Look for worker count step-up in Panel 2\n",
    "2. **What's the latency at different loads?** ‚Üí Compare Panel 5 across different message rates in Panel 1\n",
    "3. **Is the pipeline keeping up?** ‚Üí Panel 3 system lag should stay near zero\n",
    "4. **Where's the bottleneck?** ‚Üí If latency is high but system lag is low, it's windowing. If system lag is high, it's processing capacity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "phase2b_viz",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize endpoint stress tests\n",
    "for i, (result, latency_df) in enumerate(zip(phase2b_results, phase2b_latencies)):\n",
    "    test_config = STRESS_ENDPOINT_TESTS[i]\n",
    "    \n",
    "    # Get metrics for this specific test\n",
    "    test_metrics = metrics_collector.collect_combined_metrics(\n",
    "        start_time=result['start_time'],\n",
    "        end_time=result['end_time'],\n",
    "        resolution_seconds=10\n",
    "    )\n",
    "    \n",
    "    fig = plot_combined_timeline(\n",
    "        combined_metrics=test_metrics,\n",
    "        latency_df=latency_df,\n",
    "        load_pattern_data=result['message_data'],\n",
    "        test_name=test_config['name']\n",
    "    )\n",
    "    fig.show(renderer=\"png\", width=1400, height=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç Queuing Diagnostic: Where Does the Latency Come From?\n",
    "\n",
    "The visualization above shows WHAT happened. Now let's understand WHY.\n",
    "\n",
    "This diagnostic reveals:\n",
    "- **Latency Attribution**: What % is window assignment vs internal queuing\n",
    "- **Queue Growth**: How queue depth evolves over time\n",
    "- **Autoscaling Response**: Did workers/replicas scale in response to queuing?\n",
    "\n",
    "**How to Read the Diagnostic:**\n",
    "\n",
    "**Panel 1 (Pie Chart)**: Latency Attribution\n",
    "- üîµ **Blue = Window Assignment** (just metadata timestamps, 0-1 second)\n",
    "- üî¥ **Red = Internal Queuing** (ACTUAL bottleneck - messages waiting inside workers)\n",
    "- ‚úÖ Good: Red < 20% (window config is appropriate)\n",
    "- ‚ö†Ô∏è  Concern: Red > 50% (workers can't keep up)\n",
    "- ‚ùå Problem: Red > 80% (severe worker saturation)\n",
    "\n",
    "**Panel 2 (Stacked Area)**: Components Over Time\n",
    "- Shows how latency components change as load increases\n",
    "- Watch for red area growing ‚Üí queue building up\n",
    "\n",
    "**Panel 3 (Heatmap)**: Queue Depth Over Time\n",
    "- Dark red = Messages severely delayed\n",
    "- Shows WHEN queuing starts (correlate with load changes)\n",
    "\n",
    "**Panel 4 (Autoscaling)**: THE SMOKING GUN\n",
    "- üîµ Blue line = Worker count (Dataflow)\n",
    "- üü¢ Green dashed = Replica count (Vertex Endpoint)\n",
    "- üî¥ Red line = Queue depth\n",
    "- **Key insight**: If queue grows but workers stay flat ‚Üí autoscaling failed\n",
    "\n",
    "**Common Patterns:**\n",
    "\n",
    "1. **Healthy System**:\n",
    "   - Window assignment: 5-15%\n",
    "   - Internal queue: <20%\n",
    "   - Workers scale up as queue grows\n",
    "   - Queue depth stays low (<1 second)\n",
    "\n",
    "2. **Worker Autoscaling Failure** (most common):\n",
    "   - Internal queue: 80-95%\n",
    "   - Workers stay at 1 despite configuration\n",
    "   - Queue depth grows to 10+ seconds\n",
    "   - **Fix**: See recommended fixes below\n",
    "\n",
    "3. **Endpoint Bottleneck**:\n",
    "   - Internal queue: 30-50%\n",
    "   - Replicas stay at min\n",
    "   - Endpoint CPU at 100%\n",
    "   - **Fix**: Increase max replicas or machine type\n",
    "\n",
    "**If You See Workers NOT Scaling:**\n",
    "\n",
    "The autoscaling algorithm may not be triggering because:\n",
    "1. Pub/Sub backlog stays small (workers pull fast, process slow)\n",
    "2. System lag metric doesn't capture internal worker queuing\n",
    "3. Wrong autoscaling algorithm for the workload\n",
    "\n",
    "**Recommended Fixes:**\n",
    "\n",
    "```python\n",
    "# In pipeline options, add:\n",
    "\"--autoscaling_algorithm=THROUGHPUT_BASED\",  # Scale based on throughput, not backlog\n",
    "\"--target_worker_utilization=0.7\",            # Scale at 70% capacity (more aggressive)\n",
    "\"--num_workers=2\",                             # Ensure proper initial worker count\n",
    "```\n",
    "\n",
    "Additionally in the pipeline code:\n",
    "```python\n",
    "# Reduce buffering to expose backpressure faster\n",
    "| \"Batch elements\" >> beam.BatchElements(\n",
    "    min_batch_size=1,\n",
    "    max_batch_size=50,\n",
    "    max_batch_duration_secs=0.001  # 1ms instead of 10ms\n",
    ")\n",
    "```\n",
    "\n",
    "**Understanding the Metrics:**\n",
    "\n",
    "- **Window wait time**: Time from publish to window_end timestamp (metadata only, not actual waiting)\n",
    "- **Internal queue time**: Time from window_end to pipeline output (REAL delay - workers saturated)\n",
    "- **P95 queue depth**: 95th percentile worst-case delay (most sensitive metric)\n",
    "\n",
    "The key insight: Window configuration is almost NEVER the bottleneck. The real issue is always worker saturation or endpoint throttling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create queuing diagnostic\n",
    "from scale_testing_combined_utils import create_queuing_diagnostic, print_queuing_summary\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"QUEUING DIAGNOSTIC\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create visualization\n",
    "fig_diagnostic = create_queuing_diagnostic(\n",
    "    latency_df=phase2b_latencies[0],\n",
    "    dataflow_metrics=phase2b_metrics['dataflow'],\n",
    "    endpoint_metrics=phase2b_metrics['endpoint'],\n",
    "    test_name=\"Phase 2B: Stress Vertex Endpoint\"\n",
    ")\n",
    "\n",
    "fig_diagnostic.show(renderer=\"png\", width=1400, height=1200)\n",
    "\n",
    "# Print summary\n",
    "print_queuing_summary(\n",
    "    latency_df=phase2b_latencies[0],\n",
    "    dataflow_metrics=phase2b_metrics['dataflow'],\n",
    "    endpoint_metrics=phase2b_metrics['endpoint']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phase3_header",
   "metadata": {},
   "source": [
    "---\n",
    "## Phase 3: Ramp Test\n",
    "\n",
    "**Goal**: Find exact autoscaling trigger point with gradual load increase\n",
    "\n",
    "**Test**: Single ramp test (~15 minutes)\n",
    "- Gradually increase from 0 ‚Üí 500 msg/sec over 15 minutes\n",
    "- Linear ramp to simulate traffic growth\n",
    "\n",
    "**What We're Looking For**:\n",
    "- At what exact message rate does autoscaling trigger?\n",
    "- How does the pipeline respond to gradual load increase?\n",
    "- Is there a \"sweet spot\" for sustained throughput?\n",
    "- When does latency start degrading?\n",
    "\n",
    "‚è≥ **This phase will take approximately 15 minutes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "phase3_init",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Phase 3 storage\n",
    "phase3_results = []\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"PHASE 2C: BALANCED RAMP TEST\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Ramping from 0 ‚Üí {RAMP_TEST['target_rate']} msg/sec over {RAMP_TEST['duration']//60} minutes\")\n",
    "print(f\"Total estimated time: ~{RAMP_TEST['duration']//60} minutes\")\n",
    "print(\"\\nThis test gradually increases load to find autoscaling threshold.\")\n",
    "print(\"=\"*70 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34dc188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run ramp test\n",
    "ramp_result = await load_generator.run_load_test(\n",
    "    pattern=\"ramp\",\n",
    "    target_rate=RAMP_TEST[\"target_rate\"],\n",
    "    duration=RAMP_TEST[\"duration\"],\n",
    "    test_name=RAMP_TEST[\"name\"]\n",
    ")\n",
    "phase3_results.append(ramp_result)\n",
    "\n",
    "print(f\"\\n‚úÖ Phase 3 complete: Ramp test finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phase3_metrics_header",
   "metadata": {},
   "source": [
    "### Collect Metrics for Phase 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "phase3_metrics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect metrics for ramp test\n",
    "phase3_start = ramp_result['start_time']\n",
    "phase3_end = ramp_result['end_time']\n",
    "\n",
    "print(f\"Collecting metrics for Phase 3\")\n",
    "print(f\"Time window: {phase3_start.strftime('%H:%M:%S')} ‚Üí {phase3_end.strftime('%H:%M:%S')}\")\n",
    "\n",
    "phase3_metrics = metrics_collector.collect_combined_metrics(\n",
    "    start_time=phase3_start,\n",
    "    end_time=phase3_end,\n",
    "    resolution_seconds=10\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Metrics collected for Phase 3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "phase3_latency",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect end-to-end latency for ramp test\n",
    "phase3_latency = metrics_collector.dataflow_collector.collect_end_to_end_latency(\n",
    "    test_id=ramp_result['test_id'],\n",
    "    expected_messages=ramp_result['num_messages'],\n",
    "    timeout=300\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Latency data collected: {len(phase3_latency):,} messages\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phase3_analysis_header",
   "metadata": {},
   "source": [
    "### Analyze Phase 3 Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "phase3_analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze ramp test to find autoscaling trigger point\n",
    "print(\"=\"*70)\n",
    "print(\"PHASE 3 ANALYSIS: AUTOSCALING TRIGGER POINT\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\n{RAMP_TEST['name']}:\")\n",
    "print(f\"  Duration: {ramp_result['elapsed_seconds']:.0f}s ({ramp_result['elapsed_seconds']//60:.0f} mins)\")\n",
    "print(f\"  Messages sent: {ramp_result['num_messages']:,}\")\n",
    "print(f\"  Average rate: {ramp_result['actual_rate']:.1f} msg/sec\")\n",
    "print(f\"  Peak rate: {RAMP_TEST['target_rate']} msg/sec\")\n",
    "\n",
    "if len(phase3_latency) > 0:\n",
    "    print(f\"\\n  Latency Statistics:\")\n",
    "    print(f\"    Mean E2E: {phase3_latency['pipeline_latency_ms'].mean():,.1f}ms\")\n",
    "    print(f\"    P50 E2E:  {phase3_latency['pipeline_latency_ms'].quantile(0.50):,.1f}ms\")\n",
    "    print(f\"    P95 E2E:  {phase3_latency['pipeline_latency_ms'].quantile(0.95):,.1f}ms\")\n",
    "    print(f\"    P99 E2E:  {phase3_latency['pipeline_latency_ms'].quantile(0.99):,.1f}ms\")\n",
    "    \n",
    "    # Analyze latency trend over time (early vs late)\n",
    "    phase3_latency_sorted = phase3_latency.sort_values('publish_time')\n",
    "    \n",
    "    # Split into quartiles to see latency progression\n",
    "    quartile_size = len(phase3_latency_sorted) // 4\n",
    "    q1 = phase3_latency_sorted.iloc[:quartile_size]\n",
    "    q2 = phase3_latency_sorted.iloc[quartile_size:2*quartile_size]\n",
    "    q3 = phase3_latency_sorted.iloc[2*quartile_size:3*quartile_size]\n",
    "    q4 = phase3_latency_sorted.iloc[3*quartile_size:]\n",
    "    \n",
    "    print(f\"\\n  Latency Progression (as rate increases):\")\n",
    "    print(f\"    Q1 (0-25%):   {q1['pipeline_latency_ms'].mean():,.1f}ms mean\")\n",
    "    print(f\"    Q2 (25-50%):  {q2['pipeline_latency_ms'].mean():,.1f}ms mean\")\n",
    "    print(f\"    Q3 (50-75%):  {q3['pipeline_latency_ms'].mean():,.1f}ms mean\")\n",
    "    print(f\"    Q4 (75-100%): {q4['pipeline_latency_ms'].mean():,.1f}ms mean\")\n",
    "    \n",
    "    # Identify when latency starts degrading significantly\n",
    "    q1_mean = q1['pipeline_latency_ms'].mean()\n",
    "    if q4['pipeline_latency_ms'].mean() > q1_mean * 1.5:\n",
    "        print(f\"\\n    ‚ö†Ô∏è  Latency degraded significantly in later quartiles\")\n",
    "        print(f\"       Suggests capacity limits reached at higher rates\")\n",
    "    else:\n",
    "        print(f\"\\n    ‚úÖ Latency remained stable throughout ramp\")\n",
    "\n",
    "# Analyze autoscaling during ramp\n",
    "print(f\"\\n\" + \"=\"*70)\n",
    "print(\"AUTOSCALING BEHAVIOR DURING RAMP\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "autoscaling_events = metrics_collector.dataflow_collector.analyze_autoscaling(phase3_metrics['dataflow'])\n",
    "\n",
    "if len(autoscaling_events) > 0:\n",
    "    print(f\"\\n‚úÖ Autoscaling triggered {len(autoscaling_events)} time(s) during ramp\\n\")\n",
    "    \n",
    "    for idx, event in autoscaling_events.iterrows():\n",
    "        # Calculate approximate message rate at trigger time\n",
    "        elapsed_at_trigger = (pd.Timestamp(event['trigger_time']) - pd.Timestamp(phase3_start)).total_seconds()\n",
    "        rate_at_trigger = (elapsed_at_trigger / RAMP_TEST['duration']) * RAMP_TEST['target_rate']\n",
    "        \n",
    "        print(f\"Event {idx + 1}:\")\n",
    "        print(f\"  Trigger time:       {event['trigger_time'].strftime('%H:%M:%S')}\")\n",
    "        print(f\"  Elapsed:            {elapsed_at_trigger:.0f}s into ramp\")\n",
    "        print(f\"  Approx rate:        {rate_at_trigger:.0f} msg/sec\")\n",
    "        print(f\"  Workers:            {event['workers_before']:.0f} ‚Üí {event['workers_after']:.0f}\")\n",
    "        print(f\"  Provisioning time:  {event['scale_up_lag_seconds']:.0f}s\")\n",
    "        print()\n",
    "    \n",
    "    # Identify threshold\n",
    "    first_trigger = autoscaling_events.iloc[0]\n",
    "    first_trigger_elapsed = (pd.Timestamp(first_trigger['trigger_time']) - pd.Timestamp(phase3_start)).total_seconds()\n",
    "    first_trigger_rate = (first_trigger_elapsed / RAMP_TEST['duration']) * RAMP_TEST['target_rate']\n",
    "    \n",
    "    print(f\"üìä Key Finding:\")\n",
    "    print(f\"   Autoscaling triggered at approximately {first_trigger_rate:.0f} msg/sec\")\n",
    "    print(f\"   This is the throughput threshold for current configuration\")\n",
    "else:\n",
    "    print(\"\\n‚ÑπÔ∏è  No autoscaling triggered during ramp\")\n",
    "    print(f\"   Current workers handled up to {RAMP_TEST['target_rate']} msg/sec\")\n",
    "    print(\"   Pipeline has significant headroom at this configuration\")\n",
    "\n",
    "# Analyze backlog behavior during ramp\n",
    "if 'backlog' in phase3_metrics and len(phase3_metrics['backlog']) > 0:\n",
    "    print(f\"\\n\" + \"=\"*70)\n",
    "    print(\"BACKLOG BEHAVIOR DURING RAMP\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    backlog_df = phase3_metrics['backlog'].sort_values('timestamp')\n",
    "    max_backlog = backlog_df['value'].max()\n",
    "    \n",
    "    print(f\"\\nBacklog Statistics:\")\n",
    "    print(f\"  Max backlog: {max_backlog:,.0f} messages\")\n",
    "    print(f\"  Mean backlog: {backlog_df['value'].mean():,.0f} messages\")\n",
    "    \n",
    "    # Find when backlog started building\n",
    "    backlog_threshold = 100  # Consider backlog \"building\" at 100+ messages\n",
    "    backlog_building = backlog_df[backlog_df['value'] > backlog_threshold]\n",
    "    \n",
    "    if len(backlog_building) > 0:\n",
    "        first_backlog_time = backlog_building.iloc[0]['timestamp']\n",
    "        elapsed_at_backlog = (pd.Timestamp(first_backlog_time) - pd.Timestamp(phase3_start)).total_seconds()\n",
    "        rate_at_backlog = (elapsed_at_backlog / RAMP_TEST['duration']) * RAMP_TEST['target_rate']\n",
    "        \n",
    "        print(f\"\\n  Backlog started building:\")\n",
    "        print(f\"    Time: {first_backlog_time.strftime('%H:%M:%S')}\")\n",
    "        print(f\"    Elapsed: {elapsed_at_backlog:.0f}s into ramp\")\n",
    "        print(f\"    Approx rate: {rate_at_backlog:.0f} msg/sec\")\n",
    "        print(f\"\\n    üí° This indicates pipeline capacity threshold\")\n",
    "    else:\n",
    "        print(f\"\\n  ‚úÖ Backlog remained under control throughout ramp\")\n",
    "\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db57294",
   "metadata": {},
   "source": [
    "### üìä How to Read the Visualization\n",
    "\n",
    "The timeline visualization below shows 5 panels that track different aspects of pipeline performance. Here's what each panel tells you:\n",
    "\n",
    "#### **Panel 1: Incoming Message Rate**\n",
    "- **What it shows:** Messages published per second to the input Pub/Sub topic\n",
    "- **What to look for:**\n",
    "  - Flat line = steady rate (good for controlled testing)\n",
    "  - Spikes = burst traffic\n",
    "  - Gradual increase = ramp test pattern\n",
    "- **Interpretation:** This is your **input load** - what you're sending to the pipeline\n",
    "\n",
    "#### **Panel 2: Worker Count (Autoscaling)**\n",
    "- **What it shows:** Number of active Dataflow workers over time\n",
    "- **What to look for:**\n",
    "  - Flat line = no autoscaling needed\n",
    "  - Step up = autoscaling triggered (workers being added)\n",
    "  - Step down = scale-down (workers being removed after low load period)\n",
    "- **Interpretation:** Shows when pipeline capacity changes. Worker provisioning takes **2-4 minutes**, so you'll see a delay between load increase and worker scale-up.\n",
    "\n",
    "#### **Panel 3: System Lag (Processing Delay: Oldest To Current Message)**\n",
    "- **What it shows:** How far behind real-time the pipeline is processing (in milliseconds)\n",
    "- **What to look for:**\n",
    "  - Near-zero lag = pipeline keeping up with real-time\n",
    "  - Increasing lag = pipeline falling behind\n",
    "  - High lag (>60s) = serious capacity issues\n",
    "- **Interpretation:** This is your **real-time health indicator**. Rising lag means you need more workers or better machine type.\n",
    "\n",
    "#### **Panel 4: Output Subscription Queue**\n",
    "- **What it shows:** Number of messages waiting in the output Pub/Sub subscription\n",
    "- **What to look for:**\n",
    "  - Zero or low = messages being consumed quickly\n",
    "  - Linearly increasing = messages accumulating (expected during testing)\n",
    "  - Very high (>10,000) = potential capacity issues\n",
    "- **Interpretation:** This backlog includes **both** pipeline processing queue AND messages waiting for the test framework to pull them. During tests, this will grow because we're not consuming as fast as we're producing.\n",
    "\n",
    "#### **Panel 5: P95 Pipeline Latency (publish ‚Üí output queue, excludes queue wait)**\n",
    "- **What it shows:** 95th percentile latency from message publish to arriving in output queue\n",
    "- **What to look for:**\n",
    "  - Steady line = consistent performance\n",
    "  - Gradual increase = latency degrading under load\n",
    "  - Spikes at test end = shutdown artifacts (normal)\n",
    "- **Interpretation:** This is your **actual pipeline performance metric** - the one you care about most. It excludes the time messages sit in the queue waiting for the test to pull them.\n",
    "- **Y-axis note:** The axis is auto-scaled to focus on steady-state latency using the 99th percentile. Very high spikes (like test shutdown) may be clipped but won't distort the main trend.\n",
    "\n",
    "---\n",
    "\n",
    "**üí° Key Insights to Extract:**\n",
    "1. **When does autoscaling trigger?** ‚Üí Look for worker count step-up in Panel 2\n",
    "2. **What's the latency at different loads?** ‚Üí Compare Panel 5 across different message rates in Panel 1\n",
    "3. **Is the pipeline keeping up?** ‚Üí Panel 3 system lag should stay near zero\n",
    "4. **Where's the bottleneck?** ‚Üí If latency is high but system lag is low, it's windowing. If system lag is high, it's processing capacity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "phase3_viz",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize ramp test\n",
    "fig = plot_combined_timeline(\n",
    "    combined_metrics=phase3_metrics,\n",
    "    latency_df=phase3_latency,\n",
    "    load_pattern_data=ramp_result['message_data'],\n",
    "    test_name=\"Phase 3: Ramp Test - Finding Autoscaling Threshold\"\n",
    ")\n",
    "\n",
    "fig.show(renderer=\"png\", width=1400, height=1000)\n",
    "\n",
    "print(\"\\nüí° This visualization shows:\")\n",
    "print(f\"   - Gradual message rate increase: 0 ‚Üí {RAMP_TEST['target_rate']} msg/sec\")\n",
    "print(f\"   - Exact moment when workers scaled up\")\n",
    "print(f\"   - System lag progression as load increased\")\n",
    "print(f\"   - Backlog buildup and recovery\")\n",
    "print(f\"   - Latency trends throughout the ramp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç Queuing Diagnostic: Where Does the Latency Come From?\n",
    "\n",
    "The visualization above shows WHAT happened. Now let's understand WHY.\n",
    "\n",
    "This diagnostic reveals:\n",
    "- **Latency Attribution**: What % is window assignment vs internal queuing\n",
    "- **Queue Growth**: How queue depth evolves over time\n",
    "- **Autoscaling Response**: Did workers/replicas scale in response to queuing?\n",
    "\n",
    "**How to Read the Diagnostic:**\n",
    "\n",
    "**Panel 1 (Pie Chart)**: Latency Attribution\n",
    "- üîµ **Blue = Window Assignment** (just metadata timestamps, 0-1 second)\n",
    "- üî¥ **Red = Internal Queuing** (ACTUAL bottleneck - messages waiting inside workers)\n",
    "- ‚úÖ Good: Red < 20% (window config is appropriate)\n",
    "- ‚ö†Ô∏è  Concern: Red > 50% (workers can't keep up)\n",
    "- ‚ùå Problem: Red > 80% (severe worker saturation)\n",
    "\n",
    "**Panel 2 (Stacked Area)**: Components Over Time\n",
    "- Shows how latency components change as load increases\n",
    "- Watch for red area growing ‚Üí queue building up\n",
    "\n",
    "**Panel 3 (Heatmap)**: Queue Depth Over Time\n",
    "- Dark red = Messages severely delayed\n",
    "- Shows WHEN queuing starts (correlate with load changes)\n",
    "\n",
    "**Panel 4 (Autoscaling)**: THE SMOKING GUN\n",
    "- üîµ Blue line = Worker count (Dataflow)\n",
    "- üü¢ Green dashed = Replica count (Vertex Endpoint)\n",
    "- üî¥ Red line = Queue depth\n",
    "- **Key insight**: If queue grows but workers stay flat ‚Üí autoscaling failed\n",
    "\n",
    "**Common Patterns:**\n",
    "\n",
    "1. **Healthy System**:\n",
    "   - Window assignment: 5-15%\n",
    "   - Internal queue: <20%\n",
    "   - Workers scale up as queue grows\n",
    "   - Queue depth stays low (<1 second)\n",
    "\n",
    "2. **Worker Autoscaling Failure** (most common):\n",
    "   - Internal queue: 80-95%\n",
    "   - Workers stay at 1 despite configuration\n",
    "   - Queue depth grows to 10+ seconds\n",
    "   - **Fix**: See recommended fixes below\n",
    "\n",
    "3. **Endpoint Bottleneck**:\n",
    "   - Internal queue: 30-50%\n",
    "   - Replicas stay at min\n",
    "   - Endpoint CPU at 100%\n",
    "   - **Fix**: Increase max replicas or machine type\n",
    "\n",
    "**If You See Workers NOT Scaling:**\n",
    "\n",
    "The autoscaling algorithm may not be triggering because:\n",
    "1. Pub/Sub backlog stays small (workers pull fast, process slow)\n",
    "2. System lag metric doesn't capture internal worker queuing\n",
    "3. Wrong autoscaling algorithm for the workload\n",
    "\n",
    "**Recommended Fixes:**\n",
    "\n",
    "```python\n",
    "# In pipeline options, add:\n",
    "\"--autoscaling_algorithm=THROUGHPUT_BASED\",  # Scale based on throughput, not backlog\n",
    "\"--target_worker_utilization=0.7\",            # Scale at 70% capacity (more aggressive)\n",
    "\"--num_workers=2\",                             # Ensure proper initial worker count\n",
    "```\n",
    "\n",
    "Additionally in the pipeline code:\n",
    "```python\n",
    "# Reduce buffering to expose backpressure faster\n",
    "| \"Batch elements\" >> beam.BatchElements(\n",
    "    min_batch_size=1,\n",
    "    max_batch_size=50,\n",
    "    max_batch_duration_secs=0.001  # 1ms instead of 10ms\n",
    ")\n",
    "```\n",
    "\n",
    "**Understanding the Metrics:**\n",
    "\n",
    "- **Window wait time**: Time from publish to window_end timestamp (metadata only, not actual waiting)\n",
    "- **Internal queue time**: Time from window_end to pipeline output (REAL delay - workers saturated)\n",
    "- **P95 queue depth**: 95th percentile worst-case delay (most sensitive metric)\n",
    "\n",
    "The key insight: Window configuration is almost NEVER the bottleneck. The real issue is always worker saturation or endpoint throttling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create queuing diagnostic\n",
    "from scale_testing_combined_utils import create_queuing_diagnostic, print_queuing_summary\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"QUEUING DIAGNOSTIC\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create visualization\n",
    "fig_diagnostic = create_queuing_diagnostic(\n",
    "    latency_df=phase3_latency,\n",
    "    dataflow_metrics=phase3_metrics['dataflow'],\n",
    "    endpoint_metrics=phase3_metrics['endpoint'],\n",
    "    test_name=\"Phase 3: Ramp Test - Finding Autoscaling Threshold\"\n",
    ")\n",
    "\n",
    "fig_diagnostic.show(renderer=\"png\", width=1400, height=1200)\n",
    "\n",
    "# Print summary\n",
    "print_queuing_summary(\n",
    "    latency_df=phase3_latency,\n",
    "    dataflow_metrics=phase3_metrics['dataflow'],\n",
    "    endpoint_metrics=phase3_metrics['endpoint']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary_header",
   "metadata": {},
   "source": [
    "---\n",
    "## Comprehensive Test Summary & Production Recommendations\n",
    "\n",
    "Based on all test phases, generate production recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comprehensive_summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive summary\n",
    "print(\"=\"*80)\n",
    "print(\"COMPREHENSIVE TEST SUMMARY & PRODUCTION RECOMMENDATIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Job configuration\n",
    "print(f\"\\nüìã Dataflow Job Configuration:\")\n",
    "print(f\"   Job ID:         {JOB_ID}\")\n",
    "print(f\"   Region:         {REGION}\")\n",
    "print(f\"   Machine Type:   n1-standard-4 (4 vCPUs, 15 GB memory)\")\n",
    "print(f\"   Window Size:    60 seconds\")\n",
    "\n",
    "# Phase 1 summary\n",
    "print(f\"\\nüìä Phase 1: Baseline Performance\")\n",
    "if len(phase1_latencies) > 0 and len(phase1_latencies[0]) > 0:\n",
    "    baseline_latency = phase1_latencies[0]['pipeline_latency_ms'].quantile(0.95)\n",
    "    print(f\"   Baseline P95 latency: {baseline_latency:,.1f}ms at {BASELINE_TESTS[0]['target_rate']} msg/sec\")\n",
    "else:\n",
    "    print(f\"   Tests completed but latency data unavailable\")\n",
    "\n",
    "# Phase 2A summary\n",
    "print(f\"\\nüìä Phase 2A: Throughput Threshold Hunt\")\n",
    "phase2a_autoscaling = metrics_collector.dataflow_collector.analyze_autoscaling(phase2a_metrics['dataflow'])\n",
    "if len(phase2a_autoscaling) > 0:\n",
    "    print(f\"   ‚úÖ Autoscaling triggered {len(phase2a_autoscaling)} time(s)\")\n",
    "    print(f\"   Average provisioning time: {phase2a_autoscaling['scale_up_lag_seconds'].mean():.0f}s\")\n",
    "else:\n",
    "    print(f\"   No autoscaling at tested rates (50-200 msg/sec)\")\n",
    "\n",
    "if len(phase2a_latencies) > 0:\n",
    "    # Compare latency degradation across sustained tests\n",
    "    latencies_50 = phase2a_latencies[0]['pipeline_latency_ms'].quantile(0.95) if len(phase2a_latencies[0]) > 0 else None\n",
    "    latencies_200 = phase2a_latencies[-1]['pipeline_latency_ms'].quantile(0.95) if len(phase2a_latencies[-1]) > 0 else None\n",
    "    \n",
    "    if latencies_50 and latencies_200:\n",
    "        degradation = ((latencies_200 - latencies_50) / latencies_50) * 100\n",
    "        print(f\"   Latency at 50 msg/sec:  {latencies_50:,.1f}ms (P95)\")\n",
    "        print(f\"   Latency at 200 msg/sec: {latencies_200:,.1f}ms (P95)\")\n",
    "        print(f\"   Degradation: {degradation:+.1f}%\")\n",
    "\n",
    "# Phase 2B summary\n",
    "print(f\"\\nüìä Phase 2B: Endpoint Stress Testing\")\n",
    "if len(phase2b_results) > 0:\n",
    "    largest_burst = phase2b_results[-1]\n",
    "    print(f\"   Largest burst: {largest_burst['num_messages']:,} messages\")\n",
    "    if len(phase2b_latencies[-1]) > 0:\n",
    "        burst_p95 = phase2b_latencies[-1]['pipeline_latency_ms'].quantile(0.95)\n",
    "        print(f\"   P95 latency during burst: {burst_p95:,.1f}ms\")\n",
    "\n",
    "# Phase 3 summary\n",
    "print(f\"\\nüìä Phase 3: Ramp Test\")\n",
    "phase3_autoscaling = metrics_collector.dataflow_collector.analyze_autoscaling(phase3_metrics['dataflow'])\n",
    "if len(phase3_autoscaling) > 0:\n",
    "    first_trigger = phase3_autoscaling.iloc[0]\n",
    "    elapsed = (pd.Timestamp(first_trigger['trigger_time']) - pd.Timestamp(phase3_start)).total_seconds()\n",
    "    trigger_rate = (elapsed / RAMP_TEST['duration']) * RAMP_TEST['target_rate']\n",
    "    print(f\"   ‚úÖ Autoscaling triggered at ~{trigger_rate:.0f} msg/sec\")\n",
    "    print(f\"   This is the capacity threshold\")\n",
    "else:\n",
    "    print(f\"   No autoscaling triggered up to {RAMP_TEST['target_rate']} msg/sec\")\n",
    "    print(f\"   Pipeline has significant headroom\")\n",
    "\n",
    "if len(phase3_latency) > 0:\n",
    "    ramp_p95 = phase3_latency['pipeline_latency_ms'].quantile(0.95)\n",
    "    print(f\"   Overall P95 latency: {ramp_p95:,.1f}ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "production_recommendations",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Production recommendations\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PRODUCTION RECOMMENDATIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Determine recommended throughput\n",
    "print(\"\\nüéØ Recommended Throughput Configurations:\")\n",
    "\n",
    "# Try to determine capacity from autoscaling events\n",
    "all_autoscaling = pd.concat([\n",
    "    metrics_collector.dataflow_collector.analyze_autoscaling(phase1_metrics['dataflow']),\n",
    "    metrics_collector.dataflow_collector.analyze_autoscaling(phase2a_metrics['dataflow']),\n",
    "    metrics_collector.dataflow_collector.analyze_autoscaling(phase2b_metrics['dataflow']),\n",
    "    metrics_collector.dataflow_collector.analyze_autoscaling(phase3_metrics['dataflow'])\n",
    "], ignore_index=True)\n",
    "\n",
    "if len(all_autoscaling) > 0:\n",
    "    # Use first autoscaling event to estimate capacity\n",
    "    print(\"\\n   Based on autoscaling behavior:\")\n",
    "    print(\"   ‚Ä¢ Conservative (70% capacity): Suitable for production with headroom\")\n",
    "    print(\"   ‚Ä¢ Balanced (85% capacity): Good for predictable workloads\")\n",
    "    print(\"   ‚Ä¢ Aggressive (95% capacity): Maximum throughput, relies on autoscaling\")\n",
    "else:\n",
    "    print(\"\\n   Based on test results (no autoscaling triggered):\")\n",
    "    print(f\"   ‚Ä¢ Current configuration handled up to 200 msg/sec comfortably\")\n",
    "    print(\"   ‚Ä¢ Significant headroom available\")\n",
    "    print(\"   ‚Ä¢ Consider testing higher rates to find true limits\")\n",
    "\n",
    "# Worker configuration\n",
    "print(\"\\n‚öôÔ∏è  Worker Configuration Recommendations:\")\n",
    "if len(all_autoscaling) > 0:\n",
    "    avg_provisioning = all_autoscaling['scale_up_lag_seconds'].mean()\n",
    "    print(f\"\\n   Autoscaling Performance:\")\n",
    "    print(f\"   ‚Ä¢ Average worker provisioning: {avg_provisioning:.0f}s ({avg_provisioning/60:.1f} mins)\")\n",
    "    print(f\"   ‚Ä¢ Triggered {len(all_autoscaling)} time(s) across all tests\")\n",
    "    \n",
    "    if avg_provisioning > 300:  # 5 minutes\n",
    "        print(f\"\\n   ‚ö†Ô∏è  Slow provisioning time detected\")\n",
    "        print(f\"   ‚Ä¢ Consider increasing MIN_WORKERS for faster response\")\n",
    "        print(f\"   ‚Ä¢ Pre-warm capacity reduces latency spikes\")\n",
    "    else:\n",
    "        print(f\"\\n   ‚úÖ Autoscaling performance acceptable\")\n",
    "else:\n",
    "    print(f\"\\n   Current MIN_WORKERS=2, MAX_WORKERS=20\")\n",
    "    print(f\"   ‚Ä¢ No autoscaling needed at tested rates\")\n",
    "    print(f\"   ‚Ä¢ Current configuration appropriate\")\n",
    "\n",
    "# Machine type recommendations\n",
    "print(\"\\nüíª Machine Type Recommendations:\")\n",
    "print(f\"\\n   Current: n1-standard-4 (4 vCPUs, 15 GB memory)\")\n",
    "\n",
    "# Check if we have system lag data to determine if CPU-bound\n",
    "all_system_lag = []\n",
    "for metrics in [phase1_metrics['dataflow'], phase2a_metrics['dataflow'], phase2b_metrics['dataflow'], phase3_metrics['dataflow']]:\n",
    "    if 'system_lag' in metrics and len(metrics['system_lag']) > 0:\n",
    "        all_system_lag.extend(metrics['system_lag']['value'].tolist())\n",
    "\n",
    "if all_system_lag:\n",
    "    max_lag_ms = max(all_system_lag) / 1000  # Convert to ms\n",
    "    \n",
    "    if max_lag_ms > 60000:  # 1 minute\n",
    "        print(f\"\\n   ‚ö†Ô∏è  High system lag detected ({max_lag_ms/1000:.1f}s max)\")\n",
    "        print(f\"   ‚Ä¢ Consider c2-standard-4 for more CPU power\")\n",
    "        print(f\"   ‚Ä¢ Or increase worker count instead\")\n",
    "    else:\n",
    "        print(f\"\\n   ‚úÖ Current machine type performing well\")\n",
    "        print(f\"   ‚Ä¢ System lag remained under control (max: {max_lag_ms/1000:.1f}s)\")\n",
    "\n",
    "print(f\"\\n   When to consider alternatives:\")\n",
    "print(f\"   ‚Ä¢ CPU-intensive model ‚Üí c2-standard-4 (compute-optimized)\")\n",
    "print(f\"   ‚Ä¢ Large model size ‚Üí n1-highmem-4 (more memory)\")\n",
    "print(f\"   ‚Ä¢ GPU-compatible model ‚Üí Add --worker_gpu_type=nvidia-tesla-t4\")\n",
    "\n",
    "# Latency optimization\n",
    "print(\"\\n‚è±Ô∏è  Latency Characteristics:\")\n",
    "if len(phase1_latencies) > 0 and len(phase1_latencies[0]) > 0:\n",
    "    baseline = phase1_latencies[0]\n",
    "    \n",
    "    print(f\"\\n   Pipeline Latency (baseline test):\")\n",
    "    print(f\"   ‚Ä¢ P50:  {baseline['pipeline_latency_ms'].quantile(0.50):.1f}ms\")\n",
    "    print(f\"   ‚Ä¢ P95:  {baseline['pipeline_latency_ms'].quantile(0.95):.1f}ms\")\n",
    "    print(f\"   ‚Ä¢ P99:  {baseline['pipeline_latency_ms'].quantile(0.99):.1f}ms\")\n",
    "    print(f\"   ‚Ä¢ Mean: {baseline['pipeline_latency_ms'].mean():.1f}ms\")\n",
    "    \n",
    "    print(f\"\\n   Note: Pipeline latency includes ALL processing:\")\n",
    "    print(f\"   ‚Ä¢ Vertex AI endpoint inference\")\n",
    "    print(f\"   ‚Ä¢ Dataflow transforms\")\n",
    "    print(f\"   ‚Ä¢ Windowing (up to 60 seconds)\")\n",
    "    print(f\"   ‚Ä¢ Pub/Sub delivery\")\n",
    "    \n",
    "    # Check if endpoint metrics are available\n",
    "    if 'endpoint' in phase0_metrics and 'latency' in phase0_metrics['endpoint'] and len(phase0_metrics['endpoint']['latency']) > 0:\n",
    "        endpoint_latency = phase0_metrics['endpoint']['latency']['value'].mean()\n",
    "        endpoint_ratio = endpoint_latency / baseline['pipeline_latency_ms'].mean() * 100\n",
    "        \n",
    "        print(f\"\\n   Vertex AI Endpoint Contribution:\")\n",
    "        print(f\"   ‚Ä¢ Endpoint service latency: {endpoint_latency:.1f}ms (from Cloud Monitoring)\")\n",
    "        print(f\"   ‚Ä¢ Represents ~{endpoint_ratio:.1f}% of total pipeline latency\")\n",
    "        \n",
    "        if endpoint_ratio > 60:\n",
    "            print(f\"\\n   üí° Endpoint dominates latency\")\n",
    "            print(f\"   ‚Ä¢ Consider model optimization\")\n",
    "            print(f\"   ‚Ä¢ Or use larger endpoint machine type\")\n",
    "            print(f\"   ‚Ä¢ Or enable GPU acceleration\")\n",
    "        else:\n",
    "            print(f\"\\n   ‚ÑπÔ∏è  Latency is distributed across multiple components\")\n",
    "            print(f\"   ‚Ä¢ Windowing contributes significant time (up to 60s)\")\n",
    "            print(f\"   ‚Ä¢ To reduce latency: Use smaller windows (30s, 15s)\")\n",
    "            print(f\"   ‚Ä¢ Trade-off: Smaller windows = less batching efficiency\")\n",
    "\n",
    "# Cost optimization\n",
    "print(\"\\nüí∞ Cost Optimization Tips:\")\n",
    "print(f\"   ‚Ä¢ Monitor actual traffic patterns and adjust MIN/MAX workers\")\n",
    "print(f\"   ‚Ä¢ Monitor endpoint CPU to optimize replica counts\")\n",
    "print(f\"   ‚Ä¢ Use batch processing for historical analysis (cheaper)\")\n",
    "print(f\"   ‚Ä¢ Set up alerts for unexpected scaling\")\n",
    "print(f\"   ‚Ä¢ Drain and stop job when not actively processing\")\n",
    "\n",
    "# Monitoring\n",
    "print(\"\\nüìä Monitoring & Alerts:\")\n",
    "print(f\"   Set up Cloud Monitoring alerts for:\")\n",
    "print(f\"   ‚Ä¢ Dataflow system lag > 60 seconds (pipeline falling behind)\")\n",
    "print(f\"   ‚Ä¢ Pub/Sub backlog > 10,000 messages (capacity issues)\")\n",
    "print(f\"   ‚Ä¢ Dataflow worker count at MAX_WORKERS (may need limit increase)\")\n",
    "print(f\"   ‚Ä¢ Vertex endpoint CPU > 70% (may need more replicas)\")\n",
    "print(f\"   ‚Ä¢ Element count drops to 0 (pipeline stalled)\")\n",
    "\n",
    "# Next steps\n",
    "print(\"\\nüìù Next Steps:\")\n",
    "print(f\"   1. Review combined system metrics in Cloud Console\")\n",
    "print(f\"      - Dataflow: https://console.cloud.google.com/dataflow/jobs/{REGION}/{JOB_ID}?project={PROJECT_ID}\")\n",
    "print(f\"      - Vertex AI: https://console.cloud.google.com/vertex-ai/endpoints?project={PROJECT_ID}\")\n",
    "print(f\"   2. If bottleneck identified:\")\n",
    "print(f\"      ‚Ä¢ Dataflow bottleneck ‚Üí Increase workers or use better machine type\")\n",
    "print(f\"      ‚Ä¢ Endpoint bottleneck ‚Üí Increase replicas or optimize model\")\n",
    "print(f\"   3. Implement recommended monitoring alerts\")\n",
    "print(f\"   4. Set up dashboard for real-time visibility\")\n",
    "print(f\"   5. Document findings for future capacity planning\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion_header",
   "metadata": {},
   "source": [
    "---\n",
    "## Conclusion\n",
    "\n",
    "This notebook provides a comprehensive, scientific approach to understanding Dataflow Streaming performance through systematic testing across three dimensions:\n",
    "\n",
    "**1. Message Rate** - Baseline, sustained, and peak throughput testing  \n",
    "**2. Load Patterns** - Burst, sustained, and ramping traffic patterns  \n",
    "**3. Latency Characteristics** - End-to-end breakdown and bottleneck identification\n",
    "\n",
    "### Key Insights from This Testing Framework\n",
    "\n",
    "**Understanding Dataflow Autoscaling:**\n",
    "- Dataflow scales based on **Pub/Sub backlog** and **system lag** (not just CPU like Vertex AI)\n",
    "- Worker provisioning takes **2-4 minutes** (plan for this delay)\n",
    "- Sustained load (not bursts) typically triggers autoscaling\n",
    "- Ramp tests reveal exact throughput thresholds\n",
    "\n",
    "**Latency Composition:**\n",
    "- **Window Wait**: Time from publish to window close (up to window size)\n",
    "- **Processing**: Model inference + transforms (controllable via machine type)\n",
    "- **Pub/Sub Delivery**: Usually <100ms (network overhead)\n",
    "- Smaller windows = lower latency but less batching efficiency\n",
    "\n",
    "**Bottleneck Identification:**\n",
    "- High **system lag** ‚Üí Processing capacity issues (need more workers or better machine type)\n",
    "- High **backlog** ‚Üí Ingestion rate exceeds processing capacity (autoscaling needed)\n",
    "- High **window wait %** ‚Üí Latency dominated by batching (consider smaller windows)\n",
    "- High **processing %** ‚Üí Model inference slow (consider GPU or model optimization)\n",
    "\n",
    "### Important Notes\n",
    "\n",
    "**Results are Pipeline-Specific:**  \n",
    "All results in this notebook are specific to the tested pipeline configuration:\n",
    "- PyTorch autoencoder model with 30 features\n",
    "- n1-standard-4 workers (4 vCPUs, 15 GB memory)\n",
    "- 60-second fixed windows\n",
    "- MIN=2, MAX=20 workers\n",
    "\n",
    "Your results will vary based on:\n",
    "- Model complexity and inference time\n",
    "- Machine type and resources\n",
    "- Window size configuration\n",
    "- Message size and format\n",
    "\n",
    "**Always Test Your Own Pipeline:**  \n",
    "Before production deployment:\n",
    "1. Run this notebook with your pipeline and representative data\n",
    "2. Test with realistic traffic patterns\n",
    "3. Adjust worker configuration based on results\n",
    "4. Monitor production metrics continuously\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "**If This Notebook Revealed Issues:**\n",
    "1. **Autoscaling too slow** ‚Üí Increase MIN_WORKERS for pre-warmed capacity\n",
    "2. **High system lag** ‚Üí Use c2-standard-4 or increase MAX_WORKERS\n",
    "3. **High latency** ‚Üí Reduce window size or optimize model\n",
    "4. **Backlog building** ‚Üí Increase MAX_WORKERS or improve processing efficiency\n",
    "\n",
    "**Production Deployment Tasks:**\n",
    "- Set up Cloud Monitoring alerts (system lag, backlog, worker count)\n",
    "- Create dashboard for real-time visibility\n",
    "- Document capacity planning findings\n",
    "- Implement gradual rollout with monitoring\n",
    "\n",
    "**Related Notebooks:**\n",
    "- [Dataflow Streaming RunInference](./dataflow-streaming-runinference.ipynb) - Deploy the pipeline tested here\n",
    "- [Dataflow Batch RunInference](./dataflow-batch-runinference.ipynb) - Batch processing alternative\n",
    "- [Vertex AI Endpoint Scale Tests](./scale-tests-vertex-ai-endpoints.ipynb) - Compare with endpoint serving\n",
    "\n",
    "### Resources\n",
    "\n",
    "**Dataflow Documentation:**\n",
    "- [Streaming Pipelines](https://cloud.google.com/dataflow/docs/concepts/streaming-pipelines)\n",
    "- [Autoscaling](https://cloud.google.com/dataflow/docs/guides/deploying-a-pipeline#autoscaling)\n",
    "- [Monitoring](https://cloud.google.com/dataflow/docs/guides/using-cloud-monitoring)\n",
    "\n",
    "**Apache Beam:**\n",
    "- [RunInference](https://beam.apache.org/documentation/ml/about-ml/)\n",
    "- [Windowing](https://beam.apache.org/documentation/programming-guide/#windowing)\n",
    "- [PyTorch Handler](https://beam.apache.org/documentation/ml/pytorch-inference/)\n",
    "\n",
    "**Cloud Monitoring:**\n",
    "- [Dataflow Metrics](https://cloud.google.com/dataflow/docs/guides/using-cloud-monitoring#available_dataflow_metrics)\n",
    "- [Setting Up Alerts](https://cloud.google.com/monitoring/alerts)\n",
    "\n",
    "---\n",
    "\n",
    "**Testing Framework Created:** This comprehensive testing infrastructure (`scale_testing_dataflow_utils.py` + this notebook) can be reused for testing any Dataflow Streaming pipeline. Simply update the job ID and test parameters."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (frameworks-pytorch)",
   "language": "python",
   "name": "frameworks-pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
