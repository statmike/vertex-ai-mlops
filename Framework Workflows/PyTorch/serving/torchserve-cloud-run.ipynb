{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69b7ced9",
   "metadata": {},
   "source": [
    "![tracker](https://us-central1-vertex-ai-mlops-369716.cloudfunctions.net/pixel-tracking?path=statmike%2Fvertex-ai-mlops%2FFramework+Workflows%2FPyTorch%2Fserving&file=torchserve-cloud-run.ipynb)\n",
    "<!--- header table --->\n",
    "<table align=\"left\">\n",
    "<tr>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%20Workflows/PyTorch/serving/torchserve-cloud-run.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.svgrepo.com/download/217753/github.svg\" alt=\"GitHub logo\">\n",
    "      <br>View on<br>GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://colab.research.google.com/github/statmike/vertex-ai-mlops/blob/main/Framework%20Workflows/PyTorch/serving/torchserve-cloud-run.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.gstatic.com/pantheon/images/bigquery/welcome_page/colab-logo.svg\" alt=\"Google Colaboratory logo\">\n",
    "      <br>Run in<br>Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https%3A%2F%2Fraw.githubusercontent.com%2Fstatmike%2Fvertex-ai-mlops%2Fmain%2FFramework%2520Workflows%2FPyTorch%2Fserving%2Ftorchserve-cloud-run.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\">\n",
    "      <br>Run in<br>Colab Enterprise\n",
    "    </a>\n",
    "  </td>      \n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/bigquery/import?url=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%20Workflows/PyTorch/serving/torchserve-cloud-run.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.gstatic.com/images/branding/gcpiconscolors/bigquery/v1/32px.svg\" alt=\"BigQuery logo\">\n",
    "      <br>Open in<br>BigQuery Studio\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/statmike/vertex-ai-mlops/main/Framework%20Workflows/PyTorch/serving/torchserve-cloud-run.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.gstatic.com/images/branding/gcpiconscolors/vertexai/v1/32px.svg\" alt=\"Vertex AI logo\">\n",
    "      <br>Open in<br>Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td colspan=\"5\" style=\"text-align: right\">\n",
    "    <b>Share This On: </b> \n",
    "    <a href=\"https://www.linkedin.com/sharing/share-offsite/?url=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%2520Workflows/PyTorch/serving/torchserve-cloud-run.ipynb\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"Linkedin Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://reddit.com/submit?url=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%2520Workflows/PyTorch/serving/torchserve-cloud-run.ipynb\"><img src=\"https://redditinc.com/hubfs/Reddit%20Inc/Brand/Reddit_Logo.png\" alt=\"Reddit Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://bsky.app/intent/compose?text=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%2520Workflows/PyTorch/serving/torchserve-cloud-run.ipynb\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"BlueSky Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://twitter.com/intent/tweet?url=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%2520Workflows/PyTorch/serving/torchserve-cloud-run.ipynb\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/5/5a/X_icon_2.svg\" alt=\"X (Twitter) Logo\" width=\"20px\"></a> \n",
    "  </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td colspan=\"5\" style=\"text-align: right\">\n",
    "    <b>Connect With Author On: </b> \n",
    "    <a href=\"https://www.linkedin.com/in/statmike\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"Linkedin Logo\" width=\"20px\"></a>\n",
    "    <a href=\"https://www.github.com/statmike\"><img src=\"https://www.svgrepo.com/download/217753/github.svg\" alt=\"GitHub Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://www.youtube.com/@statmike-channel\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/f/fd/YouTube_full-color_icon_%282024%29.svg\" alt=\"YouTube Logo\" width=\"20px\"></a>\n",
    "    <a href=\"https://bsky.app/profile/statmike.bsky.social\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"BlueSky Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://x.com/statmike\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/5/5a/X_icon_2.svg\" alt=\"X (Twitter) Logo\" width=\"20px\"></a>\n",
    "  </td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# TorchServe Deployment on Cloud Run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overview",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This notebook demonstrates how to containerize TorchServe and deploy it to **Cloud Run**, Google Cloud's fully managed serverless platform for containers.\n",
    "\n",
    "### What This Notebook Does\n",
    "\n",
    "- Uses the Model Archive (.mar) file created in the training notebook\n",
    "- Creates a production-ready Dockerfile for TorchServe\n",
    "- Builds and pushes a container image using Cloud Build\n",
    "- Deploys TorchServe to Cloud Run with auto-scaling\n",
    "- Tests predictions via the deployed HTTPS endpoint\n",
    "- Demonstrates monitoring and cost optimization\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "Before running this notebook, you should have:\n",
    "- **Completed** `../pytorch-autoencoder.ipynb` to create the model archive\n",
    "  - The `.mar` file should be in `../files/pytorch-autoencoder/pytorch_autoencoder.mar`\n",
    "- Google Cloud project with billing enabled\n",
    "- Appropriate IAM permissions for Cloud Run, Cloud Build, and Artifact Registry\n",
    "\n",
    "### What is Cloud Run?\n",
    "\n",
    "Cloud Run is a serverless compute platform that automatically scales your containerized applications. Key features include:\n",
    "- **Serverless**: No infrastructure management required\n",
    "- **Auto-scaling**: Automatically scales from 0 to N based on traffic\n",
    "- **Pay-per-use**: Only pay for actual request processing time\n",
    "- **Fully managed**: Handles load balancing, health checks, and SSL certificates\n",
    "- **Fast deployment**: Deploy containers in seconds\n",
    "\n",
    "### Benefits of Cloud Run for ML Model Serving\n",
    "\n",
    "Compared to local TorchServe deployment:\n",
    "- **Production-ready**: Built-in load balancing, health checks, and HTTPS\n",
    "- **Cost-effective**: Scales to zero when not in use (no idle costs)\n",
    "- **Global availability**: Deploy to regions worldwide\n",
    "- **Easy updates**: Deploy new versions with zero downtime\n",
    "- **Integrated monitoring**: Cloud Logging and Cloud Monitoring built-in\n",
    "\n",
    "### File Organization\n",
    "\n",
    "This notebook creates an isolated workspace to avoid conflicts with other notebooks:\n",
    "\n",
    "**Local Structure:**\n",
    "```\n",
    "./files/torchserve-cloud-run/\n",
    "â””â”€â”€ Dockerfile              # Container build configuration\n",
    "```\n",
    "\n",
    "**GCS Structure:**\n",
    "```\n",
    "gs://{PROJECT_ID}/frameworks/pytorch-autoencoder/torchserve-cloud-run/\n",
    "â””â”€â”€ pytorch_autoencoder.mar # Model archive for container build\n",
    "```\n",
    "\n",
    "### Architecture\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  Local Model Archive (.mar)             â”‚\n",
    "â”‚  ../files/pytorch-autoencoder/          â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "         â”‚\n",
    "         â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  Upload to GCS Bucket                   â”‚\n",
    "â”‚  gs://{PROJECT_ID}/frameworks/          â”‚\n",
    "â”‚     pytorch-autoencoder/                â”‚\n",
    "â”‚     torchserve-cloud-run/               â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "         â”‚\n",
    "         â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  Dockerfile (TorchServe)                â”‚\n",
    "â”‚  ./files/torchserve-cloud-run/          â”‚\n",
    "â”‚  - Downloads .mar from GCS              â”‚\n",
    "â”‚  - Configures TorchServe for port 8080  â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "         â”‚\n",
    "         â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  Cloud Build                            â”‚\n",
    "â”‚  - Builds container image               â”‚\n",
    "â”‚  - Pushes to Artifact Registry          â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "         â”‚\n",
    "         â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  Cloud Run Service                      â”‚\n",
    "â”‚  - Auto-scaling (0 to N instances)      â”‚\n",
    "â”‚  - HTTPS endpoint                       â”‚\n",
    "â”‚  - Built-in monitoring                  â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### What You Will Learn\n",
    "\n",
    "1. How to prepare a model archive for cloud deployment\n",
    "2. How to create a Cloud Run-compatible Dockerfile for TorchServe\n",
    "3. How to build and push container images using Cloud Build\n",
    "4. How to deploy and configure Cloud Run services\n",
    "5. How to test deployed endpoints with predictions\n",
    "6. How to monitor service performance and logs\n",
    "7. How to configure auto-scaling for cost optimization\n",
    "8. Best practices for production ML serving on Cloud Run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "env",
   "metadata": {},
   "source": [
    "---\n",
    "## Environment Setup\n",
    "\n",
    "This section will authenticate your session, enable required Google Cloud APIs, and install necessary Python packages.\n",
    "\n",
    "**Package Installation Options (`REQ_TYPE`):**\n",
    "- `PRIMARY`: Installs only the main packages. Faster, but pip resolves sub-dependencies which may result in different versions than development.\n",
    "- `ALL` (Default): Installs exact versions of all packages and dependencies. Best for perfectly reproducing the development environment.\n",
    "- `COLAB`: Installs a Colab-optimized list that excludes pre-installed packages like `ipython` and `ipykernel`.\n",
    "\n",
    "**Installation Tool Options (`INSTALL_TOOL`):**\n",
    "- `pip` (Default): Uses pip for package installation. Standard Python package installer.\n",
    "- `uv`: Modern, fast Python package installer. Must be installed separately. See: https://github.com/astral-sh/uv\n",
    "- `poetry`: Dependency management tool. Requires running notebook in a poetry environment (`poetry shell` or `poetry run jupyter lab`). Uses `pyproject.toml` instead of requirements.txt.\n",
    "\n",
    "> **Note:** If running in Google Colab, the script will automatically detect this and set `REQ_TYPE = 'COLAB'` to prevent package conflicts, overriding any manual setting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proj",
   "metadata": {},
   "source": [
    "### Set Your Project ID\n",
    "\n",
    "âš ï¸ **Action Required:** Replace the `PROJECT_ID` value below with your Google Cloud project ID before running this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "conf",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = 'statmike-mlops-349915' # replace with GCP project ID\n",
    "REQ_TYPE = 'ALL' # Specify PRIMARY or ALL or COLAB\n",
    "INSTALL_TOOL = 'poetry' # Specify pip, uv, or poetry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "698hb9aex2t",
   "metadata": {},
   "outputs": [],
   "source": [
    "REQUIREMENTS_URL = 'https://raw.githubusercontent.com/statmike/vertex-ai-mlops/refs/heads/main/Framework%20Workflows/PyTorch/requirements.txt'\n",
    "REQUIRED_APIS = [\"run.googleapis.com\", \"cloudbuild.googleapis.com\", \"artifactregistry.googleapis.com\", \"storage.googleapis.com\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup",
   "metadata": {},
   "source": [
    "### Run Setup\n",
    "\n",
    "This cell downloads the centralized setup code and configures your environment. It will:\n",
    "- Authenticate to Google Cloud\n",
    "- Enable required APIs (Cloud Run, Cloud Build, Artifact Registry, Cloud Storage)\n",
    "- Install necessary Python packages\n",
    "- Display a setup summary\n",
    "\n",
    "> **Note:** In Colab, if packages are installed, the kernel will automatically restart. After restart, continue from the next cell without re-running earlier cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "q1eifsqdr2n",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "PYTHON GCP ENVIRONMENT SETUP\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "AUTHENTICATION\n",
      "==================================================\n",
      "Checking for existing ADC...\n",
      "âœ… Existing ADC found.\n",
      "âœ… Project is correctly set to 'statmike-mlops-349915'.\n",
      "\n",
      "==================================================\n",
      "API CHECK & ENABLE\n",
      "==================================================\n",
      "âœ… run.googleapis.com is already enabled.\n",
      "âœ… cloudbuild.googleapis.com is already enabled.\n",
      "âœ… artifactregistry.googleapis.com is already enabled.\n",
      "âœ… storage.googleapis.com is already enabled.\n",
      "\n",
      "==================================================\n",
      "PACKAGE MANAGEMENT\n",
      "==================================================\n",
      "Installation Tool: poetry\n",
      "âœ… Found poetry at: /usr/local/google/home/statmike/.local/bin/poetry\n",
      "âœ… Running in poetry environment: /usr/local/google/home/statmike/.cache/pypoetry/virtualenvs/frameworks-pytorch-0KVJlKeQ-py3.13\n",
      "â„¹ï¸  Poetry mode: Installing from pyproject.toml (REQUIREMENTS_URL ignored)\n",
      "âœ… Found pyproject.toml at: /usr/local/google/home/statmike/Git/vertex-ai-mlops/Framework Workflows/PyTorch/pyproject.toml\n",
      "   Changed working directory to: /usr/local/google/home/statmike/Git/vertex-ai-mlops/Framework Workflows/PyTorch\n",
      "Running poetry install...\n",
      "   Restored working directory to: /usr/local/google/home/statmike/Git/vertex-ai-mlops/Framework Workflows/PyTorch/serving\n",
      "âœ… All packages are already installed and up to date.\n",
      "\n",
      "==================================================\n",
      "Google Cloud Project Information\n",
      "==================================================\n",
      "PROJECT_ID     = statmike-mlops-349915\n",
      "PROJECT_NUMBER = 1026793852137\n",
      "==================================================\n",
      "\n",
      "\n",
      "==================================================\n",
      "SETUP SUMMARY\n",
      "==================================================\n",
      "âœ… Authentication:    Success\n",
      "âœ… API Configuration: Success\n",
      "âœ… Package Install:   Already up to date\n",
      "âœ… Installation Tool: poetry\n",
      "âœ… Project ID:        statmike-mlops-349915\n",
      "âœ… Project Number:    1026793852137\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os, urllib.request\n",
    "\n",
    "# Download and import setup code\n",
    "url = 'https://raw.githubusercontent.com/statmike/vertex-ai-mlops/refs/heads/main/core/notebook-template/python_setup.py'\n",
    "urllib.request.urlretrieve(url, 'python_setup_local.py')\n",
    "import python_setup_local as python_setup\n",
    "os.remove('python_setup_local.py')\n",
    "\n",
    "# Run setup\n",
    "setup_info = python_setup.setup_environment(PROJECT_ID, REQ_TYPE, REQUIREMENTS_URL, REQUIRED_APIS, INSTALL_TOOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "py",
   "metadata": {},
   "source": [
    "---\n",
    "## Python Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imp",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0za42ixsmpz",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import subprocess\n",
    "import time\n",
    "import requests\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from google.cloud import storage\n",
    "from google.cloud import artifactregistry_v1\n",
    "from google.cloud.devtools import cloudbuild_v1\n",
    "from google.cloud import run_v2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vars",
   "metadata": {},
   "source": [
    "### Variables\n",
    "\n",
    "Configuration for Cloud Run deployment and model paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bebiw88qsx",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project: statmike-mlops-349915\n",
      "Region: us-central1\n",
      "\n",
      "Model Configuration:\n",
      "  Source .mar file: ../files/pytorch-autoencoder/pytorch_autoencoder.mar\n",
      "  GCS destination: gs://statmike-mlops-349915/frameworks/pytorch-autoencoder/torchserve-cloud-run/pytorch_autoencoder.mar\n",
      "\n",
      "Local Workspace:\n",
      "  Working directory: files/torchserve-cloud-run\n",
      "\n",
      "Artifact Registry:\n",
      "  Repository: us-central1-docker.pkg.dev/statmike-mlops-349915/frameworks\n",
      "  Image: us-central1-docker.pkg.dev/statmike-mlops-349915/frameworks/pytorch-autoencoder-torchserve\n",
      "\n",
      "Cloud Run Configuration:\n",
      "  Service name: pytorch-autoencoder-torchserve\n",
      "  Resources: 2 CPU, 2Gi memory\n",
      "  Port: 8080\n",
      "  Scaling: 0 to 10 instances\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "REGION = \"us-central1\"\n",
    "SERIES = \"frameworks\"\n",
    "EXPERIMENT = \"pytorch-autoencoder\"\n",
    "\n",
    "# Model settings\n",
    "MODEL_NAME = \"pytorch_autoencoder\"\n",
    "MAR_FILE = \"pytorch_autoencoder.mar\"\n",
    "\n",
    "# Local paths\n",
    "LOCAL_MAR_PATH = f\"../files/{EXPERIMENT}/{MAR_FILE}\"\n",
    "WORK_DIR = Path(\"./files/torchserve-cloud-run\")\n",
    "\n",
    "# GCS paths\n",
    "BUCKET_NAME = PROJECT_ID\n",
    "BUCKET_URI = f\"gs://{BUCKET_NAME}\"\n",
    "GCS_DIR = f\"{BUCKET_URI}/{SERIES}/{EXPERIMENT}/torchserve-cloud-run\"\n",
    "GCS_MAR_PATH = f\"{GCS_DIR}/{MAR_FILE}\"\n",
    "\n",
    "# Artifact Registry configuration\n",
    "AR_REPO_NAME = SERIES\n",
    "AR_REPO_URI = f\"{REGION}-docker.pkg.dev/{PROJECT_ID}/{AR_REPO_NAME}\"\n",
    "\n",
    "# Cloud Run configuration\n",
    "SERVICE_NAME = f\"{EXPERIMENT}-torchserve\"\n",
    "IMAGE_NAME = f\"{AR_REPO_URI}/{SERVICE_NAME}\"\n",
    "MEMORY = \"2Gi\"\n",
    "CPU = 2\n",
    "PORT = 8080\n",
    "MAX_INSTANCES = 10\n",
    "MIN_INSTANCES = 0  # Scale to zero when idle\n",
    "\n",
    "# Initialize clients\n",
    "storage_client = storage.Client(project=PROJECT_ID)\n",
    "ar_client = artifactregistry_v1.ArtifactRegistryClient()\n",
    "cb_client = cloudbuild_v1.CloudBuildClient()\n",
    "run_client = run_v2.ServicesClient()\n",
    "\n",
    "print(f\"Project: {PROJECT_ID}\")\n",
    "print(f\"Region: {REGION}\")\n",
    "print(f\"\\nModel Configuration:\")\n",
    "print(f\"  Source .mar file: {LOCAL_MAR_PATH}\")\n",
    "print(f\"  GCS destination: {GCS_MAR_PATH}\")\n",
    "print(f\"\\nLocal Workspace:\")\n",
    "print(f\"  Working directory: {WORK_DIR}\")\n",
    "print(f\"\\nArtifact Registry:\")\n",
    "print(f\"  Repository: {AR_REPO_URI}\")\n",
    "print(f\"  Image: {IMAGE_NAME}\")\n",
    "print(f\"\\nCloud Run Configuration:\")\n",
    "print(f\"  Service name: {SERVICE_NAME}\")\n",
    "print(f\"  Resources: {CPU} CPU, {MEMORY} memory\")\n",
    "print(f\"  Port: {PORT}\")\n",
    "print(f\"  Scaling: {MIN_INSTANCES} to {MAX_INSTANCES} instances\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dockerfile",
   "metadata": {},
   "source": [
    "---\n",
    "## Upload Model Archive to GCS\n",
    "\n",
    "Upload the `.mar` file to Google Cloud Storage so it can be downloaded by Cloud Run containers at startup.\n",
    "\n",
    "**Why GCS?** Cloud Run containers download the model at runtime (not build time) so they can authenticate using the service account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ns7krtt0vk",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Local workspace: files/torchserve-cloud-run\n",
      "âœ… Found model archive: ../files/pytorch-autoencoder/pytorch_autoencoder.mar\n",
      "   Size: 29.2 KB\n",
      "\n",
      "Uploading to gs://statmike-mlops-349915/frameworks/pytorch-autoencoder/torchserve-cloud-run/pytorch_autoencoder.mar...\n",
      "âœ… Upload complete!\n"
     ]
    }
   ],
   "source": [
    "# Create local working directory\n",
    "WORK_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"âœ… Local workspace: {WORK_DIR}\")\n",
    "\n",
    "# Verify local .mar file exists\n",
    "if not Path(LOCAL_MAR_PATH).exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Model archive not found at {LOCAL_MAR_PATH}\\n\"\n",
    "        f\"Please run ../pytorch-autoencoder.ipynb first to create the model archive\"\n",
    "    )\n",
    "\n",
    "print(f\"âœ… Found model archive: {LOCAL_MAR_PATH}\")\n",
    "print(f\"   Size: {Path(LOCAL_MAR_PATH).stat().st_size / 1024:.1f} KB\")\n",
    "\n",
    "# Upload to GCS\n",
    "bucket = storage_client.bucket(BUCKET_NAME)\n",
    "blob = bucket.blob(f\"{SERIES}/{EXPERIMENT}/torchserve-cloud-run/{MAR_FILE}\")\n",
    "\n",
    "print(f\"\\nUploading to {GCS_MAR_PATH}...\")\n",
    "blob.upload_from_filename(LOCAL_MAR_PATH)\n",
    "\n",
    "print(f\"âœ… Upload complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ft5v4fw0zj",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup Artifact Registry\n",
    "\n",
    "Check for existing Artifact Registry repository or create a new one to store Docker containers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12afd937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for Artifact Registry repository...\n",
      "âœ… Found existing repository: frameworks\n",
      "   Format: DOCKER\n",
      "   URI: us-central1-docker.pkg.dev/statmike-mlops-349915/frameworks\n"
     ]
    }
   ],
   "source": [
    "# Check for existing Artifact Registry repository\n",
    "ar_parent = f\"projects/{PROJECT_ID}/locations/{REGION}\"\n",
    "\n",
    "print(\"Checking for Artifact Registry repository...\")\n",
    "for repo in ar_client.list_repositories(parent=ar_parent):\n",
    "    if repo.name.split('/')[-1] == AR_REPO_NAME:\n",
    "        print(f\"âœ… Found existing repository: {AR_REPO_NAME}\")\n",
    "        print(f\"   Format: {repo.format_.name}\")\n",
    "        print(f\"   URI: {AR_REPO_URI}\")\n",
    "        break\n",
    "else:\n",
    "    # Repository not found - create it\n",
    "    print(f\"ðŸ“¦ Creating new repository: {AR_REPO_NAME}\")\n",
    "    operation = ar_client.create_repository(\n",
    "        request=artifactregistry_v1.CreateRepositoryRequest(\n",
    "            parent=ar_parent,\n",
    "            repository_id=AR_REPO_NAME,\n",
    "            repository=artifactregistry_v1.Repository(\n",
    "                description=f\"Docker images for {SERIES} series\",\n",
    "                format_=artifactregistry_v1.Repository.Format.DOCKER,\n",
    "                labels={'series': SERIES}\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    operation.result()  # Wait for creation\n",
    "    print(f\"âœ… Repository created!\")\n",
    "    print(f\"   URI: {AR_REPO_URI}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hcvoxjkmuud",
   "metadata": {},
   "source": [
    "---\n",
    "## Create Dockerfile\n",
    "\n",
    "Create a production-ready Dockerfile that configures TorchServe for Cloud Run.\n",
    "\n",
    "**Key Design Decisions:**\n",
    "1. **Runtime Model Download**: Downloads model from GCS at container startup (not build time)\n",
    "   - Allows Cloud Run's service account to authenticate with GCS\n",
    "   - Keeps image size smaller\n",
    "   - Enables using same image with different models\n",
    "\n",
    "2. **Foreground Mode**: TorchServe runs in foreground for Cloud Run compatibility\n",
    "   - Cloud Run needs the main process to stay running\n",
    "\n",
    "3. **Dependencies**: Includes `pyyaml` required by the custom handler\n",
    "\n",
    "4. **TorchServe Config**: \n",
    "   - Port 8080 for Cloud Run standard\n",
    "   - `disable_token_authorization=true` for public access\n",
    "   - `load_models=all` to auto-load models in the model store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dockerfile_c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Dockerfile created and uploaded to GCS\n",
      "   Local: files/torchserve-cloud-run/Dockerfile\n",
      "   GCS: gs://statmike-mlops-349915/frameworks/pytorch-autoencoder/torchserve-cloud-run/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "dockerfile_content = f\"\"\"FROM python:3.9-slim\n",
    "\n",
    "# Install system dependencies\n",
    "RUN apt-get update && apt-get install -y \\\\\n",
    "    default-jdk \\\\\n",
    "    curl \\\\\n",
    "    && rm -rf /var/lib/apt/lists/*\n",
    "\n",
    "# Install TorchServe and dependencies\n",
    "RUN pip install --no-cache-dir \\\\\n",
    "    torchserve \\\\\n",
    "    torch-model-archiver \\\\\n",
    "    torch-workflow-archiver \\\\\n",
    "    torch \\\\\n",
    "    torchvision \\\\\n",
    "    google-cloud-storage \\\\\n",
    "    pyyaml\n",
    "\n",
    "# Create TorchServe directories\n",
    "RUN mkdir -p /home/model-server/model-store /home/model-server/tmp\n",
    "\n",
    "WORKDIR /home/model-server\n",
    "\n",
    "# Create TorchServe configuration\n",
    "RUN echo \"inference_address=http://0.0.0.0:8080\" > config.properties && \\\\\n",
    "    echo \"management_address=http://0.0.0.0:8081\" >> config.properties && \\\\\n",
    "    echo \"metrics_address=http://0.0.0.0:8082\" >> config.properties && \\\\\n",
    "    echo \"model_store=/home/model-server/model-store\" >> config.properties && \\\\\n",
    "    echo \"load_models=all\" >> config.properties && \\\\\n",
    "    echo \"disable_token_authorization=true\" >> config.properties\n",
    "\n",
    "# Create startup script that downloads model from GCS and starts TorchServe\n",
    "RUN echo '#!/bin/bash\\\\n\\\\\n",
    "set -e\\\\n\\\\\n",
    "echo \"Downloading model from GCS...\"\\\\n\\\\\n",
    "python -c \"from google.cloud import storage; \\\\\\\\\\\\n\\\\\n",
    "    client = storage.Client(project=\\\\047{PROJECT_ID}\\\\047); \\\\\\\\\\\\n\\\\\n",
    "    bucket = client.bucket(\\\\047{BUCKET_NAME}\\\\047); \\\\\\\\\\\\n\\\\\n",
    "    blob = bucket.blob(\\\\047{SERIES}/{EXPERIMENT}/torchserve-cloud-run/{MAR_FILE}\\\\047); \\\\\\\\\\\\n\\\\\n",
    "    blob.download_to_filename(\\\\047/home/model-server/model-store/{MAR_FILE}\\\\047)\"\\\\n\\\\\n",
    "echo \"Model downloaded successfully\"\\\\n\\\\\n",
    "echo \"Starting TorchServe...\"\\\\n\\\\\n",
    "torchserve --start --ts-config config.properties --models {MODEL_NAME}={MAR_FILE} --foreground\\\\n\\\\\n",
    "' > /home/model-server/start.sh && chmod +x /home/model-server/start.sh\n",
    "\n",
    "EXPOSE 8080\n",
    "\n",
    "CMD [\"/home/model-server/start.sh\"]\n",
    "\"\"\"\n",
    "\n",
    "# Write and upload Dockerfile\n",
    "dockerfile_path = WORK_DIR / \"Dockerfile\"\n",
    "dockerfile_path.write_text(dockerfile_content)\n",
    "\n",
    "bucket = storage_client.bucket(BUCKET_NAME)\n",
    "blob = bucket.blob(f\"{SERIES}/{EXPERIMENT}/torchserve-cloud-run/Dockerfile\")\n",
    "blob.upload_from_filename(str(dockerfile_path))\n",
    "\n",
    "print(\"âœ… Dockerfile created and uploaded to GCS\")\n",
    "print(f\"   Local: {dockerfile_path}\")\n",
    "print(f\"   GCS: gs://{BUCKET_NAME}/{SERIES}/{EXPERIMENT}/torchserve-cloud-run/Dockerfile\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "build",
   "metadata": {},
   "source": [
    "---\n",
    "## Build Container Image\n",
    "\n",
    "Use Cloud Build to build the Docker container and push to Artifact Registry.\n",
    "\n",
    "**How it works:**\n",
    "1. Cloud Build fetches the Dockerfile from GCS (uploaded in previous step)\n",
    "2. Builds the container image in Cloud Build's environment\n",
    "3. Pushes the image to Artifact Registry\n",
    "\n",
    "Cloud Build provides:\n",
    "- Scalable build infrastructure\n",
    "- Build caching for faster builds\n",
    "- Automatic vulnerability scanning\n",
    "- Build history and logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "build_c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building container image: us-central1-docker.pkg.dev/statmike-mlops-349915/frameworks/pytorch-autoencoder-torchserve\n",
      "This will take 3-5 minutes...\n",
      "============================================================\n",
      "Build submitted. View progress:\n",
      "  https://console.cloud.google.com/cloud-build/builds?project=statmike-mlops-349915\n",
      "\n",
      "============================================================\n",
      "âœ… Container built successfully!\n",
      "============================================================\n",
      "Image: us-central1-docker.pkg.dev/statmike-mlops-349915/frameworks/pytorch-autoencoder-torchserve\n",
      "\n",
      "View in Artifact Registry:\n",
      "  https://console.cloud.google.com/artifacts/docker/statmike-mlops-349915/us-central1/frameworks\n"
     ]
    }
   ],
   "source": [
    "# Configure Cloud Build to fetch Dockerfile from GCS and build container\n",
    "build = cloudbuild_v1.Build(\n",
    "    steps=[\n",
    "        # Step 1: Copy Dockerfile from GCS to build workspace\n",
    "        {\n",
    "            'name': 'gcr.io/cloud-builders/gsutil',\n",
    "            'args': ['cp', f'gs://{BUCKET_NAME}/{SERIES}/{EXPERIMENT}/torchserve-cloud-run/Dockerfile', '/workspace/Dockerfile']\n",
    "        },\n",
    "        # Step 2: Build Docker image\n",
    "        {\n",
    "            'name': 'gcr.io/cloud-builders/docker',\n",
    "            'args': ['build', '-t', IMAGE_NAME, '/workspace']\n",
    "        }\n",
    "    ],\n",
    "    images=[IMAGE_NAME]  # Push to Artifact Registry\n",
    ")\n",
    "\n",
    "print(f\"Building container image: {IMAGE_NAME}\")\n",
    "print(\"This will take 3-5 minutes...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Submit build and wait for completion\n",
    "operation = cb_client.create_build(project_id=PROJECT_ID, build=build)\n",
    "print(f\"Build submitted. View progress:\")\n",
    "print(f\"  https://console.cloud.google.com/cloud-build/builds?project={PROJECT_ID}\")\n",
    "\n",
    "build_response = operation.result()  # Wait for build\n",
    "\n",
    "if build_response.status == cloudbuild_v1.Build.Status.SUCCESS:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"âœ… Container built successfully!\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Image: {IMAGE_NAME}\")\n",
    "    print(f\"\\nView in Artifact Registry:\")\n",
    "    print(f\"  https://console.cloud.google.com/artifacts/docker/{PROJECT_ID}/{REGION}/{AR_REPO_NAME}\")\n",
    "else:\n",
    "    print(f\"\\nâŒ Build failed: {build_response.status.name}\")\n",
    "    print(f\"View logs: {build_response.log_url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deploy",
   "metadata": {},
   "source": [
    "---\n",
    "## Deploy to Cloud Run\n",
    "\n",
    "Deploy the containerized TorchServe application to Cloud Run.\n",
    "\n",
    "**Deployment Strategy**: This cell uses a delete-and-recreate pattern to ensure clean deployments:\n",
    "1. Deletes existing service (if any)\n",
    "2. Creates fresh service with new container image\n",
    "3. Configures public access via IAM policy\n",
    "\n",
    "**Why Delete First?** \n",
    "- Ensures new container is always used (no cached old revisions)\n",
    "- Simpler than managing revision traffic routing\n",
    "- Guarantees clean state on every deployment\n",
    "\n",
    "**Configuration:**\n",
    "- **CPU**: 2 vCPU for responsive inference\n",
    "- **Memory**: 2Gi for TorchServe and model\n",
    "- **Scaling**: 0-10 instances (scales to zero when idle)\n",
    "- **Port**: 8080 (Cloud Run standard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deploy_c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for existing Cloud Run service: pytorch-autoencoder-torchserve\n",
      "âœ… Found existing service\n",
      "   Deleting to ensure clean deployment...\n",
      "âœ… Service deleted\n",
      "\n",
      "ðŸ“¦ Creating Cloud Run service...\n",
      "   Region: us-central1 | CPU: 2 | Memory: 2Gi\n",
      "   Scaling: 0-10 instances\n",
      "============================================================\n",
      "Deploying... View progress:\n",
      "  https://console.cloud.google.com/run/detail/us-central1/pytorch-autoencoder-torchserve?project=statmike-mlops-349915\n"
     ]
    }
   ],
   "source": [
    "# Check for existing service and delete if found (ensures clean deployment)\n",
    "service_path = f\"projects/{PROJECT_ID}/locations/{REGION}/services/{SERVICE_NAME}\"\n",
    "\n",
    "print(f\"Checking for existing Cloud Run service: {SERVICE_NAME}\")\n",
    "try:\n",
    "    existing_service = run_client.get_service(name=service_path)\n",
    "    print(f\"âœ… Found existing service\")\n",
    "    print(f\"   Deleting to ensure clean deployment...\")\n",
    "    \n",
    "    delete_operation = run_client.delete_service(name=service_path)\n",
    "    delete_operation.result()  # Wait for deletion\n",
    "    print(\"âœ… Service deleted\")\n",
    "    \n",
    "except Exception as e:\n",
    "    if \"not found\" in str(e).lower() or \"404\" in str(e):\n",
    "        print(f\"ðŸ“¦ No existing service found\")\n",
    "\n",
    "# Configure Cloud Run service\n",
    "service_config = run_v2.Service(\n",
    "    template=run_v2.RevisionTemplate(\n",
    "        containers=[\n",
    "            run_v2.Container(\n",
    "                image=IMAGE_NAME,\n",
    "                ports=[run_v2.ContainerPort(container_port=PORT)],\n",
    "                resources=run_v2.ResourceRequirements(\n",
    "                    limits={\"cpu\": str(CPU), \"memory\": MEMORY}\n",
    "                )\n",
    "            )\n",
    "        ],\n",
    "        scaling=run_v2.RevisionScaling(\n",
    "            min_instance_count=MIN_INSTANCES,\n",
    "            max_instance_count=MAX_INSTANCES\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "print(f\"\\nðŸ“¦ Creating Cloud Run service...\")\n",
    "print(f\"   Region: {REGION} | CPU: {CPU} | Memory: {MEMORY}\")\n",
    "print(f\"   Scaling: {MIN_INSTANCES}-{MAX_INSTANCES} instances\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create service\n",
    "operation = run_client.create_service(\n",
    "    parent=f\"projects/{PROJECT_ID}/locations/{REGION}\",\n",
    "    service=service_config,\n",
    "    service_id=SERVICE_NAME\n",
    ")\n",
    "\n",
    "print(f\"Deploying... View progress:\")\n",
    "print(f\"  https://console.cloud.google.com/run/detail/{REGION}/{SERVICE_NAME}?project={PROJECT_ID}\")\n",
    "\n",
    "service = operation.result()  # Wait for deployment\n",
    "\n",
    "# Configure public access\n",
    "print(f\"\\nConfiguring public access...\")\n",
    "from google.iam.v1 import policy_pb2\n",
    "\n",
    "policy = run_client.get_iam_policy(request={'resource': service.name})\n",
    "has_all_users = any(\n",
    "    'allUsers' in binding.members \n",
    "    for binding in policy.bindings \n",
    "    if binding.role == 'roles/run.invoker'\n",
    ")\n",
    "\n",
    "if not has_all_users:\n",
    "    binding = policy_pb2.Binding(role='roles/run.invoker', members=['allUsers'])\n",
    "    policy.bindings.append(binding)\n",
    "    run_client.set_iam_policy(request={'resource': service.name, 'policy': policy})\n",
    "    print(\"âœ… Public access configured\")\n",
    "else:\n",
    "    print(\"âœ… Already publicly accessible\")\n",
    "\n",
    "SERVICE_URL = service.uri\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"âœ… Deployment complete!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Service URL: {SERVICE_URL}\")\n",
    "print(f\"Revision: {service.latest_ready_revision.split('/')[-1]}\")\n",
    "print(f\"\\nEndpoints:\")\n",
    "print(f\"  Inference: {SERVICE_URL}/predictions/{MODEL_NAME}\")\n",
    "print(f\"  Health: {SERVICE_URL}/ping\")\n",
    "print(f\"  Management: {SERVICE_URL}/models\")\n",
    "print(f\"\\nâš ï¸  Wait ~30 seconds for TorchServe to initialize before testing\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ln3cnes8h",
   "metadata": {},
   "source": [
    "**Tip:** View real-time logs in Cloud Console while testing:\n",
    "```\n",
    "https://console.cloud.google.com/run/detail/us-central1/pytorch-autoencoder-torchserve/logs\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "test",
   "metadata": {},
   "source": [
    "---\n",
    "## Test Deployment\n",
    "\n",
    "Send test predictions to the deployed Cloud Run service.\n",
    "\n",
    "**TorchServe API Format:**\n",
    "- **Input**: Raw JSON array of features (30 values)\n",
    "- **Not Vertex AI format**: Don't wrap in `{\"instances\": ...}`\n",
    "\n",
    "**Expected Response:**\n",
    "- Anomaly score (denormalized_MAE)\n",
    "- RMSE metric\n",
    "- Encoded latent representation (4D)\n",
    "- Plus 10 additional metrics\n",
    "\n",
    "**Note:** Wait ~30 seconds after deployment for TorchServe workers to initialize and load the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test_c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample test data (30 features)\n",
    "sample_transaction = np.random.randn(30).astype(np.float32)\n",
    "\n",
    "# Prepare request (TorchServe expects raw JSON array, not {\"instances\": ...})\n",
    "prediction_url = f\"{SERVICE_URL}/predictions/{MODEL_NAME}\"\n",
    "data = json.dumps(sample_transaction.tolist())\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "print(f\"Testing deployed service...\")\n",
    "print(f\"Endpoint: {prediction_url}\")\n",
    "print(f\"Input: 30 features\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Make prediction\n",
    "try:\n",
    "    response = requests.post(prediction_url, data=data, headers=headers, timeout=60)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        prediction = response.json()\n",
    "        print(\"\\nâœ… Prediction successful!\")\n",
    "        print(f\"\\nKey Results:\")\n",
    "        print(f\"  Anomaly Score (MAE): {prediction.get('denormalized_MAE', 'N/A'):.2f}\")\n",
    "        print(f\"  RMSE: {prediction.get('denormalized_RMSE', 'N/A'):.2f}\")\n",
    "        print(f\"  Encoded (4D): {prediction.get('encoded', 'N/A')}\")\n",
    "        \n",
    "        # Show available fields\n",
    "        other_fields = [k for k in prediction.keys() \n",
    "                       if k not in ['denormalized_MAE', 'denormalized_RMSE', 'encoded']]\n",
    "        if other_fields:\n",
    "            print(f\"\\n  Other fields ({len(other_fields)}): {', '.join(other_fields[:5])}\")\n",
    "            if len(other_fields) > 5:\n",
    "                print(f\"    ... and {len(other_fields) - 5} more\")\n",
    "    else:\n",
    "        print(f\"\\nâŒ Prediction failed!\")\n",
    "        print(f\"   Status: {response.status_code}\")\n",
    "        print(f\"   Response: {response.text[:200]}\")\n",
    "        \n",
    "except requests.exceptions.Timeout:\n",
    "    print(f\"\\nâš ï¸  Request timed out (60s)\")\n",
    "    print(\"   TorchServe may still be initializing. Wait and try again.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nâŒ Error: {e}\")\n",
    "\n",
    "# Test health endpoint\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Testing health endpoint...\")\n",
    "try:\n",
    "    health_response = requests.get(f\"{SERVICE_URL}/ping\", timeout=10)\n",
    "    if health_response.status_code == 200:\n",
    "        print(f\"âœ… Health check passed\")\n",
    "    else:\n",
    "        print(f\"âš ï¸  Health check returned: {health_response.status_code}\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Health check failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "monitor",
   "metadata": {},
   "source": [
    "---\n",
    "## Monitor Service\n",
    "\n",
    "Cloud Run provides integrated monitoring and logging. Access these tools to monitor your deployment:\n",
    "\n",
    "**Metrics Dashboard**: Request count, latency, errors, CPU/memory usage\n",
    "- View: Cloud Console > Cloud Run > [Service] > Metrics\n",
    "\n",
    "**Logs**: Container output, request logs, TorchServe worker logs  \n",
    "- View: Cloud Console > Cloud Run > [Service] > Logs\n",
    "\n",
    "**Service Details**: Configuration, revisions, traffic routing\n",
    "- View: `gcloud run services describe [service-name]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "monitor_c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick access to monitoring tools\n",
    "print(\"Cloud Console Links:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Metrics Dashboard:\")\n",
    "print(f\"  https://console.cloud.google.com/run/detail/{REGION}/{SERVICE_NAME}/metrics?project={PROJECT_ID}\")\n",
    "print(f\"\\nLogs Viewer:\")\n",
    "print(f\"  https://console.cloud.google.com/run/detail/{REGION}/{SERVICE_NAME}/logs?project={PROJECT_ID}\")\n",
    "print(f\"\\nService Details:\")\n",
    "print(f\"  https://console.cloud.google.com/run/detail/{REGION}/{SERVICE_NAME}?project={PROJECT_ID}\")\n",
    "\n",
    "# Get current service configuration\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Current Service Configuration:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "service = run_client.get_service(name=service_path)\n",
    "container = service.template.containers[0]\n",
    "\n",
    "print(f\"Service: {SERVICE_NAME}\")\n",
    "print(f\"Region: {REGION}\")\n",
    "print(f\"Image: {container.image.split('/')[-1]}\")\n",
    "print(f\"Resources: {container.resources.limits.get('cpu')} CPU, {container.resources.limits.get('memory')} memory\")\n",
    "print(f\"Scaling: {service.template.scaling.min_instance_count}-{service.template.scaling.max_instance_count} instances\")\n",
    "print(f\"URL: {service.uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scale",
   "metadata": {},
   "source": [
    "---\n",
    "## Auto-scaling Configuration\n",
    "\n",
    "Cloud Run automatically scales based on incoming requests.\n",
    "\n",
    "**Current Configuration:**\n",
    "- **Min instances**: 0 (scales to zero when idle - no compute costs!)\n",
    "- **Max instances**: 10 (controls maximum spend)\n",
    "- **Concurrency**: 160 requests per instance (default)\n",
    "\n",
    "**Scaling Behavior:**\n",
    "- **Cold Start**: First request after idle takes ~20-40 seconds (TorchServe initialization)\n",
    "- **Warm**: Subsequent requests are fast (~100ms)\n",
    "- **Scale Down**: Idle instances terminate after ~15 minutes\n",
    "- **Scale Up**: New instances spawn automatically based on load\n",
    "\n",
    "**Cost with Min=0**: Only pay when serving requests (~$0.10/hour when active)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scale_c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Update scaling configuration\n",
    "print(\"To update scaling configuration:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\"\"\n",
    "# Via gcloud CLI:\n",
    "gcloud run services update {SERVICE_NAME} \\\\\n",
    "    --region {REGION} \\\\\n",
    "    --min-instances 1 \\\\\n",
    "    --max-instances 20 \\\\\n",
    "    --concurrency 100\n",
    "\n",
    "# Via Python client:\n",
    "service_config = run_v2.Service(\n",
    "    name=service_path,\n",
    "    template=run_v2.RevisionTemplate(\n",
    "        scaling=run_v2.RevisionScaling(\n",
    "            min_instance_count=1,  # No cold starts\n",
    "            max_instance_count=20  # Higher ceiling\n",
    "        )\n",
    "    )\n",
    ")\n",
    "run_client.update_service(service=service_config)\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Scaling Recommendations:\")\n",
    "print(\"=\"*60)\n",
    "print(\"Development/Testing:\")\n",
    "print(\"  â€¢ min-instances=0, max-instances=5\")\n",
    "print(\"  â€¢ Minimize costs, tolerate cold starts\")\n",
    "print(\"\\nProduction (Latency-Sensitive):\")\n",
    "print(\"  â€¢ min-instances=1-2, max-instances=20+\")\n",
    "print(\"  â€¢ Always warm, handle traffic spikes\")\n",
    "print(\"\\nProduction (Cost-Optimized):\")\n",
    "print(\"  â€¢ min-instances=0, max-instances=50\")\n",
    "print(\"  â€¢ Scale to zero, high burst capacity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cleanup",
   "metadata": {},
   "source": [
    "---\n",
    "## Clean Up\n",
    "\n",
    "Remove Cloud Run resources when no longer needed.\n",
    "\n",
    "**Note:** With `min-instances=0`, idle services cost nothing. You may want to keep the service deployed for future use.\n",
    "\n",
    "**What to Delete:**\n",
    "1. **Cloud Run Service** - Stops serving and removes endpoint\n",
    "2. **Container Image** - Frees Artifact Registry storage (~$0.10/GB/month)\n",
    "3. **GCS Files** - Minimal storage costs (Dockerfile, .mar file)\n",
    "4. **Local Files** - Frees local disk space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cleanup_c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Cleanup Commands:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Cloud Run service deletion\n",
    "print(\"\\n1. Delete Cloud Run Service:\")\n",
    "print(f\"   # Python:\")\n",
    "print(f\"   run_client.delete_service(name='{service_path}')\")\n",
    "print(f\"   # gcloud:\")\n",
    "print(f\"   gcloud run services delete {SERVICE_NAME} --region {REGION} --quiet\")\n",
    "\n",
    "# Container image deletion\n",
    "print(f\"\\n2. Delete Container Image:\")\n",
    "print(f\"   # Console:\")\n",
    "print(f\"   https://console.cloud.google.com/artifacts/docker/{PROJECT_ID}/{REGION}/{AR_REPO_NAME}\")\n",
    "print(f\"   # gcloud:\")\n",
    "print(f\"   gcloud artifacts docker images delete {IMAGE_NAME} --quiet\")\n",
    "\n",
    "# GCS files deletion\n",
    "print(f\"\\n3. Delete GCS Files:\")\n",
    "print(f\"   gsutil -m rm -r {GCS_DIR}\")\n",
    "\n",
    "# Local files deletion\n",
    "print(f\"\\n4. Delete Local Files:\")\n",
    "print(f\"   rm -rf {WORK_DIR}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Cost Summary:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Active Service (min=0): $0 when idle, ~$0.10/hour when serving\")\n",
    "print(f\"Container Storage: ~$0.10/month (1GB image)\")\n",
    "print(f\"GCS Storage: <$0.01/month (small files)\")\n",
    "print(f\"\\nâœ… Recommended: Keep service deployed since idle cost is $0\")\n",
    "\n",
    "# Uncomment to actually delete:\n",
    "# run_client.delete_service(name=service_path)\n",
    "# print(\"âœ… Service deleted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "### What We Accomplished\n",
    "\n",
    "In this notebook, you learned how to:\n",
    "\n",
    "1. âœ“ Create a production-ready Dockerfile for TorchServe\n",
    "2. âœ“ Build container images using Cloud Build\n",
    "3. âœ“ Deploy TorchServe to Cloud Run with optimal configuration\n",
    "4. âœ“ Test the deployed service with real predictions\n",
    "5. âœ“ Monitor service performance using Cloud Logging and Monitoring\n",
    "6. âœ“ Configure auto-scaling for cost optimization\n",
    "7. âœ“ Understand scale-to-zero benefits\n",
    "\n",
    "### Key Benefits of Cloud Run\n",
    "\n",
    "- **Serverless**: No infrastructure management\n",
    "- **Auto-scaling**: Handles traffic spikes automatically\n",
    "- **Cost-effective**: Pay only for request processing time\n",
    "- **Production-ready**: Built-in load balancing, SSL, and health checks\n",
    "- **Fast deployment**: Update services in seconds\n",
    "\n",
    "### When to Use Cloud Run\n",
    "\n",
    "Cloud Run is ideal for:\n",
    "- **Intermittent workloads**: Scale to zero when idle\n",
    "- **Variable traffic**: Automatic scaling handles spikes\n",
    "- **Rapid iteration**: Quick deployment and updates\n",
    "- **Cost-sensitive applications**: Pay-per-use pricing\n",
    "\n",
    "### Limitations to Consider\n",
    "\n",
    "- **Request timeout**: Max 60 minutes (3600 seconds)\n",
    "- **Memory**: Max 32GB per instance\n",
    "- **Cold starts**: First request may be slower (consider min-instances=1)\n",
    "- **No GPU support**: For GPU inference, use Vertex AI or GKE\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "**Production Hardening**:\n",
    "1. Add authentication (remove `--allow-unauthenticated`)\n",
    "2. Implement request validation and error handling\n",
    "3. Set up Cloud Armor for DDoS protection\n",
    "4. Configure custom domains and SSL certificates\n",
    "5. Implement rate limiting and quotas\n",
    "\n",
    "**Monitoring and Observability**:\n",
    "1. Set up Cloud Monitoring alerts for errors and latency\n",
    "2. Create custom dashboards for model performance metrics\n",
    "3. Enable request/response logging for debugging\n",
    "4. Implement distributed tracing with Cloud Trace\n",
    "\n",
    "**Performance Optimization**:\n",
    "1. Optimize Dockerfile for smaller image size\n",
    "2. Implement model caching to reduce cold start times\n",
    "3. Tune concurrency and instance settings based on load testing\n",
    "4. Consider using min-instances=1 for latency-sensitive applications\n",
    "\n",
    "**Alternative Deployment Options**:\n",
    "- **Vertex AI Endpoints**: For managed ML serving with GPU support\n",
    "- **GKE**: For more control over Kubernetes infrastructure\n",
    "- **Cloud Functions**: For simpler, lighter-weight model serving\n",
    "\n",
    "### Resources\n",
    "\n",
    "- [Cloud Run Documentation](https://cloud.google.com/run/docs)\n",
    "- [TorchServe Documentation](https://pytorch.org/serve/)\n",
    "- [Cloud Run Best Practices](https://cloud.google.com/run/docs/tips)\n",
    "- [Cloud Run Pricing](https://cloud.google.com/run/pricing)\n",
    "\n",
    "### Questions?\n",
    "\n",
    "For issues or questions:\n",
    "- Check Cloud Run logs in Cloud Console\n",
    "- Review TorchServe logs for model-specific errors\n",
    "- Consult Google Cloud documentation and support"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (frameworks-pytorch)",
   "language": "python",
   "name": "frameworks-pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
