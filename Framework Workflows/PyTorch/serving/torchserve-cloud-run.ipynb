{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69b7ced9",
   "metadata": {},
   "source": [
    "![tracker](https://us-central1-vertex-ai-mlops-369716.cloudfunctions.net/pixel-tracking?path=statmike%2Fvertex-ai-mlops%2FFramework+Workflows%2FPyTorch%2Fserving&file=torchserve-cloud-run.ipynb)\n",
    "<!--- header table --->\n",
    "<table align=\"left\">\n",
    "<tr>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%20Workflows/PyTorch/serving/torchserve-cloud-run.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.svgrepo.com/download/217753/github.svg\" alt=\"GitHub logo\">\n",
    "      <br>View on<br>GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://colab.research.google.com/github/statmike/vertex-ai-mlops/blob/main/Framework%20Workflows/PyTorch/serving/torchserve-cloud-run.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.gstatic.com/pantheon/images/bigquery/welcome_page/colab-logo.svg\" alt=\"Google Colaboratory logo\">\n",
    "      <br>Run in<br>Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https%3A%2F%2Fraw.githubusercontent.com%2Fstatmike%2Fvertex-ai-mlops%2Fmain%2FFramework%2520Workflows%2FPyTorch%2Fserving%2Ftorchserve-cloud-run.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\">\n",
    "      <br>Run in<br>Colab Enterprise\n",
    "    </a>\n",
    "  </td>      \n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/bigquery/import?url=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%20Workflows/PyTorch/serving/torchserve-cloud-run.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.gstatic.com/images/branding/gcpiconscolors/bigquery/v1/32px.svg\" alt=\"BigQuery logo\">\n",
    "      <br>Open in<br>BigQuery Studio\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/statmike/vertex-ai-mlops/main/Framework%20Workflows/PyTorch/serving/torchserve-cloud-run.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.gstatic.com/images/branding/gcpiconscolors/vertexai/v1/32px.svg\" alt=\"Vertex AI logo\">\n",
    "      <br>Open in<br>Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td colspan=\"5\" style=\"text-align: right\">\n",
    "    <b>Share This On: </b> \n",
    "    <a href=\"https://www.linkedin.com/sharing/share-offsite/?url=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%2520Workflows/PyTorch/serving/torchserve-cloud-run.ipynb\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"Linkedin Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://reddit.com/submit?url=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%2520Workflows/PyTorch/serving/torchserve-cloud-run.ipynb\"><img src=\"https://redditinc.com/hubfs/Reddit%20Inc/Brand/Reddit_Logo.png\" alt=\"Reddit Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://bsky.app/intent/compose?text=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%2520Workflows/PyTorch/serving/torchserve-cloud-run.ipynb\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"BlueSky Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://twitter.com/intent/tweet?url=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%2520Workflows/PyTorch/serving/torchserve-cloud-run.ipynb\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/5/5a/X_icon_2.svg\" alt=\"X (Twitter) Logo\" width=\"20px\"></a> \n",
    "  </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td colspan=\"5\" style=\"text-align: right\">\n",
    "    <b>Connect With Author On: </b> \n",
    "    <a href=\"https://www.linkedin.com/in/statmike\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"Linkedin Logo\" width=\"20px\"></a>\n",
    "    <a href=\"https://www.github.com/statmike\"><img src=\"https://www.svgrepo.com/download/217753/github.svg\" alt=\"GitHub Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://www.youtube.com/@statmike-channel\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/f/fd/YouTube_full-color_icon_%282024%29.svg\" alt=\"YouTube Logo\" width=\"20px\"></a>\n",
    "    <a href=\"https://bsky.app/profile/statmike.bsky.social\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"BlueSky Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://x.com/statmike\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/5/5a/X_icon_2.svg\" alt=\"X (Twitter) Logo\" width=\"20px\"></a>\n",
    "  </td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# TorchServe Deployment on Cloud Run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overview",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This notebook demonstrates how to containerize TorchServe and deploy it to **Cloud Run**, Google Cloud's fully managed serverless platform for containers.\n",
    "\n",
    "### What is Cloud Run?\n",
    "\n",
    "Cloud Run is a serverless compute platform that automatically scales your containerized applications. Key features include:\n",
    "- **Serverless**: No infrastructure management required\n",
    "- **Auto-scaling**: Automatically scales from 0 to N based on traffic\n",
    "- **Pay-per-use**: Only pay for actual request processing time\n",
    "- **Fully managed**: Handles load balancing, health checks, and SSL certificates\n",
    "- **Fast deployment**: Deploy containers in seconds\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "Before running this notebook, you should have:\n",
    "- Completed `../pytorch-autoencoder.ipynb` to create and export the model\n",
    "- A `.mar` (Model Archive) file stored in Google Cloud Storage\n",
    "- Google Cloud SDK installed and configured\n",
    "- Appropriate IAM permissions for Cloud Run and Artifact Registry\n",
    "\n",
    "### Benefits of Cloud Run for ML Model Serving\n",
    "\n",
    "Compared to local TorchServe deployment:\n",
    "- **Production-ready**: Built-in load balancing, health checks, and HTTPS\n",
    "- **Cost-effective**: Scales to zero when not in use (no idle costs)\n",
    "- **Global availability**: Deploy to regions worldwide\n",
    "- **Easy updates**: Deploy new versions with zero downtime\n",
    "- **Integrated monitoring**: Cloud Logging and Cloud Monitoring built-in\n",
    "\n",
    "### Architecture\n",
    "\n",
    "```\n",
    "┌─────────────────┐\n",
    "│  Model Archive  │\n",
    "│  (GCS Bucket)   │\n",
    "└────────┬────────┘\n",
    "         │\n",
    "         ▼\n",
    "┌─────────────────┐\n",
    "│   Dockerfile    │──────┐\n",
    "│  (TorchServe)   │      │\n",
    "└─────────────────┘      │\n",
    "                         ▼\n",
    "                  ┌──────────────┐\n",
    "                  │ Cloud Build  │\n",
    "                  └──────┬───────┘\n",
    "                         │\n",
    "                         ▼\n",
    "                  ┌──────────────┐\n",
    "                  │   Artifact   │\n",
    "                  │   Registry   │\n",
    "                  └──────┬───────┘\n",
    "                         │\n",
    "                         ▼\n",
    "                  ┌──────────────┐\n",
    "                  │  Cloud Run   │\n",
    "                  │   Service    │\n",
    "                  └──────┬───────┘\n",
    "                         │\n",
    "                         ▼\n",
    "                  ┌──────────────┐\n",
    "                  │    HTTPS     │\n",
    "                  │   Endpoint   │\n",
    "                  └──────────────┘\n",
    "```\n",
    "\n",
    "### What You'll Learn\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "1. Create a Dockerfile for TorchServe compatible with Cloud Run\n",
    "2. Build and push container images using Cloud Build\n",
    "3. Deploy TorchServe to Cloud Run with proper configuration\n",
    "4. Test the deployed service with sample predictions\n",
    "5. Monitor service performance and logs\n",
    "6. Configure auto-scaling parameters\n",
    "7. Manage costs by scaling to zero during idle periods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "env",
   "metadata": {},
   "source": [
    "---\n",
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proj",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = !gcloud config get-value project\n",
    "PROJECT_ID = PROJECT_ID[0]\n",
    "print(f\"Project ID: {PROJECT_ID}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "REGION = \"us-central1\"\n",
    "BUCKET_NAME = f\"{PROJECT_ID}-pytorch-models\"\n",
    "SERVICE_NAME = \"pytorch-autoencoder\"\n",
    "IMAGE_NAME = f\"gcr.io/{PROJECT_ID}/{SERVICE_NAME}\"\n",
    "\n",
    "print(f\"Region: {REGION}\")\n",
    "print(f\"Bucket: {BUCKET_NAME}\")\n",
    "print(f\"Service Name: {SERVICE_NAME}\")\n",
    "print(f\"Image Name: {IMAGE_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable required APIs\n",
    "!gcloud services enable run.googleapis.com\n",
    "!gcloud services enable cloudbuild.googleapis.com\n",
    "!gcloud services enable artifactregistry.googleapis.com\n",
    "\n",
    "# Set default region\n",
    "!gcloud config set run/region {REGION}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "py",
   "metadata": {},
   "source": [
    "---\n",
    "## Python Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imp",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import requests\n",
    "import numpy as np\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vars",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runtime variables\n",
    "WORK_DIR = Path.cwd()\n",
    "MAR_FILE = \"autoencoder.mar\"\n",
    "GCS_MAR_PATH = f\"gs://{BUCKET_NAME}/models/{MAR_FILE}\"\n",
    "\n",
    "# Cloud Run configuration\n",
    "MEMORY = \"2Gi\"\n",
    "CPU = \"2\"\n",
    "PORT = 8080\n",
    "MAX_INSTANCES = 10\n",
    "MIN_INSTANCES = 0  # Scale to zero when idle\n",
    "\n",
    "print(f\"Working Directory: {WORK_DIR}\")\n",
    "print(f\"Model Archive: {GCS_MAR_PATH}\")\n",
    "print(f\"Cloud Run Config: {CPU} CPU, {MEMORY} memory, port {PORT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dockerfile",
   "metadata": {},
   "source": [
    "---\n",
    "## Create Dockerfile\n",
    "\n",
    "This Dockerfile creates a container image with TorchServe configured for Cloud Run. Key considerations:\n",
    "\n",
    "- **Base Image**: Using `python:3.9-slim` for a lightweight container\n",
    "- **Port 8080**: Cloud Run requires services to listen on the port specified by the PORT environment variable (default 8080)\n",
    "- **Model Loading**: Downloads the `.mar` file from GCS during build\n",
    "- **TorchServe Config**: Configured to start automatically and accept requests on port 8080"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dockerfile_c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dockerfile_content = f\"\"\"FROM python:3.9-slim\n",
    "\n",
    "# Install system dependencies\n",
    "RUN apt-get update && apt-get install -y \\\\\n",
    "    curl \\\\\n",
    "    && rm -rf /var/lib/apt/lists/*\n",
    "\n",
    "# Install TorchServe and dependencies\n",
    "RUN pip install --no-cache-dir \\\\\n",
    "    torchserve \\\\\n",
    "    torch-model-archiver \\\\\n",
    "    torch \\\\\n",
    "    torchvision \\\\\n",
    "    google-cloud-storage\n",
    "\n",
    "# Create directories for TorchServe\n",
    "RUN mkdir -p /home/model-server/model-store \\\\\n",
    "    /home/model-server/tmp\n",
    "\n",
    "# Set working directory\n",
    "WORKDIR /home/model-server\n",
    "\n",
    "# Copy model archive from GCS\n",
    "# Note: In production, consider copying from GCS at runtime or using a startup script\n",
    "RUN pip install gcsfs && \\\\\n",
    "    python -c \"import gcsfs; fs = gcsfs.GCSFileSystem(); fs.get('{GCS_MAR_PATH.replace('gs://', '')}', '/home/model-server/model-store/{MAR_FILE}')\"\n",
    "\n",
    "# Create TorchServe configuration\n",
    "RUN echo \"inference_address=http://0.0.0.0:8080\" > config.properties && \\\\\n",
    "    echo \"management_address=http://0.0.0.0:8081\" >> config.properties && \\\\\n",
    "    echo \"model_store=/home/model-server/model-store\" >> config.properties && \\\\\n",
    "    echo \"load_models=autoencoder.mar\" >> config.properties\n",
    "\n",
    "# Expose the inference port (Cloud Run standard)\n",
    "EXPOSE 8080\n",
    "\n",
    "# Start TorchServe\n",
    "CMD [\"torchserve\", \\\\\n",
    "     \"--start\", \\\\\n",
    "     \"--ts-config\", \"config.properties\", \\\\\n",
    "     \"--models\", \"autoencoder={MAR_FILE}\", \\\\\n",
    "     \"--foreground\"]\n",
    "\"\"\"\n",
    "\n",
    "# Write Dockerfile\n",
    "with open(WORK_DIR / \"Dockerfile\", \"w\") as f:\n",
    "    f.write(dockerfile_content)\n",
    "\n",
    "print(\"Dockerfile created successfully!\")\n",
    "print(f\"\\nLocation: {WORK_DIR / 'Dockerfile'}\")\n",
    "print(\"\\nContents:\")\n",
    "print(dockerfile_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "build",
   "metadata": {},
   "source": [
    "### Build Container\n",
    "\n",
    "Using Cloud Build to build the container image and push it to Google Container Registry (GCR). Cloud Build provides:\n",
    "- Scalable build infrastructure\n",
    "- Build caching for faster builds\n",
    "- Automatic vulnerability scanning\n",
    "- Build history and logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "build_c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and push container image using Cloud Build\n",
    "!gcloud builds submit --tag {IMAGE_NAME} {WORK_DIR}\n",
    "\n",
    "print(f\"\\nContainer image built and pushed: {IMAGE_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deploy",
   "metadata": {},
   "source": [
    "---\n",
    "## Deploy to Cloud Run\n",
    "\n",
    "Deploy the containerized TorchServe application to Cloud Run with production-ready configuration:\n",
    "\n",
    "**Key Parameters**:\n",
    "- `--image`: Container image from GCR\n",
    "- `--platform managed`: Fully managed Cloud Run (serverless)\n",
    "- `--allow-unauthenticated`: Public endpoint (change for production)\n",
    "- `--memory`: Memory allocation per instance\n",
    "- `--cpu`: CPU allocation per instance\n",
    "- `--port`: Port the container listens on\n",
    "- `--max-instances`: Maximum number of instances to scale to\n",
    "- `--min-instances`: Minimum instances (0 = scale to zero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deploy_c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy to Cloud Run\n",
    "!gcloud run deploy {SERVICE_NAME} \\\n",
    "    --image {IMAGE_NAME} \\\n",
    "    --region {REGION} \\\n",
    "    --platform managed \\\n",
    "    --allow-unauthenticated \\\n",
    "    --memory {MEMORY} \\\n",
    "    --cpu {CPU} \\\n",
    "    --port {PORT} \\\n",
    "    --max-instances {MAX_INSTANCES} \\\n",
    "    --min-instances {MIN_INSTANCES}\n",
    "\n",
    "# Get the service URL\n",
    "service_url = !gcloud run services describe {SERVICE_NAME} --region {REGION} --format='value(status.url)'\n",
    "SERVICE_URL = service_url[0]\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Service deployed successfully!\")\n",
    "print(f\"Service URL: {SERVICE_URL}\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "test",
   "metadata": {},
   "source": [
    "### Test Deployment\n",
    "\n",
    "Send test predictions to the deployed Cloud Run service. The service exposes TorchServe's inference API at the `/predictions/{model_name}` endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test_c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample test data\n",
    "test_data = np.random.randn(1, 784).astype(np.float32).tolist()\n",
    "\n",
    "# Prepare request\n",
    "prediction_url = f\"{SERVICE_URL}/predictions/autoencoder\"\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "payload = {\"instances\": test_data}\n",
    "\n",
    "print(f\"Sending prediction request to: {prediction_url}\")\n",
    "print(f\"Input shape: {np.array(test_data).shape}\")\n",
    "\n",
    "# Make prediction\n",
    "try:\n",
    "    response = requests.post(\n",
    "        prediction_url,\n",
    "        headers=headers,\n",
    "        json=payload,\n",
    "        timeout=60\n",
    "    )\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        result = response.json()\n",
    "        print(f\"\\n✓ Prediction successful!\")\n",
    "        print(f\"Response: {result}\")\n",
    "        print(f\"Output shape: {np.array(result).shape if isinstance(result, list) else 'N/A'}\")\n",
    "    else:\n",
    "        print(f\"\\n✗ Prediction failed!\")\n",
    "        print(f\"Status code: {response.status_code}\")\n",
    "        print(f\"Response: {response.text}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n✗ Error making prediction: {e}\")\n",
    "\n",
    "# Test health endpoint\n",
    "health_url = f\"{SERVICE_URL}/ping\"\n",
    "try:\n",
    "    health_response = requests.get(health_url, timeout=10)\n",
    "    print(f\"\\nHealth check: {health_response.status_code}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nHealth check failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "monitor",
   "metadata": {},
   "source": [
    "### Monitor Service\n",
    "\n",
    "Cloud Run provides integrated monitoring through Cloud Logging and Cloud Monitoring. You can view:\n",
    "- Request logs\n",
    "- Container logs\n",
    "- Metrics (requests, latency, errors)\n",
    "- Resource utilization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "monitor_c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View recent logs\n",
    "print(\"Recent Cloud Run logs:\")\n",
    "print(\"=\"*60)\n",
    "!gcloud logging read \"resource.type=cloud_run_revision AND resource.labels.service_name={SERVICE_NAME}\" \\\n",
    "    --limit 10 \\\n",
    "    --format=\"table(timestamp, severity, textPayload)\"\n",
    "\n",
    "# Get service details\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Service Details:\")\n",
    "print(\"=\"*60)\n",
    "!gcloud run services describe {SERVICE_NAME} --region {REGION}\n",
    "\n",
    "# Console links\n",
    "console_url = f\"https://console.cloud.google.com/run/detail/{REGION}/{SERVICE_NAME}/metrics?project={PROJECT_ID}\"\n",
    "logs_url = f\"https://console.cloud.google.com/run/detail/{REGION}/{SERVICE_NAME}/logs?project={PROJECT_ID}\"\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"View in Cloud Console:\")\n",
    "print(f\"Metrics: {console_url}\")\n",
    "print(f\"Logs: {logs_url}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scale",
   "metadata": {},
   "source": [
    "### Auto-scaling Configuration\n",
    "\n",
    "Cloud Run automatically scales your service based on incoming requests. Key scaling features:\n",
    "\n",
    "- **Scale to Zero**: When idle, scales down to 0 instances (no costs)\n",
    "- **Auto-scale Up**: Automatically creates instances based on request load\n",
    "- **Concurrency**: Number of requests per instance (default: 80)\n",
    "- **Min/Max Instances**: Control scaling boundaries\n",
    "\n",
    "**Cost Optimization**: With `min-instances=0`, you only pay when the service is processing requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scale_c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update scaling configuration\n",
    "print(\"Current scaling configuration:\")\n",
    "!gcloud run services describe {SERVICE_NAME} \\\n",
    "    --region {REGION} \\\n",
    "    --format=\"value(spec.template.metadata.annotations)\"\n",
    "\n",
    "# Example: Update to allow higher concurrency for better cost efficiency\n",
    "# Uncomment to apply:\n",
    "# !gcloud run services update {SERVICE_NAME} \\\n",
    "#     --region {REGION} \\\n",
    "#     --concurrency 100 \\\n",
    "#     --min-instances 0 \\\n",
    "#     --max-instances 20\n",
    "\n",
    "print(f\"\\nScaling Tips:\")\n",
    "print(\"- Set min-instances=0 to scale to zero and minimize costs\")\n",
    "print(\"- Set min-instances=1+ to reduce cold start latency\")\n",
    "print(\"- Increase concurrency for CPU-bound workloads\")\n",
    "print(\"- Set max-instances to control costs and quotas\")\n",
    "print(f\"\\nCurrent config: min={MIN_INSTANCES}, max={MAX_INSTANCES}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cleanup",
   "metadata": {},
   "source": [
    "### Clean Up\n",
    "\n",
    "Remove the Cloud Run service to stop incurring charges. Note: With scale-to-zero configuration, idle services don't incur compute costs, but you may want to clean up for organization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cleanup_c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to delete the Cloud Run service\n",
    "# !gcloud run services delete {SERVICE_NAME} --region {REGION} --quiet\n",
    "\n",
    "# Uncomment to delete the container image\n",
    "# !gcloud container images delete {IMAGE_NAME} --quiet\n",
    "\n",
    "print(\"To clean up resources, uncomment the commands above.\")\n",
    "print(f\"\\nService: {SERVICE_NAME}\")\n",
    "print(f\"Image: {IMAGE_NAME}\")\n",
    "print(f\"\\nNote: With min-instances=0, idle services don't incur compute costs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "### What We Accomplished\n",
    "\n",
    "In this notebook, you learned how to:\n",
    "\n",
    "1. ✓ Create a production-ready Dockerfile for TorchServe\n",
    "2. ✓ Build container images using Cloud Build\n",
    "3. ✓ Deploy TorchServe to Cloud Run with optimal configuration\n",
    "4. ✓ Test the deployed service with real predictions\n",
    "5. ✓ Monitor service performance using Cloud Logging and Monitoring\n",
    "6. ✓ Configure auto-scaling for cost optimization\n",
    "7. ✓ Understand scale-to-zero benefits\n",
    "\n",
    "### Key Benefits of Cloud Run\n",
    "\n",
    "- **Serverless**: No infrastructure management\n",
    "- **Auto-scaling**: Handles traffic spikes automatically\n",
    "- **Cost-effective**: Pay only for request processing time\n",
    "- **Production-ready**: Built-in load balancing, SSL, and health checks\n",
    "- **Fast deployment**: Update services in seconds\n",
    "\n",
    "### When to Use Cloud Run\n",
    "\n",
    "Cloud Run is ideal for:\n",
    "- **Intermittent workloads**: Scale to zero when idle\n",
    "- **Variable traffic**: Automatic scaling handles spikes\n",
    "- **Rapid iteration**: Quick deployment and updates\n",
    "- **Cost-sensitive applications**: Pay-per-use pricing\n",
    "\n",
    "### Limitations to Consider\n",
    "\n",
    "- **Request timeout**: Max 60 minutes (3600 seconds)\n",
    "- **Memory**: Max 32GB per instance\n",
    "- **Cold starts**: First request may be slower (consider min-instances=1)\n",
    "- **No GPU support**: For GPU inference, use Vertex AI or GKE\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "**Production Hardening**:\n",
    "1. Add authentication (remove `--allow-unauthenticated`)\n",
    "2. Implement request validation and error handling\n",
    "3. Set up Cloud Armor for DDoS protection\n",
    "4. Configure custom domains and SSL certificates\n",
    "5. Implement rate limiting and quotas\n",
    "\n",
    "**Monitoring and Observability**:\n",
    "1. Set up Cloud Monitoring alerts for errors and latency\n",
    "2. Create custom dashboards for model performance metrics\n",
    "3. Enable request/response logging for debugging\n",
    "4. Implement distributed tracing with Cloud Trace\n",
    "\n",
    "**Performance Optimization**:\n",
    "1. Optimize Dockerfile for smaller image size\n",
    "2. Implement model caching to reduce cold start times\n",
    "3. Tune concurrency and instance settings based on load testing\n",
    "4. Consider using min-instances=1 for latency-sensitive applications\n",
    "\n",
    "**Alternative Deployment Options**:\n",
    "- **Vertex AI Endpoints**: For managed ML serving with GPU support\n",
    "- **GKE**: For more control over Kubernetes infrastructure\n",
    "- **Cloud Functions**: For simpler, lighter-weight model serving\n",
    "\n",
    "### Resources\n",
    "\n",
    "- [Cloud Run Documentation](https://cloud.google.com/run/docs)\n",
    "- [TorchServe Documentation](https://pytorch.org/serve/)\n",
    "- [Cloud Run Best Practices](https://cloud.google.com/run/docs/tips)\n",
    "- [Cloud Run Pricing](https://cloud.google.com/run/pricing)\n",
    "\n",
    "### Questions?\n",
    "\n",
    "For issues or questions:\n",
    "- Check Cloud Run logs in Cloud Console\n",
    "- Review TorchServe logs for model-specific errors\n",
    "- Consult Google Cloud documentation and support"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
