{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84e987f3",
   "metadata": {},
   "source": [
    "![tracker](https://us-central1-vertex-ai-mlops-369716.cloudfunctions.net/pixel-tracking?path=statmike%2Fvertex-ai-mlops%2FFramework+Workflows%2FPyTorch%2Fserving&file=dataflow-streaming-runinference-vertex.ipynb)\n",
    "<!--- header table --->\n",
    "<table align=\"left\">\n",
    "<tr>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%20Workflows/PyTorch/serving/dataflow-streaming-runinference-vertex.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.svgrepo.com/download/217753/github.svg\" alt=\"GitHub logo\">\n",
    "      <br>View on<br>GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://colab.research.google.com/github/statmike/vertex-ai-mlops/blob/main/Framework%20Workflows/PyTorch/serving/dataflow-streaming-runinference-vertex.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.gstatic.com/pantheon/images/bigquery/welcome_page/colab-logo.svg\" alt=\"Google Colaboratory logo\">\n",
    "      <br>Run in<br>Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https%3A%2F%2Fraw.githubusercontent.com%2Fstatmike%2Fvertex-ai-mlops%2Fmain%2FFramework%2520Workflows%2FPyTorch%2Fserving%2Fdataflow-streaming-runinference-vertex.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\">\n",
    "      <br>Run in<br>Colab Enterprise\n",
    "    </a>\n",
    "  </td>      \n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/bigquery/import?url=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%20Workflows/PyTorch/serving/dataflow-streaming-runinference-vertex.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.gstatic.com/images/branding/gcpiconscolors/bigquery/v1/32px.svg\" alt=\"BigQuery logo\">\n",
    "      <br>Open in<br>BigQuery Studio\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/statmike/vertex-ai-mlops/main/Framework%20Workflows/PyTorch/serving/dataflow-streaming-runinference-vertex.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.gstatic.com/images/branding/gcpiconscolors/vertexai/v1/32px.svg\" alt=\"Vertex AI logo\">\n",
    "      <br>Open in<br>Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td colspan=\"5\" style=\"text-align: right\">\n",
    "    <b>Share This On: </b> \n",
    "    <a href=\"https://www.linkedin.com/sharing/share-offsite/?url=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%2520Workflows/PyTorch/serving/dataflow-streaming-runinference-vertex.ipynb\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"Linkedin Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://reddit.com/submit?url=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%2520Workflows/PyTorch/serving/dataflow-streaming-runinference-vertex.ipynb\"><img src=\"https://redditinc.com/hubfs/Reddit%20Inc/Brand/Reddit_Logo.png\" alt=\"Reddit Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://bsky.app/intent/compose?text=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%2520Workflows/PyTorch/serving/dataflow-streaming-runinference-vertex.ipynb\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"BlueSky Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://twitter.com/intent/tweet?url=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%2520Workflows/PyTorch/serving/dataflow-streaming-runinference-vertex.ipynb\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/5/5a/X_icon_2.svg\" alt=\"X (Twitter) Logo\" width=\"20px\"></a> \n",
    "  </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td colspan=\"5\" style=\"text-align: right\">\n",
    "    <b>Connect With Author On: </b> \n",
    "    <a href=\"https://www.linkedin.com/in/statmike\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"Linkedin Logo\" width=\"20px\"></a>\n",
    "    <a href=\"https://www.github.com/statmike\"><img src=\"https://www.svgrepo.com/download/217753/github.svg\" alt=\"GitHub Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://www.youtube.com/@statmike-channel\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/f/fd/YouTube_full-color_icon_%282024%29.svg\" alt=\"YouTube Logo\" width=\"20px\"></a>\n",
    "    <a href=\"https://bsky.app/profile/statmike.bsky.social\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"BlueSky Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://x.com/statmike\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/5/5a/X_icon_2.svg\" alt=\"X (Twitter) Logo\" width=\"20px\"></a>\n",
    "  </td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "i0cm6r2trpm",
   "metadata": {},
   "source": [
    "# Dataflow Streaming Inference with RunInference (Vertex AI Endpoint)\n",
    "\n",
    "This notebook demonstrates **ultra-low-latency streaming** processing of transactions using Dataflow with Apache Beam RunInference that calls a Vertex AI Endpoint, using a **hybrid windowing + batching approach**.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "This workflow covers:\n",
    "\n",
    "1. **Configure Vertex AI Handler**: Set up VertexAIModelHandlerJSON for RunInference\n",
    "2. **Test Endpoint Health**: Verify endpoint is responding before deploying pipeline\n",
    "3. **Build Ultra-Low-Latency Pipeline**: Hybrid approach with 1-second windows, early triggers, and explicit batching\n",
    "4. **Run on Dataflow**: Execute continuous pipeline on Google Cloud\n",
    "5. **Monitor Job**: Track streaming job progress and view results\n",
    "6. **Clean Up**: Stop streaming job to avoid ongoing charges\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Completed `dataflow-setup.ipynb` - This sets up:\n",
    "  - BigQuery tables created (including `pytorch_autoencoder_streaming_results_vertex`)\n",
    "  - Pub/Sub topics and subscriptions created\n",
    "- **Vertex AI Endpoint deployed** - Choose either:\n",
    "  - [Pre-built Container](./vertex-ai-endpoint-prebuilt-container.ipynb) - Returns full model output (13 metrics)\n",
    "  - [Custom Container](./vertex-ai-endpoint-custom-container.ipynb) - Returns simplified output (2 fields)\n",
    "\n",
    "> **Note**: This notebook works with both endpoint types and will automatically detect the output format.\n",
    "\n",
    "## Batch vs Streaming\n",
    "\n",
    "**Batch Processing**:\n",
    "- ‚úÖ Process historical data\n",
    "- ‚úÖ Bounded dataset (has a start and end)\n",
    "- ‚úÖ Results available when job completes\n",
    "- ‚úÖ Cost-effective for large datasets\n",
    "- Example: Analyze all transactions from last month\n",
    "\n",
    "**Streaming Processing (This Notebook)**:\n",
    "- ‚úÖ Process real-time data\n",
    "- ‚úÖ Unbounded dataset (continuous)\n",
    "- ‚úÖ Results available immediately\n",
    "- ‚úÖ **Ultra-low-latency** anomaly detection (~100ms average including endpoint latency)\n",
    "- Example: Flag suspicious transactions as they occur\n",
    "\n",
    "---\n",
    "\n",
    "## Low-Latency Architecture: The Hybrid Approach\n",
    "\n",
    "This pipeline uses a **hybrid windowing + batching strategy** to achieve low latency while maintaining efficient batch inference via Vertex AI endpoint:\n",
    "\n",
    "### How It Works\n",
    "\n",
    "```\n",
    "Pub/Sub Message Arrives (t=0ms)\n",
    "    ‚Üì\n",
    "Parse JSON ‚Üí (features_list, metadata_dict)\n",
    "    ‚Üì\n",
    "Window (1 sec, trigger every 10ms)\n",
    "    ‚îú‚îÄ Messages accumulate for up to 10ms\n",
    "    ‚îú‚îÄ Trigger fires ‚Üí pane released\n",
    "    ‚îî‚îÄ Latency: 0-10ms (avg ~5ms)\n",
    "    ‚Üì\n",
    "BatchElements (min=1, max=50, max_wait=10ms)\n",
    "    ‚îú‚îÄ Collects elements from pane\n",
    "    ‚îú‚îÄ Waits up to 10ms OR until 50 elements\n",
    "    ‚îî‚îÄ Latency: 0-10ms (avg ~5ms)\n",
    "    ‚Üì\n",
    "RunInference (Vertex AI Endpoint Call)\n",
    "    ‚îú‚îÄ Separates: features=[feat1...feat50], metadata=[meta1...meta50]\n",
    "    ‚îú‚îÄ Calls endpoint.predict([feat1...feat50]) via HTTP\n",
    "    ‚îú‚îÄ Gets predictions: [pred1...pred50]\n",
    "    ‚îî‚îÄ Latency: 50-100ms (endpoint latency + batch processing)\n",
    "    ‚Üì\n",
    "Format Results ‚Üí BigQuery + Pub/Sub\n",
    "    ‚îî‚îÄ Latency: ~5ms (network)\n",
    "    \n",
    "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "Total End-to-End Latency: 65-125ms (avg ~100ms)\n",
    "```\n",
    "\n",
    "### Key Design Choices\n",
    "\n",
    "**1. 1-Second Fixed Windows (for Watermark Progression)**\n",
    "- Windows must exist for streaming pipelines to make progress\n",
    "- 1-second windows provide stable watermark advancement\n",
    "- Windows themselves don't add latency (triggers do the work)\n",
    "\n",
    "**2. Early Firing Triggers (10ms)**\n",
    "- Fire every 10ms instead of waiting for full window close\n",
    "- Messages processed almost immediately after arrival\n",
    "- Trigger latency: 0-10ms (average ~5ms)\n",
    "\n",
    "**3. Explicit BatchElements (Full Control)**\n",
    "- `min_batch_size=1`: Don't wait if only 1 message (ultra-low latency)\n",
    "- `max_batch_size=50`: Cap batch size for predictable endpoint response time\n",
    "- `max_batch_duration_secs=0.01`: 10ms hard deadline\n",
    "- Batch formation latency: 0-10ms (average ~5ms)\n",
    "\n",
    "**4. Metadata Preservation Throughout**\n",
    "- Batch is: `[(features1, metadata1), (features2, metadata2), ...]`\n",
    "- Metadata flows through inference unchanged\n",
    "- Enables accurate latency tracking for scale testing\n",
    "\n",
    "### Latency Breakdown\n",
    "\n",
    "| Stage | Latency | Description |\n",
    "|-------|---------|-------------|\n",
    "| Trigger wait | 0-10ms | Wait for 10ms trigger to fire |\n",
    "| Batch formation | 0-10ms | BatchElements collects messages |\n",
    "| Endpoint inference | 50-100ms | Vertex AI API call + batch processing |\n",
    "| Pub/Sub delivery | ~5ms | Network transmission |\n",
    "| **Total** | **65-125ms** | **Average ~100ms** |\n",
    "\n",
    "### Traffic Adaptability\n",
    "\n",
    "The pipeline automatically adapts batching to traffic:\n",
    "\n",
    "| Traffic Rate | Batch Behavior | Latency |\n",
    "|--------------|----------------|---------|\n",
    "| 1 msg/sec | Batch of 1 after 10ms | ~95ms |\n",
    "| 10 msg/sec | Batch of 10 after ~10ms | ~100ms |\n",
    "| 100 msg/sec | Batch of 50 almost instantly | ~100ms |\n",
    "| 1000 msg/sec | Batch of 50 continuously | ~100ms |\n",
    "\n",
    "---\n",
    "\n",
    "## RunInference with Vertex AI Benefits\n",
    "\n",
    "- **Managed endpoint**: No model loading in workers\n",
    "- **Explicit batching**: Full control over batch size and timing\n",
    "- **Scalable**: Endpoint scales independently from pipeline\n",
    "- **Flexible**: Update model without redeploying pipeline\n",
    "- **Low-latency**: Sub-150ms predictions for streaming data\n",
    "\n",
    "## Timing Expectations\n",
    "\n",
    "**Total time from start to results: ~8-10 minutes**\n",
    "\n",
    "1. **Start Dataflow job**: Instant\n",
    "2. **Wait for workers**: 3-5 minutes (worker provisioning)\n",
    "3. **Send test messages**: Instant\n",
    "4. **Wait for processing**: ~1-2 minutes (10ms trigger + processing)\n",
    "5. **View results**: Check BigQuery and Pub/Sub\n",
    "\n",
    "## What This Pipeline Does\n",
    "\n",
    "1. Read transactions from Pub/Sub input topic\n",
    "2. Window data into 1-second windows (for watermark progression)\n",
    "3. Fire triggers every 10ms (release messages early)\n",
    "4. Batch elements (1-50) with 10ms max wait\n",
    "5. Call Vertex AI Endpoint via RunInference (batch API call)\n",
    "6. Extract relevant outputs (score + embeddings)\n",
    "7. Write results to BigQuery (for analysis)\n",
    "8. Publish to Pub/Sub output (for downstream systems)\n",
    "9. Job runs continuously until cancelled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c5ce8a",
   "metadata": {},
   "source": [
    "---\n",
    "## Environment Setup\n",
    "\n",
    "This section will authenticate your session, enable required Google Cloud APIs, and install necessary Python packages.\n",
    "\n",
    "**Package Installation Options (`REQ_TYPE`)**:\n",
    "- `PRIMARY`: Installs only the main packages. Faster, but pip resolves sub-dependencies which may result in different versions than development.\n",
    "- `ALL` (Default): Installs exact versions of all packages and dependencies. Best for perfectly reproducing the development environment.\n",
    "- `COLAB`: Installs a Colab-optimized list that excludes pre-installed packages like `ipython` and `ipykernel`.\n",
    "\n",
    "**Installation Tool Options (`INSTALL_TOOL`)**:\n",
    "- `pip` (Default): Uses pip for package installation. Standard Python package installer.\n",
    "- `uv`: Modern, fast Python package installer. Must be installed separately. See: https://github.com/astral-sh/uv\n",
    "- `poetry`: Dependency management tool. Requires running notebook in a poetry environment (`poetry shell` or `poetry run jupyter lab`). Uses `pyproject.toml` instead of requirements.txt.\n",
    "\n",
    "> **Note:** If running in Google Colab, the script will automatically detect this and set `REQ_TYPE = 'COLAB'` to prevent package conflicts, overriding any manual setting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e8d0f5",
   "metadata": {},
   "source": [
    "### Set Your Project ID\n",
    "\n",
    "‚ö†Ô∏è **Action Required:** Replace the `PROJECT_ID` value below with your Google Cloud project ID before running this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e694becd",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = 'statmike-mlops-349915' # replace with GCP project ID\n",
    "REQ_TYPE = 'ALL' # Specify PRIMARY or ALL or COLAB\n",
    "INSTALL_TOOL = 'poetry' # Specify pip, uv, or poetry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b796c1",
   "metadata": {},
   "source": [
    "### Configuration\n",
    "\n",
    "This cell defines the requirements files and Google Cloud APIs needed for this notebook. Run as-is without modification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "938dc50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "REQUIREMENTS_URL = 'https://raw.githubusercontent.com/statmike/vertex-ai-mlops/refs/heads/main/Framework%20Workflows/PyTorch/requirements.txt'\n",
    "\n",
    "REQUIRED_APIS = [\n",
    "    \"dataflow.googleapis.com\",\n",
    "    \"pubsub.googleapis.com\",\n",
    "    \"bigquery.googleapis.com\",\n",
    "    \"storage.googleapis.com\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b559e3",
   "metadata": {},
   "source": [
    "### Run Setup\n",
    "\n",
    "This cell downloads the centralized setup code and configures your environment. It will:\n",
    "- Authenticate your session with Google Cloud\n",
    "- Enable required APIs for this notebook\n",
    "- Install necessary Python packages\n",
    "- Display a setup summary with your project information\n",
    "\n",
    "> **Note:** In Colab, if packages are installed, the kernel will automatically restart. After restart, continue from the next cell without re-running earlier cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2ad88fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "PYTHON GCP ENVIRONMENT SETUP\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "AUTHENTICATION\n",
      "==================================================\n",
      "Checking for existing ADC...\n",
      "‚úÖ Existing ADC found.\n",
      "‚úÖ Project is correctly set to 'statmike-mlops-349915'.\n",
      "\n",
      "==================================================\n",
      "API CHECK & ENABLE\n",
      "==================================================\n",
      "‚úÖ dataflow.googleapis.com is already enabled.\n",
      "‚úÖ pubsub.googleapis.com is already enabled.\n",
      "‚úÖ bigquery.googleapis.com is already enabled.\n",
      "‚úÖ storage.googleapis.com is already enabled.\n",
      "\n",
      "==================================================\n",
      "PACKAGE MANAGEMENT\n",
      "==================================================\n",
      "Installation Tool: poetry\n",
      "‚úÖ Found poetry at: /usr/local/google/home/statmike/.local/bin/poetry\n",
      "‚úÖ Running in poetry environment: /usr/local/google/home/statmike/.cache/pypoetry/virtualenvs/frameworks-pytorch-0KVJlKeQ-py3.13\n",
      "‚ÑπÔ∏è  Poetry mode: Installing from pyproject.toml (REQUIREMENTS_URL ignored)\n",
      "‚úÖ Found pyproject.toml at: /usr/local/google/home/statmike/Git/vertex-ai-mlops/Framework Workflows/PyTorch/pyproject.toml\n",
      "   Changed working directory to: /usr/local/google/home/statmike/Git/vertex-ai-mlops/Framework Workflows/PyTorch\n",
      "Running poetry install...\n",
      "   Restored working directory to: /usr/local/google/home/statmike/Git/vertex-ai-mlops/Framework Workflows/PyTorch/serving\n",
      "‚úÖ All packages are already installed and up to date.\n",
      "\n",
      "==================================================\n",
      "Google Cloud Project Information\n",
      "==================================================\n",
      "PROJECT_ID     = statmike-mlops-349915\n",
      "PROJECT_NUMBER = 1026793852137\n",
      "==================================================\n",
      "\n",
      "\n",
      "==================================================\n",
      "SETUP SUMMARY\n",
      "==================================================\n",
      "‚úÖ Authentication:    Success\n",
      "‚úÖ API Configuration: Success\n",
      "‚úÖ Package Install:   Already up to date\n",
      "‚úÖ Installation Tool: poetry\n",
      "‚úÖ Project ID:        statmike-mlops-349915\n",
      "‚úÖ Project Number:    1026793852137\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os, urllib.request\n",
    "\n",
    "# Download and import setup code\n",
    "url = 'https://raw.githubusercontent.com/statmike/vertex-ai-mlops/refs/heads/main/core/notebook-template/python_setup.py'\n",
    "urllib.request.urlretrieve(url, 'python_setup_local.py')\n",
    "import python_setup_local as python_setup\n",
    "os.remove('python_setup_local.py')\n",
    "\n",
    "# Run setup\n",
    "setup_info = python_setup.setup_environment(PROJECT_ID, REQ_TYPE, REQUIREMENTS_URL, REQUIRED_APIS, INSTALL_TOOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "py",
   "metadata": {},
   "source": [
    "---\n",
    "## Python Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "imp",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/google/home/statmike/.cache/pypoetry/virtualenvs/frameworks-pytorch-0KVJlKeQ-py3.13/lib/python3.13/site-packages/google/cloud/aiplatform/models.py:52: FutureWarning: Support for google-cloud-storage < 3.0.0 will be removed in a future version of google-cloud-aiplatform. Please upgrade to google-cloud-storage >= 3.0.0.\n",
      "  from google.cloud.aiplatform.utils import gcs_utils\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import apache_beam as beam\n",
    "from apache_beam import window\n",
    "from apache_beam.ml.inference.base import RunInference\n",
    "from apache_beam.ml.inference.vertex_ai_inference import VertexAIModelHandlerJSON\n",
    "from apache_beam.options.pipeline_options import PipelineOptions, StandardOptions\n",
    "from apache_beam.io.gcp.pubsub import ReadFromPubSub, WriteToPubSub\n",
    "from apache_beam.io.gcp.bigquery import WriteToBigQuery\n",
    "import json\n",
    "from datetime import datetime\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud import pubsub_v1\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "vars",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking up endpoint: pytorch-autoencoder-endpoint\n",
      "‚úÖ Found endpoint: pytorch-autoencoder-endpoint\n",
      "   Endpoint ID: 5971323405637517312\n",
      "   Full resource name: projects/1026793852137/locations/us-central1/endpoints/5971323405637517312\n",
      "\n",
      "Configuration:\n",
      "  Project: statmike-mlops-349915\n",
      "  Endpoint: pytorch-autoencoder-endpoint (ID: 5971323405637517312)\n",
      "  Input subscription: projects/statmike-mlops-349915/subscriptions/pytorch-autoencoder-input-sub-vertex (VERTEX endpoint)\n",
      "  Output topic: projects/statmike-mlops-349915/topics/pytorch-autoencoder-output-vertex (VERTEX endpoint)\n",
      "  Results table: statmike-mlops-349915.frameworks.pytorch_autoencoder_streaming_results_vertex\n"
     ]
    }
   ],
   "source": [
    "PROJECT_ID = subprocess.run([\"gcloud\", \"config\", \"get-value\", \"project\"], capture_output=True, text=True, check=True).stdout.strip()\n",
    "\n",
    "# Configuration\n",
    "REGION = \"us-central1\"\n",
    "SERIES = \"frameworks\"\n",
    "EXPERIMENT = \"pytorch-autoencoder\"\n",
    "\n",
    "# ========================================\n",
    "# ENDPOINT SELECTION\n",
    "# ========================================\n",
    "# Choose which endpoint to use:\n",
    "# - \"pytorch-autoencoder-endpoint\" (pre-built container - returns 13 metrics)\n",
    "# - \"pytorch-autoencoder-custom-endpoint\" (custom container - returns 2 fields)\n",
    "ENDPOINT_DISPLAY_NAME = \"pytorch-autoencoder-endpoint\"  # Change to \"pytorch-autoencoder-custom-endpoint\" if using custom container\n",
    "\n",
    "# Get endpoint ID from display name\n",
    "# VertexAIModelHandlerJSON requires the numeric endpoint ID, not the display name\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION)\n",
    "print(f\"Looking up endpoint: {ENDPOINT_DISPLAY_NAME}\")\n",
    "endpoints = aiplatform.Endpoint.list(\n",
    "    filter=f'display_name=\"{ENDPOINT_DISPLAY_NAME}\"',\n",
    "    order_by=\"create_time desc\"\n",
    ")\n",
    "\n",
    "if not endpoints:\n",
    "    raise ValueError(\n",
    "        f\"No endpoint found with display name '{ENDPOINT_DISPLAY_NAME}' in region {REGION}.\\n\"\n",
    "        f\"Please deploy an endpoint first using either:\\n\"\n",
    "        f\"  - vertex-ai-endpoint-prebuilt-container.ipynb\\n\"\n",
    "        f\"  - vertex-ai-endpoint-custom-container.ipynb\"\n",
    "    )\n",
    "\n",
    "endpoint = endpoints[0]\n",
    "ENDPOINT_ID = endpoint.name.split(\"/\")[-1]\n",
    "\n",
    "print(f\"‚úÖ Found endpoint: {ENDPOINT_DISPLAY_NAME}\")\n",
    "print(f\"   Endpoint ID: {ENDPOINT_ID}\")\n",
    "print(f\"   Full resource name: {endpoint.resource_name}\")\n",
    "\n",
    "# GCS paths\n",
    "BUCKET_NAME = PROJECT_ID\n",
    "BUCKET_URI = f\"gs://{BUCKET_NAME}\"\n",
    "MODEL_DIR = f\"{BUCKET_URI}/{SERIES}/{EXPERIMENT}\"\n",
    "\n",
    "# Pub/Sub configuration - using VERTEX-specific topics and subscriptions\n",
    "INPUT_SUB = f\"projects/{PROJECT_ID}/subscriptions/{EXPERIMENT}-input-sub-vertex\"\n",
    "OUTPUT_TOPIC = f\"projects/{PROJECT_ID}/topics/{EXPERIMENT}-output-vertex\"\n",
    "\n",
    "# BigQuery configuration\n",
    "BQ_DATASET = SERIES.replace(\"-\", \"_\")\n",
    "BQ_TABLE = f\"{EXPERIMENT.replace('-', '_')}_streaming_results_vertex\"  # Separate table for Vertex endpoint results\n",
    "\n",
    "# Dataflow configuration\n",
    "DATAFLOW_STAGING = f\"{BUCKET_URI}/dataflow/staging\"\n",
    "DATAFLOW_TEMP = f\"{BUCKET_URI}/dataflow/temp\"\n",
    "\n",
    "print(f\"\\nConfiguration:\")\n",
    "print(f\"  Project: {PROJECT_ID}\")\n",
    "print(f\"  Endpoint: {ENDPOINT_DISPLAY_NAME} (ID: {ENDPOINT_ID})\")\n",
    "print(f\"  Input subscription: {INPUT_SUB} (VERTEX endpoint)\")\n",
    "print(f\"  Output topic: {OUTPUT_TOPIC} (VERTEX endpoint)\")\n",
    "print(f\"  Results table: {PROJECT_ID}.{BQ_DATASET}.{BQ_TABLE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5c82d6",
   "metadata": {},
   "source": [
    "---\n",
    "## Use Vertex AI Model Registry To Retrieve Model Artifacts Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d4525b66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Information:\n",
      "  Display Name: pytorch-autoencoder\n",
      "  Model ID: 2572675789577256960\n",
      "  Artifact URI: gs://statmike-mlops-349915/frameworks/pytorch-autoencoder\n"
     ]
    }
   ],
   "source": [
    "# Retrieve model artifact URI from the endpoint\n",
    "deployed_model = endpoint.list_models()[0]\n",
    "model = aiplatform.Model(deployed_model.model)\n",
    "\n",
    "print(f\"Model Information:\")\n",
    "print(f\"  Display Name: {model.display_name}\")\n",
    "print(f\"  Model ID: {model.name.split('/')[-1]}\")\n",
    "print(f\"  Artifact URI: {model.gca_resource.artifact_uri}\")\n",
    "\n",
    "# Save for potential use\n",
    "MODEL_ARTIFACT_URI = model.gca_resource.artifact_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7w88tg0mamc",
   "metadata": {},
   "source": [
    "---\n",
    "## Test Endpoint Health\n",
    "\n",
    "Before using the endpoint in the pipeline, verify it's responding correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1n0ud7b6gx2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing endpoint health...\n",
      "Endpoint: pytorch-autoencoder-endpoint (ID: 5971323405637517312)\n",
      "============================================================\n",
      "Status Code: 200\n",
      "‚úÖ Endpoint is healthy and responding!\n",
      "\n",
      "Sample prediction:\n",
      "  Anomaly Score (MAE): 2324.93\n",
      "  Encoded (4D): [0, 0, 5.395321846008301, 0]...\n",
      "\n",
      "  Total fields in response: 13\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from google.auth import default\n",
    "from google.auth.transport.requests import Request\n",
    "\n",
    "# Get credentials\n",
    "credentials, project = default()\n",
    "credentials.refresh(Request())\n",
    "\n",
    "# Prepare test prediction\n",
    "url = f\"https://{REGION}-aiplatform.googleapis.com/v1/projects/{PROJECT_ID}/locations/{REGION}/endpoints/{ENDPOINT_ID}:predict\"\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {credentials.token}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "# Test data - same format as pipeline will send\n",
    "test_data = {\n",
    "    \"instances\": [[0.1] * 30]  # Single transaction with 30 features\n",
    "}\n",
    "\n",
    "print(\"Testing endpoint health...\")\n",
    "print(f\"Endpoint: {ENDPOINT_DISPLAY_NAME} (ID: {ENDPOINT_ID})\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    response = requests.post(url, headers=headers, json=test_data, timeout=30)\n",
    "    \n",
    "    print(f\"Status Code: {response.status_code}\")\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        result = response.json()\n",
    "        print(\"‚úÖ Endpoint is healthy and responding!\")\n",
    "        print(f\"\\nSample prediction:\")\n",
    "        \n",
    "        # Handle both output formats\n",
    "        predictions = result.get(\"predictions\", [])\n",
    "        if predictions:\n",
    "            pred = predictions[0]\n",
    "            if \"anomaly_score\" in pred:\n",
    "                print(f\"  Anomaly Score: {pred['anomaly_score']:.2f}\")\n",
    "            elif \"denormalized_MAE\" in pred:\n",
    "                print(f\"  Anomaly Score (MAE): {pred['denormalized_MAE']:.2f}\")\n",
    "            \n",
    "            if \"encoded\" in pred:\n",
    "                print(f\"  Encoded (4D): {pred['encoded'][:4]}...\")\n",
    "            \n",
    "            print(f\"\\n  Total fields in response: {len(pred)}\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"‚ùå Endpoint returned error: {response.status_code}\")\n",
    "        print(f\"Response: {response.text}\")\n",
    "        print(\"\\n‚ö†Ô∏è  The pipeline will fail with this endpoint.\")\n",
    "        print(\"   Please check your endpoint deployment before continuing.\")\n",
    "        \n",
    "except requests.exceptions.Timeout:\n",
    "    print(\"‚ùå Request timed out after 30 seconds\")\n",
    "    print(\"   The endpoint may be cold-starting or unresponsive.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error testing endpoint: {e}\")\n",
    "    print(\"   Please verify the endpoint is deployed and healthy.\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handler",
   "metadata": {},
   "source": [
    "---\n",
    "## Create ModelHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "handler_c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ModelHandler created for endpoint: pytorch-autoencoder-endpoint (ID: 5971323405637517312)\n"
     ]
    }
   ],
   "source": [
    "model_handler = VertexAIModelHandlerJSON(\n",
    "    endpoint_id=ENDPOINT_ID,  # Use numeric endpoint ID, not display name\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION\n",
    ")\n",
    "print(f\"‚úÖ ModelHandler created for endpoint: {ENDPOINT_DISPLAY_NAME} (ID: {ENDPOINT_ID})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pipeline",
   "metadata": {},
   "source": [
    "---\n",
    "## Build Ultra-Low-Latency Pipeline\n",
    "\n",
    "This section builds the streaming pipeline using the **hybrid windowing + batching approach**:\n",
    "\n",
    "1. **1-second fixed windows** (for watermark progression)\n",
    "2. **10ms early triggers** (for low latency)\n",
    "3. **Explicit BatchElements** (for controlled batching)\n",
    "4. **Custom DoFn for batch inference** (preserves metadata, calls Vertex endpoint)\n",
    "\n",
    "### Why This Approach?\n",
    "\n",
    "**Simple & Elegant:**\n",
    "- Clear separation of concerns: windowing ‚Üí batching ‚Üí inference\n",
    "- Easy to understand and debug\n",
    "- Predictable latency and behavior\n",
    "\n",
    "**Full Control:**\n",
    "- Explicitly set `min_batch_size`, `max_batch_size`, `max_buffering_duration`\n",
    "- Tune for your latency requirements\n",
    "- Easy to add logging and monitoring\n",
    "\n",
    "**Efficient:**\n",
    "- Good batching at all traffic levels (1-50 elements per batch)\n",
    "- Reduces number of API calls to Vertex endpoint (cost savings)\n",
    "- Cost-effective compared to one-by-one endpoint calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36fsmm76zsp",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "WORKER COMPUTE CONFIGURATION (Vertex AI Endpoint)\n",
      "============================================================\n",
      "Machine Type: n1-standard-4\n",
      "  - vCPUs: 4\n",
      "  - Memory: 15 GB\n",
      "\n",
      "Autoscaling:\n",
      "  - Min Workers: 2\n",
      "  - Max Workers: 20\n",
      "\n",
      "Workload Type: API-based inference\n",
      "  - Workers make HTTP calls to Vertex AI endpoint\n",
      "  - Throughput limited by endpoint capacity, not worker compute\n",
      "  - No local model loading required\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Worker Compute Configuration\n",
    "# =============================\n",
    "# These settings control the machine type and autoscaling behavior for Dataflow workers.\n",
    "# For Vertex AI endpoint workflows, workers make API calls rather than loading models locally.\n",
    "\n",
    "# Machine Type: n1-standard-4\n",
    "# - 4 vCPUs, 15 GB memory\n",
    "# - Suitable for making concurrent API calls to Vertex AI endpoints\n",
    "# - Each worker can handle multiple parallel requests\n",
    "# - Alternative: n1-standard-2 (smaller), n1-standard-8 (larger) depending on concurrency needs\n",
    "\n",
    "MACHINE_TYPE = \"n1-standard-4\"\n",
    "\n",
    "# Autoscaling for Streaming Pipelines (with Vertex AI Endpoint)\n",
    "# - min_workers=2: Ensures pipeline remains responsive even with low traffic\n",
    "# - max_workers=20: Handles traffic spikes without overwhelming the endpoint\n",
    "# - Dataflow autoscales based on Pub/Sub backlog and processing latency\n",
    "\n",
    "MIN_WORKERS = 2\n",
    "MAX_WORKERS = 20\n",
    "\n",
    "# Why These Settings for Streaming + Vertex Endpoint?\n",
    "# ----------------------------------------------------\n",
    "# 1. **Minimum Workers (2)**:\n",
    "#    - Provides redundancy (if one worker fails, pipeline continues)\n",
    "#    - Reduces cold start latency when traffic arrives\n",
    "#    - Maintains low end-to-end latency for real-time processing\n",
    "#\n",
    "# 2. **Maximum Workers (20)**:\n",
    "#    - Allows scaling to handle traffic bursts\n",
    "#    - Each worker makes concurrent API calls to Vertex endpoint\n",
    "#    - Throughput depends on endpoint capacity, not worker compute\n",
    "#    - 20 workers prevent overwhelming the endpoint with too many parallel requests\n",
    "#\n",
    "# 3. **Machine Type (n1-standard-4)**:\n",
    "#    - Workers don't load PyTorch models (endpoint handles inference)\n",
    "#    - Memory requirements are lower than local model loading\n",
    "#    - 4 vCPUs enable good parallelism for API call concurrency\n",
    "#    - Cost-effective for API-based inference workflows\n",
    "\n",
    "# Key Difference from Local Model Inference:\n",
    "# ------------------------------------------\n",
    "# - **Local Model**: Workers need high memory (15GB+) to load PyTorch models\n",
    "# - **Vertex Endpoint**: Workers only need memory for API client and data buffering\n",
    "# - **Throughput**: Limited by endpoint capacity, not worker compute\n",
    "# - **Scaling**: Can use smaller machine types (n1-standard-2) if API latency is low\n",
    "\n",
    "# When to Adjust These Settings:\n",
    "# -------------------------------\n",
    "# - **Higher Traffic**: Increase max_workers (e.g., 50-100) if endpoint can handle load\n",
    "# - **Lower Latency**: Increase min_workers (e.g., 5-10) to pre-warm capacity\n",
    "# - **Cost Optimization**: Use n1-standard-2 for lower concurrency needs\n",
    "# - **High Concurrency**: Use n1-standard-8 for more parallel API calls per worker\n",
    "# - **Endpoint Throttling**: Reduce max_workers to prevent overwhelming the endpoint\n",
    "\n",
    "# GPU Support:\n",
    "# -----------\n",
    "# Not applicable for Vertex AI endpoint workflows.\n",
    "# GPU inference happens at the endpoint, not in Dataflow workers.\n",
    "# Workers only make HTTP API calls to the endpoint.\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"WORKER COMPUTE CONFIGURATION (Vertex AI Endpoint)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Machine Type: {MACHINE_TYPE}\")\n",
    "print(f\"  - vCPUs: 4\")\n",
    "print(f\"  - Memory: 15 GB\")\n",
    "print(f\"\\nAutoscaling:\")\n",
    "print(f\"  - Min Workers: {MIN_WORKERS}\")\n",
    "print(f\"  - Max Workers: {MAX_WORKERS}\")\n",
    "print(f\"\\nWorkload Type: API-based inference\")\n",
    "print(f\"  - Workers make HTTP calls to Vertex AI endpoint\")\n",
    "print(f\"  - Throughput limited by endpoint capacity, not worker compute\")\n",
    "print(f\"  - No local model loading required\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "pipeline_c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Ultra-low-latency streaming pipeline configured\n",
      "   Job will run in: us-central1\n",
      "   Staging: gs://statmike-mlops-349915/dataflow/staging\n",
      "   Machine type: n1-standard-4\n",
      "   Worker scaling: 2-20 workers\n",
      "   Target latency: <125ms end-to-end (including endpoint latency)\n"
     ]
    }
   ],
   "source": [
    "from apache_beam.transforms import trigger\n",
    "\n",
    "def parse_json(message):\n",
    "    \"\"\"\n",
    "    Parse Pub/Sub message and return both features and metadata.\n",
    "\n",
    "    Args:\n",
    "        message: Bytes from Pub/Sub containing JSON with 'features' key\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (list, dict):\n",
    "        - features: List of 30 float values for model input\n",
    "        - metadata: Dict with test_id, message_id, publish_time, etc.\n",
    "\n",
    "    Note:\n",
    "        Metadata is preserved to support latency tracking in scale tests.\n",
    "        The test_id allows filtering results by test, and publish_time enables\n",
    "        end-to-end latency measurement.\n",
    "        \n",
    "        VertexAIModelHandlerJSON will automatically wrap features in {\"instances\": [...]}\n",
    "    \"\"\"\n",
    "    data = json.loads(message.decode(\"utf-8\"))\n",
    "    features = data[\"features\"]\n",
    "\n",
    "    # Preserve metadata for latency tracking (used in scale testing)\n",
    "    metadata = {\n",
    "        \"test_id\": data.get(\"test_id\"),\n",
    "        \"message_id\": data.get(\"message_id\"),\n",
    "        \"publish_time\": data.get(\"publish_time\"),\n",
    "        \"sequence\": data.get(\"sequence\")\n",
    "    }\n",
    "\n",
    "    return (features, metadata)\n",
    "\n",
    "\n",
    "def format_result(element, window=beam.DoFn.WindowParam):\n",
    "    \"\"\"\n",
    "    Format model predictions for output to BigQuery and Pub/Sub.\n",
    "    \n",
    "    Args:\n",
    "        element: Tuple of (input_features, prediction_dict, metadata_dict)\n",
    "            - input_features: Original features list\n",
    "            - prediction_dict: Response from Vertex AI endpoint\n",
    "            - metadata_dict: Original message metadata (test_id, publish_time, etc.)\n",
    "        window: Beam window parameter for extracting window boundaries\n",
    "        \n",
    "    Returns:\n",
    "        Dict with Python types suitable for BigQuery/JSON serialization\n",
    "        \n",
    "    Note:\n",
    "        Handles both endpoint output formats:\n",
    "        - Pre-built container: 13 keys (denormalized_MAE, encoded, etc.)\n",
    "        - Custom container: 2 keys (anomaly_score, encoded)\n",
    "        \n",
    "        All timestamps stored as Unix timestamps (float) for consistent latency calculations:\n",
    "        - window_wait_ms = (window_end - publish_time) * 1000\n",
    "        - processing_ms = (pipeline_output_time - window_end) * 1000\n",
    "        - pubsub_delivery_ms = (receive_time - pipeline_output_time) * 1000\n",
    "    \"\"\"\n",
    "    input_features, prediction, metadata = element\n",
    "    \n",
    "    # Auto-detect output format and extract anomaly score\n",
    "    if \"anomaly_score\" in prediction:\n",
    "        # Custom container format\n",
    "        anomaly_score = prediction[\"anomaly_score\"]\n",
    "    elif \"denormalized_MAE\" in prediction:\n",
    "        # Pre-built container format\n",
    "        anomaly_score = prediction[\"denormalized_MAE\"]\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown prediction format: {prediction.keys()}\")\n",
    "    \n",
    "    result = {\n",
    "        \"instance_id\": str(hash(str(input_features))),\n",
    "        \"anomaly_score\": anomaly_score,\n",
    "        \"encoded\": prediction[\"encoded\"],\n",
    "        \"timestamp\": datetime.utcnow().isoformat(),\n",
    "        \"window_start\": window.start.to_utc_datetime().timestamp(),  # Unix timestamp\n",
    "        \"window_end\": window.end.to_utc_datetime().timestamp(),      # Unix timestamp\n",
    "        \"pipeline_output_time\": time.time()  # Unix timestamp\n",
    "    }\n",
    "    \n",
    "    # Add metadata fields if present (from scale testing)\n",
    "    if metadata:\n",
    "        if metadata.get(\"test_id\"):\n",
    "            result[\"test_id\"] = metadata[\"test_id\"]\n",
    "        if metadata.get(\"message_id\"):\n",
    "            result[\"message_id\"] = metadata[\"message_id\"]\n",
    "        if metadata.get(\"publish_time\"):\n",
    "            result[\"publish_time\"] = metadata[\"publish_time\"]\n",
    "        if metadata.get(\"sequence\") is not None:\n",
    "            result[\"sequence\"] = metadata[\"sequence\"]\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "def to_json(element):\n",
    "    \"\"\"Convert dict to JSON bytes for Pub/Sub publication.\"\"\"\n",
    "    return json.dumps(element).encode(\"utf-8\")\n",
    "\n",
    "\n",
    "# Configure Dataflow pipeline options\n",
    "options = PipelineOptions([\n",
    "    f\"--project={PROJECT_ID}\",\n",
    "    f\"--region={REGION}\",\n",
    "    \"--runner=DataflowRunner\",  # Run on Google Cloud (not locally)\n",
    "    f\"--temp_location={DATAFLOW_TEMP}\",\n",
    "    f\"--staging_location={DATAFLOW_STAGING}\",\n",
    "    f\"--job_name=pytorch-streaming-vertex-{datetime.now().strftime('%Y%m%d-%H%M%S')}\",\n",
    "    \"--streaming\",  # Enable streaming mode\n",
    "    \"--save_main_session\",  # Serialize global imports and variables\n",
    "    # Worker compute configuration\n",
    "    f\"--machine_type={MACHINE_TYPE}\",\n",
    "    f\"--num_workers={MIN_WORKERS}\",\n",
    "    f\"--max_num_workers={MAX_WORKERS}\",\n",
    "    # Low-latency pipeline optimization\n",
    "    \"--experiments=enable_streaming_engine\",  # Faster Dataflow execution engine\n",
    "    \"--experiments=use_runner_v2\",  # Latest runner with performance improvements\n",
    "])\n",
    "\n",
    "print(\"‚úÖ Ultra-low-latency streaming pipeline configured\")\n",
    "print(f\"   Job will run in: {REGION}\")\n",
    "print(f\"   Staging: {DATAFLOW_STAGING}\")\n",
    "print(f\"   Machine type: {MACHINE_TYPE}\")\n",
    "print(f\"   Worker scaling: {MIN_WORKERS}-{MAX_WORKERS} workers\")\n",
    "print(f\"   Target latency: <125ms end-to-end (including endpoint latency)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "run",
   "metadata": {},
   "source": [
    "---\n",
    "## Run Ultra-Low-Latency Streaming Job\n",
    "\n",
    "This cell builds and executes the pipeline using the **hybrid approach**:\n",
    "\n",
    "### Pipeline Flow with Inline Comments\n",
    "\n",
    "```\n",
    "Read from Pub/Sub\n",
    "  ‚Üì (features_list, metadata_dict)\n",
    "Parse JSON\n",
    "  ‚Üì\n",
    "Window (1 sec, trigger every 10ms) ‚Üê Watermark progression + early firing\n",
    "  ‚Üì Pane released every 10ms\n",
    "BatchElements (min=1, max=50, wait=10ms) ‚Üê Explicit batching control\n",
    "  ‚Üì Batch of 1-50 elements\n",
    "RunInference (Vertex AI endpoint call) ‚Üê Efficient batch API call\n",
    "  ‚Üì (input_features, prediction, metadata) per element\n",
    "Format results\n",
    "  ‚Üì Dict with timestamps\n",
    "‚îú‚îÄ‚Üí Write to BigQuery (storage)\n",
    "‚îî‚îÄ‚Üí Write to Pub/Sub (downstream)\n",
    "```\n",
    "\n",
    "### What Happens\n",
    "\n",
    "1. **Messages arrive** at Pub/Sub topic\n",
    "2. **Every 10ms**: Trigger fires, releasing accumulated messages as a pane\n",
    "3. **Immediately**: BatchElements collects pane messages (up to 50, max 10ms wait)\n",
    "4. **Batch API call**: Vertex endpoint receives batch request (1-50 instances)\n",
    "5. **Results flow**: Each prediction (with metadata) goes to BigQuery + Pub/Sub\n",
    "6. **Continuous**: Job runs until cancelled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "run_c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:apache_beam.options.pipeline_options:Bucket specified in temp_location has soft-delete policy enabled. To avoid being billed for unnecessary storage costs, turn off the soft delete feature on buckets that your Dataflow jobs use for temporary and staging storage. For more information, see https://cloud.google.com/storage/docs/use-soft-delete#remove-soft-delete-policy.\n",
      "WARNING:apache_beam.options.pipeline_options:Bucket specified in staging_location has soft-delete policy enabled. To avoid being billed for unnecessary storage costs, turn off the soft delete feature on buckets that your Dataflow jobs use for temporary and staging storage. For more information, see https://cloud.google.com/storage/docs/use-soft-delete#remove-soft-delete-policy.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    div.alert {\n",
       "      white-space: pre-line;\n",
       "    }\n",
       "  </style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <link rel=\"stylesheet\" href=\"https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/css/bootstrap.min.css\" integrity=\"sha384-Vkoo8x4CGsO3+Hhxv8T/Q5PaXtkKtu6ug5TOeNV6gBiFeWPGFN9MuhOf23Q9Ifjh\" crossorigin=\"anonymous\">\n",
       "            <div class=\"alert alert-info\">No cache_root detected. Defaulting to staging_location gs://statmike-mlops-349915/dataflow/staging for cache location.</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:apache_beam.transforms.core:Key coder FastPrimitivesCoder for transform <ParDo(PTransform) label=[ParDo(_StatefulBatchElementsDoFn)]> with stateful DoFn may not be deterministic. This may cause incorrect behavior for complex key types. Consider adding an input type hint for this transform.\n",
      "WARNING:apache_beam.transforms.core:('No iterator is returned by the process method in %s.', <class 'apache_beam.io.gcp.pubsub._PubSubWriteDoFn'>)\n",
      "WARNING:apache_beam.options.pipeline_options:Bucket specified in temp_location has soft-delete policy enabled. To avoid being billed for unnecessary storage costs, turn off the soft delete feature on buckets that your Dataflow jobs use for temporary and staging storage. For more information, see https://cloud.google.com/storage/docs/use-soft-delete#remove-soft-delete-policy.\n",
      "WARNING:apache_beam.options.pipeline_options:Bucket specified in staging_location has soft-delete policy enabled. To avoid being billed for unnecessary storage costs, turn off the soft delete feature on buckets that your Dataflow jobs use for temporary and staging storage. For more information, see https://cloud.google.com/storage/docs/use-soft-delete#remove-soft-delete-policy.\n",
      "WARNING:apache_beam.transforms.core:('No iterator is returned by the process method in %s.', <class 'apache_beam.io.gcp.pubsub._PubSubWriteDoFn'>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Ultra-low-latency streaming job started!\n",
      "Monitor: https://console.cloud.google.com/dataflow/jobs/us-central1?project=statmike-mlops-349915\n",
      "\n",
      "======================================================================\n",
      "‚è≥ IMPORTANT: Wait 3-5 minutes for Dataflow workers to start\n",
      "======================================================================\n",
      "The pipeline needs time to:\n",
      "  1. Provision workers (2-3 minutes)\n",
      "  2. Initialize the environment\n",
      "  3. Connect to Pub/Sub subscription and Vertex AI endpoint\n",
      "\n",
      "Once workers are running, you can send test data.\n",
      "Check the Dataflow console to see when workers are ready.\n",
      "\n",
      "üí° This pipeline achieves ~100ms average latency:\n",
      "   - Trigger fires: Every 1 second (coarse watermark)\n",
      "   - Batch formation: 0-10ms (BatchElements max_batch_duration_secs=0.01)\n",
      "   - Endpoint inference: ~80ms (Vertex AI API call + batch processing)\n",
      "   - Pub/Sub delivery: ~5ms\n",
      "   - BatchElements provides the actual low-latency control!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "from apache_beam.transforms import trigger\n",
    "\n",
    "class RunInferenceOnBatch(beam.DoFn):\n",
    "    \"\"\"\n",
    "    Run inference on batches of elements while preserving metadata (Vertex AI endpoint version).\n",
    "    \n",
    "    This DoFn receives a batch of (features, metadata) tuples from BatchElements,\n",
    "    calls the Vertex AI endpoint with all features at once, then yields individual\n",
    "    (input_features, prediction, metadata) tuples for downstream processing.\n",
    "    \n",
    "    Key behaviors:\n",
    "    - Efficient: Makes single API call to endpoint with batch of instances\n",
    "    - Metadata-preserving: Each prediction paired with its original metadata\n",
    "    - Input-preserving: Vertex endpoint doesn't return input, so we preserve it\n",
    "    - Transparent: Batch processing is invisible to downstream transforms\n",
    "    \"\"\"\n",
    "    def __init__(self, model_handler):\n",
    "        self.model_handler = model_handler\n",
    "        self.model = None\n",
    "        \n",
    "    def setup(self):\n",
    "        \"\"\"Initialize endpoint connection once per worker.\"\"\"\n",
    "        self.model = self.model_handler.load_model()\n",
    "        \n",
    "    def process(self, batch_of_elements):\n",
    "        \"\"\"\n",
    "        Process a batch of (features, metadata) tuples via Vertex AI endpoint.\n",
    "        \n",
    "        Args:\n",
    "            batch_of_elements: List of (features_list, metadata_dict) tuples\n",
    "                               from BatchElements transform\n",
    "        \n",
    "        Yields:\n",
    "            (input_features, prediction_dict, metadata_dict) tuples - one per input element\n",
    "        \"\"\"\n",
    "        # Separate features and metadata from batch\n",
    "        features_list = [elem[0] for elem in batch_of_elements]  # Extract feature lists\n",
    "        metadata_list = [elem[1] for elem in batch_of_elements]  # Extract metadata\n",
    "        \n",
    "        # Call Vertex AI endpoint with entire batch at once (efficient!)\n",
    "        # VertexAIModelHandlerJSON returns list of PredictionResult objects\n",
    "        prediction_results = self.model_handler.run_inference(features_list, self.model, None)\n",
    "        \n",
    "        # Extract predictions from PredictionResult objects\n",
    "        predictions = [pr.inference for pr in prediction_results]\n",
    "        \n",
    "        # Yield (input_features, prediction, metadata) for each instance in batch\n",
    "        for features, prediction, metadata in zip(features_list, predictions, metadata_list):\n",
    "            yield (features, prediction, metadata)\n",
    "\n",
    "\n",
    "# Build the pipeline\n",
    "p = beam.Pipeline(options=options)\n",
    "\n",
    "results = (\n",
    "    p\n",
    "    # Step 1: Read messages from Pub/Sub subscription (unbounded stream)\n",
    "    | \"Read from Pub/Sub\" >> ReadFromPubSub(subscription=INPUT_SUB)\n",
    "    \n",
    "    # Step 2: Parse JSON bytes to (features_list, metadata_dict)\n",
    "    | \"Parse JSON\" >> beam.Map(parse_json)\n",
    "    \n",
    "    # Step 3: Assign 1-second windows with 1-second early triggers\n",
    "    # - Windows provide watermark progression (required for streaming)\n",
    "    # - Triggers fire every 1 second to release messages early\n",
    "    # - BatchElements (next step) provides the actual low-latency batching\n",
    "    | \"Window (1 sec, trigger 1 sec)\" >> beam.WindowInto(\n",
    "        window.FixedWindows(1),  # 1-second windows for watermark\n",
    "        trigger=trigger.Repeatedly(\n",
    "            trigger.AfterProcessingTime(1)  # Fire every 1 second\n",
    "        ),\n",
    "        accumulation_mode=trigger.AccumulationMode.DISCARDING  # Don't re-process\n",
    "    )\n",
    "    \n",
    "    # Step 4: Batch elements for efficient endpoint calls (THIS provides low latency)\n",
    "    # - Waits for min 1 element (don't delay single messages)\n",
    "    # - Collects up to max 50 elements (reasonable batch size for endpoint)\n",
    "    # - Hard deadline of 10ms (ensures low latency even at low traffic)\n",
    "    # - This is where the actual batching and low latency control happens!\n",
    "    | \"Batch elements\" >> beam.BatchElements(\n",
    "        min_batch_size=1,      # Process immediately if only 1 message\n",
    "        max_batch_size=50,     # Cap batch size for predictable endpoint latency\n",
    "        max_batch_duration_secs=0.01  # 10ms hard deadline - KEY for low latency!\n",
    "    )\n",
    "    \n",
    "    # Step 5: Call Vertex AI endpoint with batches\n",
    "    # - Receives batch of (features, metadata) tuples\n",
    "    # - Makes single API call to endpoint with all features\n",
    "    # - Yields (input_features, prediction, metadata) for each element\n",
    "    | \"RunInference on batch\" >> beam.ParDo(RunInferenceOnBatch(model_handler))\n",
    "    \n",
    "    # Step 6: Format predictions for output\n",
    "    # - Converts to Python types\n",
    "    # - Adds window timestamps (Unix format for accurate latency calculation)\n",
    "    # - Preserves metadata for scale testing\n",
    "    | \"Format results\" >> beam.Map(format_result)\n",
    ")\n",
    "\n",
    "# Write to Pub/Sub output topic (for downstream real-time processing)\n",
    "_ = (\n",
    "    results \n",
    "    | \"To JSON\" >> beam.Map(to_json) \n",
    "    | \"Write to Pub/Sub\" >> WriteToPubSub(topic=OUTPUT_TOPIC)\n",
    ")\n",
    "\n",
    "# Write to BigQuery (for storage and analysis)\n",
    "_ = (\n",
    "    results \n",
    "    | \"Write to BigQuery\" >> WriteToBigQuery(\n",
    "        table=f\"{PROJECT_ID}:{BQ_DATASET}.{BQ_TABLE}\",\n",
    "        write_disposition=beam.io.BigQueryDisposition.WRITE_APPEND\n",
    "    )\n",
    ")\n",
    "\n",
    "result = p.run()\n",
    "print(\"\\n‚úÖ Ultra-low-latency streaming job started!\")\n",
    "print(f\"Monitor: https://console.cloud.google.com/dataflow/jobs/{REGION}?project={PROJECT_ID}\")\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"‚è≥ IMPORTANT: Wait 3-5 minutes for Dataflow workers to start\")\n",
    "print(\"=\" * 70)\n",
    "print(\"The pipeline needs time to:\")\n",
    "print(\"  1. Provision workers (2-3 minutes)\")\n",
    "print(\"  2. Initialize the environment\")\n",
    "print(\"  3. Connect to Pub/Sub subscription and Vertex AI endpoint\")\n",
    "print(\"\\nOnce workers are running, you can send test data.\")\n",
    "print(\"Check the Dataflow console to see when workers are ready.\")\n",
    "print(\"\\nüí° This pipeline achieves ~100ms average latency:\")\n",
    "print(\"   - Trigger fires: Every 1 second (coarse watermark)\")\n",
    "print(\"   - Batch formation: 0-10ms (BatchElements max_batch_duration_secs=0.01)\")\n",
    "print(\"   - Endpoint inference: ~80ms (Vertex AI API call + batch processing)\")\n",
    "print(\"   - Pub/Sub delivery: ~5ms\")\n",
    "print(\"   - BatchElements provides the actual low-latency control!\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "simulate",
   "metadata": {},
   "source": [
    "---\n",
    "## Simulate Streaming Data\n",
    "\n",
    "**‚ö†Ô∏è Wait 3-5 minutes** after starting the Dataflow job before running this cell. Check the [Dataflow Console](https://console.cloud.google.com/dataflow/jobs/us-central1?project=statmike-mlops-349915) to verify workers are running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b685609c",
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(5*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "simulate_c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Published message 1\n",
      "Published message 2\n",
      "Published message 3\n",
      "Published message 4\n",
      "Published message 5\n",
      "\n",
      "‚úÖ Sent 5 test messages to VERTEX input topic: projects/statmike-mlops-349915/topics/pytorch-autoencoder-input-vertex\n"
     ]
    }
   ],
   "source": [
    "publisher = pubsub_v1.PublisherClient()\n",
    "# Publish to VERTEX-specific input topic\n",
    "topic_path = publisher.topic_path(PROJECT_ID, f\"{EXPERIMENT}-input-vertex\")\n",
    "\n",
    "# Send test messages\n",
    "for i in range(5):\n",
    "    message = {\"features\": [0.1] * 30}  # Dummy transaction\n",
    "    publisher.publish(topic_path, json.dumps(message).encode(\"utf-8\"))\n",
    "    print(f\"Published message {i+1}\")\n",
    "    time.sleep(2)\n",
    "\n",
    "print(f\"\\n‚úÖ Sent 5 test messages to VERTEX input topic: {topic_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "monitor",
   "metadata": {},
   "source": [
    "### Monitor Results\n",
    "\n",
    "**‚è≥ Wait 2-3 minutes** after sending test messages for the pipeline to process them.\n",
    "\n",
    "Monitor results from both output destinations: BigQuery (storage/analysis) and Pub/Sub (downstream processing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd014a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "monitor_c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "BIGQUERY RESULTS (Storage & Analysis)\n",
      "============================================================\n",
      "‚úÖ Found 5 results in BigQuery\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instance_id</th>\n",
       "      <th>anomaly_score</th>\n",
       "      <th>encoded</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>window_start</th>\n",
       "      <th>window_end</th>\n",
       "      <th>pipeline_output_time</th>\n",
       "      <th>test_id</th>\n",
       "      <th>message_id</th>\n",
       "      <th>publish_time</th>\n",
       "      <th>sequence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8136260069931320451</td>\n",
       "      <td>2324.92627</td>\n",
       "      <td>[0.0, 0.0, 5.395321846008301, 0.0]</td>\n",
       "      <td>2025-11-12 23:23:57.291086+00:00</td>\n",
       "      <td>2025-11-12 23:23:56+00:00</td>\n",
       "      <td>2025-11-12 23:23:57+00:00</td>\n",
       "      <td>1.762990e+09</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8136260069931320451</td>\n",
       "      <td>2324.92627</td>\n",
       "      <td>[0.0, 0.0, 5.395321846008301, 0.0]</td>\n",
       "      <td>2025-11-12 23:23:56.482524+00:00</td>\n",
       "      <td>2025-11-12 23:23:52+00:00</td>\n",
       "      <td>2025-11-12 23:23:53+00:00</td>\n",
       "      <td>1.762990e+09</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-8138816769772226127</td>\n",
       "      <td>2324.92627</td>\n",
       "      <td>[0.0, 0.0, 5.395321846008301, 0.0]</td>\n",
       "      <td>2025-11-12 23:23:56.434316+00:00</td>\n",
       "      <td>2025-11-12 23:23:54+00:00</td>\n",
       "      <td>2025-11-12 23:23:55+00:00</td>\n",
       "      <td>1.762990e+09</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-8138816769772226127</td>\n",
       "      <td>2324.92627</td>\n",
       "      <td>[0.0, 0.0, 5.395321846008301, 0.0]</td>\n",
       "      <td>2025-11-12 23:23:55.518961+00:00</td>\n",
       "      <td>2025-11-12 23:23:50+00:00</td>\n",
       "      <td>2025-11-12 23:23:51+00:00</td>\n",
       "      <td>1.762990e+09</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-8138816769772226127</td>\n",
       "      <td>2324.92627</td>\n",
       "      <td>[0.0, 0.0, 5.395321846008301, 0.0]</td>\n",
       "      <td>2025-11-12 23:23:53.880361+00:00</td>\n",
       "      <td>2025-11-12 23:23:48+00:00</td>\n",
       "      <td>2025-11-12 23:23:49+00:00</td>\n",
       "      <td>1.762990e+09</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            instance_id  anomaly_score                             encoded  \\\n",
       "0   8136260069931320451     2324.92627  [0.0, 0.0, 5.395321846008301, 0.0]   \n",
       "1   8136260069931320451     2324.92627  [0.0, 0.0, 5.395321846008301, 0.0]   \n",
       "2  -8138816769772226127     2324.92627  [0.0, 0.0, 5.395321846008301, 0.0]   \n",
       "3  -8138816769772226127     2324.92627  [0.0, 0.0, 5.395321846008301, 0.0]   \n",
       "4  -8138816769772226127     2324.92627  [0.0, 0.0, 5.395321846008301, 0.0]   \n",
       "\n",
       "                         timestamp              window_start  \\\n",
       "0 2025-11-12 23:23:57.291086+00:00 2025-11-12 23:23:56+00:00   \n",
       "1 2025-11-12 23:23:56.482524+00:00 2025-11-12 23:23:52+00:00   \n",
       "2 2025-11-12 23:23:56.434316+00:00 2025-11-12 23:23:54+00:00   \n",
       "3 2025-11-12 23:23:55.518961+00:00 2025-11-12 23:23:50+00:00   \n",
       "4 2025-11-12 23:23:53.880361+00:00 2025-11-12 23:23:48+00:00   \n",
       "\n",
       "                 window_end  pipeline_output_time test_id message_id  \\\n",
       "0 2025-11-12 23:23:57+00:00          1.762990e+09    None       None   \n",
       "1 2025-11-12 23:23:53+00:00          1.762990e+09    None       None   \n",
       "2 2025-11-12 23:23:55+00:00          1.762990e+09    None       None   \n",
       "3 2025-11-12 23:23:51+00:00          1.762990e+09    None       None   \n",
       "4 2025-11-12 23:23:49+00:00          1.762990e+09    None       None   \n",
       "\n",
       "   publish_time  sequence  \n",
       "0           NaN      <NA>  \n",
       "1           NaN      <NA>  \n",
       "2           NaN      <NA>  \n",
       "3           NaN      <NA>  \n",
       "4           NaN      <NA>  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "PUB/SUB OUTPUT TOPIC (Downstream Processing - VERTEX)\n",
      "============================================================\n",
      "Pulling messages from: projects/statmike-mlops-349915/subscriptions/pytorch-autoencoder-output-sub-vertex\n",
      "‚úÖ Found 5 messages in output topic\n",
      "\n",
      "Sample messages:\n",
      "\n",
      "Message 1:\n",
      "  Anomaly Score: 2324.92626953125\n",
      "  Instance ID: -8138816769772226127\n",
      "  Timestamp: 2025-11-12T23:23:53.880361\n",
      "\n",
      "Message 2:\n",
      "  Anomaly Score: 2324.92626953125\n",
      "  Instance ID: 8136260069931320451\n",
      "  Timestamp: 2025-11-12T23:23:56.482524\n",
      "\n",
      "Message 3:\n",
      "  Anomaly Score: 2324.92626953125\n",
      "  Instance ID: -8138816769772226127\n",
      "  Timestamp: 2025-11-12T23:23:55.518961\n",
      "\n",
      "‚úÖ Acknowledged 5 messages\n",
      "\n",
      "============================================================\n",
      "üí° Pipeline Status Summary\n",
      "============================================================\n",
      "‚úÖ Pipeline is working correctly!\n",
      "   Total results in BigQuery: 5\n",
      "   Results are being written to both:\n",
      "     - BigQuery table: statmike-mlops-349915.frameworks.pytorch_autoencoder_streaming_results_vertex\n",
      "     - Pub/Sub topic: projects/statmike-mlops-349915/topics/pytorch-autoencoder-output-vertex\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import bigquery\n",
    "\n",
    "# Monitor BigQuery results\n",
    "print(\"=\" * 60)\n",
    "print(\"BIGQUERY RESULTS (Storage & Analysis)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "bq = bigquery.Client(project=PROJECT_ID)\n",
    "query = f\"SELECT * FROM `{PROJECT_ID}.{BQ_DATASET}.{BQ_TABLE}` ORDER BY timestamp DESC LIMIT 10\"\n",
    "df = bq.query(query).to_dataframe()\n",
    "\n",
    "if len(df) > 0:\n",
    "    print(f\"‚úÖ Found {len(df)} results in BigQuery\")\n",
    "    display(df)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No results yet in BigQuery\")\n",
    "    print(\"   Wait a few minutes for the pipeline to process data\")\n",
    "\n",
    "# Monitor Pub/Sub output topic (VERTEX-specific)\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PUB/SUB OUTPUT TOPIC (Downstream Processing - VERTEX)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "subscriber = pubsub_v1.SubscriberClient()\n",
    "output_sub_path = subscriber.subscription_path(PROJECT_ID, f\"{EXPERIMENT}-output-sub-vertex\")\n",
    "\n",
    "print(f\"Pulling messages from: {output_sub_path}\")\n",
    "\n",
    "# Pull messages from output subscription\n",
    "try:\n",
    "    response = subscriber.pull(\n",
    "        request={\"subscription\": output_sub_path, \"max_messages\": 5},\n",
    "        timeout=10\n",
    "    )\n",
    "    \n",
    "    if response.received_messages:\n",
    "        print(f\"‚úÖ Found {len(response.received_messages)} messages in output topic\")\n",
    "        print(\"\\nSample messages:\")\n",
    "        for i, msg in enumerate(response.received_messages[:3], 1):\n",
    "            data = json.loads(msg.message.data.decode(\"utf-8\"))\n",
    "            print(f\"\\nMessage {i}:\")\n",
    "            print(f\"  Anomaly Score: {data.get('anomaly_score', 'N/A')}\")\n",
    "            print(f\"  Instance ID: {data.get('instance_id', 'N/A')}\")\n",
    "            print(f\"  Timestamp: {data.get('timestamp', 'N/A')}\")\n",
    "        \n",
    "        # Acknowledge messages (optional - remove if you want to keep them)\n",
    "        ack_ids = [msg.ack_id for msg in response.received_messages]\n",
    "        subscriber.acknowledge(request={\"subscription\": output_sub_path, \"ack_ids\": ack_ids})\n",
    "        print(f\"\\n‚úÖ Acknowledged {len(ack_ids)} messages\")\n",
    "    else:\n",
    "        print(\"‚ÑπÔ∏è  No messages currently in output subscription\")\n",
    "        print(\"   Messages may have been consumed already or not yet published\")\n",
    "        \n",
    "except Exception as e:\n",
    "    if \"DeadlineExceeded\" in str(type(e).__name__):\n",
    "        print(\"‚ÑπÔ∏è  No messages available in output subscription (timeout)\")\n",
    "        print(\"   This is normal - messages are consumed quickly or not yet available\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è  Error pulling messages: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üí° Pipeline Status Summary\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Get total count from BigQuery\n",
    "count_query = f\"SELECT COUNT(*) as total FROM `{PROJECT_ID}.{BQ_DATASET}.{BQ_TABLE}`\"\n",
    "count_result = bq.query(count_query).to_dataframe()\n",
    "total_results = count_result['total'].iloc[0] if len(count_result) > 0 else 0\n",
    "\n",
    "if total_results > 0:\n",
    "    print(f\"‚úÖ Pipeline is working correctly!\")\n",
    "    print(f\"   Total results in BigQuery: {total_results}\")\n",
    "    print(f\"   Results are being written to both:\")\n",
    "    print(f\"     - BigQuery table: {PROJECT_ID}.{BQ_DATASET}.{BQ_TABLE}\")\n",
    "    print(f\"     - Pub/Sub topic: {OUTPUT_TOPIC}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No results yet in BigQuery\")\n",
    "    print(\"   Pipeline may still be processing or waiting for data\")\n",
    "    \n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0rrkkvi4ri0f",
   "metadata": {},
   "source": [
    "---\n",
    "## Understanding Scaling and Performance\n",
    "\n",
    "Now that your streaming pipeline is deployed with Vertex AI endpoint integration, understanding how the combined system scales is critical for production deployments.\n",
    "\n",
    "### Factors Affecting Performance\n",
    "\n",
    "**Dataflow Service-Side Factors**:\n",
    "- **Worker Count**: Number of Dataflow workers making API calls (min/max worker settings)\n",
    "- **Machine Type**: CPU and memory for workers (less critical than local model, as workers just make API calls)\n",
    "- **Autoscaling Configuration**: How quickly workers scale based on Pub/Sub backlog\n",
    "\n",
    "**Vertex Endpoint Service-Side Factors**:\n",
    "- **Replica Count**: Number of endpoint instances serving predictions\n",
    "- **Machine Type**: Resources per endpoint replica\n",
    "- **Endpoint Autoscaling**: How quickly new replicas come online\n",
    "\n",
    "**Usage-Side Factors**:\n",
    "- **Message Rate**: Messages published to Pub/Sub per second\n",
    "- **Endpoint Latency**: Time for endpoint to respond (affects overall throughput)\n",
    "- **Window Size**: Duration of windows for micro-batching\n",
    "- **Traffic Pattern**: Constant flow vs. spikes (both services must handle)\n",
    "\n",
    "### Current Configuration\n",
    "\n",
    "Your combined system is configured with:\n",
    "\n",
    "**Dataflow**:\n",
    "- **Machine Type**: `n1-standard-4` (4 vCPUs, 15 GB memory)\n",
    "- **Min Workers**: 2 (baseline)\n",
    "- **Max Workers**: 20 (autoscaling limit)\n",
    "- **Window Size**: 1 minute\n",
    "\n",
    "**Vertex Endpoint**:\n",
    "- **Machine Type**: `n1-standard-4` (4 vCPUs, 15 GB memory)\n",
    "- **Min Replicas**: 1 (always-on)\n",
    "- **Max Replicas**: 4 (autoscaling limit)\n",
    "\n",
    "This configuration provides:\n",
    "- ‚úÖ Two-layer autoscaling (Dataflow + Vertex)\n",
    "- ‚úÖ Independent scaling (each service adapts to load)\n",
    "- ‚ö†Ô∏è **Potential bottleneck**: Endpoint may limit overall throughput\n",
    "\n",
    "### Performance Testing\n",
    "\n",
    "To understand how this combined system scales and identify bottlenecks, see:\n",
    "\n",
    "**[scale-tests-dataflow-streaming-vertex.ipynb](./scale-tests-dataflow-streaming-vertex.ipynb)**\n",
    "\n",
    "This notebook demonstrates:\n",
    "- **Finding System Bottlenecks**: Identify which component limits performance\n",
    "  - Test message rates from 10 to 1000+ messages/second\n",
    "  - Determine if Dataflow workers or Vertex endpoint is the bottleneck\n",
    "  - Measure end-to-end latency breakdown (pipeline vs endpoint)\n",
    "  - Identify CPU saturation points for both services\n",
    "- **Latency Analysis**: Measure combined system latency components\n",
    "  - Pipeline processing (Dataflow transforms + windowing)\n",
    "  - Endpoint inference (Vertex AI API call + model execution)\n",
    "  - Test framework overhead (output queue wait time)\n",
    "  - P50/P95/P99 latency percentiles across both services\n",
    "- **Dual-Autoscaling Behavior**: Understand two-layer scaling patterns\n",
    "  - Dataflow worker scaling (based on Pub/Sub backlog)\n",
    "  - Vertex endpoint replica scaling (based on CPU utilization)\n",
    "  - Worker-to-replica ratio analysis\n",
    "  - Correlation between worker count and endpoint replicas\n",
    "- **Load Pattern Testing**: Test combined system under various scenarios\n",
    "  - Constant load (steady-state throughput for both services)\n",
    "  - Gradual ramp-up (observe both autoscaling systems)\n",
    "  - Traffic spikes (which service recovers faster?)\n",
    "  - Backpressure handling (endpoint throttling impact on pipeline)\n",
    "- **Tuning Recommendations**: Data-driven configuration guidance\n",
    "  - When to scale Dataflow workers vs. Vertex replicas\n",
    "  - How to balance worker-to-replica ratios\n",
    "  - Cost implications of different configurations\n",
    "  - Identifying which service to optimize first\n",
    "\n",
    "### When to Run Scale Testing\n",
    "\n",
    "Run scale tests when:\n",
    "- üîπ **Before production launch**: Understand combined system capacity\n",
    "- üîπ **After model/endpoint changes**: Endpoint latency affects pipeline throughput\n",
    "- üîπ **For capacity planning**: Estimate costs for both services\n",
    "- üîπ **During incidents**: Identify which service is the bottleneck"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cleanup",
   "metadata": {},
   "source": [
    "---\n",
    "## Clean Up\n",
    "\n",
    "**‚ö†Ô∏è IMPORTANT: Streaming jobs run continuously until explicitly cancelled**, incurring ongoing costs for workers and resources.\n",
    "\n",
    "### Centralized Cleanup Notebook\n",
    "\n",
    "For comprehensive cleanup of all Dataflow resources, use the centralized cleanup notebook:\n",
    "\n",
    "**[dataflow-cleanup.ipynb](./dataflow-cleanup.ipynb)**\n",
    "\n",
    "This notebook provides:\n",
    "- ‚úÖ **Stop Dataflow Jobs**: Cancel running streaming/batch jobs created by these notebooks\n",
    "- ‚úÖ **Clean BigQuery Tables**: Truncate or delete result tables\n",
    "- ‚úÖ **Clean Pub/Sub Resources**: Delete topics and subscriptions\n",
    "- ‚úÖ **Clean GCS Files**: Delete model files uploaded for Dataflow\n",
    "- ‚úÖ **Granular Control**: Use flags to choose exactly what to clean up\n",
    "- ‚úÖ **Safety Checks**: Warnings for risky operations\n",
    "- ‚úÖ **Confirmation Prompts**: Review before executing\n",
    "\n",
    "### Why Use Centralized Cleanup?\n",
    "\n",
    "- **One location**: Manage all Dataflow infrastructure from a single notebook\n",
    "- **Comprehensive**: Clean up jobs, tables, Pub/Sub, and GCS files together\n",
    "- **Flexible**: Truncate tables without deleting schema (useful for testing)\n",
    "- **Safe**: Built-in safety checks and confirmation prompts\n",
    "- **Efficient**: Clean up resources from all 4 Dataflow notebooks at once\n",
    "\n",
    "### Quick Cleanup (This Job Only)\n",
    "\n",
    "If you only need to stop the streaming job created by this notebook, uncomment and run the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "usa985119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the line below to stop the streaming job created by this notebook\n",
    "#result.cancel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "In this notebook, you successfully:\n",
    "\n",
    "‚úÖ **Tested Endpoint Health**: Verified Vertex AI endpoint is responding correctly\n",
    "\n",
    "‚úÖ **Configured Vertex AI Handler**: Set up VertexAIModelHandlerJSON for Vertex AI Endpoint calls\n",
    "\n",
    "‚úÖ **Built Streaming Pipeline**: Created real-time processing with 1-minute windows\n",
    "\n",
    "‚úÖ **Applied RunInference**: Called Vertex AI Endpoint for predictions on streaming data\n",
    "\n",
    "‚úÖ **Dual Output**: Wrote results to both Pub/Sub (for downstream systems) and BigQuery (for analysis)\n",
    "\n",
    "‚úÖ **Monitored Job**: Tracked continuous job execution and viewed results\n",
    "\n",
    "‚úÖ **Cleaned Up**: Stopped streaming job to avoid ongoing charges\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "- **Endpoint Health Check**: Always test endpoint before deploying pipeline to catch issues early\n",
    "- **Data Format**: `VertexAIModelHandlerJSON` automatically wraps data in `{\"instances\": ...}` - just pass the features array\n",
    "- **Streaming vs Batch**: Continuous processing for real-time anomaly detection\n",
    "- **Vertex AI Integration**: RunInference seamlessly calls managed endpoints\n",
    "- **Automatic Batching**: Efficient inference through request aggregation\n",
    "- **Dual Output**: Results flow to both storage and downstream services\n",
    "- **Independent Scaling**: Endpoint and pipeline scale separately\n",
    "- **Format Flexibility**: Pipeline handles both pre-built and custom container outputs\n",
    "- **Patience Required**: Allow 8-10 minutes total for workers to start and data to process\n",
    "\n",
    "## Troubleshooting\n",
    "\n",
    "If you encounter issues:\n",
    "\n",
    "1. **Endpoint Health Test Fails**:\n",
    "   - Check endpoint is deployed and has active replicas\n",
    "   - Verify endpoint display name matches `ENDPOINT_DISPLAY_NAME` variable\n",
    "   - Check Cloud Console for endpoint errors\n",
    "\n",
    "2. **No Results in BigQuery**:\n",
    "   - Wait full 8-10 minutes (workers + processing)\n",
    "   - Check Dataflow job logs for errors\n",
    "   - Verify messages were published to Pub/Sub topic\n",
    "   - Re-run endpoint health test\n",
    "\n",
    "3. **Pub/Sub Timeout**:\n",
    "   - This is normal - messages are consumed quickly\n",
    "   - Check BigQuery instead (source of truth)\n",
    "   - Pipeline Status Summary shows total results\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "### Continue with Other Dataflow Workflows:\n",
    "\n",
    "**Local Model Inference (Comparison):**\n",
    "- [dataflow-batch-runinference.ipynb](./dataflow-batch-runinference.ipynb)\n",
    "  - Batch processing with local PyTorch model\n",
    "  - Compare cost and performance vs Vertex endpoint\n",
    "\n",
    "- [dataflow-streaming-runinference.ipynb](./dataflow-streaming-runinference.ipynb)\n",
    "  - Real-time processing with local PyTorch model\n",
    "  - No endpoint dependency, lower latency\n",
    "\n",
    "**Batch Inference with Vertex Endpoint:**\n",
    "- [dataflow-batch-runinference-vertex.ipynb](./dataflow-batch-runinference-vertex.ipynb)\n",
    "  - Process historical data via Vertex AI Endpoint\n",
    "  - Compare batch vs streaming approaches\n",
    "\n",
    "### Production Enhancements:\n",
    "\n",
    "1. **Error Handling**: Add retry logic for endpoint failures with exponential backoff\n",
    "2. **Dead Letter Queue**: Route failed predictions to separate Pub/Sub topic for analysis\n",
    "3. **Monitoring**: Set up Cloud Monitoring alerts for job failures and high latency\n",
    "4. **Cost Optimization**: Tune window size and batching parameters based on traffic patterns\n",
    "5. **A/B Testing**: Route traffic between multiple endpoint versions for model comparison\n",
    "6. **Authentication**: Add service account authentication for production deployments\n",
    "7. **Scaling**: Configure autoscaling based on Pub/Sub backlog\n",
    "\n",
    "### Resources:\n",
    "\n",
    "- [Apache Beam Streaming](https://beam.apache.org/documentation/programming-guide/#streaming)\n",
    "- [Dataflow Monitoring](https://cloud.google.com/dataflow/docs/guides/monitoring-jobs)\n",
    "- [Vertex AI Endpoints](https://cloud.google.com/vertex-ai/docs/predictions/overview)\n",
    "- [Pub/Sub Documentation](https://cloud.google.com/pubsub/docs)\n",
    "- [RunInference with Vertex AI](https://beam.apache.org/documentation/ml/vertex-ai/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (frameworks-pytorch)",
   "language": "python",
   "name": "frameworks-pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
