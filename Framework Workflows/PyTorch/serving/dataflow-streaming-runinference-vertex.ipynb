{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84e987f3",
   "metadata": {},
   "source": [
    "![tracker](https://us-central1-vertex-ai-mlops-369716.cloudfunctions.net/pixel-tracking?path=statmike%2Fvertex-ai-mlops%2FFramework+Workflows%2FPyTorch%2Fserving&file=dataflow-streaming-runinference-vertex.ipynb)\n",
    "<!--- header table --->\n",
    "<table align=\"left\">\n",
    "<tr>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%20Workflows/PyTorch/serving/dataflow-streaming-runinference-vertex.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.svgrepo.com/download/217753/github.svg\" alt=\"GitHub logo\">\n",
    "      <br>View on<br>GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://colab.research.google.com/github/statmike/vertex-ai-mlops/blob/main/Framework%20Workflows/PyTorch/serving/dataflow-streaming-runinference-vertex.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.gstatic.com/pantheon/images/bigquery/welcome_page/colab-logo.svg\" alt=\"Google Colaboratory logo\">\n",
    "      <br>Run in<br>Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https%3A%2F%2Fraw.githubusercontent.com%2Fstatmike%2Fvertex-ai-mlops%2Fmain%2FFramework%2520Workflows%2FPyTorch%2Fserving%2Fdataflow-streaming-runinference-vertex.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\">\n",
    "      <br>Run in<br>Colab Enterprise\n",
    "    </a>\n",
    "  </td>      \n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/bigquery/import?url=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%20Workflows/PyTorch/serving/dataflow-streaming-runinference-vertex.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.gstatic.com/images/branding/gcpiconscolors/bigquery/v1/32px.svg\" alt=\"BigQuery logo\">\n",
    "      <br>Open in<br>BigQuery Studio\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/statmike/vertex-ai-mlops/main/Framework%20Workflows/PyTorch/serving/dataflow-streaming-runinference-vertex.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.gstatic.com/images/branding/gcpiconscolors/vertexai/v1/32px.svg\" alt=\"Vertex AI logo\">\n",
    "      <br>Open in<br>Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td colspan=\"5\" style=\"text-align: right\">\n",
    "    <b>Share This On: </b> \n",
    "    <a href=\"https://www.linkedin.com/sharing/share-offsite/?url=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%2520Workflows/PyTorch/serving/dataflow-streaming-runinference-vertex.ipynb\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"Linkedin Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://reddit.com/submit?url=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%2520Workflows/PyTorch/serving/dataflow-streaming-runinference-vertex.ipynb\"><img src=\"https://redditinc.com/hubfs/Reddit%20Inc/Brand/Reddit_Logo.png\" alt=\"Reddit Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://bsky.app/intent/compose?text=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%2520Workflows/PyTorch/serving/dataflow-streaming-runinference-vertex.ipynb\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"BlueSky Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://twitter.com/intent/tweet?url=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%2520Workflows/PyTorch/serving/dataflow-streaming-runinference-vertex.ipynb\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/5/5a/X_icon_2.svg\" alt=\"X (Twitter) Logo\" width=\"20px\"></a> \n",
    "  </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td colspan=\"5\" style=\"text-align: right\">\n",
    "    <b>Connect With Author On: </b> \n",
    "    <a href=\"https://www.linkedin.com/in/statmike\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"Linkedin Logo\" width=\"20px\"></a>\n",
    "    <a href=\"https://www.github.com/statmike\"><img src=\"https://www.svgrepo.com/download/217753/github.svg\" alt=\"GitHub Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://www.youtube.com/@statmike-channel\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/f/fd/YouTube_full-color_icon_%282024%29.svg\" alt=\"YouTube Logo\" width=\"20px\"></a>\n",
    "    <a href=\"https://bsky.app/profile/statmike.bsky.social\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"BlueSky Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://x.com/statmike\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/5/5a/X_icon_2.svg\" alt=\"X (Twitter) Logo\" width=\"20px\"></a>\n",
    "  </td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "i0cm6r2trpm",
   "source": "# Dataflow Streaming Inference with RunInference (Vertex AI Endpoint)\n\nThis notebook demonstrates streaming processing of transactions using Dataflow with Apache Beam RunInference that calls a Vertex AI Endpoint.\n\n## What You'll Learn\n\nThis workflow covers:\n\n1. **Configure Vertex AI Handler**: Set up VertexAIModelHandlerJSON for RunInference\n2. **Build Streaming Pipeline**: Read from Pub/Sub, apply model, write to BigQuery and Pub/Sub\n3. **Run on Dataflow**: Execute continuous pipeline on Google Cloud\n4. **Monitor Job**: Track streaming job progress in Cloud Console\n5. **Analyze Results**: Query and visualize real-time anomaly scores\n\n## Prerequisites\n\n- Completed `dataflow-setup.ipynb` (infrastructure ready)\n- **Vertex AI Endpoint deployed** - Choose either:\n  - [Pre-built Container](./vertex-ai-endpoint-prebuilt-container.ipynb) - Returns full model output (13 metrics)\n  - [Custom Container](./vertex-ai-endpoint-custom-container.ipynb) - Returns simplified output (2 fields)\n- Pub/Sub topics and subscriptions created\n- BigQuery tables created\n\n> **Note**: This notebook works with both endpoint types and will automatically detect the output format.\n\n## Batch vs Streaming\n\n**Batch Processing (Previous Notebook)**:\n- ✅ Process historical data\n- ✅ Bounded dataset (has a start and end)\n- ✅ Results available when job completes\n- ✅ Cost-effective for large datasets\n- Example: Analyze all transactions from last month\n\n**Streaming Processing (This Notebook)**:\n- ✅ Process real-time data\n- ✅ Unbounded dataset (continuous)\n- ✅ Results available immediately\n- ✅ Low-latency anomaly detection\n- Example: Flag suspicious transactions as they occur\n\n## Architecture\n\n```\nPub/Sub Input Topic\n  ↓ Read transactions in real-time\nDataflow Pipeline\n  ↓ 1-minute windows\nRunInference (Vertex AI Endpoint)\n  ↓ Generate anomaly scores via API\nTransform Results\n  ↓ Extract scores and embeddings\n  ├─→ BigQuery (storage & analysis)\n  └─→ Pub/Sub Output (downstream processing)\n```\n\n## RunInference with Vertex AI Benefits\n\n- **Managed endpoint**: No model loading in workers\n- **Automatic batching**: Combines instances for efficient inference\n- **Scalable**: Endpoint scales independently from pipeline\n- **Flexible**: Update model without redeploying pipeline\n- **Real-time**: Low-latency predictions for streaming data\n\n## What This Pipeline Does\n\n1. Read transactions from Pub/Sub input topic\n2. Window data into 1-minute batches\n3. Format data as JSON for Vertex AI\n4. Call Vertex AI Endpoint via RunInference\n5. Extract relevant outputs (score + embeddings)\n6. Write results to BigQuery (for analysis)\n7. Publish to Pub/Sub output (for downstream systems)\n8. Job runs continuously until cancelled",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "env",
   "metadata": {},
   "source": [
    "---\n",
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proj",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = 'statmike-mlops-349915'\n",
    "REQ_TYPE = 'ALL'\n",
    "INSTALL_TOOL = 'poetry'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conf",
   "metadata": {},
   "outputs": [],
   "source": [
    "REQUIREMENTS_URL = 'https://raw.githubusercontent.com/statmike/vertex-ai-mlops/refs/heads/main/Framework%20Workflows/PyTorch/requirements.txt'\n",
    "REQUIRED_APIS = [\"dataflow.googleapis.com\", \"pubsub.googleapis.com\", \"bigquery.googleapis.com\", \"storage.googleapis.com\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, urllib.request\n",
    "url = 'https://raw.githubusercontent.com/statmike/vertex-ai-mlops/refs/heads/main/core/notebook-template/python_setup.py'\n",
    "urllib.request.urlretrieve(url, 'python_setup_local.py')\n",
    "import python_setup_local as python_setup\n",
    "os.remove('python_setup_local.py')\n",
    "setup_info = python_setup.setup_environment(PROJECT_ID, REQ_TYPE, REQUIREMENTS_URL, REQUIRED_APIS, INSTALL_TOOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "py",
   "metadata": {},
   "source": [
    "---\n",
    "## Python Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imp",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import apache_beam as beam\n",
    "from apache_beam import window\n",
    "from apache_beam.ml.inference.base import RunInference\n",
    "from apache_beam.ml.inference.vertex_ai_inference import VertexAIModelHandlerJSON\n",
    "from apache_beam.options.pipeline_options import PipelineOptions, StandardOptions\n",
    "from apache_beam.io.gcp.pubsub import ReadFromPubSub, WriteToPubSub\n",
    "from apache_beam.io.gcp.bigquery import WriteToBigQuery\n",
    "import json\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vars",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = subprocess.run([\"gcloud\", \"config\", \"get-value\", \"project\"], capture_output=True, text=True, check=True).stdout.strip()\n",
    "\n",
    "# Configuration\n",
    "REGION = \"us-central1\"\n",
    "SERIES = \"frameworks\"\n",
    "EXPERIMENT = \"pytorch-autoencoder\"\n",
    "\n",
    "# ========================================\n",
    "# ENDPOINT SELECTION\n",
    "# ========================================\n",
    "# Choose which endpoint to use:\n",
    "# - \"pytorch-autoencoder-endpoint\" (pre-built container - returns 13 metrics)\n",
    "# - \"pytorch-autoencoder-custom-endpoint\" (custom container - returns 2 fields)\n",
    "ENDPOINT_DISPLAY_NAME = \"pytorch-autoencoder-endpoint\"  # Change to \"pytorch-autoencoder-custom-endpoint\" if using custom container\n",
    "\n",
    "# GCS paths\n",
    "BUCKET_NAME = PROJECT_ID\n",
    "BUCKET_URI = f\"gs://{BUCKET_NAME}\"\n",
    "MODEL_DIR = f\"{BUCKET_URI}/{SERIES}/{EXPERIMENT}\"\n",
    "\n",
    "# Pub/Sub configuration\n",
    "INPUT_SUB = f\"projects/{PROJECT_ID}/subscriptions/{EXPERIMENT}-input-sub\"\n",
    "OUTPUT_TOPIC = f\"projects/{PROJECT_ID}/topics/{EXPERIMENT}-output\"\n",
    "\n",
    "# BigQuery configuration\n",
    "BQ_DATASET = SERIES.replace(\"-\", \"_\")\n",
    "BQ_TABLE = f\"{EXPERIMENT.replace('-', '_')}_streaming_results_vertex\"\n",
    "\n",
    "# Dataflow configuration\n",
    "DATAFLOW_STAGING = f\"{BUCKET_URI}/dataflow/staging\"\n",
    "DATAFLOW_TEMP = f\"{BUCKET_URI}/dataflow/temp\"\n",
    "\n",
    "print(f\"Project: {PROJECT_ID}\")\n",
    "print(f\"Endpoint: {ENDPOINT_DISPLAY_NAME}\")\n",
    "print(f\"Input subscription: {INPUT_SUB}\")\n",
    "print(f\"Output topic: {OUTPUT_TOPIC}\")\n",
    "print(f\"Results table: {PROJECT_ID}.{BQ_DATASET}.{BQ_TABLE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handler",
   "metadata": {},
   "source": [
    "---\n",
    "## Create ModelHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handler_c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_handler = VertexAIModelHandlerJSON(\n",
    "    endpoint_id=ENDPOINT_DISPLAY_NAME,\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION\n",
    ")\n",
    "print(\"✅ ModelHandler created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pipeline",
   "metadata": {},
   "source": [
    "---\n",
    "## Build Streaming Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pipeline_c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_json(message):\n",
    "    \"\"\"Parse Pub/Sub message\"\"\"\n",
    "    data = json.loads(message.decode(\"utf-8\"))\n",
    "    return {\"instances\": [data[\"features\"]]}\n",
    "\n",
    "def format_result(element, window=beam.DoFn.WindowParam):\n",
    "    \"\"\"\n",
    "    Format for Pub/Sub and BigQuery.\n",
    "    \n",
    "    Handles both endpoint output formats:\n",
    "    - Pre-built container: 13 keys (denormalized_MAE, encoded, etc.)\n",
    "    - Custom container: 2 keys (anomaly_score, encoded)\n",
    "    \"\"\"\n",
    "    prediction = element[1]\n",
    "    \n",
    "    # Auto-detect output format and extract anomaly score\n",
    "    if \"anomaly_score\" in prediction:\n",
    "        # Custom container format\n",
    "        anomaly_score = prediction[\"anomaly_score\"]\n",
    "    elif \"denormalized_MAE\" in prediction:\n",
    "        # Pre-built container format\n",
    "        anomaly_score = prediction[\"denormalized_MAE\"]\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown prediction format: {prediction.keys()}\")\n",
    "    \n",
    "    return {\n",
    "        \"instance_id\": str(hash(str(element[0]))),\n",
    "        \"anomaly_score\": anomaly_score,\n",
    "        \"encoded\": prediction[\"encoded\"],\n",
    "        \"timestamp\": datetime.utcnow().isoformat(),\n",
    "        \"window_start\": window.start.to_utc_datetime().isoformat(),\n",
    "        \"window_end\": window.end.to_utc_datetime().isoformat()\n",
    "    }\n",
    "\n",
    "def to_json(element):\n",
    "    \"\"\"Convert to JSON for Pub/Sub\"\"\"\n",
    "    return json.dumps(element).encode(\"utf-8\")\n",
    "\n",
    "options = PipelineOptions([\n",
    "    f\"--project={PROJECT_ID}\",\n",
    "    f\"--region={REGION}\",\n",
    "    \"--runner=DataflowRunner\",\n",
    "    f\"--temp_location={DATAFLOW_TEMP}\",\n",
    "    f\"--staging_location={DATAFLOW_STAGING}\",\n",
    "    f\"--job_name=pytorch-streaming-vertex-{datetime.now().strftime('%Y%m%d-%H%M%S')}\",\n",
    "    \"--streaming\",\n",
    "    \"--save_main_session=True\"\n",
    "])\n",
    "\n",
    "print(\"✅ Streaming pipeline configured\")\n",
    "print(f\"   Job will run in: {REGION}\")\n",
    "print(f\"   Staging: {DATAFLOW_STAGING}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "run",
   "metadata": {},
   "source": [
    "### Run Streaming Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run_c",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = beam.Pipeline(options=options)\n",
    "\n",
    "results = (\n",
    "    p\n",
    "    | \"Read from Pub/Sub\" >> ReadFromPubSub(subscription=INPUT_SUB)\n",
    "    | \"Parse JSON\" >> beam.Map(parse_json)\n",
    "    | \"Window (1 min)\" >> beam.WindowInto(window.FixedWindows(60))\n",
    "    | \"RunInference\" >> RunInference(model_handler)\n",
    "    | \"Format results\" >> beam.Map(format_result)\n",
    ")\n",
    "\n",
    "# Write to Pub/Sub\n",
    "_ = results | \"To JSON\" >> beam.Map(to_json) | \"Write to Pub/Sub\" >> WriteToPubSub(topic=OUTPUT_TOPIC)\n",
    "\n",
    "# Write to BigQuery\n",
    "_ = results | \"Write to BigQuery\" >> WriteToBigQuery(\n",
    "    table=f\"{PROJECT_ID}:{BQ_DATASET}.{BQ_TABLE}\",\n",
    "    write_disposition=beam.io.BigQueryDisposition.WRITE_APPEND\n",
    ")\n",
    "\n",
    "result = p.run()\n",
    "print(\"\\n✅ Streaming job started!\")\n",
    "print(f\"Monitor: https://console.cloud.google.com/dataflow/jobs/{REGION}?project={PROJECT_ID}\")\n",
    "print(\"\\n⚠️  Job will run continuously until canceled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "simulate",
   "metadata": {},
   "source": [
    "---\n",
    "## Simulate Streaming Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "simulate_c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import pubsub_v1\n",
    "import time\n",
    "\n",
    "publisher = pubsub_v1.PublisherClient()\n",
    "topic_path = publisher.topic_path(PROJECT_ID, f\"{EXPERIMENT}-input\")\n",
    "\n",
    "# Send test messages\n",
    "for i in range(5):\n",
    "    message = {\"features\": [0.1] * 30}  # Dummy transaction\n",
    "    publisher.publish(topic_path, json.dumps(message).encode(\"utf-8\"))\n",
    "    print(f\"Published message {i+1}\")\n",
    "    time.sleep(2)\n",
    "\n",
    "print(\"\\n✅ Sent 5 test messages\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "monitor",
   "metadata": {},
   "source": [
    "### Monitor Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "monitor_c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "bq = bigquery.Client(project=PROJECT_ID)\n",
    "query = f\"SELECT * FROM `{PROJECT_ID}.{BQ_DATASET}.{BQ_TABLE}` ORDER BY timestamp DESC LIMIT 10\"\n",
    "df = bq.query(query).to_dataframe()\n",
    "print(f\"Latest {len(df)} results:\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cleanup",
   "metadata": {},
   "source": [
    "---\n",
    "## Clean Up\n",
    "\n",
    "⚠️ **Important**: Cancel streaming job to stop charges\n",
    "\n",
    "```python\n",
    "# Cancel job in Cloud Console or use:\n",
    "# gcloud dataflow jobs cancel JOB_ID --region=us-central1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "✅ Built streaming Dataflow pipeline\n",
    "\n",
    "✅ Real-time RunInference with Vertex AI Endpoint\n",
    "\n",
    "✅ Windowed processing (1-min windows)\n",
    "\n",
    "✅ Dual output (Pub/Sub + BigQuery)\n",
    "\n",
    "### Next: [Vertex Endpoint Integration](./dataflow-vertex-endpoint.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}