{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84e987f3",
   "metadata": {},
   "source": [
    "![tracker](https://us-central1-vertex-ai-mlops-369716.cloudfunctions.net/pixel-tracking?path=statmike%2Fvertex-ai-mlops%2FFramework+Workflows%2FPyTorch%2Fserving&file=dataflow-streaming-runinference-vertex.ipynb)\n",
    "<!--- header table --->\n",
    "<table align=\"left\">\n",
    "<tr>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%20Workflows/PyTorch/serving/dataflow-streaming-runinference-vertex.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.svgrepo.com/download/217753/github.svg\" alt=\"GitHub logo\">\n",
    "      <br>View on<br>GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://colab.research.google.com/github/statmike/vertex-ai-mlops/blob/main/Framework%20Workflows/PyTorch/serving/dataflow-streaming-runinference-vertex.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.gstatic.com/pantheon/images/bigquery/welcome_page/colab-logo.svg\" alt=\"Google Colaboratory logo\">\n",
    "      <br>Run in<br>Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https%3A%2F%2Fraw.githubusercontent.com%2Fstatmike%2Fvertex-ai-mlops%2Fmain%2FFramework%2520Workflows%2FPyTorch%2Fserving%2Fdataflow-streaming-runinference-vertex.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\">\n",
    "      <br>Run in<br>Colab Enterprise\n",
    "    </a>\n",
    "  </td>      \n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/bigquery/import?url=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%20Workflows/PyTorch/serving/dataflow-streaming-runinference-vertex.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.gstatic.com/images/branding/gcpiconscolors/bigquery/v1/32px.svg\" alt=\"BigQuery logo\">\n",
    "      <br>Open in<br>BigQuery Studio\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/statmike/vertex-ai-mlops/main/Framework%20Workflows/PyTorch/serving/dataflow-streaming-runinference-vertex.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.gstatic.com/images/branding/gcpiconscolors/vertexai/v1/32px.svg\" alt=\"Vertex AI logo\">\n",
    "      <br>Open in<br>Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td colspan=\"5\" style=\"text-align: right\">\n",
    "    <b>Share This On: </b> \n",
    "    <a href=\"https://www.linkedin.com/sharing/share-offsite/?url=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%2520Workflows/PyTorch/serving/dataflow-streaming-runinference-vertex.ipynb\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"Linkedin Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://reddit.com/submit?url=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%2520Workflows/PyTorch/serving/dataflow-streaming-runinference-vertex.ipynb\"><img src=\"https://redditinc.com/hubfs/Reddit%20Inc/Brand/Reddit_Logo.png\" alt=\"Reddit Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://bsky.app/intent/compose?text=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%2520Workflows/PyTorch/serving/dataflow-streaming-runinference-vertex.ipynb\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"BlueSky Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://twitter.com/intent/tweet?url=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%2520Workflows/PyTorch/serving/dataflow-streaming-runinference-vertex.ipynb\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/5/5a/X_icon_2.svg\" alt=\"X (Twitter) Logo\" width=\"20px\"></a> \n",
    "  </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td colspan=\"5\" style=\"text-align: right\">\n",
    "    <b>Connect With Author On: </b> \n",
    "    <a href=\"https://www.linkedin.com/in/statmike\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"Linkedin Logo\" width=\"20px\"></a>\n",
    "    <a href=\"https://www.github.com/statmike\"><img src=\"https://www.svgrepo.com/download/217753/github.svg\" alt=\"GitHub Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://www.youtube.com/@statmike-channel\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/f/fd/YouTube_full-color_icon_%282024%29.svg\" alt=\"YouTube Logo\" width=\"20px\"></a>\n",
    "    <a href=\"https://bsky.app/profile/statmike.bsky.social\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"BlueSky Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://x.com/statmike\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/5/5a/X_icon_2.svg\" alt=\"X (Twitter) Logo\" width=\"20px\"></a>\n",
    "  </td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "i0cm6r2trpm",
   "metadata": {},
   "source": [
    "# Dataflow Streaming Inference with RunInference (Vertex AI Endpoint)\n",
    "\n",
    "This notebook demonstrates streaming processing of transactions using Dataflow with Apache Beam RunInference that calls a Vertex AI Endpoint.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "This workflow covers:\n",
    "\n",
    "1. **Configure Vertex AI Handler**: Set up VertexAIModelHandlerJSON for RunInference\n",
    "2. **Test Endpoint Health**: Verify endpoint is responding before deploying pipeline\n",
    "3. **Build Streaming Pipeline**: Read from Pub/Sub, apply model, write to BigQuery and Pub/Sub\n",
    "4. **Run on Dataflow**: Execute continuous pipeline on Google Cloud\n",
    "5. **Monitor Job**: Track streaming job progress and view results\n",
    "6. **Clean Up**: Stop streaming job to avoid ongoing charges\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Completed `dataflow-setup.ipynb` - This sets up:\n",
    "  - BigQuery tables created (including `pytorch_autoencoder_streaming_results_vertex`)\n",
    "  - Pub/Sub topics and subscriptions created\n",
    "- **Vertex AI Endpoint deployed** - Choose either:\n",
    "  - [Pre-built Container](./vertex-ai-endpoint-prebuilt-container.ipynb) - Returns full model output (13 metrics)\n",
    "  - [Custom Container](./vertex-ai-endpoint-custom-container.ipynb) - Returns simplified output (2 fields)\n",
    "\n",
    "> **Note**: This notebook works with both endpoint types and will automatically detect the output format.\n",
    "\n",
    "## Batch vs Streaming\n",
    "\n",
    "**Batch Processing (Previous Notebook)**:\n",
    "- ‚úÖ Process historical data\n",
    "- ‚úÖ Bounded dataset (has a start and end)\n",
    "- ‚úÖ Results available when job completes\n",
    "- ‚úÖ Cost-effective for large datasets\n",
    "- Example: Analyze all transactions from last month\n",
    "\n",
    "**Streaming Processing (This Notebook)**:\n",
    "- ‚úÖ Process real-time data\n",
    "- ‚úÖ Unbounded dataset (continuous)\n",
    "- ‚úÖ Results available immediately\n",
    "- ‚úÖ Low-latency anomaly detection\n",
    "- Example: Flag suspicious transactions as they occur\n",
    "\n",
    "## Architecture\n",
    "\n",
    "```\n",
    "Pub/Sub Input Topic\n",
    "  ‚Üì Read transactions in real-time\n",
    "Dataflow Pipeline\n",
    "  ‚Üì 1-minute windows\n",
    "RunInference (Vertex AI Endpoint)\n",
    "  ‚Üì Generate anomaly scores via API\n",
    "Transform Results\n",
    "  ‚Üì Extract scores and embeddings\n",
    "  ‚îú‚îÄ‚Üí BigQuery (storage & analysis)\n",
    "  ‚îî‚îÄ‚Üí Pub/Sub Output (downstream processing)\n",
    "```\n",
    "\n",
    "## RunInference with Vertex AI Benefits\n",
    "\n",
    "- **Managed endpoint**: No model loading in workers\n",
    "- **Automatic batching**: Combines instances for efficient inference\n",
    "- **Scalable**: Endpoint scales independently from pipeline\n",
    "- **Flexible**: Update model without redeploying pipeline\n",
    "- **Real-time**: Low-latency predictions for streaming data\n",
    "\n",
    "## Timing Expectations\n",
    "\n",
    "**Total time from start to results: ~8-10 minutes**\n",
    "\n",
    "1. **Start Dataflow job**: Instant\n",
    "2. **Wait for workers**: 3-5 minutes (worker provisioning)\n",
    "3. **Send test messages**: Instant\n",
    "4. **Wait for processing**: 2-3 minutes (1-min window + processing)\n",
    "5. **View results**: Check BigQuery and Pub/Sub\n",
    "\n",
    "## What This Pipeline Does\n",
    "\n",
    "1. Read transactions from Pub/Sub input topic\n",
    "2. Window data into 1-minute batches\n",
    "3. Format data for Vertex AI (extract features array)\n",
    "4. Call Vertex AI Endpoint via RunInference\n",
    "5. Extract relevant outputs (score + embeddings)\n",
    "6. Write results to BigQuery (for analysis)\n",
    "7. Publish to Pub/Sub output (for downstream systems)\n",
    "8. Job runs continuously until cancelled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "env",
   "metadata": {},
   "source": [
    "---\n",
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "proj",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = 'statmike-mlops-349915'\n",
    "REQ_TYPE = 'ALL'\n",
    "INSTALL_TOOL = 'poetry'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "conf",
   "metadata": {},
   "outputs": [],
   "source": [
    "REQUIREMENTS_URL = 'https://raw.githubusercontent.com/statmike/vertex-ai-mlops/refs/heads/main/Framework%20Workflows/PyTorch/requirements.txt'\n",
    "REQUIRED_APIS = [\"dataflow.googleapis.com\", \"pubsub.googleapis.com\", \"bigquery.googleapis.com\", \"storage.googleapis.com\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "setup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "PYTHON GCP ENVIRONMENT SETUP\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "AUTHENTICATION\n",
      "==================================================\n",
      "Checking for existing ADC...\n",
      "‚úÖ Existing ADC found.\n",
      "‚úÖ Project is correctly set to 'statmike-mlops-349915'.\n",
      "\n",
      "==================================================\n",
      "API CHECK & ENABLE\n",
      "==================================================\n",
      "‚úÖ dataflow.googleapis.com is already enabled.\n",
      "‚úÖ pubsub.googleapis.com is already enabled.\n",
      "‚úÖ bigquery.googleapis.com is already enabled.\n",
      "‚úÖ storage.googleapis.com is already enabled.\n",
      "\n",
      "==================================================\n",
      "PACKAGE MANAGEMENT\n",
      "==================================================\n",
      "Installation Tool: poetry\n",
      "‚úÖ Found poetry at: /usr/local/google/home/statmike/.local/bin/poetry\n",
      "‚úÖ Running in poetry environment: /usr/local/google/home/statmike/.cache/pypoetry/virtualenvs/frameworks-pytorch-0KVJlKeQ-py3.13\n",
      "‚ÑπÔ∏è  Poetry mode: Installing from pyproject.toml (REQUIREMENTS_URL ignored)\n",
      "‚úÖ Found pyproject.toml at: /usr/local/google/home/statmike/Git/vertex-ai-mlops/Framework Workflows/PyTorch/pyproject.toml\n",
      "   Changed working directory to: /usr/local/google/home/statmike/Git/vertex-ai-mlops/Framework Workflows/PyTorch\n",
      "Running poetry install...\n",
      "   Restored working directory to: /usr/local/google/home/statmike/Git/vertex-ai-mlops/Framework Workflows/PyTorch/serving\n",
      "‚úÖ All packages are already installed and up to date.\n",
      "\n",
      "==================================================\n",
      "Google Cloud Project Information\n",
      "==================================================\n",
      "PROJECT_ID     = statmike-mlops-349915\n",
      "PROJECT_NUMBER = 1026793852137\n",
      "==================================================\n",
      "\n",
      "\n",
      "==================================================\n",
      "SETUP SUMMARY\n",
      "==================================================\n",
      "‚úÖ Authentication:    Success\n",
      "‚úÖ API Configuration: Success\n",
      "‚úÖ Package Install:   Already up to date\n",
      "‚úÖ Installation Tool: poetry\n",
      "‚úÖ Project ID:        statmike-mlops-349915\n",
      "‚úÖ Project Number:    1026793852137\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os, urllib.request\n",
    "url = 'https://raw.githubusercontent.com/statmike/vertex-ai-mlops/refs/heads/main/core/notebook-template/python_setup.py'\n",
    "urllib.request.urlretrieve(url, 'python_setup_local.py')\n",
    "import python_setup_local as python_setup\n",
    "os.remove('python_setup_local.py')\n",
    "setup_info = python_setup.setup_environment(PROJECT_ID, REQ_TYPE, REQUIREMENTS_URL, REQUIRED_APIS, INSTALL_TOOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "py",
   "metadata": {},
   "source": [
    "---\n",
    "## Python Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "imp",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/google/home/statmike/.cache/pypoetry/virtualenvs/frameworks-pytorch-0KVJlKeQ-py3.13/lib/python3.13/site-packages/google/cloud/aiplatform/models.py:52: FutureWarning: Support for google-cloud-storage < 3.0.0 will be removed in a future version of google-cloud-aiplatform. Please upgrade to google-cloud-storage >= 3.0.0.\n",
      "  from google.cloud.aiplatform.utils import gcs_utils\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import apache_beam as beam\n",
    "from apache_beam import window\n",
    "from apache_beam.ml.inference.base import RunInference\n",
    "from apache_beam.ml.inference.vertex_ai_inference import VertexAIModelHandlerJSON\n",
    "from apache_beam.options.pipeline_options import PipelineOptions, StandardOptions\n",
    "from apache_beam.io.gcp.pubsub import ReadFromPubSub, WriteToPubSub\n",
    "from apache_beam.io.gcp.bigquery import WriteToBigQuery\n",
    "import json\n",
    "from datetime import datetime\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud import pubsub_v1\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "vars",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking up endpoint: pytorch-autoencoder-endpoint\n",
      "‚úÖ Found endpoint: pytorch-autoencoder-endpoint\n",
      "   Endpoint ID: 2741468416626917376\n",
      "   Full resource name: projects/1026793852137/locations/us-central1/endpoints/2741468416626917376\n",
      "\n",
      "Configuration:\n",
      "  Project: statmike-mlops-349915\n",
      "  Endpoint: pytorch-autoencoder-endpoint (ID: 2741468416626917376)\n",
      "  Input subscription: projects/statmike-mlops-349915/subscriptions/pytorch-autoencoder-input-sub\n",
      "  Output topic: projects/statmike-mlops-349915/topics/pytorch-autoencoder-output\n",
      "  Results table: statmike-mlops-349915.frameworks.pytorch_autoencoder_streaming_results_vertex\n"
     ]
    }
   ],
   "source": [
    "PROJECT_ID = subprocess.run([\"gcloud\", \"config\", \"get-value\", \"project\"], capture_output=True, text=True, check=True).stdout.strip()\n",
    "\n",
    "# Configuration\n",
    "REGION = \"us-central1\"\n",
    "SERIES = \"frameworks\"\n",
    "EXPERIMENT = \"pytorch-autoencoder\"\n",
    "\n",
    "# ========================================\n",
    "# ENDPOINT SELECTION\n",
    "# ========================================\n",
    "# Choose which endpoint to use:\n",
    "# - \"pytorch-autoencoder-endpoint\" (pre-built container - returns 13 metrics)\n",
    "# - \"pytorch-autoencoder-custom-endpoint\" (custom container - returns 2 fields)\n",
    "ENDPOINT_DISPLAY_NAME = \"pytorch-autoencoder-endpoint\"  # Change to \"pytorch-autoencoder-custom-endpoint\" if using custom container\n",
    "\n",
    "# Get endpoint ID from display name\n",
    "# VertexAIModelHandlerJSON requires the numeric endpoint ID, not the display name\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION)\n",
    "print(f\"Looking up endpoint: {ENDPOINT_DISPLAY_NAME}\")\n",
    "endpoints = aiplatform.Endpoint.list(\n",
    "    filter=f'display_name=\"{ENDPOINT_DISPLAY_NAME}\"',\n",
    "    order_by=\"create_time desc\"\n",
    ")\n",
    "\n",
    "if not endpoints:\n",
    "    raise ValueError(\n",
    "        f\"No endpoint found with display name '{ENDPOINT_DISPLAY_NAME}' in region {REGION}.\\n\"\n",
    "        f\"Please deploy an endpoint first using either:\\n\"\n",
    "        f\"  - vertex-ai-endpoint-prebuilt-container.ipynb\\n\"\n",
    "        f\"  - vertex-ai-endpoint-custom-container.ipynb\"\n",
    "    )\n",
    "\n",
    "endpoint = endpoints[0]\n",
    "ENDPOINT_ID = endpoint.name.split(\"/\")[-1]\n",
    "\n",
    "print(f\"‚úÖ Found endpoint: {ENDPOINT_DISPLAY_NAME}\")\n",
    "print(f\"   Endpoint ID: {ENDPOINT_ID}\")\n",
    "print(f\"   Full resource name: {endpoint.resource_name}\")\n",
    "\n",
    "# GCS paths\n",
    "BUCKET_NAME = PROJECT_ID\n",
    "BUCKET_URI = f\"gs://{BUCKET_NAME}\"\n",
    "MODEL_DIR = f\"{BUCKET_URI}/{SERIES}/{EXPERIMENT}\"\n",
    "\n",
    "# Pub/Sub configuration\n",
    "INPUT_SUB = f\"projects/{PROJECT_ID}/subscriptions/{EXPERIMENT}-input-sub\"\n",
    "OUTPUT_TOPIC = f\"projects/{PROJECT_ID}/topics/{EXPERIMENT}-output\"\n",
    "\n",
    "# BigQuery configuration\n",
    "BQ_DATASET = SERIES.replace(\"-\", \"_\")\n",
    "BQ_TABLE = f\"{EXPERIMENT.replace('-', '_')}_streaming_results_vertex\"  # Separate table for Vertex endpoint results\n",
    "\n",
    "# Dataflow configuration\n",
    "DATAFLOW_STAGING = f\"{BUCKET_URI}/dataflow/staging\"\n",
    "DATAFLOW_TEMP = f\"{BUCKET_URI}/dataflow/temp\"\n",
    "\n",
    "print(f\"\\nConfiguration:\")\n",
    "print(f\"  Project: {PROJECT_ID}\")\n",
    "print(f\"  Endpoint: {ENDPOINT_DISPLAY_NAME} (ID: {ENDPOINT_ID})\")\n",
    "print(f\"  Input subscription: {INPUT_SUB}\")\n",
    "print(f\"  Output topic: {OUTPUT_TOPIC}\")\n",
    "print(f\"  Results table: {PROJECT_ID}.{BQ_DATASET}.{BQ_TABLE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7w88tg0mamc",
   "metadata": {},
   "source": [
    "---\n",
    "## Test Endpoint Health\n",
    "\n",
    "Before using the endpoint in the pipeline, verify it's responding correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1n0ud7b6gx2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing endpoint health...\n",
      "Endpoint: pytorch-autoencoder-endpoint (ID: 2741468416626917376)\n",
      "============================================================\n",
      "Status Code: 200\n",
      "‚úÖ Endpoint is healthy and responding!\n",
      "\n",
      "Sample prediction:\n",
      "  Anomaly Score (MAE): 2647.89\n",
      "  Encoded (4D): [0, 0, 0.4922903776168823, 0]...\n",
      "\n",
      "  Total fields in response: 13\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from google.auth import default\n",
    "from google.auth.transport.requests import Request\n",
    "\n",
    "# Get credentials\n",
    "credentials, project = default()\n",
    "credentials.refresh(Request())\n",
    "\n",
    "# Prepare test prediction\n",
    "url = f\"https://{REGION}-aiplatform.googleapis.com/v1/projects/{PROJECT_ID}/locations/{REGION}/endpoints/{ENDPOINT_ID}:predict\"\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {credentials.token}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "# Test data - same format as pipeline will send\n",
    "test_data = {\n",
    "    \"instances\": [[0.1] * 30]  # Single transaction with 30 features\n",
    "}\n",
    "\n",
    "print(\"Testing endpoint health...\")\n",
    "print(f\"Endpoint: {ENDPOINT_DISPLAY_NAME} (ID: {ENDPOINT_ID})\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    response = requests.post(url, headers=headers, json=test_data, timeout=30)\n",
    "    \n",
    "    print(f\"Status Code: {response.status_code}\")\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        result = response.json()\n",
    "        print(\"‚úÖ Endpoint is healthy and responding!\")\n",
    "        print(f\"\\nSample prediction:\")\n",
    "        \n",
    "        # Handle both output formats\n",
    "        predictions = result.get(\"predictions\", [])\n",
    "        if predictions:\n",
    "            pred = predictions[0]\n",
    "            if \"anomaly_score\" in pred:\n",
    "                print(f\"  Anomaly Score: {pred['anomaly_score']:.2f}\")\n",
    "            elif \"denormalized_MAE\" in pred:\n",
    "                print(f\"  Anomaly Score (MAE): {pred['denormalized_MAE']:.2f}\")\n",
    "            \n",
    "            if \"encoded\" in pred:\n",
    "                print(f\"  Encoded (4D): {pred['encoded'][:4]}...\")\n",
    "            \n",
    "            print(f\"\\n  Total fields in response: {len(pred)}\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"‚ùå Endpoint returned error: {response.status_code}\")\n",
    "        print(f\"Response: {response.text}\")\n",
    "        print(\"\\n‚ö†Ô∏è  The pipeline will fail with this endpoint.\")\n",
    "        print(\"   Please check your endpoint deployment before continuing.\")\n",
    "        \n",
    "except requests.exceptions.Timeout:\n",
    "    print(\"‚ùå Request timed out after 30 seconds\")\n",
    "    print(\"   The endpoint may be cold-starting or unresponsive.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error testing endpoint: {e}\")\n",
    "    print(\"   Please verify the endpoint is deployed and healthy.\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handler",
   "metadata": {},
   "source": [
    "---\n",
    "## Create ModelHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "handler_c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ModelHandler created for endpoint: pytorch-autoencoder-endpoint (ID: 2741468416626917376)\n"
     ]
    }
   ],
   "source": [
    "model_handler = VertexAIModelHandlerJSON(\n",
    "    endpoint_id=ENDPOINT_ID,  # Use numeric endpoint ID, not display name\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION\n",
    ")\n",
    "print(f\"‚úÖ ModelHandler created for endpoint: {ENDPOINT_DISPLAY_NAME} (ID: {ENDPOINT_ID})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pipeline",
   "metadata": {},
   "source": [
    "---\n",
    "## Configure Worker Compute Resources\n",
    "\n",
    "Before building the pipeline, configure the compute resources (machine type and autoscaling) for Dataflow workers. These settings directly impact performance, cost, and latency when calling Vertex AI endpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36fsmm76zsp",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "WORKER COMPUTE CONFIGURATION (Vertex AI Endpoint)\n",
      "============================================================\n",
      "Machine Type: n1-standard-4\n",
      "  - vCPUs: 4\n",
      "  - Memory: 15 GB\n",
      "\n",
      "Autoscaling:\n",
      "  - Min Workers: 2\n",
      "  - Max Workers: 20\n",
      "\n",
      "Workload Type: API-based inference\n",
      "  - Workers make HTTP calls to Vertex AI endpoint\n",
      "  - Throughput limited by endpoint capacity, not worker compute\n",
      "  - No local model loading required\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Worker Compute Configuration\n",
    "# =============================\n",
    "# These settings control the machine type and autoscaling behavior for Dataflow workers.\n",
    "# For Vertex AI endpoint workflows, workers make API calls rather than loading models locally.\n",
    "\n",
    "# Machine Type: n1-standard-4\n",
    "# - 4 vCPUs, 15 GB memory\n",
    "# - Suitable for making concurrent API calls to Vertex AI endpoints\n",
    "# - Each worker can handle multiple parallel requests\n",
    "# - Alternative: n1-standard-2 (smaller), n1-standard-8 (larger) depending on concurrency needs\n",
    "\n",
    "MACHINE_TYPE = \"n1-standard-4\"\n",
    "\n",
    "# Autoscaling for Streaming Pipelines (with Vertex AI Endpoint)\n",
    "# - min_workers=2: Ensures pipeline remains responsive even with low traffic\n",
    "# - max_workers=20: Handles traffic spikes without overwhelming the endpoint\n",
    "# - Dataflow autoscales based on Pub/Sub backlog and processing latency\n",
    "\n",
    "MIN_WORKERS = 2\n",
    "MAX_WORKERS = 20\n",
    "\n",
    "# Why These Settings for Streaming + Vertex Endpoint?\n",
    "# ----------------------------------------------------\n",
    "# 1. **Minimum Workers (2)**:\n",
    "#    - Provides redundancy (if one worker fails, pipeline continues)\n",
    "#    - Reduces cold start latency when traffic arrives\n",
    "#    - Maintains low end-to-end latency for real-time processing\n",
    "#\n",
    "# 2. **Maximum Workers (20)**:\n",
    "#    - Allows scaling to handle traffic bursts\n",
    "#    - Each worker makes concurrent API calls to Vertex endpoint\n",
    "#    - Throughput depends on endpoint capacity, not worker compute\n",
    "#    - 20 workers prevent overwhelming the endpoint with too many parallel requests\n",
    "#\n",
    "# 3. **Machine Type (n1-standard-4)**:\n",
    "#    - Workers don't load PyTorch models (endpoint handles inference)\n",
    "#    - Memory requirements are lower than local model loading\n",
    "#    - 4 vCPUs enable good parallelism for API call concurrency\n",
    "#    - Cost-effective for API-based inference workflows\n",
    "\n",
    "# Key Difference from Local Model Inference:\n",
    "# ------------------------------------------\n",
    "# - **Local Model**: Workers need high memory (15GB+) to load PyTorch models\n",
    "# - **Vertex Endpoint**: Workers only need memory for API client and data buffering\n",
    "# - **Throughput**: Limited by endpoint capacity, not worker compute\n",
    "# - **Scaling**: Can use smaller machine types (n1-standard-2) if API latency is low\n",
    "\n",
    "# When to Adjust These Settings:\n",
    "# -------------------------------\n",
    "# - **Higher Traffic**: Increase max_workers (e.g., 50-100) if endpoint can handle load\n",
    "# - **Lower Latency**: Increase min_workers (e.g., 5-10) to pre-warm capacity\n",
    "# - **Cost Optimization**: Use n1-standard-2 for lower concurrency needs\n",
    "# - **High Concurrency**: Use n1-standard-8 for more parallel API calls per worker\n",
    "# - **Endpoint Throttling**: Reduce max_workers to prevent overwhelming the endpoint\n",
    "\n",
    "# GPU Support:\n",
    "# -----------\n",
    "# Not applicable for Vertex AI endpoint workflows.\n",
    "# GPU inference happens at the endpoint, not in Dataflow workers.\n",
    "# Workers only make HTTP API calls to the endpoint.\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"WORKER COMPUTE CONFIGURATION (Vertex AI Endpoint)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Machine Type: {MACHINE_TYPE}\")\n",
    "print(f\"  - vCPUs: 4\")\n",
    "print(f\"  - Memory: 15 GB\")\n",
    "print(f\"\\nAutoscaling:\")\n",
    "print(f\"  - Min Workers: {MIN_WORKERS}\")\n",
    "print(f\"  - Max Workers: {MAX_WORKERS}\")\n",
    "print(f\"\\nWorkload Type: API-based inference\")\n",
    "print(f\"  - Workers make HTTP calls to Vertex AI endpoint\")\n",
    "print(f\"  - Throughput limited by endpoint capacity, not worker compute\")\n",
    "print(f\"  - No local model loading required\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "pipeline_c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Streaming pipeline configured\n",
      "   Job will run in: us-central1\n",
      "   Staging: gs://statmike-mlops-349915/dataflow/staging\n",
      "   Machine type: n1-standard-4\n",
      "   Worker scaling: 2-20 workers\n"
     ]
    }
   ],
   "source": [
    "def parse_json(message):\n",
    "    \"\"\"Parse Pub/Sub message and extract features\"\"\"\n",
    "    data = json.loads(message.decode(\"utf-8\"))\n",
    "    # Return just the features - VertexAIModelHandlerJSON will wrap in {\"instances\": ...}\n",
    "    return data[\"features\"]\n",
    "\n",
    "def format_result(element, window=beam.DoFn.WindowParam):\n",
    "    \"\"\"\n",
    "    Format for Pub/Sub and BigQuery.\n",
    "    \n",
    "    Handles both endpoint output formats:\n",
    "    - Pre-built container: 13 keys (denormalized_MAE, encoded, etc.)\n",
    "    - Custom container: 2 keys (anomaly_score, encoded)\n",
    "    \"\"\"\n",
    "    prediction = element[1]\n",
    "    \n",
    "    # Auto-detect output format and extract anomaly score\n",
    "    if \"anomaly_score\" in prediction:\n",
    "        # Custom container format\n",
    "        anomaly_score = prediction[\"anomaly_score\"]\n",
    "    elif \"denormalized_MAE\" in prediction:\n",
    "        # Pre-built container format\n",
    "        anomaly_score = prediction[\"denormalized_MAE\"]\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown prediction format: {prediction.keys()}\")\n",
    "    \n",
    "    return {\n",
    "        \"instance_id\": str(hash(str(element[0]))),\n",
    "        \"anomaly_score\": anomaly_score,\n",
    "        \"encoded\": prediction[\"encoded\"],\n",
    "        \"timestamp\": datetime.utcnow().isoformat(),\n",
    "        \"window_start\": window.start.to_utc_datetime().isoformat(),\n",
    "        \"window_end\": window.end.to_utc_datetime().isoformat()\n",
    "    }\n",
    "\n",
    "def to_json(element):\n",
    "    \"\"\"Convert to JSON for Pub/Sub\"\"\"\n",
    "    return json.dumps(element).encode(\"utf-8\")\n",
    "\n",
    "options = PipelineOptions([\n",
    "    f\"--project={PROJECT_ID}\",\n",
    "    f\"--region={REGION}\",\n",
    "    \"--runner=DataflowRunner\",\n",
    "    f\"--temp_location={DATAFLOW_TEMP}\",\n",
    "    f\"--staging_location={DATAFLOW_STAGING}\",\n",
    "    f\"--job_name=pytorch-streaming-vertex-{datetime.now().strftime('%Y%m%d-%H%M%S')}\",\n",
    "    \"--streaming\",\n",
    "    \"--save_main_session\",  # Serialize global imports and variables\n",
    "    # Worker compute configuration\n",
    "    f\"--machine_type={MACHINE_TYPE}\",  # Machine type for workers\n",
    "    f\"--num_workers={MIN_WORKERS}\",  # Initial/minimum number of workers\n",
    "    f\"--max_num_workers={MAX_WORKERS}\",  # Maximum workers for autoscaling\n",
    "])\n",
    "\n",
    "print(\"‚úÖ Streaming pipeline configured\")\n",
    "print(f\"   Job will run in: {REGION}\")\n",
    "print(f\"   Staging: {DATAFLOW_STAGING}\")\n",
    "print(f\"   Machine type: {MACHINE_TYPE}\")\n",
    "print(f\"   Worker scaling: {MIN_WORKERS}-{MAX_WORKERS} workers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "run",
   "metadata": {},
   "source": [
    "### Run Streaming Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "run_c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:apache_beam.options.pipeline_options:Bucket specified in temp_location has soft-delete policy enabled. To avoid being billed for unnecessary storage costs, turn off the soft delete feature on buckets that your Dataflow jobs use for temporary and staging storage. For more information, see https://cloud.google.com/storage/docs/use-soft-delete#remove-soft-delete-policy.\n",
      "WARNING:apache_beam.options.pipeline_options:Bucket specified in staging_location has soft-delete policy enabled. To avoid being billed for unnecessary storage costs, turn off the soft delete feature on buckets that your Dataflow jobs use for temporary and staging storage. For more information, see https://cloud.google.com/storage/docs/use-soft-delete#remove-soft-delete-policy.\n",
      "WARNING:apache_beam.runners.interactive.interactive_environment:Dependencies required for Interactive Beam PCollection visualization are not available, please use: `pip install apache-beam[interactive]` to install necessary dependencies to enable all data visualization features.\n"
     ]
    },
    {
     "data": {
      "application/javascript": "\n        if (typeof window.interactive_beam_jquery == 'undefined') {\n          var jqueryScript = document.createElement('script');\n          jqueryScript.src = 'https://code.jquery.com/jquery-3.4.1.slim.min.js';\n          jqueryScript.type = 'text/javascript';\n          jqueryScript.onload = function() {\n            var datatableScript = document.createElement('script');\n            datatableScript.src = 'https://cdn.datatables.net/1.10.20/js/jquery.dataTables.min.js';\n            datatableScript.type = 'text/javascript';\n            datatableScript.onload = function() {\n              window.interactive_beam_jquery = jQuery.noConflict(true);\n              window.interactive_beam_jquery(document).ready(function($){\n                \n              });\n            }\n            document.head.appendChild(datatableScript);\n          };\n          document.head.appendChild(jqueryScript);\n        } else {\n          window.interactive_beam_jquery(document).ready(function($){\n            \n          });\n        }"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    div.alert {\n",
       "      white-space: pre-line;\n",
       "    }\n",
       "  </style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <link rel=\"stylesheet\" href=\"https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/css/bootstrap.min.css\" integrity=\"sha384-Vkoo8x4CGsO3+Hhxv8T/Q5PaXtkKtu6ug5TOeNV6gBiFeWPGFN9MuhOf23Q9Ifjh\" crossorigin=\"anonymous\">\n",
       "            <div class=\"alert alert-info\">No cache_root detected. Defaulting to staging_location gs://statmike-mlops-349915/dataflow/staging for cache location.</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:apache_beam.transforms.core:('No iterator is returned by the process method in %s.', <class 'apache_beam.io.gcp.pubsub._PubSubWriteDoFn'>)\n",
      "WARNING:apache_beam.options.pipeline_options:Bucket specified in temp_location has soft-delete policy enabled. To avoid being billed for unnecessary storage costs, turn off the soft delete feature on buckets that your Dataflow jobs use for temporary and staging storage. For more information, see https://cloud.google.com/storage/docs/use-soft-delete#remove-soft-delete-policy.\n",
      "WARNING:apache_beam.options.pipeline_options:Bucket specified in staging_location has soft-delete policy enabled. To avoid being billed for unnecessary storage costs, turn off the soft delete feature on buckets that your Dataflow jobs use for temporary and staging storage. For more information, see https://cloud.google.com/storage/docs/use-soft-delete#remove-soft-delete-policy.\n",
      "WARNING:apache_beam.transforms.core:('No iterator is returned by the process method in %s.', <class 'apache_beam.io.gcp.pubsub._PubSubWriteDoFn'>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Streaming job started!\n",
      "Monitor: https://console.cloud.google.com/dataflow/jobs/us-central1?project=statmike-mlops-349915\n",
      "\n",
      "============================================================\n",
      "‚è≥ IMPORTANT: Wait 3-5 minutes for Dataflow workers to start\n",
      "============================================================\n",
      "The pipeline needs time to:\n",
      "  1. Provision workers (2-3 minutes)\n",
      "  2. Initialize the environment\n",
      "  3. Connect to Pub/Sub subscription\n",
      "\n",
      "Once workers are running, you can send test data.\n",
      "Check the Dataflow console to see when workers are ready.\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "p = beam.Pipeline(options=options)\n",
    "\n",
    "results = (\n",
    "    p\n",
    "    | \"Read from Pub/Sub\" >> ReadFromPubSub(subscription=INPUT_SUB)\n",
    "    | \"Parse JSON\" >> beam.Map(parse_json)\n",
    "    | \"Window (1 min)\" >> beam.WindowInto(window.FixedWindows(60))\n",
    "    | \"RunInference\" >> RunInference(model_handler)\n",
    "    | \"Format results\" >> beam.Map(format_result)\n",
    ")\n",
    "\n",
    "# Write to Pub/Sub\n",
    "_ = results | \"To JSON\" >> beam.Map(to_json) | \"Write to Pub/Sub\" >> WriteToPubSub(topic=OUTPUT_TOPIC)\n",
    "\n",
    "# Write to BigQuery\n",
    "_ = results | \"Write to BigQuery\" >> WriteToBigQuery(\n",
    "    table=f\"{PROJECT_ID}:{BQ_DATASET}.{BQ_TABLE}\",\n",
    "    write_disposition=beam.io.BigQueryDisposition.WRITE_APPEND\n",
    ")\n",
    "\n",
    "result = p.run()\n",
    "print(\"\\n‚úÖ Streaming job started!\")\n",
    "print(f\"Monitor: https://console.cloud.google.com/dataflow/jobs/{REGION}?project={PROJECT_ID}\")\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚è≥ IMPORTANT: Wait 3-5 minutes for Dataflow workers to start\")\n",
    "print(\"=\" * 60)\n",
    "print(\"The pipeline needs time to:\")\n",
    "print(\"  1. Provision workers (2-3 minutes)\")\n",
    "print(\"  2. Initialize the environment\")\n",
    "print(\"  3. Connect to Pub/Sub subscription\")\n",
    "print(\"\\nOnce workers are running, you can send test data.\")\n",
    "print(\"Check the Dataflow console to see when workers are ready.\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "simulate",
   "metadata": {},
   "source": [
    "---\n",
    "## Simulate Streaming Data\n",
    "\n",
    "**‚ö†Ô∏è Wait 3-5 minutes** after starting the Dataflow job before running this cell. Check the [Dataflow Console](https://console.cloud.google.com/dataflow/jobs/us-central1?project=statmike-mlops-349915) to verify workers are running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b685609c",
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(5*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "simulate_c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Published message 1\n",
      "Published message 2\n",
      "Published message 3\n",
      "Published message 4\n",
      "Published message 5\n",
      "\n",
      "‚úÖ Sent 5 test messages\n"
     ]
    }
   ],
   "source": [
    "publisher = pubsub_v1.PublisherClient()\n",
    "topic_path = publisher.topic_path(PROJECT_ID, f\"{EXPERIMENT}-input\")\n",
    "\n",
    "# Send test messages\n",
    "for i in range(5):\n",
    "    message = {\"features\": [0.1] * 30}  # Dummy transaction\n",
    "    publisher.publish(topic_path, json.dumps(message).encode(\"utf-8\"))\n",
    "    print(f\"Published message {i+1}\")\n",
    "    time.sleep(2)\n",
    "\n",
    "print(\"\\n‚úÖ Sent 5 test messages\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "monitor",
   "metadata": {},
   "source": [
    "### Monitor Results\n",
    "\n",
    "**‚è≥ Wait 2-3 minutes** after sending test messages for the pipeline to process them.\n",
    "\n",
    "Monitor results from both output destinations: BigQuery (storage/analysis) and Pub/Sub (downstream processing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8dd014a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "monitor_c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "BIGQUERY RESULTS (Storage & Analysis)\n",
      "============================================================\n",
      "‚úÖ Found 5 results in BigQuery\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instance_id</th>\n",
       "      <th>anomaly_score</th>\n",
       "      <th>encoded</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>window_start</th>\n",
       "      <th>window_end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1873740870990298768</td>\n",
       "      <td>2647.894531</td>\n",
       "      <td>[0.0, 0.0, 0.4922903776168823, 0.0]</td>\n",
       "      <td>2025-11-07 22:38:10.691485+00:00</td>\n",
       "      <td>2025-11-07 22:38:00+00:00</td>\n",
       "      <td>2025-11-07 22:39:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-6251579571237998899</td>\n",
       "      <td>2647.894531</td>\n",
       "      <td>[0.0, 0.0, 0.4922903776168823, 0.0]</td>\n",
       "      <td>2025-11-07 22:38:09.036252+00:00</td>\n",
       "      <td>2025-11-07 22:38:00+00:00</td>\n",
       "      <td>2025-11-07 22:39:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1873740870990298768</td>\n",
       "      <td>2647.894531</td>\n",
       "      <td>[0.0, 0.0, 0.4922903776168823, 0.0]</td>\n",
       "      <td>2025-11-07 22:37:38.122867+00:00</td>\n",
       "      <td>2025-11-07 22:37:00+00:00</td>\n",
       "      <td>2025-11-07 22:38:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-6251579571237998899</td>\n",
       "      <td>2647.894531</td>\n",
       "      <td>[0.0, 0.0, 0.4922903776168823, 0.0]</td>\n",
       "      <td>2025-11-07 22:37:36.962659+00:00</td>\n",
       "      <td>2025-11-07 22:37:00+00:00</td>\n",
       "      <td>2025-11-07 22:38:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1873740870990298768</td>\n",
       "      <td>2647.894531</td>\n",
       "      <td>[0.0, 0.0, 0.4922903776168823, 0.0]</td>\n",
       "      <td>2025-11-07 22:37:35.574497+00:00</td>\n",
       "      <td>2025-11-07 22:37:00+00:00</td>\n",
       "      <td>2025-11-07 22:38:00+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            instance_id  anomaly_score                              encoded  \\\n",
       "0  -1873740870990298768    2647.894531  [0.0, 0.0, 0.4922903776168823, 0.0]   \n",
       "1  -6251579571237998899    2647.894531  [0.0, 0.0, 0.4922903776168823, 0.0]   \n",
       "2  -1873740870990298768    2647.894531  [0.0, 0.0, 0.4922903776168823, 0.0]   \n",
       "3  -6251579571237998899    2647.894531  [0.0, 0.0, 0.4922903776168823, 0.0]   \n",
       "4  -1873740870990298768    2647.894531  [0.0, 0.0, 0.4922903776168823, 0.0]   \n",
       "\n",
       "                         timestamp              window_start  \\\n",
       "0 2025-11-07 22:38:10.691485+00:00 2025-11-07 22:38:00+00:00   \n",
       "1 2025-11-07 22:38:09.036252+00:00 2025-11-07 22:38:00+00:00   \n",
       "2 2025-11-07 22:37:38.122867+00:00 2025-11-07 22:37:00+00:00   \n",
       "3 2025-11-07 22:37:36.962659+00:00 2025-11-07 22:37:00+00:00   \n",
       "4 2025-11-07 22:37:35.574497+00:00 2025-11-07 22:37:00+00:00   \n",
       "\n",
       "                 window_end  \n",
       "0 2025-11-07 22:39:00+00:00  \n",
       "1 2025-11-07 22:39:00+00:00  \n",
       "2 2025-11-07 22:38:00+00:00  \n",
       "3 2025-11-07 22:38:00+00:00  \n",
       "4 2025-11-07 22:38:00+00:00  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "PUB/SUB OUTPUT TOPIC (Downstream Processing)\n",
      "============================================================\n",
      "Pulling messages from: projects/statmike-mlops-349915/subscriptions/pytorch-autoencoder-output-sub\n",
      "‚úÖ Found 5 messages in output topic\n",
      "\n",
      "Sample messages:\n",
      "\n",
      "Message 1:\n",
      "  Anomaly Score: 2647.89453125\n",
      "  Instance ID: -1873740870990298768\n",
      "  Timestamp: 2025-11-07T22:37:35.574497\n",
      "\n",
      "Message 2:\n",
      "  Anomaly Score: 2647.89453125\n",
      "  Instance ID: -1873740870990298768\n",
      "  Timestamp: 2025-11-07T22:38:10.691485\n",
      "\n",
      "Message 3:\n",
      "  Anomaly Score: 2647.89453125\n",
      "  Instance ID: 3044125753005103097\n",
      "  Timestamp: 2025-11-07T22:38:12.624519\n",
      "\n",
      "‚úÖ Acknowledged 5 messages\n",
      "\n",
      "============================================================\n",
      "üí° Pipeline Status Summary\n",
      "============================================================\n",
      "‚úÖ Pipeline is working correctly!\n",
      "   Total results in BigQuery: 5\n",
      "   Results are being written to both:\n",
      "     - BigQuery table: statmike-mlops-349915.frameworks.pytorch_autoencoder_streaming_results_vertex\n",
      "     - Pub/Sub topic: projects/statmike-mlops-349915/topics/pytorch-autoencoder-output\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import bigquery\n",
    "\n",
    "# Monitor BigQuery results\n",
    "print(\"=\" * 60)\n",
    "print(\"BIGQUERY RESULTS (Storage & Analysis)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "bq = bigquery.Client(project=PROJECT_ID)\n",
    "query = f\"SELECT * FROM `{PROJECT_ID}.{BQ_DATASET}.{BQ_TABLE}` ORDER BY timestamp DESC LIMIT 10\"\n",
    "df = bq.query(query).to_dataframe()\n",
    "\n",
    "if len(df) > 0:\n",
    "    print(f\"‚úÖ Found {len(df)} results in BigQuery\")\n",
    "    display(df)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No results yet in BigQuery\")\n",
    "    print(\"   Wait a few minutes for the pipeline to process data\")\n",
    "\n",
    "# Monitor Pub/Sub output topic\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PUB/SUB OUTPUT TOPIC (Downstream Processing)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "subscriber = pubsub_v1.SubscriberClient()\n",
    "output_sub_path = subscriber.subscription_path(PROJECT_ID, f\"{EXPERIMENT}-output-sub\")\n",
    "\n",
    "print(f\"Pulling messages from: {output_sub_path}\")\n",
    "\n",
    "# Pull messages from output subscription\n",
    "try:\n",
    "    response = subscriber.pull(\n",
    "        request={\"subscription\": output_sub_path, \"max_messages\": 5},\n",
    "        timeout=10\n",
    "    )\n",
    "    \n",
    "    if response.received_messages:\n",
    "        print(f\"‚úÖ Found {len(response.received_messages)} messages in output topic\")\n",
    "        print(\"\\nSample messages:\")\n",
    "        for i, msg in enumerate(response.received_messages[:3], 1):\n",
    "            data = json.loads(msg.message.data.decode(\"utf-8\"))\n",
    "            print(f\"\\nMessage {i}:\")\n",
    "            print(f\"  Anomaly Score: {data.get('anomaly_score', 'N/A')}\")\n",
    "            print(f\"  Instance ID: {data.get('instance_id', 'N/A')}\")\n",
    "            print(f\"  Timestamp: {data.get('timestamp', 'N/A')}\")\n",
    "        \n",
    "        # Acknowledge messages (optional - remove if you want to keep them)\n",
    "        ack_ids = [msg.ack_id for msg in response.received_messages]\n",
    "        subscriber.acknowledge(request={\"subscription\": output_sub_path, \"ack_ids\": ack_ids})\n",
    "        print(f\"\\n‚úÖ Acknowledged {len(ack_ids)} messages\")\n",
    "    else:\n",
    "        print(\"‚ÑπÔ∏è  No messages currently in output subscription\")\n",
    "        print(\"   Messages may have been consumed already or not yet published\")\n",
    "        \n",
    "except Exception as e:\n",
    "    if \"DeadlineExceeded\" in str(type(e).__name__):\n",
    "        print(\"‚ÑπÔ∏è  No messages available in output subscription (timeout)\")\n",
    "        print(\"   This is normal - messages are consumed quickly or not yet available\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è  Error pulling messages: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üí° Pipeline Status Summary\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Get total count from BigQuery\n",
    "count_query = f\"SELECT COUNT(*) as total FROM `{PROJECT_ID}.{BQ_DATASET}.{BQ_TABLE}`\"\n",
    "count_result = bq.query(count_query).to_dataframe()\n",
    "total_results = count_result['total'].iloc[0] if len(count_result) > 0 else 0\n",
    "\n",
    "if total_results > 0:\n",
    "    print(f\"‚úÖ Pipeline is working correctly!\")\n",
    "    print(f\"   Total results in BigQuery: {total_results}\")\n",
    "    print(f\"   Results are being written to both:\")\n",
    "    print(f\"     - BigQuery table: {PROJECT_ID}.{BQ_DATASET}.{BQ_TABLE}\")\n",
    "    print(f\"     - Pub/Sub topic: {OUTPUT_TOPIC}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No results yet in BigQuery\")\n",
    "    print(\"   Pipeline may still be processing or waiting for data\")\n",
    "    \n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cleanup",
   "metadata": {},
   "source": "---\n## Clean Up\n\n**‚ö†Ô∏è IMPORTANT: Streaming jobs run continuously until explicitly cancelled**, incurring ongoing costs for workers and resources.\n\n### Centralized Cleanup Notebook\n\nFor comprehensive cleanup of all Dataflow resources, use the centralized cleanup notebook:\n\n**[dataflow-cleanup.ipynb](./dataflow-cleanup.ipynb)**\n\nThis notebook provides:\n- ‚úÖ **Stop Dataflow Jobs**: Cancel running streaming/batch jobs created by these notebooks\n- ‚úÖ **Clean BigQuery Tables**: Truncate or delete result tables\n- ‚úÖ **Clean Pub/Sub Resources**: Delete topics and subscriptions\n- ‚úÖ **Clean GCS Files**: Delete model files uploaded for Dataflow\n- ‚úÖ **Granular Control**: Use flags to choose exactly what to clean up\n- ‚úÖ **Safety Checks**: Warnings for risky operations\n- ‚úÖ **Confirmation Prompts**: Review before executing\n\n### Why Use Centralized Cleanup?\n\n- **One location**: Manage all Dataflow infrastructure from a single notebook\n- **Comprehensive**: Clean up jobs, tables, Pub/Sub, and GCS files together\n- **Flexible**: Truncate tables without deleting schema (useful for testing)\n- **Safe**: Built-in safety checks and confirmation prompts\n- **Efficient**: Clean up resources from all 4 Dataflow notebooks at once\n\n### Quick Cleanup (This Job Only)\n\nIf you only need to stop the streaming job created by this notebook, uncomment and run the cell below."
  },
  {
   "cell_type": "markdown",
   "id": "0rrkkvi4ri0f",
   "source": "---\n## Understanding Scaling and Performance\n\nNow that your streaming pipeline is deployed with Vertex AI endpoint integration, understanding how the combined system scales is critical for production deployments.\n\n### Factors Affecting Performance\n\n**Dataflow Service-Side Factors**:\n- **Worker Count**: Number of Dataflow workers making API calls (min/max worker settings)\n- **Machine Type**: CPU and memory for workers (less critical than local model, as workers just make API calls)\n- **Autoscaling Configuration**: How quickly workers scale based on Pub/Sub backlog\n\n**Vertex Endpoint Service-Side Factors**:\n- **Replica Count**: Number of endpoint instances serving predictions\n- **Machine Type**: Resources per endpoint replica\n- **Endpoint Autoscaling**: How quickly new replicas come online\n\n**Usage-Side Factors**:\n- **Message Rate**: Messages published to Pub/Sub per second\n- **Endpoint Latency**: Time for endpoint to respond (affects overall throughput)\n- **Window Size**: Duration of windows for micro-batching\n- **Traffic Pattern**: Constant flow vs. spikes (both services must handle)\n\n### Current Configuration\n\nYour combined system is configured with:\n\n**Dataflow**:\n- **Machine Type**: `n1-standard-4` (4 vCPUs, 15 GB memory)\n- **Min Workers**: 2 (baseline)\n- **Max Workers**: 20 (autoscaling limit)\n- **Window Size**: 1 minute\n\n**Vertex Endpoint**:\n- **Machine Type**: `n1-standard-4` (4 vCPUs, 15 GB memory)\n- **Min Replicas**: 1 (always-on)\n- **Max Replicas**: 4 (autoscaling limit)\n\nThis configuration provides:\n- ‚úÖ Two-layer autoscaling (Dataflow + Vertex)\n- ‚úÖ Independent scaling (each service adapts to load)\n- ‚ö†Ô∏è **Potential bottleneck**: Endpoint may limit overall throughput\n\n### Performance Testing\n\nTo understand how this combined system scales and identify bottlenecks, see:\n\n**[scaling-dataflow-streaming-runinference-vertex.ipynb](./scaling-dataflow-streaming-runinference-vertex.ipynb)**\n\nThis notebook demonstrates:\n- **Finding System Bottlenecks**: Identify which component limits performance\n  - Test message rates to find where latency spikes\n  - Determine if Dataflow or Vertex endpoint is the bottleneck\n  - Measure end-to-end latency breakdown\n- **Load Pattern Testing**: Understand dual-autoscaling behavior\n  - Constant load (how both services reach steady state)\n  - Gradual ramp-up (observe both autoscaling systems)\n  - Traffic spikes (which service recovers faster?)\n- **Tuning Guidance**: Optimize both services together\n  - When to scale Dataflow workers vs. Vertex replicas\n  - How to balance worker-to-replica ratios\n  - Cost implications of different configurations\n  - Failure scenarios (endpoint throttling, backpressure handling)\n\n### When to Run Scale Testing\n\nRun scale tests when:\n- üîπ **Before production launch**: Understand combined system capacity\n- üîπ **After model/endpoint changes**: Endpoint latency affects pipeline throughput\n- üîπ **For capacity planning**: Estimate costs for both services\n- üîπ **During incidents**: Identify which service is the bottleneck",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "usa985119",
   "source": "# Uncomment the line below to stop the streaming job created by this notebook\n# result.cancel()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "In this notebook, you successfully:\n",
    "\n",
    "‚úÖ **Tested Endpoint Health**: Verified Vertex AI endpoint is responding correctly\n",
    "\n",
    "‚úÖ **Configured Vertex AI Handler**: Set up VertexAIModelHandlerJSON for Vertex AI Endpoint calls\n",
    "\n",
    "‚úÖ **Built Streaming Pipeline**: Created real-time processing with 1-minute windows\n",
    "\n",
    "‚úÖ **Applied RunInference**: Called Vertex AI Endpoint for predictions on streaming data\n",
    "\n",
    "‚úÖ **Dual Output**: Wrote results to both Pub/Sub (for downstream systems) and BigQuery (for analysis)\n",
    "\n",
    "‚úÖ **Monitored Job**: Tracked continuous job execution and viewed results\n",
    "\n",
    "‚úÖ **Cleaned Up**: Stopped streaming job to avoid ongoing charges\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "- **Endpoint Health Check**: Always test endpoint before deploying pipeline to catch issues early\n",
    "- **Data Format**: `VertexAIModelHandlerJSON` automatically wraps data in `{\"instances\": ...}` - just pass the features array\n",
    "- **Streaming vs Batch**: Continuous processing for real-time anomaly detection\n",
    "- **Vertex AI Integration**: RunInference seamlessly calls managed endpoints\n",
    "- **Automatic Batching**: Efficient inference through request aggregation\n",
    "- **Dual Output**: Results flow to both storage and downstream services\n",
    "- **Independent Scaling**: Endpoint and pipeline scale separately\n",
    "- **Format Flexibility**: Pipeline handles both pre-built and custom container outputs\n",
    "- **Patience Required**: Allow 8-10 minutes total for workers to start and data to process\n",
    "\n",
    "## Troubleshooting\n",
    "\n",
    "If you encounter issues:\n",
    "\n",
    "1. **Endpoint Health Test Fails**:\n",
    "   - Check endpoint is deployed and has active replicas\n",
    "   - Verify endpoint display name matches `ENDPOINT_DISPLAY_NAME` variable\n",
    "   - Check Cloud Console for endpoint errors\n",
    "\n",
    "2. **No Results in BigQuery**:\n",
    "   - Wait full 8-10 minutes (workers + processing)\n",
    "   - Check Dataflow job logs for errors\n",
    "   - Verify messages were published to Pub/Sub topic\n",
    "   - Re-run endpoint health test\n",
    "\n",
    "3. **Pub/Sub Timeout**:\n",
    "   - This is normal - messages are consumed quickly\n",
    "   - Check BigQuery instead (source of truth)\n",
    "   - Pipeline Status Summary shows total results\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "### Continue with Other Dataflow Workflows:\n",
    "\n",
    "**Local Model Inference (Comparison):**\n",
    "- [dataflow-batch-runinference.ipynb](./dataflow-batch-runinference.ipynb)\n",
    "  - Batch processing with local PyTorch model\n",
    "  - Compare cost and performance vs Vertex endpoint\n",
    "\n",
    "- [dataflow-streaming-runinference.ipynb](./dataflow-streaming-runinference.ipynb)\n",
    "  - Real-time processing with local PyTorch model\n",
    "  - No endpoint dependency, lower latency\n",
    "\n",
    "**Batch Inference with Vertex Endpoint:**\n",
    "- [dataflow-batch-runinference-vertex.ipynb](./dataflow-batch-runinference-vertex.ipynb)\n",
    "  - Process historical data via Vertex AI Endpoint\n",
    "  - Compare batch vs streaming approaches\n",
    "\n",
    "### Production Enhancements:\n",
    "\n",
    "1. **Error Handling**: Add retry logic for endpoint failures with exponential backoff\n",
    "2. **Dead Letter Queue**: Route failed predictions to separate Pub/Sub topic for analysis\n",
    "3. **Monitoring**: Set up Cloud Monitoring alerts for job failures and high latency\n",
    "4. **Cost Optimization**: Tune window size and batching parameters based on traffic patterns\n",
    "5. **A/B Testing**: Route traffic between multiple endpoint versions for model comparison\n",
    "6. **Authentication**: Add service account authentication for production deployments\n",
    "7. **Scaling**: Configure autoscaling based on Pub/Sub backlog\n",
    "\n",
    "### Resources:\n",
    "\n",
    "- [Apache Beam Streaming](https://beam.apache.org/documentation/programming-guide/#streaming)\n",
    "- [Dataflow Monitoring](https://cloud.google.com/dataflow/docs/guides/monitoring-jobs)\n",
    "- [Vertex AI Endpoints](https://cloud.google.com/vertex-ai/docs/predictions/overview)\n",
    "- [Pub/Sub Documentation](https://cloud.google.com/pubsub/docs)\n",
    "- [RunInference with Vertex AI](https://beam.apache.org/documentation/ml/vertex-ai/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (frameworks-pytorch)",
   "language": "python",
   "name": "frameworks-pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}