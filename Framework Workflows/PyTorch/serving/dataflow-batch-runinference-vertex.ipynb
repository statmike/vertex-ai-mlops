{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7dfad844",
   "metadata": {},
   "source": [
    "![tracker](https://us-central1-vertex-ai-mlops-369716.cloudfunctions.net/pixel-tracking?path=statmike%2Fvertex-ai-mlops%2FFramework+Workflows%2FPyTorch%2Fserving&file=dataflow-batch-runinference-vertex.ipynb)\n",
    "<!--- header table --->\n",
    "<table align=\"left\">\n",
    "<tr>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%20Workflows/PyTorch/serving/dataflow-batch-runinference-vertex.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.svgrepo.com/download/217753/github.svg\" alt=\"GitHub logo\">\n",
    "      <br>View on<br>GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://colab.research.google.com/github/statmike/vertex-ai-mlops/blob/main/Framework%20Workflows/PyTorch/serving/dataflow-batch-runinference-vertex.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.gstatic.com/pantheon/images/bigquery/welcome_page/colab-logo.svg\" alt=\"Google Colaboratory logo\">\n",
    "      <br>Run in<br>Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https%3A%2F%2Fraw.githubusercontent.com%2Fstatmike%2Fvertex-ai-mlops%2Fmain%2FFramework%2520Workflows%2FPyTorch%2Fserving%2Fdataflow-batch-runinference-vertex.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\">\n",
    "      <br>Run in<br>Colab Enterprise\n",
    "    </a>\n",
    "  </td>      \n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/bigquery/import?url=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%20Workflows/PyTorch/serving/dataflow-batch-runinference-vertex.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.gstatic.com/images/branding/gcpiconscolors/bigquery/v1/32px.svg\" alt=\"BigQuery logo\">\n",
    "      <br>Open in<br>BigQuery Studio\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/statmike/vertex-ai-mlops/main/Framework%20Workflows/PyTorch/serving/dataflow-batch-runinference-vertex.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.gstatic.com/images/branding/gcpiconscolors/vertexai/v1/32px.svg\" alt=\"Vertex AI logo\">\n",
    "      <br>Open in<br>Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td colspan=\"5\" style=\"text-align: right\">\n",
    "    <b>Share This On: </b> \n",
    "    <a href=\"https://www.linkedin.com/sharing/share-offsite/?url=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%2520Workflows/PyTorch/serving/dataflow-batch-runinference-vertex.ipynb\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"Linkedin Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://reddit.com/submit?url=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%2520Workflows/PyTorch/serving/dataflow-batch-runinference-vertex.ipynb\"><img src=\"https://redditinc.com/hubfs/Reddit%20Inc/Brand/Reddit_Logo.png\" alt=\"Reddit Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://bsky.app/intent/compose?text=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%2520Workflows/PyTorch/serving/dataflow-batch-runinference-vertex.ipynb\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"BlueSky Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://twitter.com/intent/tweet?url=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%2520Workflows/PyTorch/serving/dataflow-batch-runinference-vertex.ipynb\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/5/5a/X_icon_2.svg\" alt=\"X (Twitter) Logo\" width=\"20px\"></a> \n",
    "  </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td colspan=\"5\" style=\"text-align: right\">\n",
    "    <b>Connect With Author On: </b> \n",
    "    <a href=\"https://www.linkedin.com/in/statmike\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"Linkedin Logo\" width=\"20px\"></a>\n",
    "    <a href=\"https://www.github.com/statmike\"><img src=\"https://www.svgrepo.com/download/217753/github.svg\" alt=\"GitHub Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://www.youtube.com/@statmike-channel\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/f/fd/YouTube_full-color_icon_%282024%29.svg\" alt=\"YouTube Logo\" width=\"20px\"></a>\n",
    "    <a href=\"https://bsky.app/profile/statmike.bsky.social\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"BlueSky Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://x.com/statmike\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/5/5a/X_icon_2.svg\" alt=\"X (Twitter) Logo\" width=\"20px\"></a>\n",
    "  </td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overview",
   "metadata": {},
   "source": "# Dataflow Batch Inference with RunInference (Vertex AI Endpoint)\n\nThis notebook demonstrates batch processing of transactions using Dataflow with Apache Beam RunInference that calls a Vertex AI Endpoint.\n\n## What You'll Learn\n\nThis workflow covers:\n\n1. **Configure Vertex AI Handler**: Set up VertexAIModelHandlerJSON for RunInference\n2. **Test Endpoint Health**: Verify endpoint is responding before deploying pipeline\n3. **Build Batch Pipeline**: Read from BigQuery, apply model, write results\n4. **Run on Dataflow**: Execute pipeline on Google Cloud\n5. **Monitor Job**: Track progress in Cloud Console\n6. **Analyze Results**: Query and visualize anomaly scores\n\n## Prerequisites\n\n- Completed `dataflow-setup.ipynb` - This sets up:\n  - BigQuery tables created (including `pytorch_autoencoder_batch_results_vertex`)\n  - Pub/Sub topics and subscriptions created\n- **Vertex AI Endpoint deployed** - Choose either:\n  - [Pre-built Container](./vertex-ai-endpoint-prebuilt-container.ipynb) - Returns full model output (13 metrics)\n  - [Custom Container](./vertex-ai-endpoint-custom-container.ipynb) - Returns simplified output (2 fields)\n\n> **Note**: This notebook works with both endpoint types and will automatically detect the output format.\n\n## Batch vs Streaming\n\n**Batch Processing (This Notebook)**:\n- ‚úÖ Process historical data\n- ‚úÖ Bounded dataset (has a start and end)\n- ‚úÖ Results available when job completes\n- ‚úÖ Cost-effective for large datasets\n- Example: Analyze all transactions from last month\n\n**Streaming Processing (Next Notebook)**:\n- ‚úÖ Process real-time data\n- ‚úÖ Unbounded dataset (continuous)\n- ‚úÖ Results available immediately\n- ‚úÖ Low-latency anomaly detection\n- Example: Flag suspicious transactions as they occur\n\n## Architecture\n\n```\nBigQuery Source Table\n  ‚Üì Read test transactions\nDataflow Pipeline\n  ‚Üì Format instances\nRunInference (Vertex AI Endpoint)\n  ‚Üì Generate anomaly scores via API\nTransform Results\n  ‚Üì Extract scores and embeddings\nBigQuery Results Table\n```\n\n## RunInference with Vertex AI Benefits\n\n- **Managed endpoint**: No model loading in workers\n- **Automatic batching**: Combines instances for efficient inference\n- **Scalable**: Endpoint scales independently from pipeline\n- **Flexible**: Update model without redeploying pipeline\n\n## Timing Expectations\n\n**Total time: ~5-10 minutes**\n\n1. **Start Dataflow job**: Instant\n2. **Worker provisioning**: 2-3 minutes\n3. **Processing 1000 records**: 2-5 minutes (depends on endpoint response time)\n4. **View results**: Immediate after job completes\n\n## What This Pipeline Does\n\n1. Read TEST transactions from BigQuery (1000 records)\n2. Extract features from each record\n3. Call Vertex AI Endpoint via RunInference (automatic batching)\n4. Extract relevant outputs (score + embeddings)\n5. Write results to BigQuery\n6. Job completes when all data processed"
  },
  {
   "cell_type": "markdown",
   "id": "env",
   "metadata": {},
   "source": [
    "---\n",
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proj",
   "metadata": {},
   "source": [
    "### Set Your Project ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proj_c",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = 'statmike-mlops-349915'\n",
    "REQ_TYPE = 'ALL'\n",
    "INSTALL_TOOL = 'poetry'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conf",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conf_c",
   "metadata": {},
   "outputs": [],
   "source": [
    "REQUIREMENTS_URL = 'https://raw.githubusercontent.com/statmike/vertex-ai-mlops/refs/heads/main/Framework%20Workflows/PyTorch/requirements.txt'\n",
    "REQUIRED_APIS = [\"dataflow.googleapis.com\", \"bigquery.googleapis.com\", \"storage.googleapis.com\", \"aiplatform.googleapis.com\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup",
   "metadata": {},
   "source": [
    "### Run Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup_c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, urllib.request\n",
    "url = 'https://raw.githubusercontent.com/statmike/vertex-ai-mlops/refs/heads/main/core/notebook-template/python_setup.py'\n",
    "urllib.request.urlretrieve(url, 'python_setup_local.py')\n",
    "import python_setup_local as python_setup\n",
    "os.remove('python_setup_local.py')\n",
    "setup_info = python_setup.setup_environment(PROJECT_ID, REQ_TYPE, REQUIREMENTS_URL, REQUIRED_APIS, INSTALL_TOOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "py",
   "metadata": {},
   "source": [
    "---\n",
    "## Python Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imp",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imp_c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import apache_beam as beam\n",
    "from apache_beam.ml.inference.base import RunInference\n",
    "from apache_beam.ml.inference.vertex_ai_inference import VertexAIModelHandlerJSON\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "from apache_beam.io.gcp.bigquery import ReadFromBigQuery, WriteToBigQuery\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vars",
   "metadata": {},
   "source": [
    "### Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vars_c",
   "metadata": {},
   "outputs": [],
   "source": "PROJECT_ID = subprocess.run([\"gcloud\", \"config\", \"get-value\", \"project\"], capture_output=True, text=True, check=True).stdout.strip()\n\n# Configuration\nREGION = \"us-central1\"\nSERIES = \"frameworks\"\nEXPERIMENT = \"pytorch-autoencoder\"\n\n# ========================================\n# ENDPOINT SELECTION\n# ========================================\n# Choose which endpoint to use:\n# - \"pytorch-autoencoder-endpoint\" (pre-built container - returns 13 metrics)\n# - \"pytorch-autoencoder-custom-endpoint\" (custom container - returns 2 fields)\nENDPOINT_DISPLAY_NAME = \"pytorch-autoencoder-endpoint\"  # Change to \"pytorch-autoencoder-custom-endpoint\" if using custom container\n\n# Get endpoint ID from display name\n# VertexAIModelHandlerJSON requires the numeric endpoint ID, not the display name\nfrom google.cloud import aiplatform\naiplatform.init(project=PROJECT_ID, location=REGION)\n\nprint(f\"Looking up endpoint: {ENDPOINT_DISPLAY_NAME}\")\nendpoints = aiplatform.Endpoint.list(\n    filter=f'display_name=\"{ENDPOINT_DISPLAY_NAME}\"',\n    order_by=\"create_time desc\"\n)\n\nif not endpoints:\n    raise ValueError(\n        f\"No endpoint found with display name '{ENDPOINT_DISPLAY_NAME}' in region {REGION}.\\n\"\n        f\"Please deploy an endpoint first using either:\\n\"\n        f\"  - vertex-ai-endpoint-prebuilt-container.ipynb\\n\"\n        f\"  - vertex-ai-endpoint-custom-container.ipynb\"\n    )\n\nendpoint = endpoints[0]\nENDPOINT_ID = endpoint.name.split(\"/\")[-1]\n\nprint(f\"‚úÖ Found endpoint: {ENDPOINT_DISPLAY_NAME}\")\nprint(f\"   Endpoint ID: {ENDPOINT_ID}\")\nprint(f\"   Full resource name: {endpoint.resource_name}\")\n\n# GCS paths\nBUCKET_NAME = PROJECT_ID\nBUCKET_URI = f\"gs://{BUCKET_NAME}\"\n\n# BigQuery configuration\nBQ_DATASET = SERIES.replace(\"-\", \"_\")\nBQ_TABLE_RESULTS = f\"{EXPERIMENT.replace('-', '_')}_batch_results_vertex\"\n\n# Dataflow configuration\nDATAFLOW_STAGING = f\"{BUCKET_URI}/dataflow/staging\"\nDATAFLOW_TEMP = f\"{BUCKET_URI}/dataflow/temp\"\n\nprint(f\"\\nConfiguration:\")\nprint(f\"  Project: {PROJECT_ID}\")\nprint(f\"  Endpoint: {ENDPOINT_DISPLAY_NAME} (ID: {ENDPOINT_ID})\")\nprint(f\"  Results table: {PROJECT_ID}.{BQ_DATASET}.{BQ_TABLE_RESULTS}\")\nprint(f\"  Dataflow staging: {DATAFLOW_STAGING}\")"
  },
  {
   "cell_type": "markdown",
   "id": "aawd1d55wpv",
   "source": "---\n## Test Endpoint Health\n\nBefore using the endpoint in the pipeline, verify it's responding correctly.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "mfyycuubyoi",
   "source": "import requests\nfrom google.auth import default\nfrom google.auth.transport.requests import Request\n\n# Get credentials\ncredentials, project = default()\ncredentials.refresh(Request())\n\n# Prepare test prediction\nurl = f\"https://{REGION}-aiplatform.googleapis.com/v1/projects/{PROJECT_ID}/locations/{REGION}/endpoints/{ENDPOINT_ID}:predict\"\nheaders = {\n    \"Authorization\": f\"Bearer {credentials.token}\",\n    \"Content-Type\": \"application/json\"\n}\n\n# Test data - same format as pipeline will send\ntest_data = {\n    \"instances\": [[0.1] * 30]  # Single transaction with 30 features\n}\n\nprint(\"Testing endpoint health...\")\nprint(f\"Endpoint: {ENDPOINT_DISPLAY_NAME} (ID: {ENDPOINT_ID})\")\nprint(\"=\" * 60)\n\ntry:\n    response = requests.post(url, headers=headers, json=test_data, timeout=30)\n    \n    print(f\"Status Code: {response.status_code}\")\n    \n    if response.status_code == 200:\n        result = response.json()\n        print(\"‚úÖ Endpoint is healthy and responding!\")\n        print(f\"\\nSample prediction:\")\n        \n        # Handle both output formats\n        predictions = result.get(\"predictions\", [])\n        if predictions:\n            pred = predictions[0]\n            if \"anomaly_score\" in pred:\n                print(f\"  Anomaly Score: {pred['anomaly_score']:.2f}\")\n            elif \"denormalized_MAE\" in pred:\n                print(f\"  Anomaly Score (MAE): {pred['denormalized_MAE']:.2f}\")\n            \n            if \"encoded\" in pred:\n                print(f\"  Encoded (4D): {pred['encoded'][:4]}...\")\n            \n            print(f\"\\n  Total fields in response: {len(pred)}\")\n        \n    else:\n        print(f\"‚ùå Endpoint returned error: {response.status_code}\")\n        print(f\"Response: {response.text}\")\n        print(\"\\n‚ö†Ô∏è  The pipeline will fail with this endpoint.\")\n        print(\"   Please check your endpoint deployment before continuing.\")\n        \nexcept requests.exceptions.Timeout:\n    print(\"‚ùå Request timed out after 30 seconds\")\n    print(\"   The endpoint may be cold-starting or unresponsive.\")\n    \nexcept Exception as e:\n    print(f\"‚ùå Error testing endpoint: {e}\")\n    print(\"   Please verify the endpoint is deployed and healthy.\")\n\nprint(\"=\" * 60)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "handler",
   "metadata": {},
   "source": [
    "---\n",
    "## Create Custom ModelHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handler_c",
   "metadata": {},
   "outputs": [],
   "source": "# Create Vertex AI handler\nmodel_handler = VertexAIModelHandlerJSON(\n    endpoint_id=ENDPOINT_ID,  # Use numeric endpoint ID, not display name\n    project=PROJECT_ID,\n    location=REGION\n)\nprint(f\"‚úÖ VertexAIModelHandlerJSON created for endpoint: {ENDPOINT_DISPLAY_NAME} (ID: {ENDPOINT_ID})\")"
  },
  {
   "cell_type": "markdown",
   "id": "pipeline",
   "metadata": {},
   "source": "---\n## Configure Worker Compute Resources\n\nBefore building the pipeline, configure the compute resources (machine type and autoscaling) for Dataflow workers. These settings directly impact performance and cost when calling Vertex AI endpoints."
  },
  {
   "cell_type": "code",
   "id": "ypjog8c3jxe",
   "source": "# Worker Compute Configuration\n# =============================\n# These settings control the machine type and autoscaling behavior for Dataflow workers.\n# For Vertex AI endpoint workflows, workers make API calls rather than loading models locally.\n\n# Machine Type: n1-standard-4\n# - 4 vCPUs, 15 GB memory\n# - Suitable for making concurrent API calls to Vertex AI endpoints\n# - Each worker can handle multiple parallel requests\n# - Alternative: n1-standard-2 (smaller), n1-standard-8 (larger) depending on concurrency needs\n\nMACHINE_TYPE = \"n1-standard-4\"\n\n# Autoscaling for Batch Pipelines (with Vertex AI Endpoint)\n# - min_workers=2: Provides baseline parallelism for faster job completion\n# - max_workers=10: Balances throughput with cost for bounded datasets\n# - Dataflow autoscales based on data volume and endpoint response time\n\nMIN_WORKERS = 2\nMAX_WORKERS = 10\n\n# Why These Settings for Batch + Vertex Endpoint?\n# ------------------------------------------------\n# 1. **Minimum Workers (2)**:\n#    - Establishes baseline parallelism for efficient data processing\n#    - Reduces overall job runtime by distributing API calls\n#    - Lower than streaming since batch jobs are temporary\n#\n# 2. **Maximum Workers (10)**:\n#    - Allows scaling for large datasets while controlling costs\n#    - Each worker makes concurrent API calls to Vertex endpoint\n#    - Throughput depends on endpoint capacity, not worker compute\n#    - Conservative limit to avoid overwhelming the endpoint\n#\n# 3. **Machine Type (n1-standard-4)**:\n#    - Workers don't load PyTorch models (endpoint handles inference)\n#    - Memory requirements are lower than local model loading\n#    - 4 vCPUs enable good parallelism for API call concurrency\n#    - Cost-effective for API-based inference workflows\n\n# Key Difference from Local Model Inference:\n# ------------------------------------------\n# - **Local Model**: Workers need high memory (15GB+) to load PyTorch models\n# - **Vertex Endpoint**: Workers only need memory for API client and data buffering\n# - **Throughput**: Limited by endpoint capacity, not worker compute\n# - **Scaling**: Can use smaller machine types (n1-standard-2) if API latency is low\n\n# When to Adjust These Settings:\n# -------------------------------\n# - **Large Datasets**: Increase max_workers (e.g., 50-100) if endpoint can handle load\n# - **Faster Completion**: Increase min_workers (e.g., 5-10) to reduce job runtime\n# - **Cost Optimization**: Use n1-standard-2 for lower concurrency needs\n# - **High Concurrency**: Use n1-standard-8 for more parallel API calls per worker\n# - **Endpoint Throttling**: Reduce max_workers to prevent overwhelming the endpoint\n\n# GPU Support:\n# -----------\n# Not applicable for Vertex AI endpoint workflows.\n# GPU inference happens at the endpoint, not in Dataflow workers.\n# Workers only make HTTP API calls to the endpoint.\n\nprint(\"=\" * 60)\nprint(\"WORKER COMPUTE CONFIGURATION (Vertex AI Endpoint)\")\nprint(\"=\" * 60)\nprint(f\"Machine Type: {MACHINE_TYPE}\")\nprint(f\"  - vCPUs: 4\")\nprint(f\"  - Memory: 15 GB\")\nprint(f\"\\nAutoscaling:\")\nprint(f\"  - Min Workers: {MIN_WORKERS}\")\nprint(f\"  - Max Workers: {MAX_WORKERS}\")\nprint(f\"\\nWorkload Type: API-based inference\")\nprint(f\"  - Workers make HTTP calls to Vertex AI endpoint\")\nprint(f\"  - Throughput limited by endpoint capacity, not worker compute\")\nprint(f\"  - No local model loading required\")\nprint(\"=\" * 60)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pipeline_c",
   "metadata": {},
   "outputs": [],
   "source": "def format_for_bq(element):\n    \"\"\"\n    Format results for BigQuery.\n    \n    Handles both endpoint output formats:\n    - Pre-built container: 13 keys (denormalized_MAE, encoded, etc.)\n    - Custom container: 2 keys (anomaly_score, encoded)\n    \"\"\"\n    prediction = element[1]  # RunInference returns (input, output)\n    \n    # Auto-detect output format and extract anomaly score\n    if \"anomaly_score\" in prediction:\n        # Custom container format\n        anomaly_score = prediction[\"anomaly_score\"]\n    elif \"denormalized_MAE\" in prediction:\n        # Pre-built container format\n        anomaly_score = prediction[\"denormalized_MAE\"]\n    else:\n        raise ValueError(f\"Unknown prediction format: {prediction.keys()}\")\n    \n    return {\n        \"instance_id\": str(hash(str(element[0]))),\n        \"anomaly_score\": anomaly_score,\n        \"encoded\": prediction[\"encoded\"],\n        \"timestamp\": datetime.utcnow().isoformat()\n    }\n\n# Pipeline options\noptions = PipelineOptions([\n    f\"--project={PROJECT_ID}\",\n    f\"--region={REGION}\",\n    \"--runner=DataflowRunner\",\n    f\"--temp_location={DATAFLOW_TEMP}\",\n    f\"--staging_location={DATAFLOW_STAGING}\",\n    f\"--job_name=pytorch-batch-vertex-{datetime.now().strftime('%Y%m%d-%H%M%S')}\",\n    \"--save_main_session\",  # Serialize global imports and variables\n    # Worker compute configuration\n    f\"--machine_type={MACHINE_TYPE}\",  # Machine type for workers\n    f\"--num_workers={MIN_WORKERS}\",  # Initial/minimum number of workers\n    f\"--max_num_workers={MAX_WORKERS}\",  # Maximum workers for autoscaling\n])\n\nprint(\"‚úÖ Pipeline options configured\")\nprint(f\"   Job will run in: {REGION}\")\nprint(f\"   Staging: {DATAFLOW_STAGING}\")\nprint(f\"   Machine type: {MACHINE_TYPE}\")\nprint(f\"   Worker scaling: {MIN_WORKERS}-{MAX_WORKERS} workers\")"
  },
  {
   "cell_type": "markdown",
   "id": "run",
   "metadata": {},
   "source": [
    "### Run Batch Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run_c",
   "metadata": {},
   "outputs": [],
   "source": "# Build and run pipeline\nwith beam.Pipeline(options=options) as p:\n    results = (\n        p\n        | \"Read from BigQuery\" >> ReadFromBigQuery(\n            query=f\"\"\"\n            SELECT * EXCEPT(splits, transaction_id, Class)\n            FROM `{PROJECT_ID}.{BQ_DATASET}.{SERIES}`\n            WHERE splits = \"TEST\"\n            LIMIT 1000\n            \"\"\",\n            use_standard_sql=True\n        )\n        | \"Convert to JSON\" >> beam.Map(lambda row: dict(row))\n        | \"RunInference\" >> RunInference(model_handler)\n        | \"Format for BigQuery\" >> beam.Map(format_for_bq)\n        | \"Write to BigQuery\" >> WriteToBigQuery(\n            table=f\"{PROJECT_ID}:{BQ_DATASET}.{BQ_TABLE_RESULTS}\",\n            write_disposition=beam.io.BigQueryDisposition.WRITE_APPEND\n        )\n    )\n\nprint(\"\\n‚úÖ Dataflow job submitted!\")\nprint(f\"Monitor at: https://console.cloud.google.com/dataflow/jobs/{REGION}?project={PROJECT_ID}\")\nprint(\"\\n\" + \"=\" * 60)\nprint(\"‚è≥ TIMING EXPECTATIONS\")\nprint(\"=\" * 60)\nprint(\"The pipeline will take approximately 5-10 minutes:\")\nprint(\"  1. Worker provisioning: 2-3 minutes\")\nprint(\"  2. Processing 1000 records: 2-5 minutes\")\nprint(\"  3. Writing to BigQuery: < 1 minute\")\nprint(\"\\nJob will complete automatically when all data is processed.\")\nprint(\"Check the Dataflow console to monitor progress.\")\nprint(\"=\" * 60)"
  },
  {
   "cell_type": "markdown",
   "id": "results",
   "metadata": {},
   "source": [
    "---\n",
    "## Analyze Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "results_c",
   "metadata": {},
   "outputs": [],
   "source": "from google.cloud import bigquery\nimport pandas as pd\n\nbq = bigquery.Client(project=PROJECT_ID)\n\nprint(\"=\" * 60)\nprint(\"BIGQUERY RESULTS (Batch Inference)\")\nprint(\"=\" * 60)\n\n# Get recent results\nquery = f\"\"\"\nSELECT *\nFROM `{PROJECT_ID}.{BQ_DATASET}.{BQ_TABLE_RESULTS}`\nORDER BY timestamp DESC\nLIMIT 10\n\"\"\"\ndf = bq.query(query).to_dataframe()\n\nif len(df) > 0:\n    print(f\"‚úÖ Found {len(df)} recent results\")\n    display(df)\nelse:\n    print(\"‚ö†Ô∏è  No results yet in BigQuery\")\n    print(\"   Wait for Dataflow job to complete\")\n\n# Get total count\nprint(\"\\n\" + \"=\" * 60)\nprint(\"üí° Pipeline Status Summary\")\nprint(\"=\" * 60)\n\ncount_query = f\"SELECT COUNT(*) as total FROM `{PROJECT_ID}.{BQ_DATASET}.{BQ_TABLE_RESULTS}`\"\ncount_result = bq.query(count_query).to_dataframe()\ntotal_results = count_result['total'].iloc[0] if len(count_result) > 0 else 0\n\nif total_results > 0:\n    print(f\"‚úÖ Pipeline completed successfully!\")\n    print(f\"   Total results in BigQuery: {total_results}\")\n    print(f\"   Results table: {PROJECT_ID}.{BQ_DATASET}.{BQ_TABLE_RESULTS}\")\nelse:\n    print(\"‚ö†Ô∏è  No results yet in BigQuery\")\n    print(\"   Check Dataflow job status for errors\")\n\nprint(\"=\" * 60)"
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": "---\n## Summary\n\n‚úÖ Configured VertexAIModelHandlerJSON for RunInference\n\n‚úÖ Built batch Dataflow pipeline\n\n‚úÖ Processed transactions via Vertex AI Endpoint\n\n‚úÖ Wrote anomaly scores to BigQuery\n\n## Next Steps\n\n### Continue with Dataflow Workflows:\n\n**Streaming Inference with Vertex Endpoint:**\n- [dataflow-streaming-runinference-vertex.ipynb](./dataflow-streaming-runinference-vertex.ipynb)\n  - Real-time endpoint calls from Dataflow\n  - Process streaming data via Vertex AI\n  - Continuous anomaly detection\n\n**Local Model Inference (Comparison):**\n- [dataflow-batch-runinference.ipynb](./dataflow-batch-runinference.ipynb)\n  - In-process model loading (no endpoint)\n  - Compare cost and performance\n  - Lower latency for batch workloads\n\n- [dataflow-streaming-runinference.ipynb](./dataflow-streaming-runinference.ipynb)\n  - Real-time local model inference\n  - No endpoint dependency\n\n### Resources\n\n- [Vertex AI Endpoints](https://cloud.google.com/vertex-ai/docs/predictions/overview)\n- [Dataflow ML Integration](https://cloud.google.com/dataflow/docs/machine-learning)\n- [RunInference with Vertex AI](https://beam.apache.org/documentation/ml/vertex-ai/)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}