{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7dfad844",
   "metadata": {},
   "source": [
    "![tracker](https://us-central1-vertex-ai-mlops-369716.cloudfunctions.net/pixel-tracking?path=statmike%2Fvertex-ai-mlops%2FFramework+Workflows%2FPyTorch%2Fserving&file=dataflow-batch-runinference-vertex.ipynb)\n",
    "<!--- header table --->\n",
    "<table align=\"left\">\n",
    "<tr>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%20Workflows/PyTorch/serving/dataflow-batch-runinference-vertex.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.svgrepo.com/download/217753/github.svg\" alt=\"GitHub logo\">\n",
    "      <br>View on<br>GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://colab.research.google.com/github/statmike/vertex-ai-mlops/blob/main/Framework%20Workflows/PyTorch/serving/dataflow-batch-runinference-vertex.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.gstatic.com/pantheon/images/bigquery/welcome_page/colab-logo.svg\" alt=\"Google Colaboratory logo\">\n",
    "      <br>Run in<br>Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https%3A%2F%2Fraw.githubusercontent.com%2Fstatmike%2Fvertex-ai-mlops%2Fmain%2FFramework%2520Workflows%2FPyTorch%2Fserving%2Fdataflow-batch-runinference-vertex.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\">\n",
    "      <br>Run in<br>Colab Enterprise\n",
    "    </a>\n",
    "  </td>      \n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/bigquery/import?url=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%20Workflows/PyTorch/serving/dataflow-batch-runinference-vertex.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.gstatic.com/images/branding/gcpiconscolors/bigquery/v1/32px.svg\" alt=\"BigQuery logo\">\n",
    "      <br>Open in<br>BigQuery Studio\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/statmike/vertex-ai-mlops/main/Framework%20Workflows/PyTorch/serving/dataflow-batch-runinference-vertex.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.gstatic.com/images/branding/gcpiconscolors/vertexai/v1/32px.svg\" alt=\"Vertex AI logo\">\n",
    "      <br>Open in<br>Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td colspan=\"5\" style=\"text-align: right\">\n",
    "    <b>Share This On: </b> \n",
    "    <a href=\"https://www.linkedin.com/sharing/share-offsite/?url=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%2520Workflows/PyTorch/serving/dataflow-batch-runinference-vertex.ipynb\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"Linkedin Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://reddit.com/submit?url=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%2520Workflows/PyTorch/serving/dataflow-batch-runinference-vertex.ipynb\"><img src=\"https://redditinc.com/hubfs/Reddit%20Inc/Brand/Reddit_Logo.png\" alt=\"Reddit Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://bsky.app/intent/compose?text=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%2520Workflows/PyTorch/serving/dataflow-batch-runinference-vertex.ipynb\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"BlueSky Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://twitter.com/intent/tweet?url=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%2520Workflows/PyTorch/serving/dataflow-batch-runinference-vertex.ipynb\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/5/5a/X_icon_2.svg\" alt=\"X (Twitter) Logo\" width=\"20px\"></a> \n",
    "  </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td colspan=\"5\" style=\"text-align: right\">\n",
    "    <b>Connect With Author On: </b> \n",
    "    <a href=\"https://www.linkedin.com/in/statmike\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"Linkedin Logo\" width=\"20px\"></a>\n",
    "    <a href=\"https://www.github.com/statmike\"><img src=\"https://www.svgrepo.com/download/217753/github.svg\" alt=\"GitHub Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://www.youtube.com/@statmike-channel\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/f/fd/YouTube_full-color_icon_%282024%29.svg\" alt=\"YouTube Logo\" width=\"20px\"></a>\n",
    "    <a href=\"https://bsky.app/profile/statmike.bsky.social\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"BlueSky Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://x.com/statmike\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/5/5a/X_icon_2.svg\" alt=\"X (Twitter) Logo\" width=\"20px\"></a>\n",
    "  </td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overview",
   "metadata": {},
   "source": [
    "# Dataflow Batch Inference with RunInference (Vertex AI Endpoint)\n",
    "\n",
    "This notebook demonstrates batch processing of transactions using Dataflow with Apache Beam RunInference that calls a Vertex AI Endpoint.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "This workflow covers:\n",
    "\n",
    "1. **Configure Vertex AI Handler**: Set up VertexAIModelHandlerJSON for RunInference\n",
    "2. **Test Endpoint Health**: Verify endpoint is responding before deploying pipeline\n",
    "3. **Configure Worker Resources**: Set machine type and autoscaling parameters  \n",
    "4. **Build Batch Pipeline**: Read from BigQuery, apply model, write results\n",
    "5. **Run on Dataflow**: Execute pipeline on Google Cloud (non-blocking)\n",
    "6. **Monitor Job Progress**: Track job status programmatically until completion\n",
    "7. **Analyze Results**: Query and visualize anomaly scores\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Completed `dataflow-setup.ipynb` - This sets up:\n",
    "  - BigQuery tables created (including `pytorch_autoencoder_batch_results_vertex`)\n",
    "  - Pub/Sub topics and subscriptions created\n",
    "- **Vertex AI Endpoint deployed** - Choose either:\n",
    "  - [Pre-built Container](./vertex-ai-endpoint-prebuilt-container.ipynb) - Returns full model output (13 metrics)\n",
    "  - [Custom Container](./vertex-ai-endpoint-custom-container.ipynb) - Returns simplified output (2 fields)\n",
    "\n",
    "> **Note**: This notebook works with both endpoint types and will automatically detect the output format.\n",
    "\n",
    "## Batch vs Streaming\n",
    "\n",
    "**Batch Processing (This Notebook)**:\n",
    "- ✅ Process historical data\n",
    "- ✅ Bounded dataset (has a start and end)\n",
    "- ✅ Job completes automatically when data is exhausted\n",
    "- ✅ Cost-effective for large datasets\n",
    "- ✅ Ideal for periodic analytics and reporting\n",
    "- Example: Analyze all transactions from last month\n",
    "\n",
    "**Streaming Processing (Next Notebook)**:\n",
    "- ✅ Process real-time data\n",
    "- ✅ Unbounded dataset (continuous)\n",
    "- ✅ Results available immediately as data arrives\n",
    "- ✅ Low-latency anomaly detection\n",
    "- ✅ Requires manual job cancellation\n",
    "- Example: Flag suspicious transactions as they occur\n",
    "\n",
    "## Architecture\n",
    "\n",
    "```\n",
    "BigQuery Source Table\n",
    "  ↓ Read test transactions (bounded)\n",
    "Dataflow Pipeline\n",
    "  ↓ Extract features\n",
    "RunInference (Vertex AI Endpoint)\n",
    "  ↓ Generate anomaly scores via API\n",
    "Transform Results\n",
    "  ↓ Extract scores and embeddings\n",
    "BigQuery Results Table\n",
    "```\n",
    "\n",
    "## RunInference with Vertex AI Benefits\n",
    "\n",
    "- **Managed endpoint**: No model loading in workers\n",
    "- **Automatic batching**: Combines instances for efficient API calls\n",
    "- **Scalable**: Endpoint scales independently from pipeline\n",
    "- **Flexible**: Update model without redeploying pipeline\n",
    "- **Centralized**: Share endpoint across multiple pipelines\n",
    "\n",
    "## Timing Expectations\n",
    "\n",
    "**Total time: ~5-10 minutes**\n",
    "\n",
    "1. **Start Dataflow job**: Instant (non-blocking execution)\n",
    "2. **Worker provisioning**: 2-3 minutes\n",
    "3. **Processing 1000 records**: 2-5 minutes (depends on endpoint response time)\n",
    "4. **Job completion**: Automatic when all data processed\n",
    "\n",
    "## What This Pipeline Does\n",
    "\n",
    "1. Read TEST transactions from BigQuery (1000 records)\n",
    "2. Extract features from each record\n",
    "3. Call Vertex AI Endpoint via RunInference (automatic batching)\n",
    "4. Extract relevant outputs (score + embeddings)\n",
    "5. Write results to BigQuery\n",
    "6. Job completes automatically when all data processed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "env",
   "metadata": {},
   "source": [
    "---\n",
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proj",
   "metadata": {},
   "source": [
    "### Set Your Project ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "proj_c",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = 'statmike-mlops-349915'\n",
    "REQ_TYPE = 'ALL'\n",
    "INSTALL_TOOL = 'poetry'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conf",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "conf_c",
   "metadata": {},
   "outputs": [],
   "source": [
    "REQUIREMENTS_URL = 'https://raw.githubusercontent.com/statmike/vertex-ai-mlops/refs/heads/main/Framework%20Workflows/PyTorch/requirements.txt'\n",
    "REQUIRED_APIS = [\"dataflow.googleapis.com\", \"bigquery.googleapis.com\", \"storage.googleapis.com\", \"aiplatform.googleapis.com\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup",
   "metadata": {},
   "source": [
    "### Run Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "setup_c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "PYTHON GCP ENVIRONMENT SETUP\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "AUTHENTICATION\n",
      "==================================================\n",
      "Checking for existing ADC...\n",
      "✅ Existing ADC found.\n",
      "✅ Project is correctly set to 'statmike-mlops-349915'.\n",
      "\n",
      "==================================================\n",
      "API CHECK & ENABLE\n",
      "==================================================\n",
      "✅ dataflow.googleapis.com is already enabled.\n",
      "✅ bigquery.googleapis.com is already enabled.\n",
      "✅ storage.googleapis.com is already enabled.\n",
      "✅ aiplatform.googleapis.com is already enabled.\n",
      "\n",
      "==================================================\n",
      "PACKAGE MANAGEMENT\n",
      "==================================================\n",
      "Installation Tool: poetry\n",
      "✅ Found poetry at: /usr/local/google/home/statmike/.local/bin/poetry\n",
      "✅ Running in poetry environment: /usr/local/google/home/statmike/.cache/pypoetry/virtualenvs/frameworks-pytorch-0KVJlKeQ-py3.13\n",
      "ℹ️  Poetry mode: Installing from pyproject.toml (REQUIREMENTS_URL ignored)\n",
      "✅ Found pyproject.toml at: /usr/local/google/home/statmike/Git/vertex-ai-mlops/Framework Workflows/PyTorch/pyproject.toml\n",
      "   Changed working directory to: /usr/local/google/home/statmike/Git/vertex-ai-mlops/Framework Workflows/PyTorch\n",
      "Running poetry install...\n",
      "   Restored working directory to: /usr/local/google/home/statmike/Git/vertex-ai-mlops/Framework Workflows/PyTorch/serving\n",
      "✅ All packages are already installed and up to date.\n",
      "\n",
      "==================================================\n",
      "Google Cloud Project Information\n",
      "==================================================\n",
      "PROJECT_ID     = statmike-mlops-349915\n",
      "PROJECT_NUMBER = 1026793852137\n",
      "==================================================\n",
      "\n",
      "\n",
      "==================================================\n",
      "SETUP SUMMARY\n",
      "==================================================\n",
      "✅ Authentication:    Success\n",
      "✅ API Configuration: Success\n",
      "✅ Package Install:   Already up to date\n",
      "✅ Installation Tool: poetry\n",
      "✅ Project ID:        statmike-mlops-349915\n",
      "✅ Project Number:    1026793852137\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os, urllib.request\n",
    "url = 'https://raw.githubusercontent.com/statmike/vertex-ai-mlops/refs/heads/main/core/notebook-template/python_setup.py'\n",
    "urllib.request.urlretrieve(url, 'python_setup_local.py')\n",
    "import python_setup_local as python_setup\n",
    "os.remove('python_setup_local.py')\n",
    "setup_info = python_setup.setup_environment(PROJECT_ID, REQ_TYPE, REQUIREMENTS_URL, REQUIRED_APIS, INSTALL_TOOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "py",
   "metadata": {},
   "source": [
    "---\n",
    "## Python Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imp",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "imp_c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/google/home/statmike/.cache/pypoetry/virtualenvs/frameworks-pytorch-0KVJlKeQ-py3.13/lib/python3.13/site-packages/google/cloud/aiplatform/models.py:52: FutureWarning: Support for google-cloud-storage < 3.0.0 will be removed in a future version of google-cloud-aiplatform. Please upgrade to google-cloud-storage >= 3.0.0.\n",
      "  from google.cloud.aiplatform.utils import gcs_utils\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import apache_beam as beam\n",
    "from apache_beam.ml.inference.base import RunInference\n",
    "from apache_beam.ml.inference.vertex_ai_inference import VertexAIModelHandlerJSON\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "from apache_beam.io.gcp.bigquery import ReadFromBigQuery, WriteToBigQuery\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vars",
   "metadata": {},
   "source": [
    "### Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "vars_c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking up endpoint: pytorch-autoencoder-endpoint\n",
      "✅ Found endpoint: pytorch-autoencoder-endpoint\n",
      "   Endpoint ID: 2741468416626917376\n",
      "   Full resource name: projects/1026793852137/locations/us-central1/endpoints/2741468416626917376\n",
      "\n",
      "Configuration:\n",
      "  Project: statmike-mlops-349915\n",
      "  Endpoint: pytorch-autoencoder-endpoint (ID: 2741468416626917376)\n",
      "  Results table: statmike-mlops-349915.frameworks.pytorch_autoencoder_batch_results_vertex\n",
      "  Dataflow staging: gs://statmike-mlops-349915/dataflow/staging\n"
     ]
    }
   ],
   "source": [
    "PROJECT_ID = subprocess.run([\"gcloud\", \"config\", \"get-value\", \"project\"], capture_output=True, text=True, check=True).stdout.strip()\n",
    "\n",
    "# Configuration\n",
    "REGION = \"us-central1\"\n",
    "SERIES = \"frameworks\"\n",
    "EXPERIMENT = \"pytorch-autoencoder\"\n",
    "\n",
    "# ========================================\n",
    "# ENDPOINT SELECTION\n",
    "# ========================================\n",
    "# Choose which endpoint to use:\n",
    "# - \"pytorch-autoencoder-endpoint\" (pre-built container - returns 13 metrics)\n",
    "# - \"pytorch-autoencoder-custom-endpoint\" (custom container - returns 2 fields)\n",
    "ENDPOINT_DISPLAY_NAME = \"pytorch-autoencoder-endpoint\"  # Change to \"pytorch-autoencoder-custom-endpoint\" if using custom container\n",
    "\n",
    "# Get endpoint ID from display name\n",
    "# VertexAIModelHandlerJSON requires the numeric endpoint ID, not the display name\n",
    "from google.cloud import aiplatform\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION)\n",
    "\n",
    "print(f\"Looking up endpoint: {ENDPOINT_DISPLAY_NAME}\")\n",
    "endpoints = aiplatform.Endpoint.list(\n",
    "    filter=f'display_name=\"{ENDPOINT_DISPLAY_NAME}\"',\n",
    "    order_by=\"create_time desc\"\n",
    ")\n",
    "\n",
    "if not endpoints:\n",
    "    raise ValueError(\n",
    "        f\"No endpoint found with display name '{ENDPOINT_DISPLAY_NAME}' in region {REGION}.\\n\"\n",
    "        f\"Please deploy an endpoint first using either:\\n\"\n",
    "        f\"  - vertex-ai-endpoint-prebuilt-container.ipynb\\n\"\n",
    "        f\"  - vertex-ai-endpoint-custom-container.ipynb\"\n",
    "    )\n",
    "\n",
    "endpoint = endpoints[0]\n",
    "ENDPOINT_ID = endpoint.name.split(\"/\")[-1]\n",
    "\n",
    "print(f\"✅ Found endpoint: {ENDPOINT_DISPLAY_NAME}\")\n",
    "print(f\"   Endpoint ID: {ENDPOINT_ID}\")\n",
    "print(f\"   Full resource name: {endpoint.resource_name}\")\n",
    "\n",
    "# GCS paths\n",
    "BUCKET_NAME = PROJECT_ID\n",
    "BUCKET_URI = f\"gs://{BUCKET_NAME}\"\n",
    "\n",
    "# BigQuery configuration\n",
    "BQ_DATASET = SERIES.replace(\"-\", \"_\")\n",
    "BQ_TABLE_RESULTS = f\"{EXPERIMENT.replace('-', '_')}_batch_results_vertex\"\n",
    "\n",
    "# Dataflow configuration\n",
    "DATAFLOW_STAGING = f\"{BUCKET_URI}/dataflow/staging\"\n",
    "DATAFLOW_TEMP = f\"{BUCKET_URI}/dataflow/temp\"\n",
    "\n",
    "print(f\"\\nConfiguration:\")\n",
    "print(f\"  Project: {PROJECT_ID}\")\n",
    "print(f\"  Endpoint: {ENDPOINT_DISPLAY_NAME} (ID: {ENDPOINT_ID})\")\n",
    "print(f\"  Results table: {PROJECT_ID}.{BQ_DATASET}.{BQ_TABLE_RESULTS}\")\n",
    "print(f\"  Dataflow staging: {DATAFLOW_STAGING}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f3d59c",
   "metadata": {},
   "source": [
    "---\n",
    "## Use Vertex AI Model Registry To Retrieve Model Artifacts Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884fbd0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Information:\n",
      "  Display Name: pytorch-autoencoder\n",
      "  Model ID: 2572675789577256960\n",
      "  Artifact URI: gs://statmike-mlops-349915/frameworks/pytorch-autoencoder\n"
     ]
    }
   ],
   "source": [
    "# Retrieve model artifact URI from the endpoint\n",
    "deployed_model = endpoint.list_models()[0]\n",
    "model = aiplatform.Model(deployed_model.model)\n",
    "\n",
    "print(f\"Model Information:\")\n",
    "print(f\"  Display Name: {model.display_name}\")\n",
    "print(f\"  Model ID: {model.name.split('/')[-1]}\")\n",
    "print(f\"  Artifact URI: {model.gca_resource.artifact_uri}\")\n",
    "\n",
    "# Save for potential use\n",
    "MODEL_ARTIFACT_URI = model.gca_resource.artifact_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aawd1d55wpv",
   "metadata": {},
   "source": [
    "---\n",
    "## Test Endpoint Health\n",
    "\n",
    "Before using the endpoint in the pipeline, verify it's responding correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "mfyycuubyoi",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing endpoint health...\n",
      "Endpoint: pytorch-autoencoder-endpoint (ID: 2741468416626917376)\n",
      "============================================================\n",
      "Status Code: 200\n",
      "✅ Endpoint is healthy and responding!\n",
      "\n",
      "Sample prediction:\n",
      "  Anomaly Score (MAE): 2647.89\n",
      "  Encoded (4D): [0, 0, 0.4922903776168823, 0]...\n",
      "\n",
      "  Total fields in response: 13\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from google.auth import default\n",
    "from google.auth.transport.requests import Request\n",
    "\n",
    "# Get credentials\n",
    "credentials, project = default()\n",
    "credentials.refresh(Request())\n",
    "\n",
    "# Prepare test prediction\n",
    "url = f\"https://{REGION}-aiplatform.googleapis.com/v1/projects/{PROJECT_ID}/locations/{REGION}/endpoints/{ENDPOINT_ID}:predict\"\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {credentials.token}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "# Test data - same format as pipeline will send\n",
    "test_data = {\n",
    "    \"instances\": [[0.1] * 30]  # Single transaction with 30 features\n",
    "}\n",
    "\n",
    "print(\"Testing endpoint health...\")\n",
    "print(f\"Endpoint: {ENDPOINT_DISPLAY_NAME} (ID: {ENDPOINT_ID})\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    response = requests.post(url, headers=headers, json=test_data, timeout=30)\n",
    "    \n",
    "    print(f\"Status Code: {response.status_code}\")\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        result = response.json()\n",
    "        print(\"✅ Endpoint is healthy and responding!\")\n",
    "        print(f\"\\nSample prediction:\")\n",
    "        \n",
    "        # Handle both output formats\n",
    "        predictions = result.get(\"predictions\", [])\n",
    "        if predictions:\n",
    "            pred = predictions[0]\n",
    "            if \"anomaly_score\" in pred:\n",
    "                print(f\"  Anomaly Score: {pred['anomaly_score']:.2f}\")\n",
    "            elif \"denormalized_MAE\" in pred:\n",
    "                print(f\"  Anomaly Score (MAE): {pred['denormalized_MAE']:.2f}\")\n",
    "            \n",
    "            if \"encoded\" in pred:\n",
    "                print(f\"  Encoded (4D): {pred['encoded'][:4]}...\")\n",
    "            \n",
    "            print(f\"\\n  Total fields in response: {len(pred)}\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"❌ Endpoint returned error: {response.status_code}\")\n",
    "        print(f\"Response: {response.text}\")\n",
    "        print(\"\\n⚠️  The pipeline will fail with this endpoint.\")\n",
    "        print(\"   Please check your endpoint deployment before continuing.\")\n",
    "        \n",
    "except requests.exceptions.Timeout:\n",
    "    print(\"❌ Request timed out after 30 seconds\")\n",
    "    print(\"   The endpoint may be cold-starting or unresponsive.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error testing endpoint: {e}\")\n",
    "    print(\"   Please verify the endpoint is deployed and healthy.\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handler",
   "metadata": {},
   "source": [
    "---\n",
    "## Create Custom ModelHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "handler_c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ VertexAIModelHandlerJSON created for endpoint: pytorch-autoencoder-endpoint (ID: 2741468416626917376)\n"
     ]
    }
   ],
   "source": [
    "# Create Vertex AI handler\n",
    "model_handler = VertexAIModelHandlerJSON(\n",
    "    endpoint_id=ENDPOINT_ID,  # Use numeric endpoint ID, not display name\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION\n",
    ")\n",
    "print(f\"✅ VertexAIModelHandlerJSON created for endpoint: {ENDPOINT_DISPLAY_NAME} (ID: {ENDPOINT_ID})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pipeline",
   "metadata": {},
   "source": [
    "---\n",
    "## Configure Worker Compute Resources\n",
    "\n",
    "Before building the pipeline, configure the compute resources (machine type and autoscaling) for Dataflow workers. These settings directly impact performance and cost when calling Vertex AI endpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ypjog8c3jxe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "WORKER COMPUTE CONFIGURATION (Vertex AI Endpoint)\n",
      "============================================================\n",
      "Machine Type: n1-standard-4\n",
      "  - vCPUs: 4\n",
      "  - Memory: 15 GB\n",
      "\n",
      "Autoscaling:\n",
      "  - Min Workers: 2\n",
      "  - Max Workers: 10\n",
      "\n",
      "Workload Type: API-based inference\n",
      "  - Workers make HTTP calls to Vertex AI endpoint\n",
      "  - Throughput limited by endpoint capacity, not worker compute\n",
      "  - No local model loading required\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Worker Compute Configuration\n",
    "# =============================\n",
    "# These settings control the machine type and autoscaling behavior for Dataflow workers.\n",
    "# For Vertex AI endpoint workflows, workers make API calls rather than loading models locally.\n",
    "\n",
    "# Machine Type: n1-standard-4\n",
    "# - 4 vCPUs, 15 GB memory\n",
    "# - Suitable for making concurrent API calls to Vertex AI endpoints\n",
    "# - Each worker can handle multiple parallel requests\n",
    "# - Alternative: n1-standard-2 (smaller), n1-standard-8 (larger) depending on concurrency needs\n",
    "\n",
    "MACHINE_TYPE = \"n1-standard-4\"\n",
    "\n",
    "# Autoscaling for Batch Pipelines (with Vertex AI Endpoint)\n",
    "# - min_workers=2: Provides baseline parallelism for faster job completion\n",
    "# - max_workers=10: Balances throughput with cost for bounded datasets\n",
    "# - Dataflow autoscales based on data volume and endpoint response time\n",
    "\n",
    "MIN_WORKERS = 2\n",
    "MAX_WORKERS = 10\n",
    "\n",
    "# Why These Settings for Batch + Vertex Endpoint?\n",
    "# ------------------------------------------------\n",
    "# 1. **Minimum Workers (2)**:\n",
    "#    - Establishes baseline parallelism for efficient data processing\n",
    "#    - Reduces overall job runtime by distributing API calls\n",
    "#    - Lower than streaming since batch jobs are temporary\n",
    "#\n",
    "# 2. **Maximum Workers (10)**:\n",
    "#    - Allows scaling for large datasets while controlling costs\n",
    "#    - Each worker makes concurrent API calls to Vertex endpoint\n",
    "#    - Throughput depends on endpoint capacity, not worker compute\n",
    "#    - Conservative limit to avoid overwhelming the endpoint\n",
    "#\n",
    "# 3. **Machine Type (n1-standard-4)**:\n",
    "#    - Workers don't load PyTorch models (endpoint handles inference)\n",
    "#    - Memory requirements are lower than local model loading\n",
    "#    - 4 vCPUs enable good parallelism for API call concurrency\n",
    "#    - Cost-effective for API-based inference workflows\n",
    "\n",
    "# Key Difference from Local Model Inference:\n",
    "# ------------------------------------------\n",
    "# - **Local Model**: Workers need high memory (15GB+) to load PyTorch models\n",
    "# - **Vertex Endpoint**: Workers only need memory for API client and data buffering\n",
    "# - **Throughput**: Limited by endpoint capacity, not worker compute\n",
    "# - **Scaling**: Can use smaller machine types (n1-standard-2) if API latency is low\n",
    "\n",
    "# When to Adjust These Settings:\n",
    "# -------------------------------\n",
    "# - **Large Datasets**: Increase max_workers (e.g., 50-100) if endpoint can handle load\n",
    "# - **Faster Completion**: Increase min_workers (e.g., 5-10) to reduce job runtime\n",
    "# - **Cost Optimization**: Use n1-standard-2 for lower concurrency needs\n",
    "# - **High Concurrency**: Use n1-standard-8 for more parallel API calls per worker\n",
    "# - **Endpoint Throttling**: Reduce max_workers to prevent overwhelming the endpoint\n",
    "\n",
    "# GPU Support:\n",
    "# -----------\n",
    "# Not applicable for Vertex AI endpoint workflows.\n",
    "# GPU inference happens at the endpoint, not in Dataflow workers.\n",
    "# Workers only make HTTP API calls to the endpoint.\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"WORKER COMPUTE CONFIGURATION (Vertex AI Endpoint)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Machine Type: {MACHINE_TYPE}\")\n",
    "print(f\"  - vCPUs: 4\")\n",
    "print(f\"  - Memory: 15 GB\")\n",
    "print(f\"\\nAutoscaling:\")\n",
    "print(f\"  - Min Workers: {MIN_WORKERS}\")\n",
    "print(f\"  - Max Workers: {MAX_WORKERS}\")\n",
    "print(f\"\\nWorkload Type: API-based inference\")\n",
    "print(f\"  - Workers make HTTP calls to Vertex AI endpoint\")\n",
    "print(f\"  - Throughput limited by endpoint capacity, not worker compute\")\n",
    "print(f\"  - No local model loading required\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "pipeline_c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Pipeline options configured\n",
      "   Job will run in: us-central1\n",
      "   Staging: gs://statmike-mlops-349915/dataflow/staging\n",
      "   Machine type: n1-standard-4\n",
      "   Worker scaling: 2-10 workers\n"
     ]
    }
   ],
   "source": [
    "def format_for_bq(element):\n",
    "    \"\"\"\n",
    "    Format results for BigQuery.\n",
    "    \n",
    "    Handles both endpoint output formats:\n",
    "    - Pre-built container: 13 keys (denormalized_MAE, encoded, etc.)\n",
    "    - Custom container: 2 keys (anomaly_score, encoded)\n",
    "    \"\"\"\n",
    "    prediction = element[1]  # RunInference returns (input, output)\n",
    "    \n",
    "    # Auto-detect output format and extract anomaly score\n",
    "    if \"anomaly_score\" in prediction:\n",
    "        # Custom container format\n",
    "        anomaly_score = prediction[\"anomaly_score\"]\n",
    "    elif \"denormalized_MAE\" in prediction:\n",
    "        # Pre-built container format\n",
    "        anomaly_score = prediction[\"denormalized_MAE\"]\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown prediction format: {prediction.keys()}\")\n",
    "    \n",
    "    return {\n",
    "        \"instance_id\": str(hash(str(element[0]))),\n",
    "        \"anomaly_score\": anomaly_score,\n",
    "        \"encoded\": prediction[\"encoded\"],\n",
    "        \"timestamp\": datetime.utcnow().isoformat()\n",
    "    }\n",
    "\n",
    "# Pipeline options\n",
    "options = PipelineOptions([\n",
    "    f\"--project={PROJECT_ID}\",\n",
    "    f\"--region={REGION}\",\n",
    "    \"--runner=DataflowRunner\",\n",
    "    f\"--temp_location={DATAFLOW_TEMP}\",\n",
    "    f\"--staging_location={DATAFLOW_STAGING}\",\n",
    "    f\"--job_name=pytorch-batch-vertex-{datetime.now().strftime('%Y%m%d-%H%M%S')}\",\n",
    "    \"--save_main_session\",  # Serialize global imports and variables\n",
    "    # Worker compute configuration\n",
    "    f\"--machine_type={MACHINE_TYPE}\",  # Machine type for workers\n",
    "    f\"--num_workers={MIN_WORKERS}\",  # Initial/minimum number of workers\n",
    "    f\"--max_num_workers={MAX_WORKERS}\",  # Maximum workers for autoscaling\n",
    "])\n",
    "\n",
    "print(\"✅ Pipeline options configured\")\n",
    "print(f\"   Job will run in: {REGION}\")\n",
    "print(f\"   Staging: {DATAFLOW_STAGING}\")\n",
    "print(f\"   Machine type: {MACHINE_TYPE}\")\n",
    "print(f\"   Worker scaling: {MIN_WORKERS}-{MAX_WORKERS} workers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "run",
   "metadata": {},
   "source": [
    "### Run Batch Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "run_c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:apache_beam.options.pipeline_options:Bucket specified in temp_location has soft-delete policy enabled. To avoid being billed for unnecessary storage costs, turn off the soft delete feature on buckets that your Dataflow jobs use for temporary and staging storage. For more information, see https://cloud.google.com/storage/docs/use-soft-delete#remove-soft-delete-policy.\n",
      "WARNING:apache_beam.options.pipeline_options:Bucket specified in staging_location has soft-delete policy enabled. To avoid being billed for unnecessary storage costs, turn off the soft delete feature on buckets that your Dataflow jobs use for temporary and staging storage. For more information, see https://cloud.google.com/storage/docs/use-soft-delete#remove-soft-delete-policy.\n",
      "WARNING:apache_beam.runners.interactive.interactive_environment:Dependencies required for Interactive Beam PCollection visualization are not available, please use: `pip install apache-beam[interactive]` to install necessary dependencies to enable all data visualization features.\n"
     ]
    },
    {
     "data": {
      "application/javascript": "\n        if (typeof window.interactive_beam_jquery == 'undefined') {\n          var jqueryScript = document.createElement('script');\n          jqueryScript.src = 'https://code.jquery.com/jquery-3.4.1.slim.min.js';\n          jqueryScript.type = 'text/javascript';\n          jqueryScript.onload = function() {\n            var datatableScript = document.createElement('script');\n            datatableScript.src = 'https://cdn.datatables.net/1.10.20/js/jquery.dataTables.min.js';\n            datatableScript.type = 'text/javascript';\n            datatableScript.onload = function() {\n              window.interactive_beam_jquery = jQuery.noConflict(true);\n              window.interactive_beam_jquery(document).ready(function($){\n                \n              });\n            }\n            document.head.appendChild(datatableScript);\n          };\n          document.head.appendChild(jqueryScript);\n        } else {\n          window.interactive_beam_jquery(document).ready(function($){\n            \n          });\n        }"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    div.alert {\n",
       "      white-space: pre-line;\n",
       "    }\n",
       "  </style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <link rel=\"stylesheet\" href=\"https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/css/bootstrap.min.css\" integrity=\"sha384-Vkoo8x4CGsO3+Hhxv8T/Q5PaXtkKtu6ug5TOeNV6gBiFeWPGFN9MuhOf23Q9Ifjh\" crossorigin=\"anonymous\">\n",
       "            <div class=\"alert alert-info\">No cache_root detected. Defaulting to staging_location gs://statmike-mlops-349915/dataflow/staging for cache location.</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:apache_beam.transforms.core:('No iterator is returned by the process method in %s.', <class 'apache_beam.io.gcp.bigquery_read_internal._PassThroughThenCleanup.expand.<locals>.RemoveExtractedFiles'>)\n",
      "WARNING:apache_beam.transforms.core:('No iterator is returned by the process method in %s.', <class 'apache_beam.io.gcp.bigquery_file_loads.UpdateDestinationSchema'>)\n",
      "WARNING:apache_beam.transforms.core:('No iterator is returned by the process method in %s.', <class 'apache_beam.io.gcp.bigquery_file_loads.TriggerCopyJobs'>)\n",
      "WARNING:apache_beam.transforms.core:('No iterator is returned by the process method in %s.', <class 'apache_beam.io.gcp.bigquery_file_loads.DeleteTablesFn'>)\n",
      "WARNING:apache_beam.options.pipeline_options:Bucket specified in temp_location has soft-delete policy enabled. To avoid being billed for unnecessary storage costs, turn off the soft delete feature on buckets that your Dataflow jobs use for temporary and staging storage. For more information, see https://cloud.google.com/storage/docs/use-soft-delete#remove-soft-delete-policy.\n",
      "WARNING:apache_beam.options.pipeline_options:Bucket specified in staging_location has soft-delete policy enabled. To avoid being billed for unnecessary storage costs, turn off the soft delete feature on buckets that your Dataflow jobs use for temporary and staging storage. For more information, see https://cloud.google.com/storage/docs/use-soft-delete#remove-soft-delete-policy.\n",
      "WARNING:apache_beam.transforms.core:('No iterator is returned by the process method in %s.', <class 'apache_beam.io.gcp.bigquery_read_internal._PassThroughThenCleanup.expand.<locals>.RemoveExtractedFiles'>)\n",
      "WARNING:apache_beam.transforms.core:('No iterator is returned by the process method in %s.', <class 'apache_beam.io.gcp.bigquery_file_loads.UpdateDestinationSchema'>)\n",
      "WARNING:apache_beam.transforms.core:('No iterator is returned by the process method in %s.', <class 'apache_beam.io.gcp.bigquery_file_loads.TriggerCopyJobs'>)\n",
      "WARNING:apache_beam.transforms.core:('No iterator is returned by the process method in %s.', <class 'apache_beam.io.gcp.bigquery_file_loads.DeleteTablesFn'>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Dataflow job submitted!\n",
      "Job ID: 2025-11-07_17_45_32-6876832538906994548\n",
      "Monitor at: https://console.cloud.google.com/dataflow/jobs/us-central1/2025-11-07_17_45_32-6876832538906994548?project=statmike-mlops-349915\n",
      "\n",
      "============================================================\n",
      "⏳ TIMING EXPECTATIONS\n",
      "============================================================\n",
      "The pipeline will take approximately 5-10 minutes:\n",
      "  1. Worker provisioning: 2-3 minutes\n",
      "  2. Processing 1000 records: 2-5 minutes\n",
      "  3. Writing to BigQuery: < 1 minute\n",
      "\n",
      "Job will complete automatically when all data is processed.\n",
      "Run the next cell to monitor job progress.\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Build and run pipeline (non-blocking)\n",
    "p = beam.Pipeline(options=options)\n",
    "\n",
    "results = (\n",
    "    p\n",
    "    | \"Read from BigQuery\" >> ReadFromBigQuery(\n",
    "        query=f\"\"\"\n",
    "        SELECT * EXCEPT(splits, transaction_id, Class)\n",
    "        FROM `{PROJECT_ID}.{BQ_DATASET}.{SERIES}`\n",
    "        WHERE splits = \"TEST\"\n",
    "        LIMIT 1000\n",
    "        \"\"\",\n",
    "        use_standard_sql=True\n",
    "    )\n",
    "    | \"Extract features\" >> beam.Map(lambda row: list(row.values()))\n",
    "    | \"RunInference\" >> RunInference(model_handler)\n",
    "    | \"Format for BigQuery\" >> beam.Map(format_for_bq)\n",
    "    | \"Write to BigQuery\" >> WriteToBigQuery(\n",
    "        table=f\"{PROJECT_ID}:{BQ_DATASET}.{BQ_TABLE_RESULTS}\",\n",
    "        write_disposition=beam.io.BigQueryDisposition.WRITE_APPEND\n",
    "    )\n",
    ")\n",
    "\n",
    "# Run pipeline asynchronously (non-blocking)\n",
    "result = p.run()\n",
    "\n",
    "print(\"\\n✅ Dataflow job submitted!\")\n",
    "print(f\"Job ID: {result.job_id()}\")\n",
    "print(f\"Monitor at: https://console.cloud.google.com/dataflow/jobs/{REGION}/{result.job_id()}?project={PROJECT_ID}\")\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"⏳ TIMING EXPECTATIONS\")\n",
    "print(\"=\" * 60)\n",
    "print(\"The pipeline will take approximately 5-10 minutes:\")\n",
    "print(\"  1. Worker provisioning: 2-3 minutes\")\n",
    "print(\"  2. Processing 1000 records: 2-5 minutes\")\n",
    "print(\"  3. Writing to BigQuery: < 1 minute\")\n",
    "print(\"\\nJob will complete automatically when all data is processed.\")\n",
    "print(\"Run the next cell to monitor job progress.\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "obebomv0hf",
   "metadata": {},
   "source": [
    "### Monitor Job Progress\n",
    "\n",
    "Wait for the batch job to complete by polling its status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "nrzih3a9q79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monitoring Dataflow job: 2025-11-07_17_45_32-6876832538906994548\n",
      "Console: https://console.cloud.google.com/dataflow/jobs/us-central1/2025-11-07_17_45_32-6876832538906994548?project=statmike-mlops-349915\n",
      "============================================================\n",
      "[0s] Job state: JOB_STATE_RUNNING\n",
      "[751s] Job state: JOB_STATE_DONE\n",
      "============================================================\n",
      "✅ Job completed successfully!\n",
      "   Total time: 12 minutes\n",
      "   You can now check results in BigQuery\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import dataflow_v1beta3\n",
    "import time\n",
    "\n",
    "# Initialize Dataflow client\n",
    "dataflow_client = dataflow_v1beta3.JobsV1Beta3Client()\n",
    "\n",
    "# Get the job ID from the result object\n",
    "job_id = result.job_id()\n",
    "\n",
    "print(f\"Monitoring Dataflow job: {job_id}\")\n",
    "print(f\"Console: https://console.cloud.google.com/dataflow/jobs/{REGION}/{job_id}?project={PROJECT_ID}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Poll job status every 30 seconds\n",
    "start_time = time.time()\n",
    "last_state = None\n",
    "\n",
    "while True:\n",
    "    # Get current job status\n",
    "    request = dataflow_v1beta3.GetJobRequest(\n",
    "        project_id=PROJECT_ID,\n",
    "        location=REGION,\n",
    "        job_id=job_id\n",
    "    )\n",
    "    \n",
    "    job = dataflow_client.get_job(request=request)\n",
    "    current_state = job.current_state.name\n",
    "    \n",
    "    # Print status update if state changed\n",
    "    if current_state != last_state:\n",
    "        elapsed = int(time.time() - start_time)\n",
    "        print(f\"[{elapsed}s] Job state: {current_state}\")\n",
    "        last_state = current_state\n",
    "    \n",
    "    # Check if job is in terminal state\n",
    "    if job.current_state in [\n",
    "        dataflow_v1beta3.JobState.JOB_STATE_DONE,\n",
    "        dataflow_v1beta3.JobState.JOB_STATE_FAILED,\n",
    "        dataflow_v1beta3.JobState.JOB_STATE_CANCELLED,\n",
    "        dataflow_v1beta3.JobState.JOB_STATE_DRAINED\n",
    "    ]:\n",
    "        break\n",
    "    \n",
    "    # Wait 30 seconds before next check\n",
    "    time.sleep(30)\n",
    "\n",
    "# Final status\n",
    "elapsed_minutes = int((time.time() - start_time) / 60)\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if job.current_state == dataflow_v1beta3.JobState.JOB_STATE_DONE:\n",
    "    print(f\"✅ Job completed successfully!\")\n",
    "    print(f\"   Total time: {elapsed_minutes} minutes\")\n",
    "    print(f\"   You can now check results in BigQuery\")\n",
    "elif job.current_state == dataflow_v1beta3.JobState.JOB_STATE_FAILED:\n",
    "    print(f\"❌ Job failed after {elapsed_minutes} minutes\")\n",
    "    print(f\"   Check logs in console for details\")\n",
    "elif job.current_state == dataflow_v1beta3.JobState.JOB_STATE_CANCELLED:\n",
    "    print(f\"⚠️  Job was cancelled after {elapsed_minutes} minutes\")\n",
    "else:\n",
    "    print(f\"ℹ️  Job ended with state: {job.current_state.name}\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "results",
   "metadata": {},
   "source": [
    "---\n",
    "## Analyze Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "results_c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "BIGQUERY RESULTS (Batch Inference)\n",
      "============================================================\n",
      "✅ Found 10 recent results\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instance_id</th>\n",
       "      <th>anomaly_score</th>\n",
       "      <th>encoded</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>554888716750174190</td>\n",
       "      <td>2705.457520</td>\n",
       "      <td>[0.0, 0.0, 2.628177165985107, 0.0]</td>\n",
       "      <td>2025-11-08 01:53:02.969502+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-3641110859865901016</td>\n",
       "      <td>1320.756104</td>\n",
       "      <td>[3.761604785919189, 0.0, 9.868988990783691, 0.0]</td>\n",
       "      <td>2025-11-08 01:53:02.969427+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7108313115711558563</td>\n",
       "      <td>3013.436035</td>\n",
       "      <td>[0.0, 0.0, 2.615923881530762, 0.0]</td>\n",
       "      <td>2025-11-08 01:53:02.969353+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4876745214744798803</td>\n",
       "      <td>2542.620605</td>\n",
       "      <td>[0.0, 0.0, 0.4010606408119202, 0.0]</td>\n",
       "      <td>2025-11-08 01:53:02.969279+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-6145642637583889166</td>\n",
       "      <td>106.213814</td>\n",
       "      <td>[0.01215388439595699, 0.0, 8.333025932312012, ...</td>\n",
       "      <td>2025-11-08 01:53:02.969203+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-566709172729238786</td>\n",
       "      <td>901.961731</td>\n",
       "      <td>[0.3761498630046844, 0.7020396590232849, 0.606...</td>\n",
       "      <td>2025-11-08 01:53:02.969125+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7342723600041289136</td>\n",
       "      <td>3199.615479</td>\n",
       "      <td>[0.0, 0.0, 0.08318693935871124, 0.0]</td>\n",
       "      <td>2025-11-08 01:53:02.969039+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-6785816373577670536</td>\n",
       "      <td>218.590530</td>\n",
       "      <td>[0.1295188963413239, 1.110034227371216, 0.8382...</td>\n",
       "      <td>2025-11-08 01:53:02.968879+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5979682317830200369</td>\n",
       "      <td>259.608673</td>\n",
       "      <td>[0.1256755888462067, 1.09805154800415, 0.82613...</td>\n",
       "      <td>2025-11-08 01:53:02.968790+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5401649693787842364</td>\n",
       "      <td>954.889648</td>\n",
       "      <td>[0.1446773111820221, 1.182241439819336, 0.9164...</td>\n",
       "      <td>2025-11-08 01:53:02.968713+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            instance_id  anomaly_score  \\\n",
       "0    554888716750174190    2705.457520   \n",
       "1  -3641110859865901016    1320.756104   \n",
       "2   7108313115711558563    3013.436035   \n",
       "3   4876745214744798803    2542.620605   \n",
       "4  -6145642637583889166     106.213814   \n",
       "5   -566709172729238786     901.961731   \n",
       "6   7342723600041289136    3199.615479   \n",
       "7  -6785816373577670536     218.590530   \n",
       "8   5979682317830200369     259.608673   \n",
       "9   5401649693787842364     954.889648   \n",
       "\n",
       "                                             encoded  \\\n",
       "0                 [0.0, 0.0, 2.628177165985107, 0.0]   \n",
       "1   [3.761604785919189, 0.0, 9.868988990783691, 0.0]   \n",
       "2                 [0.0, 0.0, 2.615923881530762, 0.0]   \n",
       "3                [0.0, 0.0, 0.4010606408119202, 0.0]   \n",
       "4  [0.01215388439595699, 0.0, 8.333025932312012, ...   \n",
       "5  [0.3761498630046844, 0.7020396590232849, 0.606...   \n",
       "6               [0.0, 0.0, 0.08318693935871124, 0.0]   \n",
       "7  [0.1295188963413239, 1.110034227371216, 0.8382...   \n",
       "8  [0.1256755888462067, 1.09805154800415, 0.82613...   \n",
       "9  [0.1446773111820221, 1.182241439819336, 0.9164...   \n",
       "\n",
       "                         timestamp  \n",
       "0 2025-11-08 01:53:02.969502+00:00  \n",
       "1 2025-11-08 01:53:02.969427+00:00  \n",
       "2 2025-11-08 01:53:02.969353+00:00  \n",
       "3 2025-11-08 01:53:02.969279+00:00  \n",
       "4 2025-11-08 01:53:02.969203+00:00  \n",
       "5 2025-11-08 01:53:02.969125+00:00  \n",
       "6 2025-11-08 01:53:02.969039+00:00  \n",
       "7 2025-11-08 01:53:02.968879+00:00  \n",
       "8 2025-11-08 01:53:02.968790+00:00  \n",
       "9 2025-11-08 01:53:02.968713+00:00  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "💡 Pipeline Status Summary\n",
      "============================================================\n",
      "✅ Pipeline completed successfully!\n",
      "   Total results in BigQuery: 1000\n",
      "   Results table: statmike-mlops-349915.frameworks.pytorch_autoencoder_batch_results_vertex\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import bigquery\n",
    "import pandas as pd\n",
    "\n",
    "bq = bigquery.Client(project=PROJECT_ID)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"BIGQUERY RESULTS (Batch Inference)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Get recent results\n",
    "query = f\"\"\"\n",
    "SELECT *\n",
    "FROM `{PROJECT_ID}.{BQ_DATASET}.{BQ_TABLE_RESULTS}`\n",
    "ORDER BY timestamp DESC\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "df = bq.query(query).to_dataframe()\n",
    "\n",
    "if len(df) > 0:\n",
    "    print(f\"✅ Found {len(df)} recent results\")\n",
    "    display(df)\n",
    "else:\n",
    "    print(\"⚠️  No results yet in BigQuery\")\n",
    "    print(\"   Wait for Dataflow job to complete\")\n",
    "\n",
    "# Get total count\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"💡 Pipeline Status Summary\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "count_query = f\"SELECT COUNT(*) as total FROM `{PROJECT_ID}.{BQ_DATASET}.{BQ_TABLE_RESULTS}`\"\n",
    "count_result = bq.query(count_query).to_dataframe()\n",
    "total_results = count_result['total'].iloc[0] if len(count_result) > 0 else 0\n",
    "\n",
    "if total_results > 0:\n",
    "    print(f\"✅ Pipeline completed successfully!\")\n",
    "    print(f\"   Total results in BigQuery: {total_results}\")\n",
    "    print(f\"   Results table: {PROJECT_ID}.{BQ_DATASET}.{BQ_TABLE_RESULTS}\")\n",
    "else:\n",
    "    print(\"⚠️  No results yet in BigQuery\")\n",
    "    print(\"   Check Dataflow job status for errors\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "✅ Configured VertexAIModelHandlerJSON for RunInference\n",
    "\n",
    "✅ Built batch Dataflow pipeline\n",
    "\n",
    "✅ Processed transactions via Vertex AI Endpoint\n",
    "\n",
    "✅ Wrote anomaly scores to BigQuery\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "### Continue with Dataflow Workflows:\n",
    "\n",
    "**Streaming Inference with Vertex Endpoint:**\n",
    "- [dataflow-streaming-runinference-vertex.ipynb](./dataflow-streaming-runinference-vertex.ipynb)\n",
    "  - Real-time endpoint calls from Dataflow\n",
    "  - Process streaming data via Vertex AI\n",
    "  - Continuous anomaly detection\n",
    "\n",
    "**Local Model Inference (Comparison):**\n",
    "- [dataflow-batch-runinference.ipynb](./dataflow-batch-runinference.ipynb)\n",
    "  - In-process model loading (no endpoint)\n",
    "  - Compare cost and performance\n",
    "  - Lower latency for batch workloads\n",
    "\n",
    "- [dataflow-streaming-runinference.ipynb](./dataflow-streaming-runinference.ipynb)\n",
    "  - Real-time local model inference\n",
    "  - No endpoint dependency\n",
    "\n",
    "### Resources\n",
    "\n",
    "- [Vertex AI Endpoints](https://cloud.google.com/vertex-ai/docs/predictions/overview)\n",
    "- [Dataflow ML Integration](https://cloud.google.com/dataflow/docs/machine-learning)\n",
    "- [RunInference with Vertex AI](https://beam.apache.org/documentation/ml/vertex-ai/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (frameworks-pytorch)",
   "language": "python",
   "name": "frameworks-pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
