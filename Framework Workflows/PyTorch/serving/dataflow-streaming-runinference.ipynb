{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![tracker](https://us-central1-vertex-ai-mlops-369716.cloudfunctions.net/pixel-tracking?path=statmike%2Fvertex-ai-mlops%2FFramework+Workflows%2FPyTorch%2Fserving&file=dataflow-streaming-runinference.ipynb)\n# Dataflow Streaming Inference with RunInference\n\nReal-time anomaly detection using Dataflow streaming with PyTorch RunInference.\n\n## Architecture\n```\nPub/Sub Input \u2192 Dataflow (RunInference) \u2192 Pub/Sub Output + BigQuery\n```"
      ],
      "id": "header"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n## Environment Setup"
      ],
      "id": "env"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "PROJECT_ID = 'statmike-mlops-349915'\nREQ_TYPE = 'ALL'\nINSTALL_TOOL = 'poetry'"
      ],
      "id": "proj"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "REQUIREMENTS_URL = 'https://raw.githubusercontent.com/statmike/vertex-ai-mlops/refs/heads/main/Framework%20Workflows/PyTorch/requirements.txt'\nREQUIRED_APIS = [\"dataflow.googleapis.com\", \"pubsub.googleapis.com\", \"bigquery.googleapis.com\", \"storage.googleapis.com\"]"
      ],
      "id": "conf"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, urllib.request\nurl = 'https://raw.githubusercontent.com/statmike/vertex-ai-mlops/refs/heads/main/core/notebook-template/python_setup.py'\nurllib.request.urlretrieve(url, 'python_setup_local.py')\nimport python_setup_local as python_setup\nos.remove('python_setup_local.py')\nsetup_info = python_setup.setup_environment(PROJECT_ID, REQ_TYPE, REQUIREMENTS_URL, REQUIRED_APIS, INSTALL_TOOL)"
      ],
      "id": "setup"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n## Python Setup"
      ],
      "id": "py"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import subprocess\nimport apache_beam as beam\nfrom apache_beam import window\nfrom apache_beam.ml.inference.base import RunInference\nfrom apache_beam.ml.inference.pytorch_inference import PytorchModelHandlerTensor\nfrom apache_beam.options.pipeline_options import PipelineOptions, StandardOptions\nfrom apache_beam.io.gcp.pubsub import ReadFromPubSub, WriteToPubSub\nfrom apache_beam.io.gcp.bigquery import WriteToBigQuery\nimport torch\nimport json\nfrom datetime import datetime"
      ],
      "id": "imp"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "PROJECT_ID = subprocess.run([\"gcloud\", \"config\", \"get-value\", \"project\"], capture_output=True, text=True, check=True).stdout.strip()\nREGION = \"us-central1\"\nSERIES = \"frameworks\"\nEXPERIMENT = \"pytorch-autoencoder\"\nBUCKET_URI = f\"gs://{PROJECT_ID}\"\nMODEL_PATH = f\"{BUCKET_URI}/{SERIES}/{EXPERIMENT}/dataflow/final_model_traced.pt\"\nINPUT_SUB = f\"projects/{PROJECT_ID}/subscriptions/{EXPERIMENT}-input-sub\"\nOUTPUT_TOPIC = f\"projects/{PROJECT_ID}/topics/{EXPERIMENT}-output\"\nBQ_DATASET = SERIES.replace(\"-\", \"_\")\nBQ_TABLE = f\"{EXPERIMENT}_streaming_results\"\nprint(f\"Input: {INPUT_SUB}\")\nprint(f\"Output: {OUTPUT_TOPIC}\")"
      ],
      "id": "vars"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n## Create ModelHandler"
      ],
      "id": "handler"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PyTorchAutoencoderHandler(PytorchModelHandlerTensor):\n    def run_inference(self, batch, model, inference_args=None):\n        with torch.no_grad():\n            predictions = model(batch)\n        results = []\n        for i in range(len(batch)):\n            results.append({\"anomaly_score\": float(predictions[\"denormalized_MAE\"][i].item()), \"encoded\": predictions[\"encoded\"][i].tolist()})\n        return results\n\nmodel_handler = PyTorchAutoencoderHandler(state_dict_path=MODEL_PATH, model_class=None, device=\"cpu\")\nprint(\"\u2705 ModelHandler created\")"
      ],
      "id": "handler_c"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n## Build Streaming Pipeline"
      ],
      "id": "pipeline"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def parse_json(message):\n",
        "    \"\"\"Parse Pub/Sub message\"\"\"\n",
        "    data = json.loads(message.decode(\"utf-8\"))\n",
        "    return torch.tensor(data[\"features\"], dtype=torch.float32)\n",
        "\n",
        "def format_result(element, window=beam.DoFn.WindowParam):\n",
        "    \"\"\"Format for Pub/Sub and BigQuery\"\"\"\n",
        "    prediction = element[1]\n",
        "    return {\n",
        "        \"instance_id\": str(hash(str(element[0]))),\n",
        "        \"anomaly_score\": prediction[\"anomaly_score\"],\n",
        "        \"encoded\": prediction[\"encoded\"],\n",
        "        \"timestamp\": datetime.utcnow().isoformat(),\n",
        "        \"window_start\": window.start.to_utc_datetime().isoformat(),\n",
        "        \"window_end\": window.end.to_utc_datetime().isoformat()\n",
        "    }\n",
        "\n",
        "def to_json(element):\n",
        "    \"\"\"Convert to JSON for Pub/Sub\"\"\"\n",
        "    return json.dumps(element).encode(\"utf-8\")\n",
        "\n",
        "options = PipelineOptions([\n",
        "    f\"--project={PROJECT_ID}\",\n",
        "    f\"--region={REGION}\",\n",
        "    \"--runner=DataflowRunner\",\n",
        "    f\"--temp_location={BUCKET_URI}/dataflow/temp\",\n",
        "    f\"--staging_location={BUCKET_URI}/dataflow/staging\",\n",
        "    f\"--job_name=pytorch-streaming-{datetime.now().strftime('%Y%m%d-%H%M%S')}\",\n",
        "    \"--streaming\",\n",
        "    \"--save_main_session=True\"\n",
        "])\n",
        "\n",
        "print(\"\u2705 Streaming pipeline configured\")"
      ],
      "id": "pipeline_c"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Run Streaming Job"
      ],
      "id": "run"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "p = beam.Pipeline(options=options)\n",
        "\n",
        "results = (\n",
        "    p\n",
        "    | \"Read from Pub/Sub\" >> ReadFromPubSub(subscription=INPUT_SUB)\n",
        "    | \"Parse JSON\" >> beam.Map(parse_json)\n",
        "    | \"Window (1 min)\" >> beam.WindowInto(window.FixedWindows(60))\n",
        "    | \"RunInference\" >> RunInference(model_handler)\n",
        "    | \"Format results\" >> beam.Map(format_result)\n",
        ")\n",
        "\n",
        "# Write to Pub/Sub\n",
        "_ = results | \"To JSON\" >> beam.Map(to_json) | \"Write to Pub/Sub\" >> WriteToPubSub(topic=OUTPUT_TOPIC)\n",
        "\n",
        "# Write to BigQuery\n",
        "_ = results | \"Write to BigQuery\" >> WriteToBigQuery(\n",
        "    table=f\"{PROJECT_ID}:{BQ_DATASET}.{BQ_TABLE}\",\n",
        "    write_disposition=beam.io.BigQueryDisposition.WRITE_APPEND\n",
        ")\n",
        "\n",
        "result = p.run()\n",
        "print(\"\\n\u2705 Streaming job started!\")\n",
        "print(f\"Monitor: https://console.cloud.google.com/dataflow/jobs/{REGION}?project={PROJECT_ID}\")\n",
        "print(\"\\n\u26a0\ufe0f  Job will run continuously until canceled\")"
      ],
      "id": "run_c"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n## Simulate Streaming Data"
      ],
      "id": "simulate"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.cloud import pubsub_v1\n",
        "import time\n",
        "\n",
        "publisher = pubsub_v1.PublisherClient()\n",
        "topic_path = publisher.topic_path(PROJECT_ID, f\"{EXPERIMENT}-input\")\n",
        "\n",
        "# Send test messages\n",
        "for i in range(5):\n",
        "    message = {\"features\": [0.1] * 30}  # Dummy transaction\n",
        "    publisher.publish(topic_path, json.dumps(message).encode(\"utf-8\"))\n",
        "    print(f\"Published message {i+1}\")\n",
        "    time.sleep(2)\n",
        "\n",
        "print(\"\\n\u2705 Sent 5 test messages\")"
      ],
      "id": "simulate_c"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Monitor Results"
      ],
      "id": "monitor"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.cloud import bigquery\nbq = bigquery.Client(project=PROJECT_ID)\nquery = f\"SELECT * FROM `{PROJECT_ID}.{BQ_DATASET}.{BQ_TABLE}` ORDER BY timestamp DESC LIMIT 10\"\ndf = bq.query(query).to_dataframe()\nprint(f\"Latest {len(df)} results:\")\ndf"
      ],
      "id": "monitor_c"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n## Clean Up\n\n\u26a0\ufe0f **Important**: Cancel streaming job to stop charges\n\n```python\n# Cancel job in Cloud Console or use:\n# gcloud dataflow jobs cancel JOB_ID --region=us-central1\n```"
      ],
      "id": "cleanup"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n## Summary\n\n\u2705 Built streaming Dataflow pipeline\n\n\u2705 Real-time RunInference with PyTorch\n\n\u2705 Windowed processing (1-min windows)\n\n\u2705 Dual output (Pub/Sub + BigQuery)\n\n### Next: [Vertex Endpoint Integration](./dataflow-vertex-endpoint.ipynb)"
      ],
      "id": "summary"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.13.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}