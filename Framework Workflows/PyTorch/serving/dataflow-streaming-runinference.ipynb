{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1351230",
   "metadata": {},
   "source": [
    "![tracker](https://us-central1-vertex-ai-mlops-369716.cloudfunctions.net/pixel-tracking?path=statmike%2Fvertex-ai-mlops%2FFramework+Workflows%2FPyTorch%2Fserving&file=dataflow-streaming-runinference.ipynb)\n",
    "<!--- header table --->\n",
    "<table align=\"left\">\n",
    "<tr>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%20Workflows/PyTorch/serving/dataflow-streaming-runinference.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.svgrepo.com/download/217753/github.svg\" alt=\"GitHub logo\">\n",
    "      <br>View on<br>GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://colab.research.google.com/github/statmike/vertex-ai-mlops/blob/main/Framework%20Workflows/PyTorch/serving/dataflow-streaming-runinference.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.gstatic.com/pantheon/images/bigquery/welcome_page/colab-logo.svg\" alt=\"Google Colaboratory logo\">\n",
    "      <br>Run in<br>Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https%3A%2F%2Fraw.githubusercontent.com%2Fstatmike%2Fvertex-ai-mlops%2Fmain%2FFramework%2520Workflows%2FPyTorch%2Fserving%2Fdataflow-streaming-runinference.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\">\n",
    "      <br>Run in<br>Colab Enterprise\n",
    "    </a>\n",
    "  </td>      \n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/bigquery/import?url=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%20Workflows/PyTorch/serving/dataflow-streaming-runinference.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.gstatic.com/images/branding/gcpiconscolors/bigquery/v1/32px.svg\" alt=\"BigQuery logo\">\n",
    "      <br>Open in<br>BigQuery Studio\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/statmike/vertex-ai-mlops/main/Framework%20Workflows/PyTorch/serving/dataflow-streaming-runinference.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.gstatic.com/images/branding/gcpiconscolors/vertexai/v1/32px.svg\" alt=\"Vertex AI logo\">\n",
    "      <br>Open in<br>Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td colspan=\"5\" style=\"text-align: right\">\n",
    "    <b>Share This On: </b> \n",
    "    <a href=\"https://www.linkedin.com/sharing/share-offsite/?url=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%2520Workflows/PyTorch/serving/dataflow-streaming-runinference.ipynb\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"Linkedin Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://reddit.com/submit?url=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%2520Workflows/PyTorch/serving/dataflow-streaming-runinference.ipynb\"><img src=\"https://redditinc.com/hubfs/Reddit%20Inc/Brand/Reddit_Logo.png\" alt=\"Reddit Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://bsky.app/intent/compose?text=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%2520Workflows/PyTorch/serving/dataflow-streaming-runinference.ipynb\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"BlueSky Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://twitter.com/intent/tweet?url=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%2520Workflows/PyTorch/serving/dataflow-streaming-runinference.ipynb\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/5/5a/X_icon_2.svg\" alt=\"X (Twitter) Logo\" width=\"20px\"></a> \n",
    "  </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td colspan=\"5\" style=\"text-align: right\">\n",
    "    <b>Connect With Author On: </b> \n",
    "    <a href=\"https://www.linkedin.com/in/statmike\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"Linkedin Logo\" width=\"20px\"></a>\n",
    "    <a href=\"https://www.github.com/statmike\"><img src=\"https://www.svgrepo.com/download/217753/github.svg\" alt=\"GitHub Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://www.youtube.com/@statmike-channel\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/f/fd/YouTube_full-color_icon_%282024%29.svg\" alt=\"YouTube Logo\" width=\"20px\"></a>\n",
    "    <a href=\"https://bsky.app/profile/statmike.bsky.social\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"BlueSky Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://x.com/statmike\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/5/5a/X_icon_2.svg\" alt=\"X (Twitter) Logo\" width=\"20px\"></a>\n",
    "  </td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ep1wl4q7e6",
   "metadata": {},
   "source": [
    "# Dataflow Streaming Inference with RunInference\n",
    "\n",
    "This notebook demonstrates **ultra-low-latency streaming** processing of transactions using Dataflow with Apache Beam RunInference and a **hybrid windowing + batching approach**.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "This workflow covers:\n",
    "\n",
    "1. **Create Custom ModelHandler**: Wrap PyTorch model for RunInference\n",
    "2. **Build Ultra-Low-Latency Pipeline**: Hybrid approach with 1-second windows, early triggers, and explicit batching\n",
    "3. **Run on Dataflow**: Execute continuous pipeline on Google Cloud\n",
    "4. **Monitor Job**: Track streaming job progress and view results\n",
    "5. **Clean Up**: Stop streaming job to avoid ongoing charges\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Completed `dataflow-setup.ipynb` - This sets up:\n",
    "  - Model .pt file extracted from .mar and uploaded to GCS\n",
    "  - BigQuery tables created (including `pytorch_autoencoder_streaming_results`)\n",
    "  - Pub/Sub topics and subscriptions created\n",
    "\n",
    "## Batch vs Streaming\n",
    "\n",
    "**Batch Processing**:\n",
    "- âœ… Process historical data\n",
    "- âœ… Bounded dataset (has a start and end)\n",
    "- âœ… Results available when job completes\n",
    "- âœ… Cost-effective for large datasets\n",
    "- Example: Analyze all transactions from last month\n",
    "\n",
    "**Streaming Processing (This Notebook)**:\n",
    "- âœ… Process real-time data\n",
    "- âœ… Unbounded dataset (continuous)\n",
    "- âœ… Results available immediately\n",
    "- âœ… **Ultra-low-latency** anomaly detection (~40ms average)\n",
    "- Example: Flag suspicious transactions as they occur\n",
    "\n",
    "---\n",
    "\n",
    "## Low-Latency Architecture: The Hybrid Approach\n",
    "\n",
    "This pipeline uses a **hybrid windowing + batching strategy** to achieve <50ms latency while maintaining efficient batch inference:\n",
    "\n",
    "### How It Works\n",
    "\n",
    "```\n",
    "Pub/Sub Message Arrives (t=0ms)\n",
    "    â†“\n",
    "Parse JSON â†’ (features_tensor, metadata_dict)\n",
    "    â†“\n",
    "Window (1 sec, trigger every 10ms)\n",
    "    â”œâ”€ Messages accumulate for up to 10ms\n",
    "    â”œâ”€ Trigger fires â†’ pane released\n",
    "    â””â”€ Latency: 0-10ms (avg ~5ms)\n",
    "    â†“\n",
    "BatchElements (min=1, max=50, max_wait=10ms)\n",
    "    â”œâ”€ Collects elements from pane\n",
    "    â”œâ”€ Waits up to 10ms OR until 50 elements\n",
    "    â””â”€ Latency: 0-10ms (avg ~5ms)\n",
    "    â†“\n",
    "RunInference (Batch Processing)\n",
    "    â”œâ”€ Separates: features=[feat1...feat50], metadata=[meta1...meta50]\n",
    "    â”œâ”€ Calls model.run_inference([feat1...feat50])\n",
    "    â”œâ”€ Gets predictions: [pred1...pred50]\n",
    "    â””â”€ Latency: 20-30ms (batch inference)\n",
    "    â†“\n",
    "Format Results â†’ BigQuery + Pub/Sub\n",
    "    â””â”€ Latency: ~5ms (network)\n",
    "    \n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "Total End-to-End Latency: 25-55ms (avg ~40ms)\n",
    "```\n",
    "\n",
    "### Key Design Choices\n",
    "\n",
    "**1. 1-Second Fixed Windows (for Watermark Progression)**\n",
    "- Windows must exist for streaming pipelines to make progress\n",
    "- 1-second windows provide stable watermark advancement\n",
    "- Windows themselves don't add latency (triggers do the work)\n",
    "\n",
    "**2. Early Firing Triggers (10ms)**\n",
    "- Fire every 10ms instead of waiting for full window close\n",
    "- Messages processed almost immediately after arrival\n",
    "- Trigger latency: 0-10ms (average ~5ms)\n",
    "\n",
    "**3. Explicit BatchElements (Full Control)**\n",
    "- `min_batch_size=1`: Don't wait if only 1 message (ultra-low latency)\n",
    "- `max_batch_size=50`: Cap batch size for predictable inference time\n",
    "- `max_batch_duration_secs=0.01`: 10ms hard deadline\n",
    "- Batch formation latency: 0-10ms (average ~5ms)\n",
    "\n",
    "**4. Metadata Preservation Throughout**\n",
    "- Batch is: `[(features1, metadata1), (features2, metadata2), ...]`\n",
    "- Metadata flows through inference unchanged\n",
    "- Enables accurate latency tracking for scale testing\n",
    "\n",
    "### Latency Breakdown\n",
    "\n",
    "| Stage | Latency | Description |\n",
    "|-------|---------|-------------|\n",
    "| Trigger wait | 0-10ms | Wait for 10ms trigger to fire |\n",
    "| Batch formation | 0-10ms | BatchElements collects messages |\n",
    "| Model inference | 20-30ms | PyTorch autoencoder (batch of 1-50) |\n",
    "| Pub/Sub delivery | ~5ms | Network transmission |\n",
    "| **Total** | **25-55ms** | **Average ~40ms** |\n",
    "\n",
    "### Traffic Adaptability\n",
    "\n",
    "The pipeline automatically adapts batching to traffic:\n",
    "\n",
    "| Traffic Rate | Batch Behavior | Latency |\n",
    "|--------------|----------------|---------|\n",
    "| 1 msg/sec | Batch of 1 after 10ms | ~35ms |\n",
    "| 10 msg/sec | Batch of 10 after ~10ms | ~40ms |\n",
    "| 100 msg/sec | Batch of 50 almost instantly | ~40ms |\n",
    "| 1000 msg/sec | Batch of 50 continuously | ~40ms |\n",
    "\n",
    "---\n",
    "\n",
    "## RunInference Benefits\n",
    "\n",
    "- **In-process**: Model loaded directly in workers (no network calls)\n",
    "- **Explicit batching**: Full control over batch size and timing\n",
    "- **Scalable**: Scales with pipeline workers\n",
    "- **Ultra-low-latency**: Sub-50ms predictions for streaming data\n",
    "- **Cost-effective**: Efficient batching reduces inference costs\n",
    "\n",
    "## Timing Expectations\n",
    "\n",
    "**Total time from start to results: ~8-10 minutes**\n",
    "\n",
    "1. **Start Dataflow job**: Instant\n",
    "2. **Wait for workers**: 3-5 minutes (worker provisioning)\n",
    "3. **Send test messages**: Instant\n",
    "4. **Wait for processing**: ~1-2 minutes (10ms trigger + processing)\n",
    "5. **View results**: Check BigQuery and Pub/Sub\n",
    "\n",
    "## What This Pipeline Does\n",
    "\n",
    "1. Read transactions from Pub/Sub input topic\n",
    "2. Window data into 1-second windows (for watermark progression)\n",
    "3. Fire triggers every 10ms (release messages early)\n",
    "4. Batch elements (1-50) with 10ms max wait\n",
    "5. Run batch inference on PyTorch model\n",
    "6. Extract relevant outputs (score + embeddings)\n",
    "7. Write results to BigQuery (for analysis)\n",
    "8. Publish to Pub/Sub output (for downstream systems)\n",
    "9. Job runs continuously until cancelled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58e8bb9",
   "metadata": {},
   "source": [
    "---\n",
    "## Environment Setup\n",
    "\n",
    "This section will authenticate your session, enable required Google Cloud APIs, and install necessary Python packages.\n",
    "\n",
    "**Package Installation Options (`REQ_TYPE`)**:\n",
    "- `PRIMARY`: Installs only the main packages. Faster, but pip resolves sub-dependencies which may result in different versions than development.\n",
    "- `ALL` (Default): Installs exact versions of all packages and dependencies. Best for perfectly reproducing the development environment.\n",
    "- `COLAB`: Installs a Colab-optimized list that excludes pre-installed packages like `ipython` and `ipykernel`.\n",
    "\n",
    "**Installation Tool Options (`INSTALL_TOOL`)**:\n",
    "- `pip` (Default): Uses pip for package installation. Standard Python package installer.\n",
    "- `uv`: Modern, fast Python package installer. Must be installed separately. See: https://github.com/astral-sh/uv\n",
    "- `poetry`: Dependency management tool. Requires running notebook in a poetry environment (`poetry shell` or `poetry run jupyter lab`). Uses `pyproject.toml` instead of requirements.txt.\n",
    "\n",
    "> **Note:** If running in Google Colab, the script will automatically detect this and set `REQ_TYPE = 'COLAB'` to prevent package conflicts, overriding any manual setting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2692647a",
   "metadata": {},
   "source": [
    "### Set Your Project ID\n",
    "\n",
    "âš ï¸ **Action Required:** Replace the `PROJECT_ID` value below with your Google Cloud project ID before running this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4a601990",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = 'statmike-mlops-349915' # replace with GCP project ID\n",
    "REQ_TYPE = 'ALL' # Specify PRIMARY or ALL or COLAB\n",
    "INSTALL_TOOL = 'poetry' # Specify pip, uv, or poetry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75112889",
   "metadata": {},
   "source": [
    "### Configuration\n",
    "\n",
    "This cell defines the requirements files and Google Cloud APIs needed for this notebook. Run as-is without modification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6ff39312",
   "metadata": {},
   "outputs": [],
   "source": [
    "REQUIREMENTS_URL = 'https://raw.githubusercontent.com/statmike/vertex-ai-mlops/refs/heads/main/Framework%20Workflows/PyTorch/requirements.txt'\n",
    "\n",
    "REQUIRED_APIS = [\n",
    "    \"dataflow.googleapis.com\",\n",
    "    \"pubsub.googleapis.com\",\n",
    "    \"bigquery.googleapis.com\",\n",
    "    \"storage.googleapis.com\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481d3c9d",
   "metadata": {},
   "source": [
    "### Run Setup\n",
    "\n",
    "This cell downloads the centralized setup code and configures your environment. It will:\n",
    "- Authenticate your session with Google Cloud\n",
    "- Enable required APIs for this notebook\n",
    "- Install necessary Python packages\n",
    "- Display a setup summary with your project information\n",
    "\n",
    "> **Note:** In Colab, if packages are installed, the kernel will automatically restart. After restart, continue from the next cell without re-running earlier cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "acb9666d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "PYTHON GCP ENVIRONMENT SETUP\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "AUTHENTICATION\n",
      "==================================================\n",
      "Checking for existing ADC...\n",
      "âœ… Existing ADC found.\n",
      "âœ… Project is correctly set to 'statmike-mlops-349915'.\n",
      "\n",
      "==================================================\n",
      "API CHECK & ENABLE\n",
      "==================================================\n",
      "âœ… dataflow.googleapis.com is already enabled.\n",
      "âœ… pubsub.googleapis.com is already enabled.\n",
      "âœ… bigquery.googleapis.com is already enabled.\n",
      "âœ… storage.googleapis.com is already enabled.\n",
      "\n",
      "==================================================\n",
      "PACKAGE MANAGEMENT\n",
      "==================================================\n",
      "Installation Tool: poetry\n",
      "âœ… Found poetry at: /usr/local/google/home/statmike/.local/bin/poetry\n",
      "âœ… Running in poetry environment: /usr/local/google/home/statmike/.cache/pypoetry/virtualenvs/frameworks-pytorch-0KVJlKeQ-py3.13\n",
      "â„¹ï¸  Poetry mode: Installing from pyproject.toml (REQUIREMENTS_URL ignored)\n",
      "âœ… Found pyproject.toml at: /usr/local/google/home/statmike/Git/vertex-ai-mlops/Framework Workflows/PyTorch/pyproject.toml\n",
      "   Changed working directory to: /usr/local/google/home/statmike/Git/vertex-ai-mlops/Framework Workflows/PyTorch\n",
      "Running poetry install...\n",
      "   Restored working directory to: /usr/local/google/home/statmike/Git/vertex-ai-mlops/Framework Workflows/PyTorch/serving\n",
      "âœ… All packages are already installed and up to date.\n",
      "\n",
      "==================================================\n",
      "Google Cloud Project Information\n",
      "==================================================\n",
      "PROJECT_ID     = statmike-mlops-349915\n",
      "PROJECT_NUMBER = 1026793852137\n",
      "==================================================\n",
      "\n",
      "\n",
      "==================================================\n",
      "SETUP SUMMARY\n",
      "==================================================\n",
      "âœ… Authentication:    Success\n",
      "âœ… API Configuration: Success\n",
      "âœ… Package Install:   Already up to date\n",
      "âœ… Installation Tool: poetry\n",
      "âœ… Project ID:        statmike-mlops-349915\n",
      "âœ… Project Number:    1026793852137\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os, urllib.request\n",
    "\n",
    "# Download and import setup code\n",
    "url = 'https://raw.githubusercontent.com/statmike/vertex-ai-mlops/refs/heads/main/core/notebook-template/python_setup.py'\n",
    "urllib.request.urlretrieve(url, 'python_setup_local.py')\n",
    "import python_setup_local as python_setup\n",
    "os.remove('python_setup_local.py')\n",
    "\n",
    "# Run setup\n",
    "setup_info = python_setup.setup_environment(PROJECT_ID, REQ_TYPE, REQUIREMENTS_URL, REQUIRED_APIS, INSTALL_TOOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "py",
   "metadata": {},
   "source": [
    "---\n",
    "## Python Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "imp",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import apache_beam as beam\n",
    "from apache_beam import window\n",
    "from apache_beam.ml.inference.base import RunInference\n",
    "from apache_beam.ml.inference.pytorch_inference import PytorchModelHandlerTensor\n",
    "from apache_beam.options.pipeline_options import PipelineOptions, StandardOptions\n",
    "from apache_beam.io.gcp.pubsub import ReadFromPubSub, WriteToPubSub\n",
    "from apache_beam.io.gcp.bigquery import WriteToBigQuery\n",
    "import torch\n",
    "import json\n",
    "from datetime import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "vars",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project: statmike-mlops-349915\n",
      "Model: gs://statmike-mlops-349915/frameworks/pytorch-autoencoder/dataflow/final_model_traced.pt\n",
      "Input subscription: projects/statmike-mlops-349915/subscriptions/pytorch-autoencoder-input-sub-local (LOCAL model)\n",
      "Output topic: projects/statmike-mlops-349915/topics/pytorch-autoencoder-output-local (LOCAL model)\n",
      "Results table: statmike-mlops-349915.frameworks.pytorch_autoencoder_streaming_results\n"
     ]
    }
   ],
   "source": [
    "PROJECT_ID = subprocess.run([\"gcloud\", \"config\", \"get-value\", \"project\"], capture_output=True, text=True, check=True).stdout.strip()\n",
    "\n",
    "# Configuration\n",
    "REGION = \"us-central1\"\n",
    "SERIES = \"frameworks\"\n",
    "EXPERIMENT = \"pytorch-autoencoder\"\n",
    "\n",
    "# GCS paths (aligned with dataflow-setup.ipynb)\n",
    "BUCKET_NAME = PROJECT_ID\n",
    "BUCKET_URI = f\"gs://{BUCKET_NAME}\"\n",
    "MODEL_PATH = f\"{BUCKET_URI}/{SERIES}/{EXPERIMENT}/dataflow/final_model_traced.pt\"\n",
    "\n",
    "# Pub/Sub configuration - using LOCAL-specific topics and subscriptions\n",
    "INPUT_SUB = f\"projects/{PROJECT_ID}/subscriptions/{EXPERIMENT}-input-sub-local\"\n",
    "OUTPUT_TOPIC = f\"projects/{PROJECT_ID}/topics/{EXPERIMENT}-output-local\"\n",
    "\n",
    "# BigQuery configuration\n",
    "BQ_DATASET = SERIES.replace(\"-\", \"_\")\n",
    "BQ_TABLE = f\"{EXPERIMENT.replace('-', '_')}_streaming_results\"\n",
    "\n",
    "# Dataflow configuration\n",
    "DATAFLOW_STAGING = f\"{BUCKET_URI}/dataflow/staging\"\n",
    "DATAFLOW_TEMP = f\"{BUCKET_URI}/dataflow/temp\"\n",
    "\n",
    "print(f\"Project: {PROJECT_ID}\")\n",
    "print(f\"Model: {MODEL_PATH}\")\n",
    "print(f\"Input subscription: {INPUT_SUB} (LOCAL model)\")\n",
    "print(f\"Output topic: {OUTPUT_TOPIC} (LOCAL model)\")\n",
    "print(f\"Results table: {PROJECT_ID}.{BQ_DATASET}.{BQ_TABLE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handler",
   "metadata": {},
   "source": [
    "---\n",
    "## Create ModelHandler\n",
    "\n",
    "The ModelHandler wraps the PyTorch model for use with Apache Beam's RunInference transform. This section explains the key design choices for working with TorchScript models in Dataflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "handler_c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ModelHandler created\n",
      "   Model: gs://statmike-mlops-349915/frameworks/pytorch-autoencoder/dataflow/final_model_traced.pt\n",
      "   Device: cpu\n"
     ]
    }
   ],
   "source": [
    "class PyTorchAutoencoderHandler(PytorchModelHandlerTensor):\n",
    "    \"\"\"\n",
    "    Custom ModelHandler for PyTorch autoencoder inference.\n",
    "    \n",
    "    Key Design Choices:\n",
    "    \n",
    "    1. TorchScript Model Loading:\n",
    "       - Uses torch_script_model_path (not state_dict_path)\n",
    "       - TorchScript models (.pt) have architecture embedded\n",
    "       - No need to provide model_class parameter\n",
    "    \n",
    "    2. Batch Processing:\n",
    "       - Input: List of tensors (one per instance)\n",
    "       - Stack into single batch tensor with torch.stack()\n",
    "       - Enables efficient GPU/CPU vectorized operations\n",
    "    \n",
    "    3. Output Format:\n",
    "       - Returns list of dicts (one per instance in batch)\n",
    "       - Each dict contains tensors for that specific instance\n",
    "       - PytorchModelHandlerTensor passes these dicts through directly\n",
    "       - Downstream format_result() function converts tensors to Python types\n",
    "    \n",
    "    4. Model Output:\n",
    "       - Autoencoder returns dict: {\"denormalized_MAE\": tensor, \"encoded\": tensor, ...}\n",
    "       - Extract only needed fields (denormalized_MAE for anomaly score, encoded for embeddings)\n",
    "       - Keep as tensors in run_inference, convert to Python types later\n",
    "    \"\"\"\n",
    "    \n",
    "    def run_inference(self, batch, model, inference_args=None):\n",
    "        \"\"\"\n",
    "        Run inference on a batch of input tensors.\n",
    "        \n",
    "        Args:\n",
    "            batch: List of torch.Tensor, each shape (30,) for our autoencoder\n",
    "            model: Loaded TorchScript model\n",
    "            inference_args: Optional additional arguments (unused)\n",
    "            \n",
    "        Returns:\n",
    "            List of dicts, one per input, containing model outputs as tensors\n",
    "        \"\"\"\n",
    "        # Stack list of tensors into single batch tensor\n",
    "        # Input: [tensor(30,), tensor(30,), ...] -> Output: tensor(batch_size, 30)\n",
    "        batch_tensor = torch.stack(batch)\n",
    "\n",
    "        # Run model inference without gradient computation\n",
    "        with torch.no_grad():\n",
    "            predictions = model(batch_tensor)\n",
    "\n",
    "        # Convert batch output to list of individual results\n",
    "        # This matches the batch size and allows proper per-instance processing\n",
    "        results = []\n",
    "        for i in range(len(batch)):\n",
    "            results.append({\n",
    "                \"denormalized_MAE\": predictions[\"denormalized_MAE\"][i],  # Scalar tensor\n",
    "                \"encoded\": predictions[\"encoded\"][i]  # 1D tensor (embedding)\n",
    "            })\n",
    "        return results\n",
    "\n",
    "\n",
    "# Initialize ModelHandler with TorchScript model from GCS\n",
    "model_handler = PyTorchAutoencoderHandler(\n",
    "    torch_script_model_path=MODEL_PATH,  # Path to .pt file in GCS\n",
    "    device=\"cpu\"  # Run on CPU in Dataflow workers\n",
    ")\n",
    "\n",
    "print(\"âœ… ModelHandler created\")\n",
    "print(f\"   Model: {MODEL_PATH}\")\n",
    "print(f\"   Device: cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pipeline",
   "metadata": {},
   "source": [
    "---\n",
    "## Build Ultra-Low-Latency Pipeline\n",
    "\n",
    "This section builds the streaming pipeline using the **hybrid windowing + batching approach**:\n",
    "\n",
    "1. **1-second fixed windows** (for watermark progression)\n",
    "2. **10ms early triggers** (for low latency)\n",
    "3. **Explicit BatchElements** (for controlled batching)\n",
    "4. **Custom DoFn for batch inference** (preserves metadata)\n",
    "\n",
    "### Why This Approach?\n",
    "\n",
    "**Simple & Elegant:**\n",
    "- Clear separation of concerns: windowing â†’ batching â†’ inference\n",
    "- Easy to understand and debug\n",
    "- Predictable latency and behavior\n",
    "\n",
    "**Full Control:**\n",
    "- Explicitly set `min_batch_size`, `max_batch_size`, `max_buffering_duration`\n",
    "- Tune for your latency requirements\n",
    "- Easy to add logging and monitoring\n",
    "\n",
    "**Efficient:**\n",
    "- Good batching at all traffic levels (1-50 elements per batch)\n",
    "- GPU/CPU vectorization benefits from batch inference\n",
    "- Cost-effective compared to one-by-one processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "qhkd671nl6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "WORKER COMPUTE CONFIGURATION\n",
      "============================================================\n",
      "Machine Type: n1-standard-4\n",
      "  - vCPUs: 4\n",
      "  - Memory: 15 GB\n",
      "\n",
      "Autoscaling:\n",
      "  - Min Workers: 2\n",
      "  - Max Workers: 20\n",
      "  - Estimated capacity: ~200-10000 transactions/sec\n",
      "\n",
      "GPU Support: Not configured (CPU inference)\n",
      "  - To enable GPU: Add --worker_gpu_type and --worker_gpu_count\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Worker Compute Configuration\n",
    "# =============================\n",
    "# These settings control the machine type and autoscaling behavior for Dataflow workers.\n",
    "# Proper configuration ensures efficient resource utilization and cost management.\n",
    "\n",
    "# Machine Type: n1-standard-4\n",
    "# - 4 vCPUs, 15 GB memory\n",
    "# - Suitable for PyTorch inference workloads that need moderate CPU and memory\n",
    "# - Each worker can handle multiple inference requests concurrently\n",
    "# - Alternative: n1-standard-2 (smaller), n1-standard-8 (larger), or custom machine types\n",
    "\n",
    "MACHINE_TYPE = \"n1-standard-4\"\n",
    "\n",
    "# Autoscaling for Streaming Pipelines\n",
    "# - min_workers=2: Ensures pipeline remains responsive even with low traffic\n",
    "# - max_workers=20: Handles traffic spikes without overwhelming resources\n",
    "# - Dataflow autoscales based on Pub/Sub backlog and processing latency\n",
    "\n",
    "MIN_WORKERS = 2\n",
    "MAX_WORKERS = 20\n",
    "\n",
    "# Why These Settings for Streaming?\n",
    "# ----------------------------------\n",
    "# 1. **Minimum Workers (2)**:\n",
    "#    - Provides redundancy (if one worker fails, pipeline continues)\n",
    "#    - Reduces cold start latency when traffic arrives\n",
    "#    - Maintains low end-to-end latency for real-time processing\n",
    "#\n",
    "# 2. **Maximum Workers (20)**:\n",
    "#    - Allows scaling to handle traffic bursts\n",
    "#    - Each n1-standard-4 worker processes ~100-500 transactions/sec\n",
    "#    - 20 workers can handle ~2,000-10,000 transactions/sec\n",
    "#    - Prevents runaway costs from unlimited scaling\n",
    "#\n",
    "# 3. **Machine Type (n1-standard-4)**:\n",
    "#    - PyTorch model loading requires ~2-4 GB memory per worker\n",
    "#    - 15 GB allows model + batch processing overhead\n",
    "#    - 4 vCPUs enable parallel batch inference\n",
    "#    - Cost-effective for moderate throughput requirements\n",
    "\n",
    "# When to Adjust These Settings:\n",
    "# -------------------------------\n",
    "# - **Higher Traffic**: Increase max_workers (e.g., 50-100)\n",
    "# - **Lower Latency**: Increase min_workers (e.g., 5-10) to pre-warm capacity\n",
    "# - **Cost Optimization**: Use smaller machine type (n1-standard-2) if memory permits\n",
    "# - **Larger Models**: Use n1-standard-8 or n1-highmem-4 for memory-intensive models\n",
    "# - **CPU-Intensive Models**: Use c2-standard-4 for compute-optimized instances\n",
    "\n",
    "# GPU Support (Optional)\n",
    "# ----------------------\n",
    "# Dataflow supports GPU workers for accelerated inference:\n",
    "# - Machine type: n1-standard-4 (host machine)\n",
    "# - GPU type: nvidia-tesla-t4, nvidia-tesla-v100, etc.\n",
    "# - GPU count: 1-4 per worker\n",
    "# - Requirements:\n",
    "#   1. Add --worker_machine_type=n1-standard-4\n",
    "#   2. Add --worker_gpu_type=nvidia-tesla-t4\n",
    "#   3. Add --worker_gpu_count=1\n",
    "#   4. Update model handler to use device=\"cuda\"\n",
    "#   5. Ensure PyTorch is GPU-enabled in requirements file\n",
    "#\n",
    "# Note: GPU workers are significantly more expensive than CPU workers.\n",
    "# Only use for models where GPU acceleration provides meaningful speedup.\n",
    "# For small models like this autoencoder, CPU inference is more cost-effective.\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"WORKER COMPUTE CONFIGURATION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Machine Type: {MACHINE_TYPE}\")\n",
    "print(f\"  - vCPUs: 4\")\n",
    "print(f\"  - Memory: 15 GB\")\n",
    "print(f\"\\nAutoscaling:\")\n",
    "print(f\"  - Min Workers: {MIN_WORKERS}\")\n",
    "print(f\"  - Max Workers: {MAX_WORKERS}\")\n",
    "print(f\"  - Estimated capacity: ~{MIN_WORKERS * 100}-{MAX_WORKERS * 500} transactions/sec\")\n",
    "print(f\"\\nGPU Support: Not configured (CPU inference)\")\n",
    "print(f\"  - To enable GPU: Add --worker_gpu_type and --worker_gpu_count\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "pipeline_c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Ultra-low-latency streaming pipeline configured\n",
      "   Job will run in: us-central1\n",
      "   Staging: gs://statmike-mlops-349915/dataflow/staging\n",
      "   Requirements: /tmp/tmpybf3jt67.txt\n",
      "   Machine type: n1-standard-4\n",
      "   Worker scaling: 2-20 workers\n",
      "   Target latency: <50ms end-to-end\n"
     ]
    }
   ],
   "source": [
    "def parse_json(message):\n",
    "    \"\"\"\n",
    "    Parse Pub/Sub message and return both features and metadata.\n",
    "\n",
    "    Args:\n",
    "        message: Bytes from Pub/Sub containing JSON with 'features' key\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (torch.Tensor, dict):\n",
    "        - tensor: Shape (30,) containing transaction features for model input\n",
    "        - metadata: Dict with test_id, message_id, publish_time, etc.\n",
    "\n",
    "    Note:\n",
    "        Metadata is preserved to support latency tracking in scale tests.\n",
    "        The test_id allows filtering results by test, and publish_time enables\n",
    "        end-to-end latency measurement.\n",
    "    \"\"\"\n",
    "    data = json.loads(message.decode(\"utf-8\"))\n",
    "    features = torch.tensor(data[\"features\"], dtype=torch.float32)\n",
    "\n",
    "    # Preserve metadata for latency tracking (used in scale testing)\n",
    "    metadata = {\n",
    "        \"test_id\": data.get(\"test_id\"),\n",
    "        \"message_id\": data.get(\"message_id\"),\n",
    "        \"publish_time\": data.get(\"publish_time\"),\n",
    "        \"sequence\": data.get(\"sequence\")\n",
    "    }\n",
    "\n",
    "    return (features, metadata)\n",
    "\n",
    "\n",
    "def format_result(element, window=beam.DoFn.WindowParam):\n",
    "    \"\"\"\n",
    "    Format model predictions for output to BigQuery and Pub/Sub.\n",
    "    \n",
    "    Args:\n",
    "        element: Tuple of (prediction_dict, metadata_dict)\n",
    "            - prediction_dict: Dict from run_inference with tensor outputs\n",
    "            - metadata_dict: Original message metadata (test_id, publish_time, etc.)\n",
    "        window: Beam window parameter for extracting window boundaries\n",
    "        \n",
    "    Returns:\n",
    "        Dict with Python types suitable for BigQuery/JSON serialization\n",
    "        \n",
    "    Note:\n",
    "        All timestamps stored as Unix timestamps (float) for consistent latency calculations:\n",
    "        - window_wait_ms = (window_end - publish_time) * 1000\n",
    "        - processing_ms = (pipeline_output_time - window_end) * 1000\n",
    "        - pubsub_delivery_ms = (receive_time - pipeline_output_time) * 1000\n",
    "    \"\"\"\n",
    "    prediction, metadata = element\n",
    "    \n",
    "    result = {\n",
    "        \"instance_id\": str(hash(str(prediction[\"denormalized_MAE\"].item()))),\n",
    "        \"anomaly_score\": float(prediction[\"denormalized_MAE\"].item()),\n",
    "        \"encoded\": prediction[\"encoded\"].tolist(),\n",
    "        \"timestamp\": datetime.utcnow().isoformat(),\n",
    "        \"window_start\": window.start.to_utc_datetime().timestamp(),  # Unix timestamp\n",
    "        \"window_end\": window.end.to_utc_datetime().timestamp(),      # Unix timestamp\n",
    "        \"pipeline_output_time\": time.time()  # Unix timestamp\n",
    "    }\n",
    "    \n",
    "    # Add metadata fields if present (from scale testing)\n",
    "    if metadata:\n",
    "        if metadata.get(\"test_id\"):\n",
    "            result[\"test_id\"] = metadata[\"test_id\"]\n",
    "        if metadata.get(\"message_id\"):\n",
    "            result[\"message_id\"] = metadata[\"message_id\"]\n",
    "        if metadata.get(\"publish_time\"):\n",
    "            result[\"publish_time\"] = metadata[\"publish_time\"]\n",
    "        if metadata.get(\"sequence\") is not None:\n",
    "            result[\"sequence\"] = metadata[\"sequence\"]\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "def to_json(element):\n",
    "    \"\"\"Convert dict to JSON bytes for Pub/Sub publication.\"\"\"\n",
    "    return json.dumps(element).encode(\"utf-8\")\n",
    "\n",
    "\n",
    "# Create requirements file for Dataflow workers\n",
    "# Workers need PyTorch installed to load and run the model\n",
    "import tempfile\n",
    "requirements_content = \"torch>=2.0.0\\n\"\n",
    "\n",
    "with tempfile.NamedTemporaryFile(mode='w', suffix='.txt', delete=False) as f:\n",
    "    f.write(requirements_content)\n",
    "    requirements_file = f.name\n",
    "\n",
    "# Configure Dataflow pipeline options\n",
    "from apache_beam.transforms import trigger\n",
    "\n",
    "options = PipelineOptions([\n",
    "    f\"--project={PROJECT_ID}\",\n",
    "    f\"--region={REGION}\",\n",
    "    \"--runner=DataflowRunner\",  # Run on Google Cloud (not locally)\n",
    "    f\"--temp_location={DATAFLOW_TEMP}\",\n",
    "    f\"--staging_location={DATAFLOW_STAGING}\",\n",
    "    f\"--requirements_file={requirements_file}\",  # Install PyTorch in workers\n",
    "    f\"--job_name=pytorch-streaming-{datetime.now().strftime('%Y%m%d-%H%M%S')}\",\n",
    "    \"--streaming\",  # Enable streaming mode\n",
    "    \"--save_main_session\",  # Serialize global imports and variables\n",
    "    # Worker compute configuration\n",
    "    f\"--machine_type={MACHINE_TYPE}\",\n",
    "    f\"--num_workers={MIN_WORKERS}\",\n",
    "    f\"--max_num_workers={MAX_WORKERS}\",\n",
    "    # Low-latency pipeline optimization\n",
    "    \"--experiments=enable_streaming_engine\",  # Faster Dataflow execution engine\n",
    "    \"--experiments=use_runner_v2\",  # Latest runner with performance improvements\n",
    "])\n",
    "\n",
    "print(\"âœ… Ultra-low-latency streaming pipeline configured\")\n",
    "print(f\"   Job will run in: {REGION}\")\n",
    "print(f\"   Staging: {DATAFLOW_STAGING}\")\n",
    "print(f\"   Requirements: {requirements_file}\")\n",
    "print(f\"   Machine type: {MACHINE_TYPE}\")\n",
    "print(f\"   Worker scaling: {MIN_WORKERS}-{MAX_WORKERS} workers\")\n",
    "print(f\"   Target latency: <50ms end-to-end\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "run",
   "metadata": {},
   "source": [
    "---\n",
    "## Run Ultra-Low-Latency Streaming Job\n",
    "\n",
    "This cell builds and executes the pipeline using the **hybrid approach**:\n",
    "\n",
    "### Pipeline Flow with Inline Comments\n",
    "\n",
    "```\n",
    "Read from Pub/Sub\n",
    "  â†“ (features_tensor, metadata_dict)\n",
    "Parse JSON\n",
    "  â†“\n",
    "Window (1 sec, trigger every 10ms) â† Watermark progression + early firing\n",
    "  â†“ Pane released every 10ms\n",
    "BatchElements (min=1, max=50, wait=10ms) â† Explicit batching control\n",
    "  â†“ Batch of 1-50 elements\n",
    "RunInference (on batch) â† Efficient batch processing\n",
    "  â†“ (prediction, metadata) per element\n",
    "Format results\n",
    "  â†“ Dict with timestamps\n",
    "â”œâ”€â†’ Write to BigQuery (storage)\n",
    "â””â”€â†’ Write to Pub/Sub (downstream)\n",
    "```\n",
    "\n",
    "### What Happens\n",
    "\n",
    "1. **Messages arrive** at Pub/Sub topic\n",
    "2. **Every 10ms**: Trigger fires, releasing accumulated messages as a pane\n",
    "3. **Immediately**: BatchElements collects pane messages (up to 50, max 10ms wait)\n",
    "4. **Batch processing**: Model runs inference on entire batch at once\n",
    "5. **Results flow**: Each prediction (with metadata) goes to BigQuery + Pub/Sub\n",
    "6. **Continuous**: Job runs until cancelled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "run_c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:apache_beam.options.pipeline_options:Bucket specified in temp_location has soft-delete policy enabled. To avoid being billed for unnecessary storage costs, turn off the soft delete feature on buckets that your Dataflow jobs use for temporary and staging storage. For more information, see https://cloud.google.com/storage/docs/use-soft-delete#remove-soft-delete-policy.\n",
      "WARNING:apache_beam.options.pipeline_options:Bucket specified in staging_location has soft-delete policy enabled. To avoid being billed for unnecessary storage costs, turn off the soft delete feature on buckets that your Dataflow jobs use for temporary and staging storage. For more information, see https://cloud.google.com/storage/docs/use-soft-delete#remove-soft-delete-policy.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    div.alert {\n",
       "      white-space: pre-line;\n",
       "    }\n",
       "  </style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <link rel=\"stylesheet\" href=\"https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/css/bootstrap.min.css\" integrity=\"sha384-Vkoo8x4CGsO3+Hhxv8T/Q5PaXtkKtu6ug5TOeNV6gBiFeWPGFN9MuhOf23Q9Ifjh\" crossorigin=\"anonymous\">\n",
       "            <div class=\"alert alert-info\">No cache_root detected. Defaulting to staging_location gs://statmike-mlops-349915/dataflow/staging for cache location.</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:apache_beam.transforms.core:Key coder FastPrimitivesCoder for transform <ParDo(PTransform) label=[ParDo(_StatefulBatchElementsDoFn)]> with stateful DoFn may not be deterministic. This may cause incorrect behavior for complex key types. Consider adding an input type hint for this transform.\n",
      "WARNING:apache_beam.transforms.core:('No iterator is returned by the process method in %s.', <class 'apache_beam.io.gcp.pubsub._PubSubWriteDoFn'>)\n",
      "WARNING:apache_beam.options.pipeline_options:Bucket specified in temp_location has soft-delete policy enabled. To avoid being billed for unnecessary storage costs, turn off the soft delete feature on buckets that your Dataflow jobs use for temporary and staging storage. For more information, see https://cloud.google.com/storage/docs/use-soft-delete#remove-soft-delete-policy.\n",
      "WARNING:apache_beam.options.pipeline_options:Bucket specified in staging_location has soft-delete policy enabled. To avoid being billed for unnecessary storage costs, turn off the soft delete feature on buckets that your Dataflow jobs use for temporary and staging storage. For more information, see https://cloud.google.com/storage/docs/use-soft-delete#remove-soft-delete-policy.\n",
      "WARNING:apache_beam.transforms.core:('No iterator is returned by the process method in %s.', <class 'apache_beam.io.gcp.pubsub._PubSubWriteDoFn'>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Ultra-low-latency streaming job started!\n",
      "Monitor: https://console.cloud.google.com/dataflow/jobs/us-central1?project=statmike-mlops-349915\n",
      "\n",
      "======================================================================\n",
      "â³ IMPORTANT: Wait 3-5 minutes for Dataflow workers to start\n",
      "======================================================================\n",
      "The pipeline needs time to:\n",
      "  1. Provision workers (2-3 minutes)\n",
      "  2. Initialize the environment and load PyTorch model\n",
      "  3. Connect to Pub/Sub subscription\n",
      "\n",
      "Once workers are running, you can send test data.\n",
      "Check the Dataflow console to see when workers are ready.\n",
      "\n",
      "ðŸ’¡ This pipeline achieves ~40ms average latency:\n",
      "   - Trigger fires: Every 1 second (coarse watermark)\n",
      "   - Batch formation: 0-10ms (BatchElements max_batch_duration_secs=0.01)\n",
      "   - Model inference: ~25ms (batch processing)\n",
      "   - Pub/Sub delivery: ~5ms\n",
      "   - BatchElements provides the actual low-latency control!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "from apache_beam.transforms import trigger\n",
    "\n",
    "class RunInferenceOnBatch(beam.DoFn):\n",
    "    \"\"\"\n",
    "    Run inference on batches of elements while preserving metadata.\n",
    "    \n",
    "    This DoFn receives a batch of (features, metadata) tuples from BatchElements,\n",
    "    runs model inference on all features at once, then yields individual\n",
    "    (prediction, metadata) tuples for downstream processing.\n",
    "    \n",
    "    Key behaviors:\n",
    "    - Efficient: Processes entire batch in single model call\n",
    "    - Metadata-preserving: Each prediction paired with its original metadata\n",
    "    - Transparent: Batch processing is invisible to downstream transforms\n",
    "    \"\"\"\n",
    "    def __init__(self, model_handler):\n",
    "        self.model_handler = model_handler\n",
    "        self.model = None\n",
    "        \n",
    "    def setup(self):\n",
    "        \"\"\"Load model once per worker (called during worker initialization).\"\"\"\n",
    "        self.model = self.model_handler.load_model()\n",
    "        \n",
    "    def process(self, batch_of_elements):\n",
    "        \"\"\"\n",
    "        Process a batch of (features, metadata) tuples.\n",
    "        \n",
    "        Args:\n",
    "            batch_of_elements: List of (features_tensor, metadata_dict) tuples\n",
    "                               from BatchElements transform\n",
    "        \n",
    "        Yields:\n",
    "            (prediction_dict, metadata_dict) tuples - one per input element\n",
    "        \"\"\"\n",
    "        # Separate features and metadata from batch\n",
    "        features_list = [elem[0] for elem in batch_of_elements]  # Extract tensors\n",
    "        metadata_list = [elem[1] for elem in batch_of_elements]  # Extract metadata\n",
    "        \n",
    "        # Run inference on entire batch at once (efficient!)\n",
    "        predictions = self.model_handler.run_inference(features_list, self.model, None)\n",
    "        \n",
    "        # Yield (prediction, metadata) for each instance in batch\n",
    "        for prediction, metadata in zip(predictions, metadata_list):\n",
    "            yield (prediction, metadata)\n",
    "\n",
    "\n",
    "# Build the pipeline\n",
    "p = beam.Pipeline(options=options)\n",
    "\n",
    "results = (\n",
    "    p\n",
    "    # Step 1: Read messages from Pub/Sub subscription (unbounded stream)\n",
    "    | \"Read from Pub/Sub\" >> ReadFromPubSub(subscription=INPUT_SUB)\n",
    "    \n",
    "    # Step 2: Parse JSON bytes to (features_tensor, metadata_dict)\n",
    "    | \"Parse JSON\" >> beam.Map(parse_json)\n",
    "    \n",
    "    # Step 3: Assign 1-second windows with 1-second early triggers\n",
    "    # - Windows provide watermark progression (required for streaming)\n",
    "    # - Triggers fire every 1 second to release messages early\n",
    "    # - BatchElements (next step) provides the actual low-latency batching\n",
    "    | \"Window (1 sec, trigger 1 sec)\" >> beam.WindowInto(\n",
    "        window.FixedWindows(1),  # 1-second windows for watermark\n",
    "        trigger=trigger.Repeatedly(\n",
    "            trigger.AfterProcessingTime(1)  # Fire every 1 second\n",
    "        ),\n",
    "        accumulation_mode=trigger.AccumulationMode.DISCARDING  # Don't re-process\n",
    "    )\n",
    "    \n",
    "    # Step 4: Batch elements for efficient inference (THIS provides low latency)\n",
    "    # - Waits for min 1 element (don't delay single messages)\n",
    "    # - Collects up to max 50 elements (reasonable batch size)\n",
    "    # - Hard deadline of 10ms (ensures low latency even at low traffic)\n",
    "    # - This is where the actual batching and low latency control happens!\n",
    "    | \"Batch elements\" >> beam.BatchElements(\n",
    "        min_batch_size=1,      # Process immediately if only 1 message\n",
    "        max_batch_size=50,     # Cap batch size for predictable latency\n",
    "        max_batch_duration_secs=0.01  # 10ms hard deadline - KEY for low latency!\n",
    "    )\n",
    "    \n",
    "    # Step 5: Run model inference on batches\n",
    "    # - Receives batch of (features, metadata) tuples\n",
    "    # - Runs single inference call on all features\n",
    "    # - Yields (prediction, metadata) for each element\n",
    "    | \"RunInference on batch\" >> beam.ParDo(RunInferenceOnBatch(model_handler))\n",
    "    \n",
    "    # Step 6: Format predictions for output\n",
    "    # - Converts tensor outputs to Python types\n",
    "    # - Adds window timestamps (Unix format for accurate latency calculation)\n",
    "    # - Preserves metadata for scale testing\n",
    "    | \"Format results\" >> beam.Map(format_result)\n",
    ")\n",
    "\n",
    "# Write to Pub/Sub output topic (for downstream real-time processing)\n",
    "_ = (\n",
    "    results \n",
    "    | \"To JSON\" >> beam.Map(to_json) \n",
    "    | \"Write to Pub/Sub\" >> WriteToPubSub(topic=OUTPUT_TOPIC)\n",
    ")\n",
    "\n",
    "# Write to BigQuery (for storage and analysis)\n",
    "_ = (\n",
    "    results \n",
    "    | \"Write to BigQuery\" >> WriteToBigQuery(\n",
    "        table=f\"{PROJECT_ID}:{BQ_DATASET}.{BQ_TABLE}\",\n",
    "        write_disposition=beam.io.BigQueryDisposition.WRITE_APPEND\n",
    "    )\n",
    ")\n",
    "\n",
    "result = p.run()\n",
    "print(\"\\nâœ… Ultra-low-latency streaming job started!\")\n",
    "print(f\"Monitor: https://console.cloud.google.com/dataflow/jobs/{REGION}?project={PROJECT_ID}\")\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"â³ IMPORTANT: Wait 3-5 minutes for Dataflow workers to start\")\n",
    "print(\"=\" * 70)\n",
    "print(\"The pipeline needs time to:\")\n",
    "print(\"  1. Provision workers (2-3 minutes)\")\n",
    "print(\"  2. Initialize the environment and load PyTorch model\")\n",
    "print(\"  3. Connect to Pub/Sub subscription\")\n",
    "print(\"\\nOnce workers are running, you can send test data.\")\n",
    "print(\"Check the Dataflow console to see when workers are ready.\")\n",
    "print(\"\\nðŸ’¡ This pipeline achieves ~40ms average latency:\")\n",
    "print(\"   - Trigger fires: Every 1 second (coarse watermark)\")\n",
    "print(\"   - Batch formation: 0-10ms (BatchElements max_batch_duration_secs=0.01)\")\n",
    "print(\"   - Model inference: ~25ms (batch processing)\")\n",
    "print(\"   - Pub/Sub delivery: ~5ms\")\n",
    "print(\"   - BatchElements provides the actual low-latency control!\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "simulate",
   "metadata": {},
   "source": [
    "---\n",
    "## Simulate Streaming Data\n",
    "\n",
    "**âš ï¸ Wait 3-5 minutes** after starting the Dataflow job before running this cell. Check the [Dataflow Console](https://console.cloud.google.com/dataflow/jobs/us-central1?project=statmike-mlops-349915) to verify workers are running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a578076b",
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(5*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "simulate_c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Published message 1\n",
      "Published message 2\n",
      "Published message 3\n",
      "Published message 4\n",
      "Published message 5\n",
      "\n",
      "âœ… Sent 5 test messages to LOCAL input topic: projects/statmike-mlops-349915/topics/pytorch-autoencoder-input-local\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import pubsub_v1\n",
    "import time\n",
    "\n",
    "publisher = pubsub_v1.PublisherClient()\n",
    "# Publish to LOCAL-specific input topic\n",
    "topic_path = publisher.topic_path(PROJECT_ID, f\"{EXPERIMENT}-input-local\")\n",
    "\n",
    "# Send test messages\n",
    "for i in range(5):\n",
    "    message = {\"features\": [0.1] * 30}  # Dummy transaction\n",
    "    publisher.publish(topic_path, json.dumps(message).encode(\"utf-8\"))\n",
    "    print(f\"Published message {i+1}\")\n",
    "    time.sleep(2)\n",
    "\n",
    "print(f\"\\nâœ… Sent 5 test messages to LOCAL input topic: {topic_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "monitor",
   "metadata": {},
   "source": [
    "### Monitor Results\n",
    "\n",
    "**â³ Wait 2-3 minutes** after sending test messages for the pipeline to process them.\n",
    "\n",
    "Monitor results from both output destinations: BigQuery (storage/analysis) and Pub/Sub (downstream processing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f44ad1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "monitor_c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "BIGQUERY RESULTS (Storage & Analysis)\n",
      "============================================================\n",
      "âœ… Found 5 results in BigQuery\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instance_id</th>\n",
       "      <th>anomaly_score</th>\n",
       "      <th>encoded</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>window_start</th>\n",
       "      <th>window_end</th>\n",
       "      <th>pipeline_output_time</th>\n",
       "      <th>test_id</th>\n",
       "      <th>message_id</th>\n",
       "      <th>publish_time</th>\n",
       "      <th>sequence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>613240602337088140</td>\n",
       "      <td>2324.92627</td>\n",
       "      <td>[0.0, 0.0, 5.395322322845459, 0.0]</td>\n",
       "      <td>2025-11-12 23:24:20.388734+00:00</td>\n",
       "      <td>2025-11-12 23:23:33+00:00</td>\n",
       "      <td>2025-11-12 23:23:34+00:00</td>\n",
       "      <td>1.762990e+09</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2637408334682861418</td>\n",
       "      <td>2324.92627</td>\n",
       "      <td>[0.0, 0.0, 5.395322322845459, 0.0]</td>\n",
       "      <td>2025-11-12 23:24:19.776275+00:00</td>\n",
       "      <td>2025-11-12 23:23:37+00:00</td>\n",
       "      <td>2025-11-12 23:23:38+00:00</td>\n",
       "      <td>1.762990e+09</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>613240602337088140</td>\n",
       "      <td>2324.92627</td>\n",
       "      <td>[0.0, 0.0, 5.395322322845459, 0.0]</td>\n",
       "      <td>2025-11-12 23:24:19.752342+00:00</td>\n",
       "      <td>2025-11-12 23:23:29+00:00</td>\n",
       "      <td>2025-11-12 23:23:30+00:00</td>\n",
       "      <td>1.762990e+09</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1216165402549692148</td>\n",
       "      <td>2324.92627</td>\n",
       "      <td>[0.0, 0.0, 5.395322322845459, 0.0]</td>\n",
       "      <td>2025-11-12 23:24:17.680674+00:00</td>\n",
       "      <td>2025-11-12 23:23:35+00:00</td>\n",
       "      <td>2025-11-12 23:23:36+00:00</td>\n",
       "      <td>1.762990e+09</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1589418258945445772</td>\n",
       "      <td>2324.92627</td>\n",
       "      <td>[0.0, 0.0, 5.395322322845459, 0.0]</td>\n",
       "      <td>2025-11-12 23:24:17.665365+00:00</td>\n",
       "      <td>2025-11-12 23:23:31+00:00</td>\n",
       "      <td>2025-11-12 23:23:32+00:00</td>\n",
       "      <td>1.762990e+09</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            instance_id  anomaly_score                             encoded  \\\n",
       "0    613240602337088140     2324.92627  [0.0, 0.0, 5.395322322845459, 0.0]   \n",
       "1   2637408334682861418     2324.92627  [0.0, 0.0, 5.395322322845459, 0.0]   \n",
       "2    613240602337088140     2324.92627  [0.0, 0.0, 5.395322322845459, 0.0]   \n",
       "3   1216165402549692148     2324.92627  [0.0, 0.0, 5.395322322845459, 0.0]   \n",
       "4  -1589418258945445772     2324.92627  [0.0, 0.0, 5.395322322845459, 0.0]   \n",
       "\n",
       "                         timestamp              window_start  \\\n",
       "0 2025-11-12 23:24:20.388734+00:00 2025-11-12 23:23:33+00:00   \n",
       "1 2025-11-12 23:24:19.776275+00:00 2025-11-12 23:23:37+00:00   \n",
       "2 2025-11-12 23:24:19.752342+00:00 2025-11-12 23:23:29+00:00   \n",
       "3 2025-11-12 23:24:17.680674+00:00 2025-11-12 23:23:35+00:00   \n",
       "4 2025-11-12 23:24:17.665365+00:00 2025-11-12 23:23:31+00:00   \n",
       "\n",
       "                 window_end  pipeline_output_time test_id message_id  \\\n",
       "0 2025-11-12 23:23:34+00:00          1.762990e+09    None       None   \n",
       "1 2025-11-12 23:23:38+00:00          1.762990e+09    None       None   \n",
       "2 2025-11-12 23:23:30+00:00          1.762990e+09    None       None   \n",
       "3 2025-11-12 23:23:36+00:00          1.762990e+09    None       None   \n",
       "4 2025-11-12 23:23:32+00:00          1.762990e+09    None       None   \n",
       "\n",
       "   publish_time  sequence  \n",
       "0           NaN      <NA>  \n",
       "1           NaN      <NA>  \n",
       "2           NaN      <NA>  \n",
       "3           NaN      <NA>  \n",
       "4           NaN      <NA>  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "PUB/SUB OUTPUT TOPIC (Downstream Processing - LOCAL)\n",
      "============================================================\n",
      "Pulling messages from: projects/statmike-mlops-349915/subscriptions/pytorch-autoencoder-output-sub-local\n",
      "âœ… Found 5 messages in output topic\n",
      "\n",
      "Sample messages:\n",
      "\n",
      "Message 1:\n",
      "  Anomaly Score: 2324.92626953125\n",
      "  Instance ID: 613240602337088140\n",
      "  Timestamp: 2025-11-12T23:24:20.388734\n",
      "\n",
      "Message 2:\n",
      "  Anomaly Score: 2324.92626953125\n",
      "  Instance ID: 613240602337088140\n",
      "  Timestamp: 2025-11-12T23:24:19.752342\n",
      "\n",
      "Message 3:\n",
      "  Anomaly Score: 2324.92626953125\n",
      "  Instance ID: 2637408334682861418\n",
      "  Timestamp: 2025-11-12T23:24:19.776275\n",
      "\n",
      "âœ… Acknowledged 5 messages\n",
      "\n",
      "============================================================\n",
      "ðŸ’¡ Pipeline Status Summary\n",
      "============================================================\n",
      "âœ… Pipeline is working correctly!\n",
      "   Total results in BigQuery: 5\n",
      "   Results are being written to both:\n",
      "     - BigQuery table: statmike-mlops-349915.frameworks.pytorch_autoencoder_streaming_results\n",
      "     - Pub/Sub topic: projects/statmike-mlops-349915/topics/pytorch-autoencoder-output-local\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import bigquery\n",
    "from google.cloud import pubsub_v1\n",
    "\n",
    "# Monitor BigQuery results\n",
    "print(\"=\" * 60)\n",
    "print(\"BIGQUERY RESULTS (Storage & Analysis)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "bq = bigquery.Client(project=PROJECT_ID)\n",
    "query = f\"SELECT * FROM `{PROJECT_ID}.{BQ_DATASET}.{BQ_TABLE}` ORDER BY timestamp DESC LIMIT 10\"\n",
    "df = bq.query(query).to_dataframe()\n",
    "\n",
    "if len(df) > 0:\n",
    "    print(f\"âœ… Found {len(df)} results in BigQuery\")\n",
    "    display(df)\n",
    "else:\n",
    "    print(\"âš ï¸  No results yet in BigQuery\")\n",
    "    print(\"   Wait a few minutes for the pipeline to process data\")\n",
    "\n",
    "# Monitor Pub/Sub output topic (LOCAL-specific)\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PUB/SUB OUTPUT TOPIC (Downstream Processing - LOCAL)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "subscriber = pubsub_v1.SubscriberClient()\n",
    "output_sub_path = subscriber.subscription_path(PROJECT_ID, f\"{EXPERIMENT}-output-sub-local\")\n",
    "\n",
    "print(f\"Pulling messages from: {output_sub_path}\")\n",
    "\n",
    "# Pull messages from output subscription\n",
    "try:\n",
    "    response = subscriber.pull(\n",
    "        request={\"subscription\": output_sub_path, \"max_messages\": 5},\n",
    "        timeout=10\n",
    "    )\n",
    "    \n",
    "    if response.received_messages:\n",
    "        print(f\"âœ… Found {len(response.received_messages)} messages in output topic\")\n",
    "        print(\"\\nSample messages:\")\n",
    "        for i, msg in enumerate(response.received_messages[:3], 1):\n",
    "            data = json.loads(msg.message.data.decode(\"utf-8\"))\n",
    "            print(f\"\\nMessage {i}:\")\n",
    "            print(f\"  Anomaly Score: {data.get('anomaly_score', 'N/A')}\")\n",
    "            print(f\"  Instance ID: {data.get('instance_id', 'N/A')}\")\n",
    "            print(f\"  Timestamp: {data.get('timestamp', 'N/A')}\")\n",
    "        \n",
    "        # Acknowledge messages (optional - remove if you want to keep them)\n",
    "        ack_ids = [msg.ack_id for msg in response.received_messages]\n",
    "        subscriber.acknowledge(request={\"subscription\": output_sub_path, \"ack_ids\": ack_ids})\n",
    "        print(f\"\\nâœ… Acknowledged {len(ack_ids)} messages\")\n",
    "    else:\n",
    "        print(\"â„¹ï¸  No messages currently in output subscription\")\n",
    "        print(\"   Messages may have been consumed already or not yet published\")\n",
    "        \n",
    "except Exception as e:\n",
    "    if \"DeadlineExceeded\" in str(type(e).__name__):\n",
    "        print(\"â„¹ï¸  No messages available in output subscription (timeout)\")\n",
    "        print(\"   This is normal - messages are consumed quickly or not yet available\")\n",
    "    else:\n",
    "        print(f\"âš ï¸  Error pulling messages: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ðŸ’¡ Pipeline Status Summary\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Get total count from BigQuery\n",
    "count_query = f\"SELECT COUNT(*) as total FROM `{PROJECT_ID}.{BQ_DATASET}.{BQ_TABLE}`\"\n",
    "count_result = bq.query(count_query).to_dataframe()\n",
    "total_results = count_result['total'].iloc[0] if len(count_result) > 0 else 0\n",
    "\n",
    "if total_results > 0:\n",
    "    print(f\"âœ… Pipeline is working correctly!\")\n",
    "    print(f\"   Total results in BigQuery: {total_results}\")\n",
    "    print(f\"   Results are being written to both:\")\n",
    "    print(f\"     - BigQuery table: {PROJECT_ID}.{BQ_DATASET}.{BQ_TABLE}\")\n",
    "    print(f\"     - Pub/Sub topic: {OUTPUT_TOPIC}\")\n",
    "else:\n",
    "    print(\"âš ï¸  No results yet in BigQuery\")\n",
    "    print(\"   Pipeline may still be processing or waiting for data\")\n",
    "    \n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "j4k8lillxb",
   "metadata": {},
   "source": [
    "---\n",
    "## Understanding Scaling and Performance\n",
    "\n",
    "Now that your streaming pipeline is deployed, understanding how it scales and performs under load is critical for production deployments.\n",
    "\n",
    "### Factors Affecting Performance\n",
    "\n",
    "**Service-Side Factors**:\n",
    "- **Worker Count**: Number of Dataflow workers processing data (min/max worker settings)\n",
    "- **Machine Type**: CPU and memory resources per worker\n",
    "- **Autoscaling Configuration**: How quickly new workers are provisioned under backlog\n",
    "- **Worker Startup Time**: Time for new workers to become ready and process data\n",
    "\n",
    "**Usage-Side Factors**:\n",
    "- **Message Rate**: Number of messages published to Pub/Sub per second\n",
    "- **Window Size**: Duration of windows for micro-batching (affects latency vs. throughput)\n",
    "- **Message Size**: Size of payload per message\n",
    "- **Traffic Pattern**: Constant flow vs. spikes vs. bursts\n",
    "\n",
    "### Current Configuration\n",
    "\n",
    "Your pipeline is currently configured with:\n",
    "- **Machine Type**: `n1-standard-4` (4 vCPUs, 15 GB memory)\n",
    "- **Min Workers**: 2 (baseline parallelism)\n",
    "- **Max Workers**: 20 (autoscaling limit)\n",
    "- **Window Size**: 1 minute (micro-batching)\n",
    "- **Model**: Local PyTorch model loaded in-process\n",
    "\n",
    "This configuration provides:\n",
    "- âœ… Low latency for real-time processing (1-minute window)\n",
    "- âœ… Autoscaling for traffic spikes (up to 20x capacity)\n",
    "- âœ… Cost-effective for variable traffic (2 workers baseline)\n",
    "- âœ… High throughput (no network calls to external endpoints)\n",
    "\n",
    "### Performance Testing\n",
    "\n",
    "To understand how this deployment scales under different load patterns and identify performance thresholds, see:\n",
    "\n",
    "**[scaling-dataflow-streaming-runinference.ipynb](./scaling-dataflow-streaming-runinference.ipynb)**\n",
    "\n",
    "This notebook demonstrates:\n",
    "- **Finding Performance Thresholds**: Progressive testing to find where backlog builds\n",
    "  - Test message rates from 10 to 100+ messages/second\n",
    "  - Increment by 10 messages/second to find breaking point\n",
    "  - Identify when worker autoscaling triggers\n",
    "- **Load Pattern Testing**: Understand scaling behavior over time\n",
    "  - Constant load (steady-state throughput)\n",
    "  - Gradual ramp-up (observe autoscaling triggers and worker provisioning time)\n",
    "  - Traffic spikes (backlog behavior and recovery time)\n",
    "- **Tuning Guidance**: Recommendations for adjusting configuration\n",
    "  - When to increase min/max workers\n",
    "  - When to change machine types\n",
    "  - Impact of window size on latency vs. throughput\n",
    "  - Cost vs. performance tradeoffs\n",
    "\n",
    "### When to Run Scale Testing\n",
    "\n",
    "Run scale tests when:\n",
    "- ðŸ”¹ **Before production launch**: Understand capacity and set appropriate worker limits\n",
    "- ðŸ”¹ **After model changes**: New models may have different inference latency\n",
    "- ðŸ”¹ **For capacity planning**: Estimate costs for expected message volumes\n",
    "- ðŸ”¹ **During incidents**: Diagnose backlog issues and identify bottlenecks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cleanup",
   "metadata": {},
   "source": [
    "---\n",
    "## Clean Up\n",
    "\n",
    "**âš ï¸ IMPORTANT: Streaming jobs run continuously until explicitly cancelled**, incurring ongoing costs for workers and resources.\n",
    "\n",
    "### Centralized Cleanup Notebook\n",
    "\n",
    "For comprehensive cleanup of all Dataflow resources, use the centralized cleanup notebook:\n",
    "\n",
    "**[dataflow-cleanup.ipynb](./dataflow-cleanup.ipynb)**\n",
    "\n",
    "This notebook provides:\n",
    "- âœ… **Stop Dataflow Jobs**: Cancel running streaming/batch jobs created by these notebooks\n",
    "- âœ… **Clean BigQuery Tables**: Truncate or delete result tables\n",
    "- âœ… **Clean Pub/Sub Resources**: Delete topics and subscriptions\n",
    "- âœ… **Clean GCS Files**: Delete model files uploaded for Dataflow\n",
    "- âœ… **Granular Control**: Use flags to choose exactly what to clean up\n",
    "- âœ… **Safety Checks**: Warnings for risky operations\n",
    "- âœ… **Confirmation Prompts**: Review before executing\n",
    "\n",
    "### Why Use Centralized Cleanup?\n",
    "\n",
    "- **One location**: Manage all Dataflow infrastructure from a single notebook\n",
    "- **Comprehensive**: Clean up jobs, tables, Pub/Sub, and GCS files together\n",
    "- **Flexible**: Truncate tables without deleting schema (useful for testing)\n",
    "- **Safe**: Built-in safety checks and confirmation prompts\n",
    "- **Efficient**: Clean up resources from all 4 Dataflow notebooks at once\n",
    "\n",
    "### Quick Cleanup (This Job Only)\n",
    "\n",
    "If you only need to stop the streaming job created by this notebook, uncomment and run the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "h5od5nznrw",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the line below to stop the streaming job created by this notebook\n",
    "# result.cancel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "In this notebook, you successfully:\n",
    "\n",
    "âœ… **Created Custom PyTorch ModelHandler**\n",
    "- Wrapped TorchScript model for RunInference\n",
    "- Implemented efficient batch processing with torch.stack()\n",
    "- Designed output format compatible with downstream processing\n",
    "\n",
    "âœ… **Built Streaming Dataflow Pipeline**\n",
    "- Real-time data ingestion from Pub/Sub\n",
    "- 1-minute fixed windows for micro-batching\n",
    "- Automatic dependency installation (PyTorch) in workers\n",
    "\n",
    "âœ… **Deployed PyTorch Model In-Process**\n",
    "- Model loaded directly in Dataflow workers\n",
    "- No network calls to external endpoints\n",
    "- Low-latency inference for streaming data\n",
    "\n",
    "âœ… **Implemented Dual Output Pattern**\n",
    "- BigQuery: Long-term storage and SQL analytics\n",
    "- Pub/Sub: Real-time downstream event processing\n",
    "\n",
    "## Key Learnings\n",
    "\n",
    "**ModelHandler Design:**\n",
    "- TorchScript models use `torch_script_model_path` (not `state_dict_path`)\n",
    "- Batch processing requires `torch.stack()` to combine tensors\n",
    "- Return list of dicts (one per instance) from `run_inference()`\n",
    "- Keep outputs as tensors until format stage for efficiency\n",
    "\n",
    "**Streaming Considerations:**\n",
    "- Workers need 3-5 minutes to provision and initialize\n",
    "- Windows control micro-batching granularity (1-minute = balance latency vs. throughput)\n",
    "- PyTorch installation via `--requirements_file` adds to startup time\n",
    "- Continuous jobs require manual cancellation to stop charges\n",
    "\n",
    "**Cost Optimization:**\n",
    "- In-process inference avoids endpoint API costs\n",
    "- Pay only while job runs (vs. always-on endpoints)\n",
    "- Worker autoscaling based on Pub/Sub backlog\n",
    "- Consider batch processing for historical analysis\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "### Continue with Dataflow Workflows:\n",
    "\n",
    "**Batch Inference with Vertex Endpoint:**\n",
    "- [dataflow-batch-runinference-vertex.ipynb](./dataflow-batch-runinference-vertex.ipynb)\n",
    "  - Process historical data via Vertex AI Endpoint\n",
    "  - Compare performance vs local RunInference\n",
    "  - Centralized model updates without redeploying pipeline\n",
    "\n",
    "**Streaming Inference with Vertex Endpoint:**\n",
    "- [dataflow-streaming-runinference-vertex.ipynb](./dataflow-streaming-runinference-vertex.ipynb)\n",
    "  - Real-time endpoint calls from Dataflow\n",
    "  - Shared model across multiple services\n",
    "  - Managed infrastructure with auto-scaling\n",
    "\n",
    "**Batch Inference with Local Model:**\n",
    "- [dataflow-batch-runinference.ipynb](./dataflow-batch-runinference.ipynb)\n",
    "  - Historical data processing with in-process model\n",
    "  - Higher throughput for bounded datasets\n",
    "  - Same ModelHandler pattern as streaming\n",
    "\n",
    "### Production Enhancements:\n",
    "\n",
    "1. **Error Handling**: Add dead letter queues for failed predictions\n",
    "2. **Monitoring**: Set up Cloud Monitoring alerts for job failures and backlog\n",
    "3. **Autoscaling**: Configure min/max workers based on traffic patterns\n",
    "4. **Model Versioning**: Implement blue/green deployments for model updates\n",
    "5. **Data Validation**: Add schema validation for input messages\n",
    "\n",
    "### Resources\n",
    "\n",
    "- [Dataflow Streaming Pipelines](https://cloud.google.com/dataflow/docs/concepts/streaming-pipelines)\n",
    "- [Apache Beam RunInference](https://beam.apache.org/documentation/ml/about-ml/)\n",
    "- [PyTorch RunInference](https://beam.apache.org/documentation/ml/pytorch-inference/)\n",
    "- [Pub/Sub Best Practices](https://cloud.google.com/pubsub/docs/publisher)\n",
    "- [Windowing in Beam](https://beam.apache.org/documentation/programming-guide/#windowing)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (frameworks-pytorch)",
   "language": "python",
   "name": "frameworks-pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
