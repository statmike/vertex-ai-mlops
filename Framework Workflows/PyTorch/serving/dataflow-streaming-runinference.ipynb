{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1351230",
   "metadata": {},
   "source": [
    "![tracker](https://us-central1-vertex-ai-mlops-369716.cloudfunctions.net/pixel-tracking?path=statmike%2Fvertex-ai-mlops%2FFramework+Workflows%2FPyTorch%2Fserving&file=dataflow-streaming-runinference.ipynb)\n",
    "<!--- header table --->\n",
    "<table align=\"left\">\n",
    "<tr>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%20Workflows/PyTorch/serving/dataflow-streaming-runinference.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.svgrepo.com/download/217753/github.svg\" alt=\"GitHub logo\">\n",
    "      <br>View on<br>GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://colab.research.google.com/github/statmike/vertex-ai-mlops/blob/main/Framework%20Workflows/PyTorch/serving/dataflow-streaming-runinference.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.gstatic.com/pantheon/images/bigquery/welcome_page/colab-logo.svg\" alt=\"Google Colaboratory logo\">\n",
    "      <br>Run in<br>Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https%3A%2F%2Fraw.githubusercontent.com%2Fstatmike%2Fvertex-ai-mlops%2Fmain%2FFramework%2520Workflows%2FPyTorch%2Fserving%2Fdataflow-streaming-runinference.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\">\n",
    "      <br>Run in<br>Colab Enterprise\n",
    "    </a>\n",
    "  </td>      \n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/bigquery/import?url=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%20Workflows/PyTorch/serving/dataflow-streaming-runinference.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.gstatic.com/images/branding/gcpiconscolors/bigquery/v1/32px.svg\" alt=\"BigQuery logo\">\n",
    "      <br>Open in<br>BigQuery Studio\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/statmike/vertex-ai-mlops/main/Framework%20Workflows/PyTorch/serving/dataflow-streaming-runinference.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.gstatic.com/images/branding/gcpiconscolors/vertexai/v1/32px.svg\" alt=\"Vertex AI logo\">\n",
    "      <br>Open in<br>Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td colspan=\"5\" style=\"text-align: right\">\n",
    "    <b>Share This On: </b> \n",
    "    <a href=\"https://www.linkedin.com/sharing/share-offsite/?url=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%2520Workflows/PyTorch/serving/dataflow-streaming-runinference.ipynb\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"Linkedin Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://reddit.com/submit?url=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%2520Workflows/PyTorch/serving/dataflow-streaming-runinference.ipynb\"><img src=\"https://redditinc.com/hubfs/Reddit%20Inc/Brand/Reddit_Logo.png\" alt=\"Reddit Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://bsky.app/intent/compose?text=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%2520Workflows/PyTorch/serving/dataflow-streaming-runinference.ipynb\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"BlueSky Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://twitter.com/intent/tweet?url=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%2520Workflows/PyTorch/serving/dataflow-streaming-runinference.ipynb\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/5/5a/X_icon_2.svg\" alt=\"X (Twitter) Logo\" width=\"20px\"></a> \n",
    "  </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td colspan=\"5\" style=\"text-align: right\">\n",
    "    <b>Connect With Author On: </b> \n",
    "    <a href=\"https://www.linkedin.com/in/statmike\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"Linkedin Logo\" width=\"20px\"></a>\n",
    "    <a href=\"https://www.github.com/statmike\"><img src=\"https://www.svgrepo.com/download/217753/github.svg\" alt=\"GitHub Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://www.youtube.com/@statmike-channel\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/f/fd/YouTube_full-color_icon_%282024%29.svg\" alt=\"YouTube Logo\" width=\"20px\"></a>\n",
    "    <a href=\"https://bsky.app/profile/statmike.bsky.social\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"BlueSky Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://x.com/statmike\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/5/5a/X_icon_2.svg\" alt=\"X (Twitter) Logo\" width=\"20px\"></a>\n",
    "  </td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ep1wl4q7e6",
   "metadata": {},
   "source": [
    "# Dataflow Streaming Inference with RunInference\n",
    "\n",
    "This notebook demonstrates streaming processing of transactions using Dataflow with Apache Beam RunInference.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "This workflow covers:\n",
    "\n",
    "1. **Create Custom ModelHandler**: Wrap PyTorch model for RunInference\n",
    "2. **Build Streaming Pipeline**: Read from Pub/Sub, apply model, write to BigQuery and Pub/Sub\n",
    "3. **Run on Dataflow**: Execute continuous pipeline on Google Cloud\n",
    "4. **Monitor Job**: Track streaming job progress and view results\n",
    "5. **Clean Up**: Stop streaming job to avoid ongoing charges\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Completed `dataflow-setup.ipynb` - This sets up:\n",
    "  - Model .pt file extracted from .mar and uploaded to GCS\n",
    "  - BigQuery tables created (including `pytorch_autoencoder_streaming_results`)\n",
    "  - Pub/Sub topics and subscriptions created\n",
    "\n",
    "## Batch vs Streaming\n",
    "\n",
    "**Batch Processing (Previous Notebook)**:\n",
    "- ‚úÖ Process historical data\n",
    "- ‚úÖ Bounded dataset (has a start and end)\n",
    "- ‚úÖ Results available when job completes\n",
    "- ‚úÖ Cost-effective for large datasets\n",
    "- Example: Analyze all transactions from last month\n",
    "\n",
    "**Streaming Processing (This Notebook)**:\n",
    "- ‚úÖ Process real-time data\n",
    "- ‚úÖ Unbounded dataset (continuous)\n",
    "- ‚úÖ Results available immediately\n",
    "- ‚úÖ Low-latency anomaly detection\n",
    "- Example: Flag suspicious transactions as they occur\n",
    "\n",
    "## Architecture\n",
    "\n",
    "```\n",
    "Pub/Sub Input Topic\n",
    "  ‚Üì Read transactions in real-time\n",
    "Dataflow Pipeline\n",
    "  ‚Üì 1-minute windows\n",
    "RunInference (PyTorch Model)\n",
    "  ‚Üì Generate anomaly scores\n",
    "Transform Results\n",
    "  ‚Üì Extract scores and embeddings\n",
    "  ‚îú‚îÄ‚Üí BigQuery (storage & analysis)\n",
    "  ‚îî‚îÄ‚Üí Pub/Sub Output (downstream processing)\n",
    "```\n",
    "\n",
    "## RunInference Benefits\n",
    "\n",
    "- **In-process**: Model loaded directly in workers (no network calls)\n",
    "- **Automatic batching**: Combines instances for efficient inference\n",
    "- **Scalable**: Scales with pipeline workers\n",
    "- **Real-time**: Low-latency predictions for streaming data\n",
    "- **Cost-effective**: Pay only when job runs\n",
    "\n",
    "## Timing Expectations\n",
    "\n",
    "**Total time from start to results: ~8-10 minutes**\n",
    "\n",
    "1. **Start Dataflow job**: Instant\n",
    "2. **Wait for workers**: 3-5 minutes (worker provisioning)\n",
    "3. **Send test messages**: Instant\n",
    "4. **Wait for processing**: 2-3 minutes (1-min window + processing)\n",
    "5. **View results**: Check BigQuery and Pub/Sub\n",
    "\n",
    "## What This Pipeline Does\n",
    "\n",
    "1. Read transactions from Pub/Sub input topic\n",
    "2. Window data into 1-minute batches\n",
    "3. Format data for PyTorch model\n",
    "4. Load TorchScript model in workers\n",
    "5. Run inference to get anomaly scores\n",
    "6. Extract relevant outputs (score + embeddings)\n",
    "7. Write results to BigQuery (for analysis)\n",
    "8. Publish to Pub/Sub output (for downstream systems)\n",
    "9. Job runs continuously until cancelled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "env",
   "metadata": {},
   "source": [
    "---\n",
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "proj",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = 'statmike-mlops-349915'\n",
    "REQ_TYPE = 'ALL'\n",
    "INSTALL_TOOL = 'poetry'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "conf",
   "metadata": {},
   "outputs": [],
   "source": [
    "REQUIREMENTS_URL = 'https://raw.githubusercontent.com/statmike/vertex-ai-mlops/refs/heads/main/Framework%20Workflows/PyTorch/requirements.txt'\n",
    "REQUIRED_APIS = [\"dataflow.googleapis.com\", \"pubsub.googleapis.com\", \"bigquery.googleapis.com\", \"storage.googleapis.com\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "setup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "PYTHON GCP ENVIRONMENT SETUP\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "AUTHENTICATION\n",
      "==================================================\n",
      "Checking for existing ADC...\n",
      "‚úÖ Existing ADC found.\n",
      "‚úÖ Project is correctly set to 'statmike-mlops-349915'.\n",
      "\n",
      "==================================================\n",
      "API CHECK & ENABLE\n",
      "==================================================\n",
      "‚úÖ dataflow.googleapis.com is already enabled.\n",
      "‚úÖ pubsub.googleapis.com is already enabled.\n",
      "‚úÖ bigquery.googleapis.com is already enabled.\n",
      "‚úÖ storage.googleapis.com is already enabled.\n",
      "\n",
      "==================================================\n",
      "PACKAGE MANAGEMENT\n",
      "==================================================\n",
      "Installation Tool: poetry\n",
      "‚úÖ Found poetry at: /usr/local/google/home/statmike/.local/bin/poetry\n",
      "‚úÖ Running in poetry environment: /usr/local/google/home/statmike/.cache/pypoetry/virtualenvs/frameworks-pytorch-0KVJlKeQ-py3.13\n",
      "‚ÑπÔ∏è  Poetry mode: Installing from pyproject.toml (REQUIREMENTS_URL ignored)\n",
      "‚úÖ Found pyproject.toml at: /usr/local/google/home/statmike/Git/vertex-ai-mlops/Framework Workflows/PyTorch/pyproject.toml\n",
      "   Changed working directory to: /usr/local/google/home/statmike/Git/vertex-ai-mlops/Framework Workflows/PyTorch\n",
      "Running poetry install...\n",
      "   Restored working directory to: /usr/local/google/home/statmike/Git/vertex-ai-mlops/Framework Workflows/PyTorch/serving\n",
      "‚úÖ All packages are already installed and up to date.\n",
      "\n",
      "==================================================\n",
      "Google Cloud Project Information\n",
      "==================================================\n",
      "PROJECT_ID     = statmike-mlops-349915\n",
      "PROJECT_NUMBER = 1026793852137\n",
      "==================================================\n",
      "\n",
      "\n",
      "==================================================\n",
      "SETUP SUMMARY\n",
      "==================================================\n",
      "‚úÖ Authentication:    Success\n",
      "‚úÖ API Configuration: Success\n",
      "‚úÖ Package Install:   Already up to date\n",
      "‚úÖ Installation Tool: poetry\n",
      "‚úÖ Project ID:        statmike-mlops-349915\n",
      "‚úÖ Project Number:    1026793852137\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os, urllib.request\n",
    "url = 'https://raw.githubusercontent.com/statmike/vertex-ai-mlops/refs/heads/main/core/notebook-template/python_setup.py'\n",
    "urllib.request.urlretrieve(url, 'python_setup_local.py')\n",
    "import python_setup_local as python_setup\n",
    "os.remove('python_setup_local.py')\n",
    "setup_info = python_setup.setup_environment(PROJECT_ID, REQ_TYPE, REQUIREMENTS_URL, REQUIRED_APIS, INSTALL_TOOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "py",
   "metadata": {},
   "source": [
    "---\n",
    "## Python Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "imp",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import apache_beam as beam\n",
    "from apache_beam import window\n",
    "from apache_beam.ml.inference.base import RunInference\n",
    "from apache_beam.ml.inference.pytorch_inference import PytorchModelHandlerTensor\n",
    "from apache_beam.options.pipeline_options import PipelineOptions, StandardOptions\n",
    "from apache_beam.io.gcp.pubsub import ReadFromPubSub, WriteToPubSub\n",
    "from apache_beam.io.gcp.bigquery import WriteToBigQuery\n",
    "import torch\n",
    "import json\n",
    "from datetime import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "vars",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project: statmike-mlops-349915\n",
      "Model: gs://statmike-mlops-349915/frameworks/pytorch-autoencoder/dataflow/final_model_traced.pt\n",
      "Input subscription: projects/statmike-mlops-349915/subscriptions/pytorch-autoencoder-input-sub\n",
      "Output topic: projects/statmike-mlops-349915/topics/pytorch-autoencoder-output\n",
      "Results table: statmike-mlops-349915.frameworks.pytorch_autoencoder_streaming_results\n"
     ]
    }
   ],
   "source": [
    "PROJECT_ID = subprocess.run([\"gcloud\", \"config\", \"get-value\", \"project\"], capture_output=True, text=True, check=True).stdout.strip()\n",
    "\n",
    "# Configuration\n",
    "REGION = \"us-central1\"\n",
    "SERIES = \"frameworks\"\n",
    "EXPERIMENT = \"pytorch-autoencoder\"\n",
    "\n",
    "# GCS paths (aligned with dataflow-setup.ipynb)\n",
    "BUCKET_NAME = PROJECT_ID\n",
    "BUCKET_URI = f\"gs://{BUCKET_NAME}\"\n",
    "MODEL_PATH = f\"{BUCKET_URI}/{SERIES}/{EXPERIMENT}/dataflow/final_model_traced.pt\"\n",
    "\n",
    "# Pub/Sub configuration\n",
    "INPUT_SUB = f\"projects/{PROJECT_ID}/subscriptions/{EXPERIMENT}-input-sub\"\n",
    "OUTPUT_TOPIC = f\"projects/{PROJECT_ID}/topics/{EXPERIMENT}-output\"\n",
    "\n",
    "# BigQuery configuration\n",
    "BQ_DATASET = SERIES.replace(\"-\", \"_\")\n",
    "BQ_TABLE = f\"{EXPERIMENT.replace('-', '_')}_streaming_results\"\n",
    "\n",
    "# Dataflow configuration\n",
    "DATAFLOW_STAGING = f\"{BUCKET_URI}/dataflow/staging\"\n",
    "DATAFLOW_TEMP = f\"{BUCKET_URI}/dataflow/temp\"\n",
    "\n",
    "print(f\"Project: {PROJECT_ID}\")\n",
    "print(f\"Model: {MODEL_PATH}\")\n",
    "print(f\"Input subscription: {INPUT_SUB}\")\n",
    "print(f\"Output topic: {OUTPUT_TOPIC}\")\n",
    "print(f\"Results table: {PROJECT_ID}.{BQ_DATASET}.{BQ_TABLE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handler",
   "metadata": {},
   "source": [
    "---\n",
    "## Create ModelHandler\n",
    "\n",
    "The ModelHandler wraps the PyTorch model for use with Apache Beam's RunInference transform. This section explains the key design choices for working with TorchScript models in Dataflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "handler_c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ModelHandler created\n",
      "   Model: gs://statmike-mlops-349915/frameworks/pytorch-autoencoder/dataflow/final_model_traced.pt\n",
      "   Device: cpu\n"
     ]
    }
   ],
   "source": [
    "class PyTorchAutoencoderHandler(PytorchModelHandlerTensor):\n",
    "    \"\"\"\n",
    "    Custom ModelHandler for PyTorch autoencoder inference.\n",
    "    \n",
    "    Key Design Choices:\n",
    "    \n",
    "    1. TorchScript Model Loading:\n",
    "       - Uses torch_script_model_path (not state_dict_path)\n",
    "       - TorchScript models (.pt) have architecture embedded\n",
    "       - No need to provide model_class parameter\n",
    "    \n",
    "    2. Batch Processing:\n",
    "       - Input: List of tensors (one per instance)\n",
    "       - Stack into single batch tensor with torch.stack()\n",
    "       - Enables efficient GPU/CPU vectorized operations\n",
    "    \n",
    "    3. Output Format:\n",
    "       - Returns list of dicts (one per instance in batch)\n",
    "       - Each dict contains tensors for that specific instance\n",
    "       - PytorchModelHandlerTensor passes these dicts through directly\n",
    "       - Downstream format_result() function converts tensors to Python types\n",
    "    \n",
    "    4. Model Output:\n",
    "       - Autoencoder returns dict: {\"denormalized_MAE\": tensor, \"encoded\": tensor, ...}\n",
    "       - Extract only needed fields (denormalized_MAE for anomaly score, encoded for embeddings)\n",
    "       - Keep as tensors in run_inference, convert to Python types later\n",
    "    \"\"\"\n",
    "    \n",
    "    def run_inference(self, batch, model, inference_args=None):\n",
    "        \"\"\"\n",
    "        Run inference on a batch of input tensors.\n",
    "        \n",
    "        Args:\n",
    "            batch: List of torch.Tensor, each shape (30,) for our autoencoder\n",
    "            model: Loaded TorchScript model\n",
    "            inference_args: Optional additional arguments (unused)\n",
    "            \n",
    "        Returns:\n",
    "            List of dicts, one per input, containing model outputs as tensors\n",
    "        \"\"\"\n",
    "        # Stack list of tensors into single batch tensor\n",
    "        # Input: [tensor(30,), tensor(30,), ...] -> Output: tensor(batch_size, 30)\n",
    "        batch_tensor = torch.stack(batch)\n",
    "\n",
    "        # Run model inference without gradient computation\n",
    "        with torch.no_grad():\n",
    "            predictions = model(batch_tensor)\n",
    "\n",
    "        # Convert batch output to list of individual results\n",
    "        # This matches the batch size and allows proper per-instance processing\n",
    "        results = []\n",
    "        for i in range(len(batch)):\n",
    "            results.append({\n",
    "                \"denormalized_MAE\": predictions[\"denormalized_MAE\"][i],  # Scalar tensor\n",
    "                \"encoded\": predictions[\"encoded\"][i]  # 1D tensor (embedding)\n",
    "            })\n",
    "        return results\n",
    "\n",
    "\n",
    "# Initialize ModelHandler with TorchScript model from GCS\n",
    "model_handler = PyTorchAutoencoderHandler(\n",
    "    torch_script_model_path=MODEL_PATH,  # Path to .pt file in GCS\n",
    "    device=\"cpu\"  # Run on CPU in Dataflow workers\n",
    ")\n",
    "\n",
    "print(\"‚úÖ ModelHandler created\")\n",
    "print(f\"   Model: {MODEL_PATH}\")\n",
    "print(f\"   Device: cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pipeline",
   "metadata": {},
   "source": [
    "---\n",
    "## Configure Worker Compute Resources\n",
    "\n",
    "Before building the pipeline, configure the compute resources (machine type and autoscaling) for Dataflow workers. These settings directly impact performance, cost, and latency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "qhkd671nl6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "WORKER COMPUTE CONFIGURATION\n",
      "============================================================\n",
      "Machine Type: n1-standard-4\n",
      "  - vCPUs: 4\n",
      "  - Memory: 15 GB\n",
      "\n",
      "Autoscaling:\n",
      "  - Min Workers: 2\n",
      "  - Max Workers: 20\n",
      "  - Estimated capacity: ~200-10000 transactions/sec\n",
      "\n",
      "GPU Support: Not configured (CPU inference)\n",
      "  - To enable GPU: Add --worker_gpu_type and --worker_gpu_count\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Worker Compute Configuration\n",
    "# =============================\n",
    "# These settings control the machine type and autoscaling behavior for Dataflow workers.\n",
    "# Proper configuration ensures efficient resource utilization and cost management.\n",
    "\n",
    "# Machine Type: n1-standard-4\n",
    "# - 4 vCPUs, 15 GB memory\n",
    "# - Suitable for PyTorch inference workloads that need moderate CPU and memory\n",
    "# - Each worker can handle multiple inference requests concurrently\n",
    "# - Alternative: n1-standard-2 (smaller), n1-standard-8 (larger), or custom machine types\n",
    "\n",
    "MACHINE_TYPE = \"n1-standard-4\"\n",
    "\n",
    "# Autoscaling for Streaming Pipelines\n",
    "# - min_workers=2: Ensures pipeline remains responsive even with low traffic\n",
    "# - max_workers=20: Handles traffic spikes without overwhelming resources\n",
    "# - Dataflow autoscales based on Pub/Sub backlog and processing latency\n",
    "\n",
    "MIN_WORKERS = 2\n",
    "MAX_WORKERS = 20\n",
    "\n",
    "# Why These Settings for Streaming?\n",
    "# ----------------------------------\n",
    "# 1. **Minimum Workers (2)**:\n",
    "#    - Provides redundancy (if one worker fails, pipeline continues)\n",
    "#    - Reduces cold start latency when traffic arrives\n",
    "#    - Maintains low end-to-end latency for real-time processing\n",
    "#\n",
    "# 2. **Maximum Workers (20)**:\n",
    "#    - Allows scaling to handle traffic bursts\n",
    "#    - Each n1-standard-4 worker processes ~100-500 transactions/sec\n",
    "#    - 20 workers can handle ~2,000-10,000 transactions/sec\n",
    "#    - Prevents runaway costs from unlimited scaling\n",
    "#\n",
    "# 3. **Machine Type (n1-standard-4)**:\n",
    "#    - PyTorch model loading requires ~2-4 GB memory per worker\n",
    "#    - 15 GB allows model + batch processing overhead\n",
    "#    - 4 vCPUs enable parallel batch inference\n",
    "#    - Cost-effective for moderate throughput requirements\n",
    "\n",
    "# When to Adjust These Settings:\n",
    "# -------------------------------\n",
    "# - **Higher Traffic**: Increase max_workers (e.g., 50-100)\n",
    "# - **Lower Latency**: Increase min_workers (e.g., 5-10) to pre-warm capacity\n",
    "# - **Cost Optimization**: Use smaller machine type (n1-standard-2) if memory permits\n",
    "# - **Larger Models**: Use n1-standard-8 or n1-highmem-4 for memory-intensive models\n",
    "# - **CPU-Intensive Models**: Use c2-standard-4 for compute-optimized instances\n",
    "\n",
    "# GPU Support (Optional)\n",
    "# ----------------------\n",
    "# Dataflow supports GPU workers for accelerated inference:\n",
    "# - Machine type: n1-standard-4 (host machine)\n",
    "# - GPU type: nvidia-tesla-t4, nvidia-tesla-v100, etc.\n",
    "# - GPU count: 1-4 per worker\n",
    "# - Requirements:\n",
    "#   1. Add --worker_machine_type=n1-standard-4\n",
    "#   2. Add --worker_gpu_type=nvidia-tesla-t4\n",
    "#   3. Add --worker_gpu_count=1\n",
    "#   4. Update model handler to use device=\"cuda\"\n",
    "#   5. Ensure PyTorch is GPU-enabled in requirements file\n",
    "#\n",
    "# Note: GPU workers are significantly more expensive than CPU workers.\n",
    "# Only use for models where GPU acceleration provides meaningful speedup.\n",
    "# For small models like this autoencoder, CPU inference is more cost-effective.\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"WORKER COMPUTE CONFIGURATION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Machine Type: {MACHINE_TYPE}\")\n",
    "print(f\"  - vCPUs: 4\")\n",
    "print(f\"  - Memory: 15 GB\")\n",
    "print(f\"\\nAutoscaling:\")\n",
    "print(f\"  - Min Workers: {MIN_WORKERS}\")\n",
    "print(f\"  - Max Workers: {MAX_WORKERS}\")\n",
    "print(f\"  - Estimated capacity: ~{MIN_WORKERS * 100}-{MAX_WORKERS * 500} transactions/sec\")\n",
    "print(f\"\\nGPU Support: Not configured (CPU inference)\")\n",
    "print(f\"  - To enable GPU: Add --worker_gpu_type and --worker_gpu_count\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "pipeline_c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Streaming pipeline configured\n",
      "   Job will run in: us-central1\n",
      "   Staging: gs://statmike-mlops-349915/dataflow/staging\n",
      "   Requirements: /tmp/tmpgi36o8co.txt\n",
      "   Machine type: n1-standard-4\n",
      "   Worker scaling: 2-20 workers\n"
     ]
    }
   ],
   "source": [
    "def parse_json(message):\n",
    "    \"\"\"\n",
    "    Parse Pub/Sub message and convert to PyTorch tensor.\n",
    "    \n",
    "    Args:\n",
    "        message: Bytes from Pub/Sub containing JSON with 'features' key\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor of shape (30,) containing transaction features\n",
    "    \"\"\"\n",
    "    data = json.loads(message.decode(\"utf-8\"))\n",
    "    return torch.tensor(data[\"features\"], dtype=torch.float32)\n",
    "\n",
    "\n",
    "def format_result(element, window=beam.DoFn.WindowParam):\n",
    "    \"\"\"\n",
    "    Format model predictions for output to BigQuery and Pub/Sub.\n",
    "    \n",
    "    Args:\n",
    "        element: Dict returned from run_inference containing tensor outputs\n",
    "        window: Beam window parameter for extracting window boundaries\n",
    "        \n",
    "    Returns:\n",
    "        Dict with Python types suitable for BigQuery/JSON serialization\n",
    "        \n",
    "    Note:\n",
    "        With our custom ModelHandler, element is the dict we returned from run_inference.\n",
    "        PytorchModelHandlerTensor passes these dicts through directly without wrapping.\n",
    "        We convert tensor values to Python types here for serialization.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"instance_id\": str(hash(str(element[\"denormalized_MAE\"].item()))),\n",
    "        \"anomaly_score\": float(element[\"denormalized_MAE\"].item()),\n",
    "        \"encoded\": element[\"encoded\"].tolist(),\n",
    "        \"timestamp\": datetime.utcnow().isoformat(),\n",
    "        \"window_start\": window.start.to_utc_datetime().isoformat(),\n",
    "        \"window_end\": window.end.to_utc_datetime().isoformat()\n",
    "    }\n",
    "\n",
    "\n",
    "def to_json(element):\n",
    "    \"\"\"Convert dict to JSON bytes for Pub/Sub publication.\"\"\"\n",
    "    return json.dumps(element).encode(\"utf-8\")\n",
    "\n",
    "\n",
    "# Create requirements file for Dataflow workers\n",
    "# Workers need PyTorch installed to load and run the model\n",
    "import tempfile\n",
    "requirements_content = \"torch>=2.0.0\\n\"\n",
    "\n",
    "with tempfile.NamedTemporaryFile(mode='w', suffix='.txt', delete=False) as f:\n",
    "    f.write(requirements_content)\n",
    "    requirements_file = f.name\n",
    "\n",
    "# Configure Dataflow pipeline options\n",
    "options = PipelineOptions([\n",
    "    f\"--project={PROJECT_ID}\",\n",
    "    f\"--region={REGION}\",\n",
    "    \"--runner=DataflowRunner\",  # Run on Google Cloud (not locally)\n",
    "    f\"--temp_location={DATAFLOW_TEMP}\",\n",
    "    f\"--staging_location={DATAFLOW_STAGING}\",\n",
    "    f\"--requirements_file={requirements_file}\",  # Install PyTorch in workers\n",
    "    f\"--job_name=pytorch-streaming-{datetime.now().strftime('%Y%m%d-%H%M%S')}\",\n",
    "    \"--streaming\",  # Enable streaming mode\n",
    "    \"--save_main_session\",  # Serialize global imports and variables\n",
    "    # Worker compute configuration\n",
    "    f\"--machine_type={MACHINE_TYPE}\",  # Machine type for workers\n",
    "    f\"--num_workers={MIN_WORKERS}\",  # Initial/minimum number of workers\n",
    "    f\"--max_num_workers={MAX_WORKERS}\",  # Maximum workers for autoscaling\n",
    "])\n",
    "\n",
    "print(\"‚úÖ Streaming pipeline configured\")\n",
    "print(f\"   Job will run in: {REGION}\")\n",
    "print(f\"   Staging: {DATAFLOW_STAGING}\")\n",
    "print(f\"   Requirements: {requirements_file}\")\n",
    "print(f\"   Machine type: {MACHINE_TYPE}\")\n",
    "print(f\"   Worker scaling: {MIN_WORKERS}-{MAX_WORKERS} workers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "run",
   "metadata": {},
   "source": [
    "### Run Streaming Job\n",
    "\n",
    "Build and execute the pipeline graph. The job will:\n",
    "1. Read streaming data from Pub/Sub\n",
    "2. Window into 1-minute fixed windows\n",
    "3. Run PyTorch model inference via RunInference\n",
    "4. Write results to both BigQuery and Pub/Sub output topic\n",
    "\n",
    "The pipeline runs continuously until explicitly cancelled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "run_c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:apache_beam.options.pipeline_options:Bucket specified in temp_location has soft-delete policy enabled. To avoid being billed for unnecessary storage costs, turn off the soft delete feature on buckets that your Dataflow jobs use for temporary and staging storage. For more information, see https://cloud.google.com/storage/docs/use-soft-delete#remove-soft-delete-policy.\n",
      "WARNING:apache_beam.options.pipeline_options:Bucket specified in staging_location has soft-delete policy enabled. To avoid being billed for unnecessary storage costs, turn off the soft delete feature on buckets that your Dataflow jobs use for temporary and staging storage. For more information, see https://cloud.google.com/storage/docs/use-soft-delete#remove-soft-delete-policy.\n",
      "WARNING:apache_beam.runners.interactive.interactive_environment:Dependencies required for Interactive Beam PCollection visualization are not available, please use: `pip install apache-beam[interactive]` to install necessary dependencies to enable all data visualization features.\n"
     ]
    },
    {
     "data": {
      "application/javascript": "\n        if (typeof window.interactive_beam_jquery == 'undefined') {\n          var jqueryScript = document.createElement('script');\n          jqueryScript.src = 'https://code.jquery.com/jquery-3.4.1.slim.min.js';\n          jqueryScript.type = 'text/javascript';\n          jqueryScript.onload = function() {\n            var datatableScript = document.createElement('script');\n            datatableScript.src = 'https://cdn.datatables.net/1.10.20/js/jquery.dataTables.min.js';\n            datatableScript.type = 'text/javascript';\n            datatableScript.onload = function() {\n              window.interactive_beam_jquery = jQuery.noConflict(true);\n              window.interactive_beam_jquery(document).ready(function($){\n                \n              });\n            }\n            document.head.appendChild(datatableScript);\n          };\n          document.head.appendChild(jqueryScript);\n        } else {\n          window.interactive_beam_jquery(document).ready(function($){\n            \n          });\n        }"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    div.alert {\n",
       "      white-space: pre-line;\n",
       "    }\n",
       "  </style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <link rel=\"stylesheet\" href=\"https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/css/bootstrap.min.css\" integrity=\"sha384-Vkoo8x4CGsO3+Hhxv8T/Q5PaXtkKtu6ug5TOeNV6gBiFeWPGFN9MuhOf23Q9Ifjh\" crossorigin=\"anonymous\">\n",
       "            <div class=\"alert alert-info\">No cache_root detected. Defaulting to staging_location gs://statmike-mlops-349915/dataflow/staging for cache location.</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:apache_beam.transforms.core:('No iterator is returned by the process method in %s.', <class 'apache_beam.io.gcp.pubsub._PubSubWriteDoFn'>)\n",
      "WARNING:apache_beam.options.pipeline_options:Bucket specified in temp_location has soft-delete policy enabled. To avoid being billed for unnecessary storage costs, turn off the soft delete feature on buckets that your Dataflow jobs use for temporary and staging storage. For more information, see https://cloud.google.com/storage/docs/use-soft-delete#remove-soft-delete-policy.\n",
      "WARNING:apache_beam.options.pipeline_options:Bucket specified in staging_location has soft-delete policy enabled. To avoid being billed for unnecessary storage costs, turn off the soft delete feature on buckets that your Dataflow jobs use for temporary and staging storage. For more information, see https://cloud.google.com/storage/docs/use-soft-delete#remove-soft-delete-policy.\n",
      "WARNING:apache_beam.transforms.core:('No iterator is returned by the process method in %s.', <class 'apache_beam.io.gcp.pubsub._PubSubWriteDoFn'>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Streaming job started!\n",
      "Monitor: https://console.cloud.google.com/dataflow/jobs/us-central1?project=statmike-mlops-349915\n",
      "\n",
      "============================================================\n",
      "‚è≥ IMPORTANT: Wait 3-5 minutes for Dataflow workers to start\n",
      "============================================================\n",
      "The pipeline needs time to:\n",
      "  1. Provision workers (2-3 minutes)\n",
      "  2. Initialize the environment\n",
      "  3. Connect to Pub/Sub subscription\n",
      "\n",
      "Once workers are running, you can send test data.\n",
      "Check the Dataflow console to see when workers are ready.\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "p = beam.Pipeline(options=options)\n",
    "\n",
    "results = (\n",
    "    p\n",
    "    | \"Read from Pub/Sub\" >> ReadFromPubSub(subscription=INPUT_SUB)\n",
    "    | \"Parse JSON\" >> beam.Map(parse_json)\n",
    "    | \"Window (1 min)\" >> beam.WindowInto(window.FixedWindows(60))\n",
    "    | \"RunInference\" >> RunInference(model_handler)\n",
    "    | \"Format results\" >> beam.Map(format_result)\n",
    ")\n",
    "\n",
    "# Write to Pub/Sub\n",
    "_ = results | \"To JSON\" >> beam.Map(to_json) | \"Write to Pub/Sub\" >> WriteToPubSub(topic=OUTPUT_TOPIC)\n",
    "\n",
    "# Write to BigQuery\n",
    "_ = results | \"Write to BigQuery\" >> WriteToBigQuery(\n",
    "    table=f\"{PROJECT_ID}:{BQ_DATASET}.{BQ_TABLE}\",\n",
    "    write_disposition=beam.io.BigQueryDisposition.WRITE_APPEND\n",
    ")\n",
    "\n",
    "result = p.run()\n",
    "print(\"\\n‚úÖ Streaming job started!\")\n",
    "print(f\"Monitor: https://console.cloud.google.com/dataflow/jobs/{REGION}?project={PROJECT_ID}\")\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚è≥ IMPORTANT: Wait 3-5 minutes for Dataflow workers to start\")\n",
    "print(\"=\" * 60)\n",
    "print(\"The pipeline needs time to:\")\n",
    "print(\"  1. Provision workers (2-3 minutes)\")\n",
    "print(\"  2. Initialize the environment\")\n",
    "print(\"  3. Connect to Pub/Sub subscription\")\n",
    "print(\"\\nOnce workers are running, you can send test data.\")\n",
    "print(\"Check the Dataflow console to see when workers are ready.\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "simulate",
   "metadata": {},
   "source": [
    "---\n",
    "## Simulate Streaming Data\n",
    "\n",
    "**‚ö†Ô∏è Wait 3-5 minutes** after starting the Dataflow job before running this cell. Check the [Dataflow Console](https://console.cloud.google.com/dataflow/jobs/us-central1?project=statmike-mlops-349915) to verify workers are running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a578076b",
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(5*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "simulate_c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Published message 1\n",
      "Published message 2\n",
      "Published message 3\n",
      "Published message 4\n",
      "Published message 5\n",
      "\n",
      "‚úÖ Sent 5 test messages\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import pubsub_v1\n",
    "import time\n",
    "\n",
    "publisher = pubsub_v1.PublisherClient()\n",
    "topic_path = publisher.topic_path(PROJECT_ID, f\"{EXPERIMENT}-input\")\n",
    "\n",
    "# Send test messages\n",
    "for i in range(5):\n",
    "    message = {\"features\": [0.1] * 30}  # Dummy transaction\n",
    "    publisher.publish(topic_path, json.dumps(message).encode(\"utf-8\"))\n",
    "    print(f\"Published message {i+1}\")\n",
    "    time.sleep(2)\n",
    "\n",
    "print(\"\\n‚úÖ Sent 5 test messages\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "monitor",
   "metadata": {},
   "source": [
    "### Monitor Results\n",
    "\n",
    "**‚è≥ Wait 2-3 minutes** after sending test messages for the pipeline to process them.\n",
    "\n",
    "Monitor results from both output destinations: BigQuery (storage/analysis) and Pub/Sub (downstream processing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f44ad1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "monitor_c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "BIGQUERY RESULTS (Storage & Analysis)\n",
      "============================================================\n",
      "‚úÖ Found 10 results in BigQuery\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instance_id</th>\n",
       "      <th>anomaly_score</th>\n",
       "      <th>encoded</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>window_start</th>\n",
       "      <th>window_end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3044125753005103097</td>\n",
       "      <td>2647.894531</td>\n",
       "      <td>[0.0, 0.0, 0.4922903776168823, 0.0]</td>\n",
       "      <td>2025-11-07 22:38:12.624519+00:00</td>\n",
       "      <td>2025-11-07 22:38:00+00:00</td>\n",
       "      <td>2025-11-07 22:39:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3044125753005103097</td>\n",
       "      <td>2647.894531</td>\n",
       "      <td>[0.0, 0.0, 0.4922903776168823, 0.0]</td>\n",
       "      <td>2025-11-07 22:38:06.815538+00:00</td>\n",
       "      <td>2025-11-07 22:38:00+00:00</td>\n",
       "      <td>2025-11-07 22:39:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-6873062838286083755</td>\n",
       "      <td>2647.894531</td>\n",
       "      <td>[0.0, 0.0, 0.4922903776168823, 0.0]</td>\n",
       "      <td>2025-11-07 22:38:05.324130+00:00</td>\n",
       "      <td>2025-11-07 22:38:00+00:00</td>\n",
       "      <td>2025-11-07 22:39:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3044125753005103097</td>\n",
       "      <td>2647.894531</td>\n",
       "      <td>[0.0, 0.0, 0.4922903776168823, 0.0]</td>\n",
       "      <td>2025-11-07 22:37:54.290268+00:00</td>\n",
       "      <td>2025-11-07 22:37:00+00:00</td>\n",
       "      <td>2025-11-07 22:38:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-6873062838286083755</td>\n",
       "      <td>2647.894531</td>\n",
       "      <td>[0.0, 0.0, 0.4922903776168823, 0.0]</td>\n",
       "      <td>2025-11-07 22:37:53.232083+00:00</td>\n",
       "      <td>2025-11-07 22:37:00+00:00</td>\n",
       "      <td>2025-11-07 22:38:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-7797140705466622838</td>\n",
       "      <td>2647.894531</td>\n",
       "      <td>[0.0, 0.0, 0.4922903776168823, 0.0]</td>\n",
       "      <td>2025-11-07 21:37:32.695437+00:00</td>\n",
       "      <td>2025-11-07 21:25:00+00:00</td>\n",
       "      <td>2025-11-07 21:26:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-7797140705466622838</td>\n",
       "      <td>2647.894531</td>\n",
       "      <td>[0.0, 0.0, 0.4922903776168823, 0.0]</td>\n",
       "      <td>2025-11-07 21:37:09.924852+00:00</td>\n",
       "      <td>2025-11-07 21:35:00+00:00</td>\n",
       "      <td>2025-11-07 21:36:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-7797140705466622838</td>\n",
       "      <td>2647.894531</td>\n",
       "      <td>[0.0, 0.0, 0.4922903776168823, 0.0]</td>\n",
       "      <td>2025-11-07 21:37:09.910588+00:00</td>\n",
       "      <td>2025-11-07 20:28:00+00:00</td>\n",
       "      <td>2025-11-07 20:29:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3106348164575065947</td>\n",
       "      <td>2647.894531</td>\n",
       "      <td>[0.0, 0.0, 0.4922903776168823, 0.0]</td>\n",
       "      <td>2025-11-07 21:37:09.908446+00:00</td>\n",
       "      <td>2025-11-07 21:35:00+00:00</td>\n",
       "      <td>2025-11-07 21:36:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-7797140705466622838</td>\n",
       "      <td>2647.894531</td>\n",
       "      <td>[0.0, 0.0, 0.4922903776168823, 0.0]</td>\n",
       "      <td>2025-11-07 21:37:09.905995+00:00</td>\n",
       "      <td>2025-11-07 20:28:00+00:00</td>\n",
       "      <td>2025-11-07 20:29:00+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            instance_id  anomaly_score                              encoded  \\\n",
       "0   3044125753005103097    2647.894531  [0.0, 0.0, 0.4922903776168823, 0.0]   \n",
       "1   3044125753005103097    2647.894531  [0.0, 0.0, 0.4922903776168823, 0.0]   \n",
       "2  -6873062838286083755    2647.894531  [0.0, 0.0, 0.4922903776168823, 0.0]   \n",
       "3   3044125753005103097    2647.894531  [0.0, 0.0, 0.4922903776168823, 0.0]   \n",
       "4  -6873062838286083755    2647.894531  [0.0, 0.0, 0.4922903776168823, 0.0]   \n",
       "5  -7797140705466622838    2647.894531  [0.0, 0.0, 0.4922903776168823, 0.0]   \n",
       "6  -7797140705466622838    2647.894531  [0.0, 0.0, 0.4922903776168823, 0.0]   \n",
       "7  -7797140705466622838    2647.894531  [0.0, 0.0, 0.4922903776168823, 0.0]   \n",
       "8   3106348164575065947    2647.894531  [0.0, 0.0, 0.4922903776168823, 0.0]   \n",
       "9  -7797140705466622838    2647.894531  [0.0, 0.0, 0.4922903776168823, 0.0]   \n",
       "\n",
       "                         timestamp              window_start  \\\n",
       "0 2025-11-07 22:38:12.624519+00:00 2025-11-07 22:38:00+00:00   \n",
       "1 2025-11-07 22:38:06.815538+00:00 2025-11-07 22:38:00+00:00   \n",
       "2 2025-11-07 22:38:05.324130+00:00 2025-11-07 22:38:00+00:00   \n",
       "3 2025-11-07 22:37:54.290268+00:00 2025-11-07 22:37:00+00:00   \n",
       "4 2025-11-07 22:37:53.232083+00:00 2025-11-07 22:37:00+00:00   \n",
       "5 2025-11-07 21:37:32.695437+00:00 2025-11-07 21:25:00+00:00   \n",
       "6 2025-11-07 21:37:09.924852+00:00 2025-11-07 21:35:00+00:00   \n",
       "7 2025-11-07 21:37:09.910588+00:00 2025-11-07 20:28:00+00:00   \n",
       "8 2025-11-07 21:37:09.908446+00:00 2025-11-07 21:35:00+00:00   \n",
       "9 2025-11-07 21:37:09.905995+00:00 2025-11-07 20:28:00+00:00   \n",
       "\n",
       "                 window_end  \n",
       "0 2025-11-07 22:39:00+00:00  \n",
       "1 2025-11-07 22:39:00+00:00  \n",
       "2 2025-11-07 22:39:00+00:00  \n",
       "3 2025-11-07 22:38:00+00:00  \n",
       "4 2025-11-07 22:38:00+00:00  \n",
       "5 2025-11-07 21:26:00+00:00  \n",
       "6 2025-11-07 21:36:00+00:00  \n",
       "7 2025-11-07 20:29:00+00:00  \n",
       "8 2025-11-07 21:36:00+00:00  \n",
       "9 2025-11-07 20:29:00+00:00  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "PUB/SUB OUTPUT TOPIC (Downstream Processing)\n",
      "============================================================\n",
      "Pulling messages from: projects/statmike-mlops-349915/subscriptions/pytorch-autoencoder-output-sub\n",
      "‚úÖ Found 3 messages in output topic\n",
      "\n",
      "Sample messages:\n",
      "\n",
      "Message 1:\n",
      "  Anomaly Score: 2647.89453125\n",
      "  Instance ID: -1873740870990298768\n",
      "  Timestamp: 2025-11-07T22:37:38.122867\n",
      "\n",
      "Message 2:\n",
      "  Anomaly Score: 2647.89453125\n",
      "  Instance ID: -6873062838286083755\n",
      "  Timestamp: 2025-11-07T22:38:05.324130\n",
      "\n",
      "Message 3:\n",
      "  Anomaly Score: 2647.89453125\n",
      "  Instance ID: 3044125753005103097\n",
      "  Timestamp: 2025-11-07T22:38:06.815538\n",
      "\n",
      "‚úÖ Acknowledged 3 messages\n",
      "\n",
      "============================================================\n",
      "üí° Pipeline Status Summary\n",
      "============================================================\n",
      "‚úÖ Pipeline is working correctly!\n",
      "   Total results in BigQuery: 40\n",
      "   Results are being written to both:\n",
      "     - BigQuery table: statmike-mlops-349915.frameworks.pytorch_autoencoder_streaming_results\n",
      "     - Pub/Sub topic: projects/statmike-mlops-349915/topics/pytorch-autoencoder-output\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import bigquery\n",
    "from google.cloud import pubsub_v1\n",
    "\n",
    "# Monitor BigQuery results\n",
    "print(\"=\" * 60)\n",
    "print(\"BIGQUERY RESULTS (Storage & Analysis)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "bq = bigquery.Client(project=PROJECT_ID)\n",
    "query = f\"SELECT * FROM `{PROJECT_ID}.{BQ_DATASET}.{BQ_TABLE}` ORDER BY timestamp DESC LIMIT 10\"\n",
    "df = bq.query(query).to_dataframe()\n",
    "\n",
    "if len(df) > 0:\n",
    "    print(f\"‚úÖ Found {len(df)} results in BigQuery\")\n",
    "    display(df)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No results yet in BigQuery\")\n",
    "    print(\"   Wait a few minutes for the pipeline to process data\")\n",
    "\n",
    "# Monitor Pub/Sub output topic\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PUB/SUB OUTPUT TOPIC (Downstream Processing)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "subscriber = pubsub_v1.SubscriberClient()\n",
    "output_sub_path = subscriber.subscription_path(PROJECT_ID, f\"{EXPERIMENT}-output-sub\")\n",
    "\n",
    "print(f\"Pulling messages from: {output_sub_path}\")\n",
    "\n",
    "# Pull messages from output subscription\n",
    "try:\n",
    "    response = subscriber.pull(\n",
    "        request={\"subscription\": output_sub_path, \"max_messages\": 5},\n",
    "        timeout=10\n",
    "    )\n",
    "    \n",
    "    if response.received_messages:\n",
    "        print(f\"‚úÖ Found {len(response.received_messages)} messages in output topic\")\n",
    "        print(\"\\nSample messages:\")\n",
    "        for i, msg in enumerate(response.received_messages[:3], 1):\n",
    "            data = json.loads(msg.message.data.decode(\"utf-8\"))\n",
    "            print(f\"\\nMessage {i}:\")\n",
    "            print(f\"  Anomaly Score: {data.get('anomaly_score', 'N/A')}\")\n",
    "            print(f\"  Instance ID: {data.get('instance_id', 'N/A')}\")\n",
    "            print(f\"  Timestamp: {data.get('timestamp', 'N/A')}\")\n",
    "        \n",
    "        # Acknowledge messages (optional - remove if you want to keep them)\n",
    "        ack_ids = [msg.ack_id for msg in response.received_messages]\n",
    "        subscriber.acknowledge(request={\"subscription\": output_sub_path, \"ack_ids\": ack_ids})\n",
    "        print(f\"\\n‚úÖ Acknowledged {len(ack_ids)} messages\")\n",
    "    else:\n",
    "        print(\"‚ÑπÔ∏è  No messages currently in output subscription\")\n",
    "        print(\"   Messages may have been consumed already or not yet published\")\n",
    "        \n",
    "except Exception as e:\n",
    "    if \"DeadlineExceeded\" in str(type(e).__name__):\n",
    "        print(\"‚ÑπÔ∏è  No messages available in output subscription (timeout)\")\n",
    "        print(\"   This is normal - messages are consumed quickly or not yet available\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è  Error pulling messages: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üí° Pipeline Status Summary\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Get total count from BigQuery\n",
    "count_query = f\"SELECT COUNT(*) as total FROM `{PROJECT_ID}.{BQ_DATASET}.{BQ_TABLE}`\"\n",
    "count_result = bq.query(count_query).to_dataframe()\n",
    "total_results = count_result['total'].iloc[0] if len(count_result) > 0 else 0\n",
    "\n",
    "if total_results > 0:\n",
    "    print(f\"‚úÖ Pipeline is working correctly!\")\n",
    "    print(f\"   Total results in BigQuery: {total_results}\")\n",
    "    print(f\"   Results are being written to both:\")\n",
    "    print(f\"     - BigQuery table: {PROJECT_ID}.{BQ_DATASET}.{BQ_TABLE}\")\n",
    "    print(f\"     - Pub/Sub topic: {OUTPUT_TOPIC}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No results yet in BigQuery\")\n",
    "    print(\"   Pipeline may still be processing or waiting for data\")\n",
    "    \n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "j4k8lillxb",
   "metadata": {},
   "source": [
    "---\n",
    "## Understanding Scaling and Performance\n",
    "\n",
    "Now that your streaming pipeline is deployed, understanding how it scales and performs under load is critical for production deployments.\n",
    "\n",
    "### Factors Affecting Performance\n",
    "\n",
    "**Service-Side Factors**:\n",
    "- **Worker Count**: Number of Dataflow workers processing data (min/max worker settings)\n",
    "- **Machine Type**: CPU and memory resources per worker\n",
    "- **Autoscaling Configuration**: How quickly new workers are provisioned under backlog\n",
    "- **Worker Startup Time**: Time for new workers to become ready and process data\n",
    "\n",
    "**Usage-Side Factors**:\n",
    "- **Message Rate**: Number of messages published to Pub/Sub per second\n",
    "- **Window Size**: Duration of windows for micro-batching (affects latency vs. throughput)\n",
    "- **Message Size**: Size of payload per message\n",
    "- **Traffic Pattern**: Constant flow vs. spikes vs. bursts\n",
    "\n",
    "### Current Configuration\n",
    "\n",
    "Your pipeline is currently configured with:\n",
    "- **Machine Type**: `n1-standard-4` (4 vCPUs, 15 GB memory)\n",
    "- **Min Workers**: 2 (baseline parallelism)\n",
    "- **Max Workers**: 20 (autoscaling limit)\n",
    "- **Window Size**: 1 minute (micro-batching)\n",
    "- **Model**: Local PyTorch model loaded in-process\n",
    "\n",
    "This configuration provides:\n",
    "- ‚úÖ Low latency for real-time processing (1-minute window)\n",
    "- ‚úÖ Autoscaling for traffic spikes (up to 20x capacity)\n",
    "- ‚úÖ Cost-effective for variable traffic (2 workers baseline)\n",
    "- ‚úÖ High throughput (no network calls to external endpoints)\n",
    "\n",
    "### Performance Testing\n",
    "\n",
    "To understand how this deployment scales under different load patterns and identify performance thresholds, see:\n",
    "\n",
    "**[scaling-dataflow-streaming-runinference.ipynb](./scaling-dataflow-streaming-runinference.ipynb)**\n",
    "\n",
    "This notebook demonstrates:\n",
    "- **Finding Performance Thresholds**: Progressive testing to find where backlog builds\n",
    "  - Test message rates from 10 to 100+ messages/second\n",
    "  - Increment by 10 messages/second to find breaking point\n",
    "  - Identify when worker autoscaling triggers\n",
    "- **Load Pattern Testing**: Understand scaling behavior over time\n",
    "  - Constant load (steady-state throughput)\n",
    "  - Gradual ramp-up (observe autoscaling triggers and worker provisioning time)\n",
    "  - Traffic spikes (backlog behavior and recovery time)\n",
    "- **Tuning Guidance**: Recommendations for adjusting configuration\n",
    "  - When to increase min/max workers\n",
    "  - When to change machine types\n",
    "  - Impact of window size on latency vs. throughput\n",
    "  - Cost vs. performance tradeoffs\n",
    "\n",
    "### When to Run Scale Testing\n",
    "\n",
    "Run scale tests when:\n",
    "- üîπ **Before production launch**: Understand capacity and set appropriate worker limits\n",
    "- üîπ **After model changes**: New models may have different inference latency\n",
    "- üîπ **For capacity planning**: Estimate costs for expected message volumes\n",
    "- üîπ **During incidents**: Diagnose backlog issues and identify bottlenecks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cleanup",
   "metadata": {},
   "source": [
    "---\n",
    "## Clean Up\n",
    "\n",
    "**‚ö†Ô∏è IMPORTANT: Streaming jobs run continuously until explicitly cancelled**, incurring ongoing costs for workers and resources.\n",
    "\n",
    "### Centralized Cleanup Notebook\n",
    "\n",
    "For comprehensive cleanup of all Dataflow resources, use the centralized cleanup notebook:\n",
    "\n",
    "**[dataflow-cleanup.ipynb](./dataflow-cleanup.ipynb)**\n",
    "\n",
    "This notebook provides:\n",
    "- ‚úÖ **Stop Dataflow Jobs**: Cancel running streaming/batch jobs created by these notebooks\n",
    "- ‚úÖ **Clean BigQuery Tables**: Truncate or delete result tables\n",
    "- ‚úÖ **Clean Pub/Sub Resources**: Delete topics and subscriptions\n",
    "- ‚úÖ **Clean GCS Files**: Delete model files uploaded for Dataflow\n",
    "- ‚úÖ **Granular Control**: Use flags to choose exactly what to clean up\n",
    "- ‚úÖ **Safety Checks**: Warnings for risky operations\n",
    "- ‚úÖ **Confirmation Prompts**: Review before executing\n",
    "\n",
    "### Why Use Centralized Cleanup?\n",
    "\n",
    "- **One location**: Manage all Dataflow infrastructure from a single notebook\n",
    "- **Comprehensive**: Clean up jobs, tables, Pub/Sub, and GCS files together\n",
    "- **Flexible**: Truncate tables without deleting schema (useful for testing)\n",
    "- **Safe**: Built-in safety checks and confirmation prompts\n",
    "- **Efficient**: Clean up resources from all 4 Dataflow notebooks at once\n",
    "\n",
    "### Quick Cleanup (This Job Only)\n",
    "\n",
    "If you only need to stop the streaming job created by this notebook, uncomment and run the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "h5od5nznrw",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the line below to stop the streaming job created by this notebook\n",
    "# result.cancel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "In this notebook, you successfully:\n",
    "\n",
    "‚úÖ **Created Custom PyTorch ModelHandler**\n",
    "- Wrapped TorchScript model for RunInference\n",
    "- Implemented efficient batch processing with torch.stack()\n",
    "- Designed output format compatible with downstream processing\n",
    "\n",
    "‚úÖ **Built Streaming Dataflow Pipeline**\n",
    "- Real-time data ingestion from Pub/Sub\n",
    "- 1-minute fixed windows for micro-batching\n",
    "- Automatic dependency installation (PyTorch) in workers\n",
    "\n",
    "‚úÖ **Deployed PyTorch Model In-Process**\n",
    "- Model loaded directly in Dataflow workers\n",
    "- No network calls to external endpoints\n",
    "- Low-latency inference for streaming data\n",
    "\n",
    "‚úÖ **Implemented Dual Output Pattern**\n",
    "- BigQuery: Long-term storage and SQL analytics\n",
    "- Pub/Sub: Real-time downstream event processing\n",
    "\n",
    "## Key Learnings\n",
    "\n",
    "**ModelHandler Design:**\n",
    "- TorchScript models use `torch_script_model_path` (not `state_dict_path`)\n",
    "- Batch processing requires `torch.stack()` to combine tensors\n",
    "- Return list of dicts (one per instance) from `run_inference()`\n",
    "- Keep outputs as tensors until format stage for efficiency\n",
    "\n",
    "**Streaming Considerations:**\n",
    "- Workers need 3-5 minutes to provision and initialize\n",
    "- Windows control micro-batching granularity (1-minute = balance latency vs. throughput)\n",
    "- PyTorch installation via `--requirements_file` adds to startup time\n",
    "- Continuous jobs require manual cancellation to stop charges\n",
    "\n",
    "**Cost Optimization:**\n",
    "- In-process inference avoids endpoint API costs\n",
    "- Pay only while job runs (vs. always-on endpoints)\n",
    "- Worker autoscaling based on Pub/Sub backlog\n",
    "- Consider batch processing for historical analysis\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "### Continue with Dataflow Workflows:\n",
    "\n",
    "**Batch Inference with Vertex Endpoint:**\n",
    "- [dataflow-batch-runinference-vertex.ipynb](./dataflow-batch-runinference-vertex.ipynb)\n",
    "  - Process historical data via Vertex AI Endpoint\n",
    "  - Compare performance vs local RunInference\n",
    "  - Centralized model updates without redeploying pipeline\n",
    "\n",
    "**Streaming Inference with Vertex Endpoint:**\n",
    "- [dataflow-streaming-runinference-vertex.ipynb](./dataflow-streaming-runinference-vertex.ipynb)\n",
    "  - Real-time endpoint calls from Dataflow\n",
    "  - Shared model across multiple services\n",
    "  - Managed infrastructure with auto-scaling\n",
    "\n",
    "**Batch Inference with Local Model:**\n",
    "- [dataflow-batch-runinference.ipynb](./dataflow-batch-runinference.ipynb)\n",
    "  - Historical data processing with in-process model\n",
    "  - Higher throughput for bounded datasets\n",
    "  - Same ModelHandler pattern as streaming\n",
    "\n",
    "### Production Enhancements:\n",
    "\n",
    "1. **Error Handling**: Add dead letter queues for failed predictions\n",
    "2. **Monitoring**: Set up Cloud Monitoring alerts for job failures and backlog\n",
    "3. **Autoscaling**: Configure min/max workers based on traffic patterns\n",
    "4. **Model Versioning**: Implement blue/green deployments for model updates\n",
    "5. **Data Validation**: Add schema validation for input messages\n",
    "\n",
    "### Resources\n",
    "\n",
    "- [Dataflow Streaming Pipelines](https://cloud.google.com/dataflow/docs/concepts/streaming-pipelines)\n",
    "- [Apache Beam RunInference](https://beam.apache.org/documentation/ml/about-ml/)\n",
    "- [PyTorch RunInference](https://beam.apache.org/documentation/ml/pytorch-inference/)\n",
    "- [Pub/Sub Best Practices](https://cloud.google.com/pubsub/docs/publisher)\n",
    "- [Windowing in Beam](https://beam.apache.org/documentation/programming-guide/#windowing)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (frameworks-pytorch)",
   "language": "python",
   "name": "frameworks-pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
