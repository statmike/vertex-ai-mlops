{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f9cd9f98",
      "metadata": {},
      "source": [
        "![tracker](https://us-central1-vertex-ai-mlops-369716.cloudfunctions.net/pixel-tracking?path=statmike%2Fvertex-ai-mlops%2FFramework+Workflows%2FPyTorch%2Fserving&file=dataflow-vertex-endpoint.ipynb)\n",
        "<!--- header table --->\n",
        "<table align=\"left\">\n",
        "<tr>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%20Workflows/PyTorch/serving/dataflow-vertex-endpoint.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://www.svgrepo.com/download/217753/github.svg\" alt=\"GitHub logo\">\n",
        "      <br>View on<br>GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://colab.research.google.com/github/statmike/vertex-ai-mlops/blob/main/Framework%20Workflows/PyTorch/serving/dataflow-vertex-endpoint.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://www.gstatic.com/pantheon/images/bigquery/welcome_page/colab-logo.svg\" alt=\"Google Colaboratory logo\">\n",
        "      <br>Run in<br>Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https%3A%2F%2Fraw.githubusercontent.com%2Fstatmike%2Fvertex-ai-mlops%2Fmain%2FFramework%2520Workflows%2FPyTorch%2Fserving%2Fdataflow-vertex-endpoint.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\">\n",
        "      <br>Run in<br>Colab Enterprise\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/bigquery/import?url=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%20Workflows/PyTorch/serving/dataflow-vertex-endpoint.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://www.gstatic.com/images/branding/gcpiconscolors/bigquery/v1/32px.svg\" alt=\"BigQuery logo\">\n",
        "      <br>Open in<br>BigQuery Studio\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/statmike/vertex-ai-mlops/main/Framework%20Workflows/PyTorch/serving/dataflow-vertex-endpoint.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://www.gstatic.com/images/branding/gcpiconscolors/vertexai/v1/32px.svg\" alt=\"Vertex AI logo\">\n",
        "      <br>Open in<br>Vertex AI Workbench\n",
        "    </a>\n",
        "  </td>\n",
        "</tr>\n",
        "<tr>\n",
        "  <td colspan=\"5\" style=\"text-align: right\">\n",
        "    <b>Share This On: </b> \n",
        "    <a href=\"https://www.linkedin.com/sharing/share-offsite/?url=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%2520Workflows/PyTorch/serving/dataflow-vertex-endpoint.ipynb\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"Linkedin Logo\" width=\"20px\"></a> \n",
        "    <a href=\"https://reddit.com/submit?url=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%2520Workflows/PyTorch/serving/dataflow-vertex-endpoint.ipynb\"><img src=\"https://redditinc.com/hubfs/Reddit%20Inc/Brand/Reddit_Logo.png\" alt=\"Reddit Logo\" width=\"20px\"></a> \n",
        "    <a href=\"https://bsky.app/intent/compose?text=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%2520Workflows/PyTorch/serving/dataflow-vertex-endpoint.ipynb\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"BlueSky Logo\" width=\"20px\"></a> \n",
        "    <a href=\"https://twitter.com/intent/tweet?url=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%2520Workflows/PyTorch/serving/dataflow-vertex-endpoint.ipynb\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/5/5a/X_icon_2.svg\" alt=\"X (Twitter) Logo\" width=\"20px\"></a> \n",
        "  </td>\n",
        "</tr>\n",
        "<tr>\n",
        "  <td colspan=\"5\" style=\"text-align: right\">\n",
        "    <b>Connect With Author On: </b> \n",
        "    <a href=\"https://www.linkedin.com/in/statmike\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"Linkedin Logo\" width=\"20px\"></a>\n",
        "    <a href=\"https://www.github.com/statmike\"><img src=\"https://www.svgrepo.com/download/217753/github.svg\" alt=\"GitHub Logo\" width=\"20px\"></a> \n",
        "    <a href=\"https://www.youtube.com/@statmike-channel\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/f/fd/YouTube_full-color_icon_%282024%29.svg\" alt=\"YouTube Logo\" width=\"20px\"></a>\n",
        "    <a href=\"https://bsky.app/profile/statmike.bsky.social\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"BlueSky Logo\" width=\"20px\"></a> \n",
        "    <a href=\"https://x.com/statmike\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/5/5a/X_icon_2.svg\" alt=\"X (Twitter) Logo\" width=\"20px\"></a>\n",
        "  </td>\n",
        "</tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "overview",
      "metadata": {},
      "source": [
        "# Dataflow with Vertex AI Endpoint\n",
        "\n",
        "This notebook demonstrates how to call a deployed Vertex AI Endpoint from a Dataflow pipeline for batch inference.\n",
        "\n",
        "## What You'll Learn\n",
        "\n",
        "This workflow covers:\n",
        "\n",
        "1. **Verify Deployed Endpoint**: Confirm endpoint is ready\n",
        "2. **Create Batch Pipeline**: Read data from BigQuery\n",
        "3. **Call Vertex Endpoint**: Make predictions via HTTP\n",
        "4. **Write Results**: Store predictions back to BigQuery\n",
        "5. **Compare Approaches**: RunInference vs Endpoint calls\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "**Required**:\n",
        "- Completed `dataflow-setup.ipynb` (infrastructure created)\n",
        "- Completed `vertex-ai-endpoint-prebuilt-container.ipynb` OR `vertex-ai-endpoint-custom-container.ipynb` (endpoint deployed)\n",
        "- Endpoint is currently deployed and running\n",
        "\n",
        "**Note**: This approach requires a deployed endpoint, which incurs hourly costs (~$0.20/hour for n1-standard-4). For cost-effective batch processing, consider using `dataflow-batch-runinference.ipynb` instead.\n",
        "\n",
        "## Core Concepts\n",
        "\n",
        "**Vertex AI Endpoint**: A managed online prediction service that serves ML models via HTTP/gRPC. The endpoint:\n",
        "- Runs continuously (incurs costs even when idle)\n",
        "- Handles autoscaling based on traffic\n",
        "- Provides model versioning and traffic splitting\n",
        "- Offers centralized model management\n",
        "\n",
        "**Dataflow Batch Processing**: Process large datasets in parallel by:\n",
        "- Reading data from BigQuery in batches\n",
        "- Making HTTP requests to Vertex Endpoint\n",
        "- Writing predictions back to BigQuery\n",
        "- Paying only for Dataflow worker time (not endpoint time)\n",
        "\n",
        "**When to Use This Approach**:\n",
        "- ✅ Endpoint is already deployed for online predictions\n",
        "- ✅ Need centralized model versioning and rollback\n",
        "- ✅ Want A/B testing or canary deployments\n",
        "- ✅ Multiple applications calling the same model\n",
        "- ❌ Cost-sensitive batch processing (use RunInference instead)\n",
        "- ❌ Very high throughput batch jobs (network overhead)\n",
        "\n",
        "## RunInference vs Vertex Endpoint\n",
        "\n",
        "| Aspect | RunInference | Vertex Endpoint |\n",
        "|--------|--------------|----------------|\n",
        "| **Model Loading** | Loaded in-process | Served separately |\n",
        "| **Network Calls** | None | HTTP/gRPC per batch |\n",
        "| **Latency** | Lower (in-process) | Higher (network overhead) |\n",
        "| **Cost (batch)** | Worker time only | Worker + endpoint time |\n",
        "| **Model Updates** | Redeploy pipeline | Update endpoint |\n",
        "| **Scaling** | Dataflow autoscaling | Endpoint autoscaling |\n",
        "| **Shared Model** | No | Yes (multiple callers) |\n",
        "| **A/B Testing** | Complex | Built-in |\n",
        "\n",
        "**Best Practice**: Use RunInference for batch jobs, use Endpoint for online predictions and multi-application model sharing."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "env_setup",
      "metadata": {},
      "source": [
        "---\n",
        "## Environment Setup\n",
        "\n",
        "This section will authenticate your session, enable required Google Cloud APIs, and install necessary Python packages.\n",
        "\n",
        "**Package Installation Options (`REQ_TYPE`):**\n",
        "- `PRIMARY`: Installs only the main packages. Faster, but pip resolves sub-dependencies which may result in different versions than development.\n",
        "- `ALL` (Default): Installs exact versions of all packages and dependencies. Best for perfectly reproducing the development environment.\n",
        "- `COLAB`: Installs a Colab-optimized list that excludes pre-installed packages like `ipython` and `ipykernel`.\n",
        "\n",
        "**Installation Tool Options (`INSTALL_TOOL`):**\n",
        "- `pip` (Default): Uses pip for package installation. Standard Python package installer.\n",
        "- `uv`: Modern, fast Python package installer. Must be installed separately. See: https://github.com/astral-sh/uv\n",
        "- `poetry`: Dependency management tool. Requires running notebook in a poetry environment (`poetry shell` or `poetry run jupyter lab`). Uses `pyproject.toml` instead of requirements.txt.\n",
        "\n",
        "> **Note:** If running in Google Colab, the script will automatically detect this and set `REQ_TYPE = 'COLAB'` to prevent package conflicts, overriding any manual setting."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "set_project",
      "metadata": {},
      "source": [
        "### Set Your Project ID\n",
        "\n",
        "⚠️ **Action Required:** Replace the `PROJECT_ID` value below with your Google Cloud project ID before running this cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "project_config",
      "metadata": {},
      "outputs": [],
      "source": [
        "PROJECT_ID = 'statmike-mlops-349915' # replace with GCP project ID\n",
        "REQ_TYPE = 'ALL' # Specify PRIMARY or ALL or COLAB\n",
        "INSTALL_TOOL = 'poetry' # Specify pip, uv, or poetry (all implemented)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "config_header",
      "metadata": {},
      "source": [
        "### Configuration\n",
        "\n",
        "This cell defines the requirements files and Google Cloud APIs needed for this notebook. Run as-is without modification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "config",
      "metadata": {},
      "outputs": [],
      "source": [
        "REQUIREMENTS_URL = 'https://raw.githubusercontent.com/statmike/vertex-ai-mlops/refs/heads/main/Framework%20Workflows/PyTorch/requirements.txt'\n",
        "\n",
        "REQUIRED_APIS = [\n",
        "    \"aiplatform.googleapis.com\",\n",
        "    \"dataflow.googleapis.com\",\n",
        "    \"bigquery.googleapis.com\",\n",
        "    \"storage.googleapis.com\",\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "run_setup_header",
      "metadata": {},
      "source": [
        "### Run Setup\n",
        "\n",
        "This cell downloads the centralized setup code and configures your environment. It will:\n",
        "- Authenticate your session with Google Cloud\n",
        "- Enable required APIs for this notebook\n",
        "- Install necessary Python packages\n",
        "- Display a setup summary with your project information\n",
        "\n",
        "> **Note:** In Colab, if packages are installed, the kernel will automatically restart. After restart, continue from the next cell without re-running earlier cells."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "run_setup",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, urllib.request\n",
        "\n",
        "# Download and import setup code\n",
        "url = 'https://raw.githubusercontent.com/statmike/vertex-ai-mlops/refs/heads/main/core/notebook-template/python_setup.py'\n",
        "urllib.request.urlretrieve(url, 'python_setup_local.py')\n",
        "import python_setup_local as python_setup\n",
        "os.remove('python_setup_local.py')\n",
        "\n",
        "# Run setup\n",
        "setup_info = python_setup.setup_environment(PROJECT_ID, REQ_TYPE, REQUIREMENTS_URL, REQUIRED_APIS, INSTALL_TOOL)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "python_setup",
      "metadata": {},
      "source": [
        "---\n",
        "## Python Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "imports_header",
      "metadata": {},
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "imports",
      "metadata": {},
      "outputs": [],
      "source": [
        "import subprocess\n",
        "import json\n",
        "import time\n",
        "from datetime import datetime\n",
        "\n",
        "# Apache Beam\n",
        "import apache_beam as beam\n",
        "from apache_beam.options.pipeline_options import PipelineOptions, GoogleCloudOptions, StandardOptions\n",
        "from apache_beam.io.gcp.bigquery import ReadFromBigQuery, WriteToBigQuery\n",
        "\n",
        "# Google Cloud\n",
        "from google.cloud import aiplatform\n",
        "from google.cloud import bigquery\n",
        "import google.auth\n",
        "import google.auth.transport.requests\n",
        "\n",
        "# Data processing\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "vars_user",
      "metadata": {},
      "source": [
        "### Variables - User Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vars_user_code",
      "metadata": {},
      "outputs": [],
      "source": [
        "REGION = 'us-central1'\n",
        "SERIES = 'frameworks'\n",
        "EXPERIMENT = 'pytorch-autoencoder'\n",
        "\n",
        "# BigQuery configuration\n",
        "BQ_PROJECT = PROJECT_ID\n",
        "BQ_DATASET = SERIES.replace('-', '_')\n",
        "BQ_TABLE = SERIES\n",
        "BQ_TABLE_RESULTS = 'endpoint_results'\n",
        "\n",
        "# Vertex AI Endpoint\n",
        "ENDPOINT_DISPLAY_NAME = f\"{EXPERIMENT}-endpoint\"\n",
        "\n",
        "# Dataflow configuration\n",
        "DATAFLOW_JOB_NAME = f\"{EXPERIMENT}-endpoint-batch\"\n",
        "DATAFLOW_TEMP_LOCATION = f\"gs://{PROJECT_ID}/dataflow/temp\"\n",
        "DATAFLOW_STAGING_LOCATION = f\"gs://{PROJECT_ID}/dataflow/staging\"\n",
        "\n",
        "# Processing configuration\n",
        "BATCH_SIZE = 100  # Number of instances per endpoint request\n",
        "MAX_WORKERS = 4   # Maximum Dataflow workers"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "vars_auto",
      "metadata": {},
      "source": [
        "### Variables - Auto Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vars_auto_code",
      "metadata": {},
      "outputs": [],
      "source": [
        "PROJECT_ID = subprocess.run(['gcloud', 'config', 'get-value', 'project'], capture_output=True, text=True, check=True).stdout.strip()\n",
        "PROJECT_NUMBER = subprocess.run(['gcloud', 'projects', 'describe', PROJECT_ID, '--format=value(projectNumber)'], capture_output=True, text=True, check=True).stdout.strip()\n",
        "\n",
        "print(f\"\\n{'='*50}\\nGoogle Cloud Project Information\\n{'='*50}\\nPROJECT_ID     = {PROJECT_ID}\\nPROJECT_NUMBER = {PROJECT_NUMBER}\\n{'='*50}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "init_vertex",
      "metadata": {},
      "source": [
        "### Initialize Vertex AI SDK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "init_vertex_code",
      "metadata": {},
      "outputs": [],
      "source": [
        "aiplatform.init(project=PROJECT_ID, location=REGION)\n",
        "\n",
        "print(f\"✅ Vertex AI initialized\")\n",
        "print(f\"   Project: {PROJECT_ID}\")\n",
        "print(f\"   Location: {REGION}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "init_bq",
      "metadata": {},
      "source": [
        "### Initialize BigQuery Client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "init_bq_code",
      "metadata": {},
      "outputs": [],
      "source": [
        "bq = bigquery.Client(project=PROJECT_ID)\n",
        "print(f\"✅ BigQuery client initialized\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "verify_endpoint",
      "metadata": {},
      "source": [
        "---\n",
        "## Verify Deployed Endpoint\n",
        "\n",
        "Check that the Vertex AI Endpoint exists and is ready to serve predictions."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "find_endpoint",
      "metadata": {},
      "source": [
        "### Find Endpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "find_endpoint_code",
      "metadata": {},
      "outputs": [],
      "source": [
        "# List existing endpoints\n",
        "endpoints = aiplatform.Endpoint.list(filter=f\"display_name={ENDPOINT_DISPLAY_NAME}\")\n",
        "\n",
        "if not endpoints:\n",
        "    print(f\"❌ No endpoint found with name: {ENDPOINT_DISPLAY_NAME}\")\n",
        "    print(f\"\\nPlease deploy an endpoint first using:\")\n",
        "    print(f\"  - vertex-ai-endpoint-prebuilt-container.ipynb\")\n",
        "    print(f\"  - vertex-ai-endpoint-custom-container.ipynb\")\n",
        "    raise ValueError(f\"Endpoint '{ENDPOINT_DISPLAY_NAME}' not found\")\n",
        "\n",
        "endpoint = endpoints[0]\n",
        "print(f\"✅ Found endpoint: {endpoint.display_name}\")\n",
        "print(f\"   Resource name: {endpoint.resource_name}\")\n",
        "print(f\"   Created: {endpoint.create_time}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "test_endpoint",
      "metadata": {},
      "source": [
        "### Test Endpoint Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "test_endpoint_code",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get a test instance from BigQuery\n",
        "query = f\"\"\"\n",
        "SELECT * EXCEPT(splits, transaction_id, Class)\n",
        "FROM `{BQ_PROJECT}.{BQ_DATASET}.{BQ_TABLE}`\n",
        "WHERE splits = \"TEST\" AND Class = 0\n",
        "LIMIT 1\n",
        "\"\"\"\n",
        "\n",
        "test_df = bq.query(query).to_dataframe()\n",
        "test_instance = test_df.values.tolist()\n",
        "\n",
        "# Make prediction\n",
        "response = endpoint.predict(instances=test_instance)\n",
        "\n",
        "print(f\"✅ Endpoint is responding\")\n",
        "print(f\"   Received prediction keys: {list(response.predictions[0].keys())}\")\n",
        "print(f\"\\nThis endpoint returns the following output structure:\")\n",
        "if 'anomaly_score' in response.predictions[0]:\n",
        "    print(f\"  - Custom container (simplified output)\")\n",
        "    print(f\"  - Keys: anomaly_score, encoded\")\n",
        "else:\n",
        "    print(f\"  - Pre-built container (full output)\")\n",
        "    print(f\"  - Keys: {list(response.predictions[0].keys())[:5]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "create_results_table",
      "metadata": {},
      "source": [
        "---\n",
        "## Create Results Table\n",
        "\n",
        "Create a BigQuery table to store predictions from the endpoint."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "create_results_table_code",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check if endpoint uses custom or pre-built container\n",
        "is_custom_endpoint = 'anomaly_score' in response.predictions[0]\n",
        "\n",
        "# Define schema based on endpoint type\n",
        "if is_custom_endpoint:\n",
        "    schema = [\n",
        "        bigquery.SchemaField(\"instance_id\", \"STRING\"),\n",
        "        bigquery.SchemaField(\"anomaly_score\", \"FLOAT64\"),\n",
        "        bigquery.SchemaField(\"encoded\", \"FLOAT64\", mode=\"REPEATED\"),\n",
        "        bigquery.SchemaField(\"timestamp\", \"TIMESTAMP\"),\n",
        "    ]\n",
        "    print(\"Using schema for custom container (simplified output)\")\n",
        "else:\n",
        "    schema = [\n",
        "        bigquery.SchemaField(\"instance_id\", \"STRING\"),\n",
        "        bigquery.SchemaField(\"denormalized_MAE\", \"FLOAT64\"),\n",
        "        bigquery.SchemaField(\"denormalized_RMSE\", \"FLOAT64\"),\n",
        "        bigquery.SchemaField(\"denormalized_MSE\", \"FLOAT64\"),\n",
        "        bigquery.SchemaField(\"denormalized_MSLE\", \"FLOAT64\"),\n",
        "        bigquery.SchemaField(\"normalized_MAE\", \"FLOAT64\"),\n",
        "        bigquery.SchemaField(\"normalized_RMSE\", \"FLOAT64\"),\n",
        "        bigquery.SchemaField(\"normalized_MSE\", \"FLOAT64\"),\n",
        "        bigquery.SchemaField(\"normalized_MSLE\", \"FLOAT64\"),\n",
        "        bigquery.SchemaField(\"encoded\", \"FLOAT64\", mode=\"REPEATED\"),\n",
        "        bigquery.SchemaField(\"timestamp\", \"TIMESTAMP\"),\n",
        "    ]\n",
        "    print(\"Using schema for pre-built container (full output)\")\n",
        "\n",
        "# Create or get table\n",
        "table_id = f\"{BQ_PROJECT}.{BQ_DATASET}.{BQ_TABLE_RESULTS}\"\n",
        "\n",
        "try:\n",
        "    table = bq.get_table(table_id)\n",
        "    print(f\"✅ Table already exists: {table_id}\")\n",
        "except:\n",
        "    table = bigquery.Table(table_id, schema=schema)\n",
        "    table = bq.create_table(table)\n",
        "    print(f\"✅ Created table: {table_id}\")\n",
        "\n",
        "print(f\"   Total rows: {table.num_rows}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "pipeline_code",
      "metadata": {},
      "source": [
        "---\n",
        "## Create Dataflow Pipeline\n",
        "\n",
        "Build a pipeline that reads from BigQuery, calls the Vertex Endpoint, and writes results back."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "endpoint_caller",
      "metadata": {},
      "source": [
        "### Define Endpoint Caller DoFn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "endpoint_caller_code",
      "metadata": {},
      "outputs": [],
      "source": [
        "class CallVertexEndpoint(beam.DoFn):\n",
        "    \"\"\"\n",
        "    DoFn that calls Vertex AI Endpoint for predictions.\n",
        "    \n",
        "    This batches multiple instances together to reduce HTTP overhead.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, project_id, region, endpoint_name, batch_size=100):\n",
        "        self.project_id = project_id\n",
        "        self.region = region\n",
        "        self.endpoint_name = endpoint_name\n",
        "        self.batch_size = batch_size\n",
        "        self.endpoint = None\n",
        "        self.credentials = None\n",
        "        self.endpoint_url = None\n",
        "    \n",
        "    def setup(self):\n",
        "        \"\"\"Initialize endpoint connection (runs once per worker)\"\"\"\n",
        "        import google.auth\n",
        "        from google.cloud import aiplatform\n",
        "        \n",
        "        # Initialize Vertex AI\n",
        "        aiplatform.init(project=self.project_id, location=self.region)\n",
        "        \n",
        "        # Find endpoint\n",
        "        endpoints = aiplatform.Endpoint.list(filter=f\"display_name={self.endpoint_name}\")\n",
        "        if not endpoints:\n",
        "            raise ValueError(f\"Endpoint '{self.endpoint_name}' not found\")\n",
        "        \n",
        "        self.endpoint = endpoints[0]\n",
        "        \n",
        "        # Get credentials for HTTP calls\n",
        "        self.credentials, _ = google.auth.default()\n",
        "        self.endpoint_url = f\"https://{self.region}-aiplatform.googleapis.com/v1/{self.endpoint.resource_name}:predict\"\n",
        "    \n",
        "    def process(self, batch):\n",
        "        \"\"\"\n",
        "        Process a batch of instances.\n",
        "        \n",
        "        Args:\n",
        "            batch: List of (instance_id, features) tuples\n",
        "        \n",
        "        Yields:\n",
        "            Dict with instance_id and prediction results\n",
        "        \"\"\"\n",
        "        import requests\n",
        "        import google.auth.transport.requests\n",
        "        from datetime import datetime\n",
        "        \n",
        "        if not batch:\n",
        "            return\n",
        "        \n",
        "        # Separate instance IDs and features\n",
        "        instance_ids = [item[0] for item in batch]\n",
        "        features = [item[1] for item in batch]\n",
        "        \n",
        "        # Refresh credentials if needed\n",
        "        auth_req = google.auth.transport.requests.Request()\n",
        "        self.credentials.refresh(auth_req)\n",
        "        \n",
        "        # Make HTTP request to endpoint\n",
        "        headers = {\n",
        "            \"Authorization\": f\"Bearer {self.credentials.token}\",\n",
        "            \"Content-Type\": \"application/json\"\n",
        "        }\n",
        "        \n",
        "        payload = {\"instances\": features}\n",
        "        \n",
        "        response = requests.post(self.endpoint_url, headers=headers, json=payload)\n",
        "        \n",
        "        if response.status_code != 200:\n",
        "            raise Exception(f\"Endpoint error: {response.status_code} - {response.text}\")\n",
        "        \n",
        "        predictions = response.json()[\"predictions\"]\n",
        "        \n",
        "        # Combine instance IDs with predictions\n",
        "        timestamp = datetime.utcnow().isoformat()\n",
        "        \n",
        "        for instance_id, prediction in zip(instance_ids, predictions):\n",
        "            result = {\n",
        "                \"instance_id\": instance_id,\n",
        "                \"timestamp\": timestamp\n",
        "            }\n",
        "            \n",
        "            # Add prediction fields based on endpoint type\n",
        "            if 'anomaly_score' in prediction:\n",
        "                # Custom container output\n",
        "                result[\"anomaly_score\"] = prediction[\"anomaly_score\"]\n",
        "                result[\"encoded\"] = prediction[\"encoded\"]\n",
        "            else:\n",
        "                # Pre-built container output (full)\n",
        "                result[\"denormalized_MAE\"] = prediction[\"denormalized_MAE\"]\n",
        "                result[\"denormalized_RMSE\"] = prediction[\"denormalized_RMSE\"]\n",
        "                result[\"denormalized_MSE\"] = prediction[\"denormalized_MSE\"]\n",
        "                result[\"denormalized_MSLE\"] = prediction[\"denormalized_MSLE\"]\n",
        "                result[\"normalized_MAE\"] = prediction[\"normalized_MAE\"]\n",
        "                result[\"normalized_RMSE\"] = prediction[\"normalized_RMSE\"]\n",
        "                result[\"normalized_MSE\"] = prediction[\"normalized_MSE\"]\n",
        "                result[\"normalized_MSLE\"] = prediction[\"normalized_MSLE\"]\n",
        "                result[\"encoded\"] = prediction[\"encoded\"]\n",
        "            \n",
        "            yield result"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "pipeline_options",
      "metadata": {},
      "source": [
        "### Define Pipeline Options"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pipeline_options_code",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create pipeline options\n",
        "options = PipelineOptions()\n",
        "\n",
        "# Google Cloud options\n",
        "google_cloud_options = options.view_as(GoogleCloudOptions)\n",
        "google_cloud_options.project = PROJECT_ID\n",
        "google_cloud_options.region = REGION\n",
        "google_cloud_options.job_name = DATAFLOW_JOB_NAME\n",
        "google_cloud_options.staging_location = DATAFLOW_STAGING_LOCATION\n",
        "google_cloud_options.temp_location = DATAFLOW_TEMP_LOCATION\n",
        "\n",
        "# Standard options\n",
        "standard_options = options.view_as(StandardOptions)\n",
        "standard_options.runner = 'DataflowRunner'\n",
        "\n",
        "# Additional options for better performance\n",
        "options.add_value_provider_argument('--max_num_workers', MAX_WORKERS)\n",
        "options.add_value_provider_argument('--autoscaling_algorithm', 'THROUGHPUT_BASED')\n",
        "\n",
        "print(f\"✅ Pipeline options configured\")\n",
        "print(f\"   Job name: {DATAFLOW_JOB_NAME}\")\n",
        "print(f\"   Region: {REGION}\")\n",
        "print(f\"   Max workers: {MAX_WORKERS}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "run_pipeline",
      "metadata": {},
      "source": [
        "### Run Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "run_pipeline_code",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the pipeline\n",
        "def run_endpoint_batch_pipeline():\n",
        "    \"\"\"\n",
        "    Batch pipeline that:\n",
        "    1. Reads test data from BigQuery\n",
        "    2. Batches instances for efficient endpoint calls\n",
        "    3. Calls Vertex Endpoint for predictions\n",
        "    4. Writes results to BigQuery\n",
        "    \"\"\"\n",
        "    \n",
        "    # BigQuery query to get test data\n",
        "    query = f\"\"\"\n",
        "    SELECT \n",
        "        transaction_id,\n",
        "        * EXCEPT(splits, transaction_id, Class)\n",
        "    FROM `{BQ_PROJECT}.{BQ_DATASET}.{BQ_TABLE}`\n",
        "    WHERE splits = \"TEST\" AND Class = 0\n",
        "    LIMIT 1000\n",
        "    \"\"\"\n",
        "    \n",
        "    with beam.Pipeline(options=options) as p:\n",
        "        results = (\n",
        "            p\n",
        "            # Read from BigQuery\n",
        "            | 'Read from BigQuery' >> ReadFromBigQuery(\n",
        "                query=query,\n",
        "                use_standard_sql=True\n",
        "            )\n",
        "            # Extract features and ID\n",
        "            | 'Extract Features' >> beam.Map(\n",
        "                lambda row: (\n",
        "                    row['transaction_id'],\n",
        "                    [row[col] for col in sorted(row.keys()) if col != 'transaction_id']\n",
        "                )\n",
        "            )\n",
        "            # Batch instances for efficient endpoint calls\n",
        "            | 'Batch Instances' >> beam.BatchElements(\n",
        "                min_batch_size=BATCH_SIZE,\n",
        "                max_batch_size=BATCH_SIZE\n",
        "            )\n",
        "            # Call Vertex Endpoint\n",
        "            | 'Call Endpoint' >> beam.ParDo(\n",
        "                CallVertexEndpoint(\n",
        "                    project_id=PROJECT_ID,\n",
        "                    region=REGION,\n",
        "                    endpoint_name=ENDPOINT_DISPLAY_NAME,\n",
        "                    batch_size=BATCH_SIZE\n",
        "                )\n",
        "            )\n",
        "            # Write to BigQuery\n",
        "            | 'Write to BigQuery' >> WriteToBigQuery(\n",
        "                table=f\"{BQ_PROJECT}:{BQ_DATASET}.{BQ_TABLE_RESULTS}\",\n",
        "                write_disposition=beam.io.BigQueryDisposition.WRITE_APPEND,\n",
        "                create_disposition=beam.io.BigQueryDisposition.CREATE_NEVER\n",
        "            )\n",
        "        )\n",
        "\n",
        "# Run the pipeline\n",
        "print(\"Starting Dataflow job...\")\n",
        "print(f\"\\nMonitor job at:\")\n",
        "print(f\"https://console.cloud.google.com/dataflow/jobs/{REGION}?project={PROJECT_ID}\\n\")\n",
        "\n",
        "run_endpoint_batch_pipeline()\n",
        "\n",
        "print(\"\\n✅ Pipeline submitted to Dataflow\")\n",
        "print(f\"   This will take several minutes to complete\")\n",
        "print(f\"   Results will be written to: {BQ_PROJECT}.{BQ_DATASET}.{BQ_TABLE_RESULTS}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "monitor",
      "metadata": {},
      "source": [
        "---\n",
        "## Monitor Pipeline\n",
        "\n",
        "Wait for the pipeline to complete and check results."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wait",
      "metadata": {},
      "source": [
        "### Wait for Completion\n",
        "\n",
        "The pipeline typically takes 5-10 minutes to complete. You can:\n",
        "\n",
        "1. **Monitor in Console**: Click the link above to see real-time job status\n",
        "2. **Check logs**: View worker logs in Cloud Logging\n",
        "3. **View metrics**: See throughput and latency graphs\n",
        "\n",
        "The cell below will poll BigQuery every 30 seconds to check for new results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wait_code",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Poll BigQuery for results\n",
        "print(\"Waiting for pipeline results...\\n\")\n",
        "\n",
        "initial_count_query = f\"SELECT COUNT(*) as count FROM `{BQ_PROJECT}.{BQ_DATASET}.{BQ_TABLE_RESULTS}`\"\n",
        "initial_count = bq.query(initial_count_query).to_dataframe()['count'][0]\n",
        "\n",
        "print(f\"Initial row count: {initial_count}\")\n",
        "print(f\"Polling every 30 seconds...\\n\")\n",
        "\n",
        "max_wait_time = 600  # 10 minutes\n",
        "elapsed_time = 0\n",
        "poll_interval = 30\n",
        "\n",
        "while elapsed_time < max_wait_time:\n",
        "    time.sleep(poll_interval)\n",
        "    elapsed_time += poll_interval\n",
        "    \n",
        "    current_count = bq.query(initial_count_query).to_dataframe()['count'][0]\n",
        "    new_rows = current_count - initial_count\n",
        "    \n",
        "    print(f\"[{elapsed_time}s] Current rows: {current_count} (+{new_rows} new)\")\n",
        "    \n",
        "    if new_rows >= 1000:\n",
        "        print(\"\\n✅ Pipeline completed successfully!\")\n",
        "        break\n",
        "else:\n",
        "    print(\"\\n⚠️  Polling timeout - check Dataflow console for job status\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "analyze_results",
      "metadata": {},
      "source": [
        "---\n",
        "## Analyze Results\n",
        "\n",
        "Query and visualize the predictions from the endpoint."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "query_results",
      "metadata": {},
      "source": [
        "### Query Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "query_results_code",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get recent results\n",
        "if is_custom_endpoint:\n",
        "    query = f\"\"\"\n",
        "    SELECT \n",
        "        instance_id,\n",
        "        anomaly_score,\n",
        "        encoded,\n",
        "        timestamp\n",
        "    FROM `{BQ_PROJECT}.{BQ_DATASET}.{BQ_TABLE_RESULTS}`\n",
        "    ORDER BY timestamp DESC\n",
        "    LIMIT 10\n",
        "    \"\"\"\n",
        "else:\n",
        "    query = f\"\"\"\n",
        "    SELECT \n",
        "        instance_id,\n",
        "        denormalized_MAE as anomaly_score,\n",
        "        encoded,\n",
        "        timestamp\n",
        "    FROM `{BQ_PROJECT}.{BQ_DATASET}.{BQ_TABLE_RESULTS}`\n",
        "    ORDER BY timestamp DESC\n",
        "    LIMIT 10\n",
        "    \"\"\"\n",
        "\n",
        "results_df = bq.query(query).to_dataframe()\n",
        "print(f\"Recent predictions ({len(results_df)} rows):\\n\")\n",
        "results_df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "summary_stats",
      "metadata": {},
      "source": [
        "### Summary Statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "summary_stats_code",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate statistics\n",
        "stats_query = f\"\"\"\n",
        "SELECT \n",
        "    COUNT(*) as total_predictions,\n",
        "    {'AVG(anomaly_score)' if is_custom_endpoint else 'AVG(denormalized_MAE)'} as avg_anomaly_score,\n",
        "    {'MIN(anomaly_score)' if is_custom_endpoint else 'MIN(denormalized_MAE)'} as min_anomaly_score,\n",
        "    {'MAX(anomaly_score)' if is_custom_endpoint else 'MAX(denormalized_MAE)'} as max_anomaly_score,\n",
        "    MIN(timestamp) as first_prediction,\n",
        "    MAX(timestamp) as last_prediction\n",
        "FROM `{BQ_PROJECT}.{BQ_DATASET}.{BQ_TABLE_RESULTS}`\n",
        "\"\"\"\n",
        "\n",
        "stats = bq.query(stats_query).to_dataframe()\n",
        "print(\"Prediction Statistics:\\n\")\n",
        "stats"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "comparison",
      "metadata": {},
      "source": [
        "---\n",
        "## Comparison: RunInference vs Endpoint\n",
        "\n",
        "Let's compare this approach with the RunInference approach from `dataflow-batch-runinference.ipynb`."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "compare_results",
      "metadata": {},
      "source": [
        "### Compare Results\n",
        "\n",
        "If you've run both notebooks, you can compare the results to verify they match:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "compare_results_code",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Try to compare with RunInference results\n",
        "try:\n",
        "    compare_query = f\"\"\"\n",
        "    SELECT \n",
        "        'RunInference' as method,\n",
        "        AVG(anomaly_score) as avg_score,\n",
        "        COUNT(*) as count\n",
        "    FROM `{BQ_PROJECT}.{BQ_DATASET}.batch_results`\n",
        "    UNION ALL\n",
        "    SELECT \n",
        "        'Vertex Endpoint' as method,\n",
        "        {'AVG(anomaly_score)' if is_custom_endpoint else 'AVG(denormalized_MAE)'} as avg_score,\n",
        "        COUNT(*) as count\n",
        "    FROM `{BQ_PROJECT}.{BQ_DATASET}.{BQ_TABLE_RESULTS}`\n",
        "    \"\"\"\n",
        "    \n",
        "    comparison_df = bq.query(compare_query).to_dataframe()\n",
        "    print(\"Comparison of Methods:\\n\")\n",
        "    print(comparison_df)\n",
        "    \n",
        "    print(\"\\nNote: Scores should be very similar (within rounding error)\")\n",
        "except Exception as e:\n",
        "    print(f\"Could not compare with RunInference results: {e}\")\n",
        "    print(f\"\\nRun dataflow-batch-runinference.ipynb first to enable comparison\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cost_analysis",
      "metadata": {},
      "source": [
        "### Cost Analysis\n",
        "\n",
        "**Endpoint Approach Costs**:\n",
        "- **Dataflow**: ~$0.10-0.50 for 5-10 minute job (worker time)\n",
        "- **Vertex Endpoint**: ~$0.20/hour × job duration (endpoint is already running)\n",
        "- **Network**: Minimal (same region)\n",
        "- **Total**: ~$0.15-0.70 per batch job (assuming endpoint is already deployed)\n",
        "\n",
        "**RunInference Approach Costs**:\n",
        "- **Dataflow**: ~$0.10-0.50 for 5-10 minute job (worker time)\n",
        "- **Vertex Endpoint**: $0 (not used)\n",
        "- **Total**: ~$0.10-0.50 per batch job\n",
        "\n",
        "**When Endpoint Approach Makes Sense**:\n",
        "1. Endpoint is already deployed for online predictions\n",
        "2. Need centralized model versioning and rollback\n",
        "3. Multiple applications calling the same model\n",
        "4. Want A/B testing or canary deployments\n",
        "5. Batch job frequency is low (weekly/monthly)\n",
        "\n",
        "**When RunInference Makes Sense**:\n",
        "1. Batch processing only (no online predictions)\n",
        "2. Very high throughput requirements\n",
        "3. Cost-sensitive applications\n",
        "4. Frequent batch jobs (daily/hourly)\n",
        "5. Want lowest latency (in-process inference)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cleanup",
      "metadata": {},
      "source": [
        "---\n",
        "## Cleanup\n",
        "\n",
        "Clean up resources to avoid ongoing charges."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cleanup_note",
      "metadata": {},
      "source": [
        "### Cleanup Notes\n",
        "\n",
        "**What to Clean Up**:\n",
        "- ✅ Results table (optional - minimal cost)\n",
        "- ⚠️ Vertex Endpoint (if no longer needed for online predictions)\n",
        "\n",
        "**What to Keep**:\n",
        "- Source data in BigQuery\n",
        "- Model in GCS (for future use)\n",
        "\n",
        "**Important**: The Vertex Endpoint incurs hourly charges (~$0.20/hour). If you're not using it for online predictions, delete it using the endpoint notebooks."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "delete_results",
      "metadata": {},
      "source": [
        "### Delete Results Table (Optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "delete_results_code",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Uncomment to delete results table\n",
        "# bq.delete_table(f\"{BQ_PROJECT}.{BQ_DATASET}.{BQ_TABLE_RESULTS}\", not_found_ok=True)\n",
        "# print(f\"✅ Deleted table: {BQ_TABLE_RESULTS}\")\n",
        "print(\"Uncomment the code above to delete the results table\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "summary",
      "metadata": {},
      "source": [
        "---\n",
        "## Summary\n",
        "\n",
        "In this notebook, you:\n",
        "\n",
        "✅ **Verified Deployed Endpoint**: Confirmed endpoint is ready to serve\n",
        "\n",
        "✅ **Created Batch Pipeline**: Built Dataflow job to call Vertex Endpoint\n",
        "\n",
        "✅ **Implemented Batching**: Grouped instances for efficient HTTP calls\n",
        "\n",
        "✅ **Stored Results**: Wrote predictions to BigQuery\n",
        "\n",
        "✅ **Analyzed Trade-offs**: Compared RunInference vs Endpoint approach\n",
        "\n",
        "### Key Learnings\n",
        "\n",
        "**Advantages of Vertex Endpoint Approach**:\n",
        "- Centralized model management and versioning\n",
        "- Built-in A/B testing and canary deployments\n",
        "- Shared model across multiple applications\n",
        "- Easier model updates (no pipeline redeployment)\n",
        "\n",
        "**Advantages of RunInference Approach**:\n",
        "- Lower cost (no endpoint charges)\n",
        "- Lower latency (in-process inference)\n",
        "- Higher throughput (no network overhead)\n",
        "- Better for batch-only workloads\n",
        "\n",
        "### When to Use Each\n",
        "\n",
        "**Use Vertex Endpoint** when:\n",
        "- Model is deployed for online predictions anyway\n",
        "- Need centralized versioning and rollback\n",
        "- Multiple teams/apps use the same model\n",
        "- Want managed A/B testing\n",
        "\n",
        "**Use RunInference** when:\n",
        "- Batch processing only\n",
        "- Cost is primary concern\n",
        "- Very high throughput needed\n",
        "- Frequent batch jobs\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "- **Compare with RunInference**: Run `dataflow-batch-runinference.ipynb` to see the difference\n",
        "- **Monitor Costs**: Track endpoint and Dataflow costs in Cloud Console\n",
        "- **Optimize Batching**: Experiment with different batch sizes for your workload\n",
        "- **Production Deployment**: Add error handling, retries, and monitoring\n",
        "\n",
        "### Related Notebooks\n",
        "\n",
        "- [Dataflow Setup](./dataflow-setup.ipynb) - Infrastructure setup\n",
        "- [Dataflow Batch RunInference](./dataflow-batch-runinference.ipynb) - In-process batch inference\n",
        "- [Dataflow Streaming RunInference](./dataflow-streaming-runinference.ipynb) - Real-time streaming\n",
        "- [Vertex AI Endpoint (Pre-built)](./vertex-ai-endpoint-prebuilt-container.ipynb) - Deploy endpoint\n",
        "- [Vertex AI Endpoint (Custom)](./vertex-ai-endpoint-custom-container.ipynb) - Custom container\n",
        "\n",
        "### Resources\n",
        "\n",
        "- [Dataflow Documentation](https://cloud.google.com/dataflow/docs)\n",
        "- [Vertex AI Endpoints](https://cloud.google.com/vertex-ai/docs/predictions/overview)\n",
        "- [Apache Beam Guide](https://beam.apache.org/documentation/)\n",
        "- [Vertex AI Pricing](https://cloud.google.com/vertex-ai/pricing)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.13.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
