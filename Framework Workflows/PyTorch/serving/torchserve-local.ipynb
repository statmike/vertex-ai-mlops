{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0adc644a",
   "metadata": {},
   "source": [
    "![tracker](https://us-central1-vertex-ai-mlops-369716.cloudfunctions.net/pixel-tracking?path=statmike%2Fvertex-ai-mlops%2FFramework+Workflows%2FPyTorch%2Fserving&file=torchserve-local.ipynb)\n",
    "<!--- header table --->\n",
    "<table align=\"left\">\n",
    "<tr>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%20Workflows/PyTorch/serving/torchserve-local.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.svgrepo.com/download/217753/github.svg\" alt=\"GitHub logo\">\n",
    "      <br>View on<br>GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://colab.research.google.com/github/statmike/vertex-ai-mlops/blob/main/Framework%20Workflows/PyTorch/serving/torchserve-local.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.gstatic.com/pantheon/images/bigquery/welcome_page/colab-logo.svg\" alt=\"Google Colaboratory logo\">\n",
    "      <br>Run in<br>Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https%3A%2F%2Fraw.githubusercontent.com%2Fstatmike%2Fvertex-ai-mlops%2Fmain%2FFramework%2520Workflows%2FPyTorch%2Fserving%2Ftorchserve-local.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\">\n",
    "      <br>Run in<br>Colab Enterprise\n",
    "    </a>\n",
    "  </td>      \n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/bigquery/import?url=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%20Workflows/PyTorch/serving/torchserve-local.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.gstatic.com/images/branding/gcpiconscolors/bigquery/v1/32px.svg\" alt=\"BigQuery logo\">\n",
    "      <br>Open in<br>BigQuery Studio\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/statmike/vertex-ai-mlops/main/Framework%20Workflows/PyTorch/serving/torchserve-local.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.gstatic.com/images/branding/gcpiconscolors/vertexai/v1/32px.svg\" alt=\"Vertex AI logo\">\n",
    "      <br>Open in<br>Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td colspan=\"5\" style=\"text-align: right\">\n",
    "    <b>Share This On: </b> \n",
    "    <a href=\"https://www.linkedin.com/sharing/share-offsite/?url=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%2520Workflows/PyTorch/serving/torchserve-local.ipynb\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"Linkedin Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://reddit.com/submit?url=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%2520Workflows/PyTorch/serving/torchserve-local.ipynb\"><img src=\"https://redditinc.com/hubfs/Reddit%20Inc/Brand/Reddit_Logo.png\" alt=\"Reddit Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://bsky.app/intent/compose?text=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%2520Workflows/PyTorch/serving/torchserve-local.ipynb\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"BlueSky Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://twitter.com/intent/tweet?url=https://github.com/statmike/vertex-ai-mlops/blob/main/Framework%2520Workflows/PyTorch/serving/torchserve-local.ipynb\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/5/5a/X_icon_2.svg\" alt=\"X (Twitter) Logo\" width=\"20px\"></a> \n",
    "  </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td colspan=\"5\" style=\"text-align: right\">\n",
    "    <b>Connect With Author On: </b> \n",
    "    <a href=\"https://www.linkedin.com/in/statmike\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"Linkedin Logo\" width=\"20px\"></a>\n",
    "    <a href=\"https://www.github.com/statmike\"><img src=\"https://www.svgrepo.com/download/217753/github.svg\" alt=\"GitHub Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://www.youtube.com/@statmike-channel\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/f/fd/YouTube_full-color_icon_%282024%29.svg\" alt=\"YouTube Logo\" width=\"20px\"></a>\n",
    "    <a href=\"https://bsky.app/profile/statmike.bsky.social\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"BlueSky Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://x.com/statmike\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/5/5a/X_icon_2.svg\" alt=\"X (Twitter) Logo\" width=\"20px\"></a>\n",
    "  </td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# TorchServe Local Development and Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overview",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This notebook demonstrates how to run **TorchServe locally** for development and testing of PyTorch model deployments. This is an essential workflow for validating model serving behavior before deploying to production environments.\n",
    "\n",
    "### What This Notebook Does\n",
    "\n",
    "- Uses the Model Archive (.mar) file created in the training notebook\n",
    "- Organizes TorchServe files under `./files/torchserve/` (model store, logs, config)\n",
    "- Starts a local TorchServe instance with the model\n",
    "- Tests predictions via the REST API\n",
    "- Demonstrates management and metrics endpoints\n",
    "- Shows proper shutdown procedures\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "Before running this notebook, you should have:\n",
    "- **Completed** `../pytorch-autoencoder.ipynb` to create the model archive\n",
    "  - The `.mar` file should be in `../files/pytorch-autoencoder/pytorch_autoencoder.mar`\n",
    "- Python 3.8+ environment\n",
    "- Sufficient local compute resources (CPU/GPU)\n",
    "\n",
    "### What is TorchServe?\n",
    "\n",
    "[TorchServe](https://pytorch.org/serve/) is PyTorch's official model serving framework that provides:\n",
    "- Production-ready REST and gRPC APIs for model inference\n",
    "- Multi-model serving with dynamic model loading\n",
    "- Built-in metrics and logging\n",
    "- Custom preprocessing and postprocessing via handlers\n",
    "- Support for batching and versioning\n",
    "\n",
    "### Benefits of Local Testing\n",
    "\n",
    "Running TorchServe locally allows you to:\n",
    "- **Validate** model behavior before cloud deployment\n",
    "- **Debug** custom handlers and preprocessing logic\n",
    "- **Test** prediction endpoints with sample data\n",
    "- **Iterate** quickly without cloud deployment overhead\n",
    "- **Understand** TorchServe configuration and APIs\n",
    "\n",
    "### File Organization\n",
    "\n",
    "This notebook creates an organized structure under `./files/torchserve/`:\n",
    "```\n",
    "./files/torchserve/\n",
    "├── model_store/           # Model archive (.mar) files\n",
    "├── logs/                  # TorchServe logs (ts_log.log, model_log.log, etc.)\n",
    "└── config.properties      # TorchServe configuration\n",
    "```\n",
    "\n",
    "### Architecture\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────┐\n",
    "│         Local Development               │\n",
    "│                                         │\n",
    "│  ┌───────────────────────────────────┐  │\n",
    "│  │      TorchServe Instance          │  │\n",
    "│  │                                   │  │\n",
    "│  │  ┌─────────────────────────────┐  │  │\n",
    "│  │  │   Inference API :8080       │  │  │\n",
    "│  │  │   - POST /predictions       │  │  │\n",
    "│  │  └─────────────────────────────┘  │  │\n",
    "│  │                                   │  │\n",
    "│  │  ┌─────────────────────────────┐  │  │\n",
    "│  │  │   Management API :8081      │  │  │\n",
    "│  │  │   - GET /models             │  │  │\n",
    "│  │  └─────────────────────────────┘  │  │\n",
    "│  │                                   │  │\n",
    "│  │  ┌─────────────────────────────┐  │  │\n",
    "│  │  │   Metrics API :8082         │  │  │\n",
    "│  │  │   - GET /metrics            │  │  │\n",
    "│  │  └─────────────────────────────┘  │  │\n",
    "│  │                                   │  │\n",
    "│  │  ┌─────────────────────────────┐  │  │\n",
    "│  │  │   Files                     │  │  │\n",
    "│  │  │   - model_store/            │  │  │\n",
    "│  │  │   - logs/                   │  │  │\n",
    "│  │  │   - config.properties       │  │  │\n",
    "│  │  └─────────────────────────────┘  │  │\n",
    "│  └───────────────────────────────────┘  │\n",
    "│                                         │\n",
    "│  ┌───────────────────────────────────┐  │\n",
    "│  │   Jupyter Notebook Client         │  │\n",
    "│  │   - HTTP requests to APIs         │  │\n",
    "│  └───────────────────────────────────┘  │\n",
    "└─────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "### What You Will Learn\n",
    "\n",
    "1. How to configure and start TorchServe locally\n",
    "2. How to load model archives into TorchServe\n",
    "3. How to make predictions via the REST API\n",
    "4. How to use management APIs to inspect models\n",
    "5. How to access metrics and monitoring data\n",
    "6. Best practices for local model serving development"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "env",
   "metadata": {},
   "source": [
    "---\n",
    "## Environment Setup\n",
    "\n",
    "This section will authenticate your session, enable required Google Cloud APIs, and install necessary Python packages.\n",
    "\n",
    "**Package Installation Options (`REQ_TYPE`):**\n",
    "- `PRIMARY`: Installs only the main packages. Faster, but pip resolves sub-dependencies which may result in different versions than development.\n",
    "- `ALL` (Default): Installs exact versions of all packages and dependencies. Best for perfectly reproducing the development environment.\n",
    "- `COLAB`: Installs a Colab-optimized list that excludes pre-installed packages like `ipython` and `ipykernel`.\n",
    "\n",
    "**Installation Tool Options (`INSTALL_TOOL`):**\n",
    "- `pip` (Default): Uses pip for package installation. Standard Python package installer.\n",
    "- `uv`: Modern, fast Python package installer. Must be installed separately. See: https://github.com/astral-sh/uv\n",
    "- `poetry`: Dependency management tool. Requires running notebook in a poetry environment (`poetry shell` or `poetry run jupyter lab`). Uses `pyproject.toml` instead of requirements.txt.\n",
    "\n",
    "> **Note:** If running in Google Colab, the script will automatically detect this and set `REQ_TYPE = 'COLAB'` to prevent package conflicts, overriding any manual setting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proj",
   "metadata": {},
   "source": [
    "### Set Your Project ID\n",
    "\n",
    "⚠️ **Action Required:** Replace the `PROJECT_ID` value below with your Google Cloud project ID before running this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "conf",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = 'statmike-mlops-349915' # replace with GCP project ID\n",
    "REQ_TYPE = 'ALL' # Specify PRIMARY or ALL or COLAB\n",
    "INSTALL_TOOL = 'poetry' # Specify pip, uv, or poetry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6taf827549y",
   "metadata": {},
   "outputs": [],
   "source": [
    "REQUIREMENTS_URL = 'https://raw.githubusercontent.com/statmike/vertex-ai-mlops/refs/heads/main/Framework%20Workflows/PyTorch/requirements.txt'\n",
    "REQUIRED_APIS = []  # No APIs required for local TorchServe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ju9f243rrao",
   "metadata": {},
   "source": [
    "### Run Setup\n",
    "\n",
    "This cell downloads the centralized setup code and configures your environment. It will:\n",
    "- Install necessary Python packages (including TorchServe)\n",
    "- Display a setup summary\n",
    "\n",
    "> **Note:** In Colab, if packages are installed, the kernel will automatically restart. After restart, continue from the next cell without re-running earlier cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "el1ryw02gdq",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "PYTHON GCP ENVIRONMENT SETUP\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "AUTHENTICATION\n",
      "==================================================\n",
      "Checking for existing ADC...\n",
      "✅ Existing ADC found.\n",
      "✅ Project is correctly set to 'statmike-mlops-349915'.\n",
      "\n",
      "==================================================\n",
      "API CHECK & ENABLE\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "PACKAGE MANAGEMENT\n",
      "==================================================\n",
      "Installation Tool: poetry\n",
      "✅ Found poetry at: /usr/local/google/home/statmike/.local/bin/poetry\n",
      "✅ Running in poetry environment: /usr/local/google/home/statmike/.cache/pypoetry/virtualenvs/frameworks-pytorch-0KVJlKeQ-py3.13\n",
      "ℹ️  Poetry mode: Installing from pyproject.toml (REQUIREMENTS_URL ignored)\n",
      "✅ Found pyproject.toml at: /usr/local/google/home/statmike/Git/vertex-ai-mlops/Framework Workflows/PyTorch/pyproject.toml\n",
      "   Changed working directory to: /usr/local/google/home/statmike/Git/vertex-ai-mlops/Framework Workflows/PyTorch\n",
      "Running poetry install...\n",
      "   Restored working directory to: /usr/local/google/home/statmike/Git/vertex-ai-mlops/Framework Workflows/PyTorch/serving\n",
      "✅ All packages are already installed and up to date.\n",
      "\n",
      "==================================================\n",
      "Google Cloud Project Information\n",
      "==================================================\n",
      "PROJECT_ID     = statmike-mlops-349915\n",
      "PROJECT_NUMBER = 1026793852137\n",
      "==================================================\n",
      "\n",
      "\n",
      "==================================================\n",
      "SETUP SUMMARY\n",
      "==================================================\n",
      "✅ Authentication:    Success\n",
      "✅ API Configuration: Success\n",
      "✅ Package Install:   Already up to date\n",
      "✅ Installation Tool: poetry\n",
      "✅ Project ID:        statmike-mlops-349915\n",
      "✅ Project Number:    1026793852137\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os, urllib.request\n",
    "\n",
    "# Download and import setup code\n",
    "url = 'https://raw.githubusercontent.com/statmike/vertex-ai-mlops/refs/heads/main/core/notebook-template/python_setup.py'\n",
    "urllib.request.urlretrieve(url, 'python_setup_local.py')\n",
    "import python_setup_local as python_setup\n",
    "os.remove('python_setup_local.py')\n",
    "\n",
    "# Run setup\n",
    "setup_info = python_setup.setup_environment(PROJECT_ID, REQ_TYPE, REQUIREMENTS_URL, REQUIRED_APIS, INSTALL_TOOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "py",
   "metadata": {},
   "source": [
    "---\n",
    "## Python Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "imp",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import subprocess\n",
    "import time\n",
    "import requests\n",
    "import numpy as np\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vars",
   "metadata": {},
   "source": [
    "### Variables\n",
    "\n",
    "Model and TorchServe configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "xq2wyndb2p",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Name: pytorch_autoencoder\n",
      "Model Archive Path: ../files/pytorch-autoencoder/pytorch_autoencoder.mar\n",
      "Model Store Directory: ./files/torchserve/model_store\n",
      "\n",
      "Inference URL: http://localhost:8080/predictions/pytorch_autoencoder\n",
      "Management URL: http://localhost:8081\n",
      "Metrics URL: http://localhost:8082/metrics\n"
     ]
    }
   ],
   "source": [
    "# Model settings\n",
    "MODEL_NAME = \"pytorch_autoencoder\"\n",
    "MAR_FILE = \"pytorch_autoencoder.mar\"\n",
    "\n",
    "# Path to the .mar file created in the training notebook\n",
    "MAR_PATH = f\"../files/pytorch-autoencoder/{MAR_FILE}\"\n",
    "\n",
    "# Local model store directory - organized under ./files/torchserve/\n",
    "MODEL_STORE_DIR = \"./files/torchserve/model_store\"\n",
    "\n",
    "# TorchServe settings\n",
    "INFERENCE_PORT = 8080\n",
    "MANAGEMENT_PORT = 8081\n",
    "METRICS_PORT = 8082\n",
    "\n",
    "# API endpoints\n",
    "INFERENCE_URL = f\"http://localhost:{INFERENCE_PORT}/predictions/{MODEL_NAME}\"\n",
    "MANAGEMENT_URL = f\"http://localhost:{MANAGEMENT_PORT}\"\n",
    "METRICS_URL = f\"http://localhost:{METRICS_PORT}/metrics\"\n",
    "\n",
    "print(f\"Model Name: {MODEL_NAME}\")\n",
    "print(f\"Model Archive Path: {MAR_PATH}\")\n",
    "print(f\"Model Store Directory: {MODEL_STORE_DIR}\")\n",
    "print(f\"\\nInference URL: {INFERENCE_URL}\")\n",
    "print(f\"Management URL: {MANAGEMENT_URL}\")\n",
    "print(f\"Metrics URL: {METRICS_URL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "download",
   "metadata": {},
   "source": [
    "---\n",
    "## Prepare Model Archive\n",
    "\n",
    "Copy the `.mar` file from the training notebook to the local model store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "download_c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model store directory: ./files/torchserve/model_store\n",
      "✅ Found model archive at ../files/pytorch-autoencoder/pytorch_autoencoder.mar\n",
      "   File size: 29.15 KB\n",
      "\n",
      "Copying ../files/pytorch-autoencoder/pytorch_autoencoder.mar to ./files/torchserve/model_store/pytorch_autoencoder.mar...\n",
      "✅ Model archive copied successfully!\n",
      "   Destination: ./files/torchserve/model_store/pytorch_autoencoder.mar\n",
      "   File size: 29.15 KB\n",
      "\n",
      "✅ TorchServe config created at: ./files/torchserve/config.properties\n",
      "   Config settings:\n",
      "     - model_store: model_store (relative path)\n",
      "     - disable_token_authorization: true\n",
      "   Model will be loaded via command line parameter\n",
      "   Logs will be created in ./files/torchserve/logs/\n"
     ]
    }
   ],
   "source": [
    "# Create directories for TorchServe\n",
    "os.makedirs(MODEL_STORE_DIR, exist_ok=True)\n",
    "print(f\"Model store directory: {MODEL_STORE_DIR}\")\n",
    "\n",
    "# Verify source .mar file exists\n",
    "if not os.path.exists(MAR_PATH):\n",
    "    print(f\"❌ Error: Model archive not found at {MAR_PATH}\")\n",
    "    print(f\"   Please run ../pytorch-autoencoder.ipynb first to create the model archive\")\n",
    "else:\n",
    "    print(f\"✅ Found model archive at {MAR_PATH}\")\n",
    "    print(f\"   File size: {os.path.getsize(MAR_PATH) / 1024:.2f} KB\")\n",
    "\n",
    "# Copy model archive to model store\n",
    "local_mar_path = os.path.join(MODEL_STORE_DIR, MAR_FILE)\n",
    "\n",
    "print(f\"\\nCopying {MAR_PATH} to {local_mar_path}...\")\n",
    "shutil.copy2(MAR_PATH, local_mar_path)\n",
    "\n",
    "print(f\"✅ Model archive copied successfully!\")\n",
    "print(f\"   Destination: {local_mar_path}\")\n",
    "print(f\"   File size: {os.path.getsize(local_mar_path) / 1024:.2f} KB\")\n",
    "\n",
    "# Create TorchServe config file\n",
    "# Note: models are loaded via command line, not in config\n",
    "config_path = \"./files/torchserve/config.properties\"\n",
    "config_content = f\"\"\"inference_address=http://0.0.0.0:{INFERENCE_PORT}\n",
    "management_address=http://0.0.0.0:{MANAGEMENT_PORT}\n",
    "metrics_address=http://0.0.0.0:{METRICS_PORT}\n",
    "model_store=model_store\n",
    "disable_token_authorization=true\n",
    "\"\"\"\n",
    "\n",
    "with open(config_path, 'w') as f:\n",
    "    f.write(config_content)\n",
    "\n",
    "print(f\"\\n✅ TorchServe config created at: {config_path}\")\n",
    "print(f\"   Config settings:\")\n",
    "print(f\"     - model_store: model_store (relative path)\")\n",
    "print(f\"     - disable_token_authorization: true\")\n",
    "print(f\"   Model will be loaded via command line parameter\")\n",
    "print(f\"   Logs will be created in ./files/torchserve/logs/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "start",
   "metadata": {},
   "source": [
    "---\n",
    "## Start TorchServe\n",
    "\n",
    "Start a local TorchServe instance with the downloaded model archive.\n",
    "\n",
    "**Command options:**\n",
    "- `--start`: Start the server\n",
    "- `--model-store`: Directory containing model archives\n",
    "- `--models`: Model to load at startup (format: `model_name=archive.mar`)\n",
    "- `--ncs`: No config snapshot (simplifies local development)\n",
    "\n",
    "The server will expose three APIs:\n",
    "- **Inference API** (port 8080): For predictions\n",
    "- **Management API** (port 8081): For model management\n",
    "- **Metrics API** (port 8082): For monitoring metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "start_c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting TorchServe...\n",
      "Model store: ./files/torchserve/model_store\n",
      "Config file: ./files/torchserve/config.properties\n",
      "Logs directory: ./files/torchserve/logs\n",
      "============================================================\n",
      "Command: torchserve --start --ts-config config.properties --models pytorch_autoencoder=pytorch_autoencoder.mar --ncs\n",
      "Working directory: /usr/local/google/home/statmike/Git/vertex-ai-mlops/Framework Workflows/PyTorch/serving/files/torchserve\n",
      "✅ Start command completed\n",
      "\n",
      "Waiting for TorchServe to be ready...\n",
      "WARNING: sun.reflect.Reflection.getCallerClass is not supported. This will impact performance.\n",
      "nvidia-smi not available or failed: Cannot run program \"nvidia-smi\": Exec failed, error: 2 (No such file or directory) \n",
      "2025-11-07T15:14:55,455 [DEBUG] main org.pytorch.serve.util.ConfigManager - xpu-smi not available or failed: Cannot run program \"xpu-smi\": Exec failed, error: 2 (No such file or directory) \n",
      "2025-11-07T15:14:55,462 [WARN ] main org.pytorch.serve.util.ConfigManager - Your torchserve instance can access any URL to load models. When deploying to production, make sure to limit the set of allowed_urls in config.properties\n",
      "2025-11-07T15:14:55,481 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...\n",
      "2025-11-07T15:14:55,587 [INFO ] main org.pytorch.serve.metrics.configuration.MetricConfiguration - Successfully loaded metrics configuration from /usr/local/google/home/statmike/.cache/pypoetry/virtualenvs/frameworks-pytorch-0KVJlKeQ-py3.13/lib/python3.13/site-packages/ts/configs/metrics.yaml\n",
      "2025-11-07T15:14:55,709 [INFO ] main org.pytorch.serve.ModelServer - \n",
      "Torchserve version: 0.12.0\n",
      "TS Home: /usr/local/google/home/statmike/.cache/pypoetry/virtualenvs/frameworks-pytorch-0KVJlKeQ-py3.13/lib/python3.13/site-packages\n",
      "Current directory: /usr/local/google/home/statmike/Git/vertex-ai-mlops/Framework Workflows/PyTorch/serving/files/torchserve\n",
      "Temp directory: /tmp\n",
      "Metrics config path: /usr/local/google/home/statmike/.cache/pypoetry/virtualenvs/frameworks-pytorch-0KVJlKeQ-py3.13/lib/python3.13/site-packages/ts/configs/metrics.yaml\n",
      "Number of GPUs: 0\n",
      "Number of CPUs: 8\n",
      "Max heap size: 8032 M\n",
      "Python executable: /usr/local/google/home/statmike/.cache/pypoetry/virtualenvs/frameworks-pytorch-0KVJlKeQ-py3.13/bin/python\n",
      "Config file: config.properties\n",
      "Inference address: http://0.0.0.0:8080\n",
      "Management address: http://0.0.0.0:8081\n",
      "Metrics address: http://0.0.0.0:8082\n",
      "Model Store: /usr/local/google/home/statmike/Git/vertex-ai-mlops/Framework Workflows/PyTorch/serving/files/torchserve/model_store\n",
      "Initial Models: pytorch_autoencoder=pytorch_autoencoder.mar\n",
      "Log dir: /usr/local/google/home/statmike/Git/vertex-ai-mlops/Framework Workflows/PyTorch/serving/files/torchserve/logs\n",
      "Metrics dir: /usr/local/google/home/statmike/Git/vertex-ai-mlops/Framework Workflows/PyTorch/serving/files/torchserve/logs\n",
      "Netty threads: 0\n",
      "Netty client threads: 0\n",
      "Default workers per model: 8\n",
      "Blacklist Regex: N/A\n",
      "Maximum Response Size: 6553500\n",
      "Maximum Request Size: 6553500\n",
      "Limit Maximum Image Pixels: true\n",
      "Prefer direct buffer: false\n",
      "Allowed Urls: [file://.*|http(s)?://.*]\n",
      "Custom python dependency for model allowed: false\n",
      "Enable metrics API: true\n",
      "Metrics mode: LOG\n",
      "Disable system metrics: false\n",
      "Workflow Store: /usr/local/google/home/statmike/Git/vertex-ai-mlops/Framework Workflows/PyTorch/serving/files/torchserve/model_store\n",
      "CPP log config: N/A\n",
      "Model config: N/A\n",
      "System metrics command: default\n",
      "Model API enabled: false\n",
      "2025-11-07T15:14:55,726 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...\n",
      "2025-11-07T15:14:55,756 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: pytorch_autoencoder.mar\n",
      "2025-11-07T15:14:55,809 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model pytorch_autoencoder\n",
      "2025-11-07T15:14:55,810 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model pytorch_autoencoder\n",
      "2025-11-07T15:14:55,810 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model pytorch_autoencoder loaded.\n",
      "2025-11-07T15:14:55,811 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: pytorch_autoencoder, count: 8\n",
      "2025-11-07T15:14:55,825 [DEBUG] W-9005-pytorch_autoencoder_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/local/google/home/statmike/.cache/pypoetry/virtualenvs/frameworks-pytorch-0KVJlKeQ-py3.13/bin/python, /usr/local/google/home/statmike/.cache/pypoetry/virtualenvs/frameworks-pytorch-0KVJlKeQ-py3.13/lib/python3.13/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9005, --metrics-config, /usr/local/google/home/statmike/.cache/pypoetry/virtualenvs/frameworks-pytorch-0KVJlKeQ-py3.13/lib/python3.13/site-packages/ts/configs/metrics.yaml]\n",
      "2025-11-07T15:14:55,826 [DEBUG] W-9001-pytorch_autoencoder_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/local/google/home/statmike/.cache/pypoetry/virtualenvs/frameworks-pytorch-0KVJlKeQ-py3.13/bin/python, /usr/local/google/home/statmike/.cache/pypoetry/virtualenvs/frameworks-pytorch-0KVJlKeQ-py3.13/lib/python3.13/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9001, --metrics-config, /usr/local/google/home/statmike/.cache/pypoetry/virtualenvs/frameworks-pytorch-0KVJlKeQ-py3.13/lib/python3.13/site-packages/ts/configs/metrics.yaml]\n",
      "2025-11-07T15:14:55,826 [DEBUG] W-9006-pytorch_autoencoder_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/local/google/home/statmike/.cache/pypoetry/virtualenvs/frameworks-pytorch-0KVJlKeQ-py3.13/bin/python, /usr/local/google/home/statmike/.cache/pypoetry/virtualenvs/frameworks-pytorch-0KVJlKeQ-py3.13/lib/python3.13/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9006, --metrics-config, /usr/local/google/home/statmike/.cache/pypoetry/virtualenvs/frameworks-pytorch-0KVJlKeQ-py3.13/lib/python3.13/site-packages/ts/configs/metrics.yaml]\n",
      "2025-11-07T15:14:55,825 [DEBUG] W-9004-pytorch_autoencoder_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/local/google/home/statmike/.cache/pypoetry/virtualenvs/frameworks-pytorch-0KVJlKeQ-py3.13/bin/python, /usr/local/google/home/statmike/.cache/pypoetry/virtualenvs/frameworks-pytorch-0KVJlKeQ-py3.13/lib/python3.13/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9004, --metrics-config, /usr/local/google/home/statmike/.cache/pypoetry/virtualenvs/frameworks-pytorch-0KVJlKeQ-py3.13/lib/python3.13/site-packages/ts/configs/metrics.yaml]\n",
      "2025-11-07T15:14:55,825 [DEBUG] W-9002-pytorch_autoencoder_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/local/google/home/statmike/.cache/pypoetry/virtualenvs/frameworks-pytorch-0KVJlKeQ-py3.13/bin/python, /usr/local/google/home/statmike/.cache/pypoetry/virtualenvs/frameworks-pytorch-0KVJlKeQ-py3.13/lib/python3.13/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9002, --metrics-config, /usr/local/google/home/statmike/.cache/pypoetry/virtualenvs/frameworks-pytorch-0KVJlKeQ-py3.13/lib/python3.13/site-packages/ts/configs/metrics.yaml]\n",
      "2025-11-07T15:14:55,825 [DEBUG] W-9003-pytorch_autoencoder_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/local/google/home/statmike/.cache/pypoetry/virtualenvs/frameworks-pytorch-0KVJlKeQ-py3.13/bin/python, /usr/local/google/home/statmike/.cache/pypoetry/virtualenvs/frameworks-pytorch-0KVJlKeQ-py3.13/lib/python3.13/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9003, --metrics-config, /usr/local/google/home/statmike/.cache/pypoetry/virtualenvs/frameworks-pytorch-0KVJlKeQ-py3.13/lib/python3.13/site-packages/ts/configs/metrics.yaml]\n",
      "2025-11-07T15:14:55,826 [DEBUG] W-9000-pytorch_autoencoder_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/local/google/home/statmike/.cache/pypoetry/virtualenvs/frameworks-pytorch-0KVJlKeQ-py3.13/bin/python, /usr/local/google/home/statmike/.cache/pypoetry/virtualenvs/frameworks-pytorch-0KVJlKeQ-py3.13/lib/python3.13/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /usr/local/google/home/statmike/.cache/pypoetry/virtualenvs/frameworks-pytorch-0KVJlKeQ-py3.13/lib/python3.13/site-packages/ts/configs/metrics.yaml]\n",
      "2025-11-07T15:14:55,829 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.\n",
      "2025-11-07T15:14:55,829 [DEBUG] W-9007-pytorch_autoencoder_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/local/google/home/statmike/.cache/pypoetry/virtualenvs/frameworks-pytorch-0KVJlKeQ-py3.13/bin/python, /usr/local/google/home/statmike/.cache/pypoetry/virtualenvs/frameworks-pytorch-0KVJlKeQ-py3.13/lib/python3.13/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9007, --metrics-config, /usr/local/google/home/statmike/.cache/pypoetry/virtualenvs/frameworks-pytorch-0KVJlKeQ-py3.13/lib/python3.13/site-packages/ts/configs/metrics.yaml]\n",
      "2025-11-07T15:14:56,046 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://0.0.0.0:8080\n",
      "2025-11-07T15:14:56,047 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.\n",
      "2025-11-07T15:14:56,055 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://0.0.0.0:8081\n",
      "2025-11-07T15:14:56,055 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.\n",
      "2025-11-07T15:14:56,062 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://0.0.0.0:8082\n",
      "2025-11-07T15:14:56,824 [INFO ] epollEventLoopGroup-3-1 ACCESS_LOG - /[0:0:0:0:0:0:0:1%0]:34166 \"GET /models HTTP/1.1\" 200 30\n",
      "2025-11-07T15:14:56,828 [INFO ] epollEventLoopGroup-3-1 TS_METRICS - Requests2XX.Count:1.0|#Level:Host|#hostname:statmike.c.googlers.com,timestamp:1762528496\n",
      "✅ TorchServe started successfully! (took ~2 seconds)\n",
      "   Loaded 1 model(s)\n",
      "     - pytorch_autoencoder\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model server started.\n",
      "2025-11-07T15:14:56,958 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.\n",
      "2025-11-07T15:14:57,255 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:100.0|#Level:Host|#hostname:statmike.c.googlers.com,timestamp:1762528497\n",
      "2025-11-07T15:14:57,262 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:320.6967468261719|#Level:Host|#hostname:statmike.c.googlers.com,timestamp:1762528497\n",
      "2025-11-07T15:14:57,263 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:117.39075469970703|#Level:Host|#hostname:statmike.c.googlers.com,timestamp:1762528497\n",
      "2025-11-07T15:14:57,264 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:26.8|#Level:Host|#hostname:statmike.c.googlers.com,timestamp:1762528497\n",
      "2025-11-07T15:14:57,264 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:18832.859375|#Level:Host|#hostname:statmike.c.googlers.com,timestamp:1762528497\n",
      "2025-11-07T15:14:57,264 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:12773.8828125|#Level:Host|#hostname:statmike.c.googlers.com,timestamp:1762528497\n",
      "2025-11-07T15:14:57,265 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:41.3|#Level:Host|#hostname:statmike.c.googlers.com,timestamp:1762528497\n",
      "2025-11-07T15:15:01,358 [INFO ] W-9005-pytorch_autoencoder_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9005, pid=1734536\n",
      "2025-11-07T15:15:01,360 [INFO ] W-9005-pytorch_autoencoder_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9005\n",
      "2025-11-07T15:15:01,378 [INFO ] W-9005-pytorch_autoencoder_1.0-stdout MODEL_LOG - Successfully loaded /usr/local/google/home/statmike/.cache/pypoetry/virtualenvs/frameworks-pytorch-0KVJlKeQ-py3.13/lib/python3.13/site-packages/ts/configs/metrics.yaml.\n",
      "2025-11-07T15:15:01,379 [INFO ] W-9005-pytorch_autoencoder_1.0-stdout MODEL_LOG - [PID]1734536\n",
      "2025-11-07T15:15:01,379 [INFO ] W-9005-pytorch_autoencoder_1.0-stdout MODEL_LOG - Torch worker started.\n",
      "2025-11-07T15:15:01,380 [INFO ] W-9005-pytorch_autoencoder_1.0-stdout MODEL_LOG - Python runtime: 3.13.3\n",
      "2025-11-07T15:15:01,379 [DEBUG] W-9005-pytorch_autoencoder_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-pytorch_autoencoder_1.0 State change null -> WORKER_STARTED\n",
      "2025-11-07T15:15:01,396 [INFO ] W-9005-pytorch_autoencoder_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9005\n",
      "2025-11-07T15:15:01,424 [INFO ] W-9005-pytorch_autoencoder_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9005.\n",
      "2025-11-07T15:15:01,436 [DEBUG] W-9005-pytorch_autoencoder_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1762528501436\n",
      "2025-11-07T15:15:01,451 [INFO ] W-9005-pytorch_autoencoder_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1762528501451\n",
      "2025-11-07T15:15:01,502 [INFO ] W-9001-pytorch_autoencoder_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9001, pid=1734524\n",
      "2025-11-07T15:15:01,504 [INFO ] W-9001-pytorch_autoencoder_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9001\n",
      "2025-11-07T15:15:01,510 [INFO ] W-9005-pytorch_autoencoder_1.0-stdout MODEL_LOG - model_name: pytorch_autoencoder, batchSize: 1\n",
      "2025-11-07T15:15:01,529 [INFO ] W-9003-pytorch_autoencoder_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9003, pid=1734532\n",
      "2025-11-07T15:15:01,531 [INFO ] W-9003-pytorch_autoencoder_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9003\n",
      "2025-11-07T15:15:01,537 [INFO ] W-9001-pytorch_autoencoder_1.0-stdout MODEL_LOG - Successfully loaded /usr/local/google/home/statmike/.cache/pypoetry/virtualenvs/frameworks-pytorch-0KVJlKeQ-py3.13/lib/python3.13/site-packages/ts/configs/metrics.yaml.\n",
      "2025-11-07T15:15:01,537 [INFO ] W-9001-pytorch_autoencoder_1.0-stdout MODEL_LOG - [PID]1734524\n",
      "2025-11-07T15:15:01,538 [INFO ] W-9001-pytorch_autoencoder_1.0-stdout MODEL_LOG - Torch worker started.\n",
      "2025-11-07T15:15:01,542 [INFO ] W-9001-pytorch_autoencoder_1.0-stdout MODEL_LOG - Python runtime: 3.13.3\n",
      "2025-11-07T15:15:01,544 [DEBUG] W-9001-pytorch_autoencoder_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-pytorch_autoencoder_1.0 State change null -> WORKER_STARTED\n",
      "2025-11-07T15:15:01,544 [INFO ] W-9001-pytorch_autoencoder_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9001\n",
      "2025-11-07T15:15:01,549 [INFO ] W-9001-pytorch_autoencoder_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9001.\n",
      "2025-11-07T15:15:01,552 [DEBUG] W-9001-pytorch_autoencoder_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1762528501552\n",
      "2025-11-07T15:15:01,552 [INFO ] W-9001-pytorch_autoencoder_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1762528501552\n",
      "2025-11-07T15:15:01,557 [INFO ] W-9003-pytorch_autoencoder_1.0-stdout MODEL_LOG - Successfully loaded /usr/local/google/home/statmike/.cache/pypoetry/virtualenvs/frameworks-pytorch-0KVJlKeQ-py3.13/lib/python3.13/site-packages/ts/configs/metrics.yaml.\n",
      "2025-11-07T15:15:01,560 [INFO ] W-9003-pytorch_autoencoder_1.0-stdout MODEL_LOG - [PID]1734532\n",
      "2025-11-07T15:15:01,562 [DEBUG] W-9003-pytorch_autoencoder_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-pytorch_autoencoder_1.0 State change null -> WORKER_STARTED\n",
      "2025-11-07T15:15:01,568 [INFO ] W-9003-pytorch_autoencoder_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9003\n",
      "2025-11-07T15:15:01,572 [INFO ] W-9003-pytorch_autoencoder_1.0-stdout MODEL_LOG - Torch worker started.\n",
      "2025-11-07T15:15:01,572 [INFO ] W-9003-pytorch_autoencoder_1.0-stdout MODEL_LOG - Python runtime: 3.13.3\n",
      "2025-11-07T15:15:01,582 [INFO ] W-9003-pytorch_autoencoder_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9003.\n",
      "2025-11-07T15:15:01,583 [DEBUG] W-9003-pytorch_autoencoder_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1762528501583\n",
      "2025-11-07T15:15:01,586 [INFO ] W-9003-pytorch_autoencoder_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1762528501586\n",
      "2025-11-07T15:15:01,596 [INFO ] W-9001-pytorch_autoencoder_1.0-stdout MODEL_LOG - model_name: pytorch_autoencoder, batchSize: 1\n",
      "2025-11-07T15:15:01,597 [INFO ] W-9004-pytorch_autoencoder_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9004, pid=1734530\n",
      "2025-11-07T15:15:01,599 [INFO ] W-9005-pytorch_autoencoder_1.0-stdout MODEL_LOG - Model loaded successfully on cpu\n",
      "2025-11-07T15:15:01,600 [INFO ] W-9004-pytorch_autoencoder_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9004\n",
      "2025-11-07T15:15:01,606 [INFO ] W-9005-pytorch_autoencoder_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 151\n",
      "2025-11-07T15:15:01,607 [DEBUG] W-9005-pytorch_autoencoder_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-pytorch_autoencoder_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED\n",
      "2025-11-07T15:15:01,608 [INFO ] W-9005-pytorch_autoencoder_1.0 TS_METRICS - WorkerLoadTime.Milliseconds:5784.0|#WorkerName:W-9005-pytorch_autoencoder_1.0,Level:Host|#hostname:statmike.c.googlers.com,timestamp:1762528501\n",
      "2025-11-07T15:15:01,609 [INFO ] W-9005-pytorch_autoencoder_1.0 TS_METRICS - WorkerThreadTime.Milliseconds:21.0|#Level:Host|#hostname:statmike.c.googlers.com,timestamp:1762528501\n",
      "2025-11-07T15:15:01,613 [INFO ] W-9003-pytorch_autoencoder_1.0-stdout MODEL_LOG - model_name: pytorch_autoencoder, batchSize: 1\n",
      "2025-11-07T15:15:01,621 [INFO ] W-9004-pytorch_autoencoder_1.0-stdout MODEL_LOG - Successfully loaded /usr/local/google/home/statmike/.cache/pypoetry/virtualenvs/frameworks-pytorch-0KVJlKeQ-py3.13/lib/python3.13/site-packages/ts/configs/metrics.yaml.\n",
      "2025-11-07T15:15:01,624 [INFO ] W-9004-pytorch_autoencoder_1.0-stdout MODEL_LOG - [PID]1734530\n",
      "2025-11-07T15:15:01,627 [INFO ] W-9004-pytorch_autoencoder_1.0-stdout MODEL_LOG - Torch worker started.\n",
      "2025-11-07T15:15:01,626 [DEBUG] W-9004-pytorch_autoencoder_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-pytorch_autoencoder_1.0 State change null -> WORKER_STARTED\n",
      "2025-11-07T15:15:01,627 [INFO ] W-9004-pytorch_autoencoder_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9004\n",
      "2025-11-07T15:15:01,629 [INFO ] W-9004-pytorch_autoencoder_1.0-stdout MODEL_LOG - Python runtime: 3.13.3\n",
      "2025-11-07T15:15:01,632 [INFO ] W-9004-pytorch_autoencoder_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9004.\n",
      "2025-11-07T15:15:01,634 [DEBUG] W-9004-pytorch_autoencoder_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1762528501634\n",
      "2025-11-07T15:15:01,635 [INFO ] W-9004-pytorch_autoencoder_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1762528501635\n",
      "2025-11-07T15:15:01,671 [INFO ] W-9004-pytorch_autoencoder_1.0-stdout MODEL_LOG - model_name: pytorch_autoencoder, batchSize: 1\n",
      "2025-11-07T15:15:01,680 [INFO ] W-9007-pytorch_autoencoder_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9007, pid=1734535\n",
      "2025-11-07T15:15:01,685 [INFO ] W-9007-pytorch_autoencoder_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9007\n",
      "2025-11-07T15:15:01,695 [INFO ] W-9003-pytorch_autoencoder_1.0-stdout MODEL_LOG - Model loaded successfully on cpu\n",
      "2025-11-07T15:15:01,697 [INFO ] W-9003-pytorch_autoencoder_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 111\n",
      "2025-11-07T15:15:01,698 [DEBUG] W-9003-pytorch_autoencoder_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-pytorch_autoencoder_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED\n",
      "2025-11-07T15:15:01,698 [INFO ] W-9003-pytorch_autoencoder_1.0 TS_METRICS - WorkerLoadTime.Milliseconds:5876.0|#WorkerName:W-9003-pytorch_autoencoder_1.0,Level:Host|#hostname:statmike.c.googlers.com,timestamp:1762528501\n",
      "2025-11-07T15:15:01,699 [INFO ] W-9003-pytorch_autoencoder_1.0 TS_METRICS - WorkerThreadTime.Milliseconds:5.0|#Level:Host|#hostname:statmike.c.googlers.com,timestamp:1762528501\n",
      "2025-11-07T15:15:01,704 [INFO ] W-9001-pytorch_autoencoder_1.0-stdout MODEL_LOG - Model loaded successfully on cpu\n",
      "2025-11-07T15:15:01,705 [INFO ] W-9001-pytorch_autoencoder_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 152\n",
      "2025-11-07T15:15:01,706 [DEBUG] W-9001-pytorch_autoencoder_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-pytorch_autoencoder_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED\n",
      "2025-11-07T15:15:01,707 [INFO ] W-9001-pytorch_autoencoder_1.0 TS_METRICS - WorkerLoadTime.Milliseconds:5886.0|#WorkerName:W-9001-pytorch_autoencoder_1.0,Level:Host|#hostname:statmike.c.googlers.com,timestamp:1762528501\n",
      "2025-11-07T15:15:01,709 [INFO ] W-9001-pytorch_autoencoder_1.0 TS_METRICS - WorkerThreadTime.Milliseconds:5.0|#Level:Host|#hostname:statmike.c.googlers.com,timestamp:1762528501\n",
      "2025-11-07T15:15:01,709 [INFO ] W-9007-pytorch_autoencoder_1.0-stdout MODEL_LOG - Successfully loaded /usr/local/google/home/statmike/.cache/pypoetry/virtualenvs/frameworks-pytorch-0KVJlKeQ-py3.13/lib/python3.13/site-packages/ts/configs/metrics.yaml.\n",
      "2025-11-07T15:15:01,709 [INFO ] W-9007-pytorch_autoencoder_1.0-stdout MODEL_LOG - [PID]1734535\n",
      "2025-11-07T15:15:01,710 [INFO ] W-9007-pytorch_autoencoder_1.0-stdout MODEL_LOG - Torch worker started.\n",
      "2025-11-07T15:15:01,710 [INFO ] W-9007-pytorch_autoencoder_1.0-stdout MODEL_LOG - Python runtime: 3.13.3\n",
      "2025-11-07T15:15:01,711 [DEBUG] W-9007-pytorch_autoencoder_1.0 org.pytorch.serve.wlm.WorkerThread - W-9007-pytorch_autoencoder_1.0 State change null -> WORKER_STARTED\n",
      "2025-11-07T15:15:01,711 [INFO ] W-9007-pytorch_autoencoder_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9007\n",
      "2025-11-07T15:15:01,714 [INFO ] W-9007-pytorch_autoencoder_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9007.\n",
      "2025-11-07T15:15:01,714 [DEBUG] W-9007-pytorch_autoencoder_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1762528501714\n",
      "2025-11-07T15:15:01,716 [INFO ] W-9007-pytorch_autoencoder_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1762528501716\n",
      "2025-11-07T15:15:01,722 [INFO ] W-9006-pytorch_autoencoder_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9006, pid=1734533\n",
      "2025-11-07T15:15:01,723 [INFO ] W-9006-pytorch_autoencoder_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9006\n",
      "2025-11-07T15:15:01,728 [INFO ] W-9004-pytorch_autoencoder_1.0-stdout MODEL_LOG - Model loaded successfully on cpu\n",
      "2025-11-07T15:15:01,730 [INFO ] W-9004-pytorch_autoencoder_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 95\n",
      "2025-11-07T15:15:01,730 [DEBUG] W-9004-pytorch_autoencoder_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-pytorch_autoencoder_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED\n",
      "2025-11-07T15:15:01,731 [INFO ] W-9004-pytorch_autoencoder_1.0 TS_METRICS - WorkerLoadTime.Milliseconds:5908.0|#WorkerName:W-9004-pytorch_autoencoder_1.0,Level:Host|#hostname:statmike.c.googlers.com,timestamp:1762528501\n",
      "2025-11-07T15:15:01,731 [INFO ] W-9004-pytorch_autoencoder_1.0 TS_METRICS - WorkerThreadTime.Milliseconds:2.0|#Level:Host|#hostname:statmike.c.googlers.com,timestamp:1762528501\n",
      "2025-11-07T15:15:01,742 [INFO ] W-9006-pytorch_autoencoder_1.0-stdout MODEL_LOG - Successfully loaded /usr/local/google/home/statmike/.cache/pypoetry/virtualenvs/frameworks-pytorch-0KVJlKeQ-py3.13/lib/python3.13/site-packages/ts/configs/metrics.yaml.\n",
      "2025-11-07T15:15:01,743 [INFO ] W-9006-pytorch_autoencoder_1.0-stdout MODEL_LOG - [PID]1734533\n",
      "2025-11-07T15:15:01,742 [INFO ] W-9007-pytorch_autoencoder_1.0-stdout MODEL_LOG - model_name: pytorch_autoencoder, batchSize: 1\n",
      "2025-11-07T15:15:01,744 [DEBUG] W-9006-pytorch_autoencoder_1.0 org.pytorch.serve.wlm.WorkerThread - W-9006-pytorch_autoencoder_1.0 State change null -> WORKER_STARTED\n",
      "2025-11-07T15:15:01,744 [INFO ] W-9006-pytorch_autoencoder_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9006\n",
      "2025-11-07T15:15:01,745 [INFO ] W-9006-pytorch_autoencoder_1.0-stdout MODEL_LOG - Torch worker started.\n",
      "2025-11-07T15:15:01,745 [INFO ] W-9006-pytorch_autoencoder_1.0-stdout MODEL_LOG - Python runtime: 3.13.3\n",
      "2025-11-07T15:15:01,748 [DEBUG] W-9006-pytorch_autoencoder_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1762528501748\n",
      "2025-11-07T15:15:01,748 [INFO ] W-9006-pytorch_autoencoder_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9006.\n",
      "2025-11-07T15:15:01,749 [INFO ] W-9006-pytorch_autoencoder_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1762528501749\n",
      "2025-11-07T15:15:01,774 [INFO ] W-9006-pytorch_autoencoder_1.0-stdout MODEL_LOG - model_name: pytorch_autoencoder, batchSize: 1\n",
      "2025-11-07T15:15:01,800 [INFO ] W-9007-pytorch_autoencoder_1.0-stdout MODEL_LOG - Model loaded successfully on cpu\n",
      "2025-11-07T15:15:01,801 [INFO ] W-9007-pytorch_autoencoder_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 85\n",
      "2025-11-07T15:15:01,801 [DEBUG] W-9007-pytorch_autoencoder_1.0 org.pytorch.serve.wlm.WorkerThread - W-9007-pytorch_autoencoder_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED\n",
      "2025-11-07T15:15:01,801 [INFO ] W-9007-pytorch_autoencoder_1.0 TS_METRICS - WorkerLoadTime.Milliseconds:5977.0|#WorkerName:W-9007-pytorch_autoencoder_1.0,Level:Host|#hostname:statmike.c.googlers.com,timestamp:1762528501\n",
      "2025-11-07T15:15:01,802 [INFO ] W-9007-pytorch_autoencoder_1.0 TS_METRICS - WorkerThreadTime.Milliseconds:3.0|#Level:Host|#hostname:statmike.c.googlers.com,timestamp:1762528501\n",
      "2025-11-07T15:15:01,823 [INFO ] W-9006-pytorch_autoencoder_1.0-stdout MODEL_LOG - Model loaded successfully on cpu\n",
      "2025-11-07T15:15:01,823 [INFO ] W-9006-pytorch_autoencoder_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 74\n",
      "2025-11-07T15:15:01,824 [DEBUG] W-9006-pytorch_autoencoder_1.0 org.pytorch.serve.wlm.WorkerThread - W-9006-pytorch_autoencoder_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED\n",
      "2025-11-07T15:15:01,824 [INFO ] W-9006-pytorch_autoencoder_1.0 TS_METRICS - WorkerLoadTime.Milliseconds:6001.0|#WorkerName:W-9006-pytorch_autoencoder_1.0,Level:Host|#hostname:statmike.c.googlers.com,timestamp:1762528501\n",
      "2025-11-07T15:15:01,824 [INFO ] W-9006-pytorch_autoencoder_1.0 TS_METRICS - WorkerThreadTime.Milliseconds:2.0|#Level:Host|#hostname:statmike.c.googlers.com,timestamp:1762528501\n",
      "2025-11-07T15:15:01,914 [INFO ] W-9002-pytorch_autoencoder_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9002, pid=1734531\n",
      "2025-11-07T15:15:01,915 [INFO ] W-9002-pytorch_autoencoder_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9002\n",
      "2025-11-07T15:15:01,927 [INFO ] W-9002-pytorch_autoencoder_1.0-stdout MODEL_LOG - Successfully loaded /usr/local/google/home/statmike/.cache/pypoetry/virtualenvs/frameworks-pytorch-0KVJlKeQ-py3.13/lib/python3.13/site-packages/ts/configs/metrics.yaml.\n",
      "2025-11-07T15:15:01,927 [INFO ] W-9002-pytorch_autoencoder_1.0-stdout MODEL_LOG - [PID]1734531\n",
      "2025-11-07T15:15:01,928 [INFO ] W-9002-pytorch_autoencoder_1.0-stdout MODEL_LOG - Torch worker started.\n",
      "2025-11-07T15:15:01,928 [INFO ] W-9002-pytorch_autoencoder_1.0-stdout MODEL_LOG - Python runtime: 3.13.3\n",
      "2025-11-07T15:15:01,928 [DEBUG] W-9002-pytorch_autoencoder_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-pytorch_autoencoder_1.0 State change null -> WORKER_STARTED\n",
      "2025-11-07T15:15:01,929 [INFO ] W-9002-pytorch_autoencoder_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9002\n",
      "2025-11-07T15:15:01,949 [INFO ] W-9002-pytorch_autoencoder_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9002.\n",
      "2025-11-07T15:15:01,949 [DEBUG] W-9002-pytorch_autoencoder_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1762528501949\n",
      "2025-11-07T15:15:01,949 [INFO ] W-9002-pytorch_autoencoder_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1762528501949\n",
      "2025-11-07T15:15:01,970 [INFO ] W-9002-pytorch_autoencoder_1.0-stdout MODEL_LOG - model_name: pytorch_autoencoder, batchSize: 1\n",
      "2025-11-07T15:15:01,978 [INFO ] W-9000-pytorch_autoencoder_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=1734534\n",
      "2025-11-07T15:15:01,980 [INFO ] W-9000-pytorch_autoencoder_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000\n",
      "2025-11-07T15:15:01,992 [INFO ] W-9000-pytorch_autoencoder_1.0-stdout MODEL_LOG - Successfully loaded /usr/local/google/home/statmike/.cache/pypoetry/virtualenvs/frameworks-pytorch-0KVJlKeQ-py3.13/lib/python3.13/site-packages/ts/configs/metrics.yaml.\n",
      "2025-11-07T15:15:01,992 [INFO ] W-9000-pytorch_autoencoder_1.0-stdout MODEL_LOG - [PID]1734534\n",
      "2025-11-07T15:15:01,993 [DEBUG] W-9000-pytorch_autoencoder_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-pytorch_autoencoder_1.0 State change null -> WORKER_STARTED\n",
      "2025-11-07T15:15:01,993 [INFO ] W-9000-pytorch_autoencoder_1.0-stdout MODEL_LOG - Torch worker started.\n",
      "2025-11-07T15:15:01,993 [INFO ] W-9000-pytorch_autoencoder_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000\n",
      "2025-11-07T15:15:01,993 [INFO ] W-9000-pytorch_autoencoder_1.0-stdout MODEL_LOG - Python runtime: 3.13.3\n",
      "2025-11-07T15:15:01,995 [DEBUG] W-9000-pytorch_autoencoder_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1762528501995\n",
      "2025-11-07T15:15:01,995 [INFO ] W-9000-pytorch_autoencoder_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.\n",
      "2025-11-07T15:15:01,995 [INFO ] W-9000-pytorch_autoencoder_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1762528501995\n",
      "2025-11-07T15:15:02,016 [INFO ] W-9002-pytorch_autoencoder_1.0-stdout MODEL_LOG - Model loaded successfully on cpu\n",
      "2025-11-07T15:15:02,016 [INFO ] W-9002-pytorch_autoencoder_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 67\n",
      "2025-11-07T15:15:02,017 [INFO ] W-9000-pytorch_autoencoder_1.0-stdout MODEL_LOG - model_name: pytorch_autoencoder, batchSize: 1\n",
      "2025-11-07T15:15:02,017 [DEBUG] W-9002-pytorch_autoencoder_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-pytorch_autoencoder_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED\n",
      "2025-11-07T15:15:02,017 [INFO ] W-9002-pytorch_autoencoder_1.0 TS_METRICS - WorkerLoadTime.Milliseconds:6195.0|#WorkerName:W-9002-pytorch_autoencoder_1.0,Level:Host|#hostname:statmike.c.googlers.com,timestamp:1762528502\n",
      "2025-11-07T15:15:02,018 [INFO ] W-9002-pytorch_autoencoder_1.0 TS_METRICS - WorkerThreadTime.Milliseconds:1.0|#Level:Host|#hostname:statmike.c.googlers.com,timestamp:1762528502\n",
      "2025-11-07T15:15:02,061 [INFO ] W-9000-pytorch_autoencoder_1.0-stdout MODEL_LOG - Model loaded successfully on cpu\n",
      "2025-11-07T15:15:02,062 [INFO ] W-9000-pytorch_autoencoder_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 67\n",
      "2025-11-07T15:15:02,063 [DEBUG] W-9000-pytorch_autoencoder_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-pytorch_autoencoder_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED\n",
      "2025-11-07T15:15:02,063 [INFO ] W-9000-pytorch_autoencoder_1.0 TS_METRICS - WorkerLoadTime.Milliseconds:6244.0|#WorkerName:W-9000-pytorch_autoencoder_1.0,Level:Host|#hostname:statmike.c.googlers.com,timestamp:1762528502\n",
      "2025-11-07T15:15:02,064 [INFO ] W-9000-pytorch_autoencoder_1.0 TS_METRICS - WorkerThreadTime.Milliseconds:1.0|#Level:Host|#hostname:statmike.c.googlers.com,timestamp:1762528502\n"
     ]
    }
   ],
   "source": [
    "# Start TorchServe with config file and organized directories\n",
    "print(f\"Starting TorchServe...\")\n",
    "print(f\"Model store: {MODEL_STORE_DIR}\")\n",
    "print(f\"Config file: ./files/torchserve/config.properties\")\n",
    "print(f\"Logs directory: ./files/torchserve/logs\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Change to torchserve directory so logs go to the right place\n",
    "original_dir = os.getcwd()\n",
    "os.chdir(\"./files/torchserve\")\n",
    "\n",
    "# Build the command - use config file and load model via command line\n",
    "# Redirect stderr to suppress startup warnings\n",
    "start_cmd = f\"torchserve --start --ts-config config.properties --models {MODEL_NAME}={MAR_FILE} --ncs 2>/dev/null\"\n",
    "\n",
    "print(f\"Command: torchserve --start --ts-config config.properties --models {MODEL_NAME}={MAR_FILE} --ncs\")\n",
    "print(f\"Working directory: {os.getcwd()}\")\n",
    "\n",
    "# Execute using shell\n",
    "result = os.system(start_cmd)\n",
    "\n",
    "# Change back to original directory\n",
    "os.chdir(original_dir)\n",
    "\n",
    "if result == 0:\n",
    "    print(f\"✅ Start command completed\")\n",
    "else:\n",
    "    print(f\"⚠️  Start command returned: {result}\")\n",
    "\n",
    "# Wait for server to actually be ready\n",
    "print(\"\\nWaiting for TorchServe to be ready...\")\n",
    "max_retries = 30\n",
    "retry_count = 0\n",
    "server_ready = False\n",
    "\n",
    "while retry_count < max_retries and not server_ready:\n",
    "    time.sleep(2)\n",
    "    retry_count += 1\n",
    "    try:\n",
    "        response = requests.get(f\"{MANAGEMENT_URL}/models\", timeout=2)\n",
    "        if response.status_code == 200:\n",
    "            server_ready = True\n",
    "            print(f\"✅ TorchServe started successfully! (took ~{retry_count * 2} seconds)\")\n",
    "            models = response.json()\n",
    "            if models.get(\"models\"):\n",
    "                print(f\"   Loaded {len(models['models'])} model(s)\")\n",
    "                for model in models[\"models\"]:\n",
    "                    print(f\"     - {model['modelName']}\")\n",
    "        else:\n",
    "            if retry_count % 5 == 0:\n",
    "                print(f\"   Attempt {retry_count}/{max_retries}: Waiting...\")\n",
    "    except requests.exceptions.RequestException:\n",
    "        if retry_count % 5 == 0:\n",
    "            print(f\"   Attempt {retry_count}/{max_retries}: Waiting...\")\n",
    "\n",
    "if not server_ready:\n",
    "    print(f\"\\n⚠️ Server may still be starting up. Check manually:\")\n",
    "    print(f\"   Logs: cat ./files/torchserve/logs/ts_log.log | tail -20\")\n",
    "    print(f\"   API: curl {MANAGEMENT_URL}/models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "test",
   "metadata": {},
   "source": [
    "### Test Predictions\n",
    "\n",
    "Make test predictions using the inference API. We'll send sample transaction data with 30 features.\n",
    "\n",
    "> **Note:** TorchServe worker logs (lines starting with timestamps and `[INFO]`) will appear in the output. These show the internal processing pipeline and are normal operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "test_c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample transaction data:\n",
      "  Shape: (30,)\n",
      "  Data type: float32\n",
      "  First 5 values: [-0.41754758 -1.8950008  -0.1050218   1.1962183   0.24943312]\n",
      "\n",
      "Sending prediction request to http://localhost:8080/predictions/pytorch_autoencoder...\n",
      "2025-11-07T15:15:16,680 [INFO ] epollEventLoopGroup-3-2 TS_METRICS - ts_inference_requests_total.Count:1.0|#model_name:pytorch_autoencoder,model_version:default|#hostname:statmike.c.googlers.com,timestamp:1762528516\n",
      "2025-11-07T15:15:16,696 [DEBUG] W-9005-pytorch_autoencoder_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd PREDICT repeats 1 to backend at: 1762528516681\n",
      "2025-11-07T15:15:16,697 [INFO ] W-9005-pytorch_autoencoder_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1762528516697\n",
      "2025-11-07T15:15:16,699 [INFO ] W-9005-pytorch_autoencoder_1.0-stdout MODEL_LOG - Backend received inference at: 1762528516\n",
      "2025-11-07T15:15:16,699 [INFO ] W-9005-pytorch_autoencoder_1.0-stdout MODEL_LOG - Received 1 request(s)\n",
      "2025-11-07T15:15:16,699 [INFO ] W-9005-pytorch_autoencoder_1.0-stdout MODEL_LOG - Preprocessing 1 instances\n",
      "2025-11-07T15:15:16,700 [INFO ] W-9005-pytorch_autoencoder_1.0-stdout MODEL_LOG - Input tensor shape: torch.Size([1, 30])\n",
      "2025-11-07T15:15:16,778 [INFO ] W-9005-pytorch_autoencoder_1.0-stdout MODEL_LOG - Postprocessing 1 predictions\n",
      "2025-11-07T15:15:16,778 [INFO ] W-9005-pytorch_autoencoder_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - result=[METRICS]PredictionTime.Milliseconds:79.11|#ModelName:pytorch_autoencoder,Level:Model|#type:GAUGE|#hostname:statmike.c.googlers.com,1762528516,89cf30a1-93af-4f39-8b10-39c1b6f015ce, pattern=[METRICS]\n",
      "2025-11-07T15:15:16,779 [INFO ] W-9005-pytorch_autoencoder_1.0-stdout MODEL_METRICS - PredictionTime.ms:79.11|#ModelName:pytorch_autoencoder,Level:Model|#hostname:statmike.c.googlers.com,requestID:89cf30a1-93af-4f39-8b10-39c1b6f015ce,timestamp:1762528516\n",
      "2025-11-07T15:15:16,780 [INFO ] W-9005-pytorch_autoencoder_1.0 org.pytorch.serve.wlm.BatchAggregator - Sending response for jobId 89cf30a1-93af-4f39-8b10-39c1b6f015ce\n",
      "2025-11-07T15:15:16,781 [INFO ] W-9005-pytorch_autoencoder_1.0 ACCESS_LOG - /[0:0:0:0:0:0:0:1%0]:37592 \"POST /predictions/pytorch_autoencoder HTTP/1.1\" 200 104\n",
      "2025-11-07T15:15:16,781 [INFO ] W-9005-pytorch_autoencoder_1.0 TS_METRICS - Requests2XX.Count:1.0|#Level:Host|#hostname:statmike.c.googlers.com,timestamp:1762528516\n",
      "2025-11-07T15:15:16,782 [INFO ] W-9005-pytorch_autoencoder_1.0 TS_METRICS - ts_inference_latency_microseconds.Microseconds:100262.079|#model_name:pytorch_autoencoder,model_version:default|#hostname:statmike.c.googlers.com,timestamp:1762528516\n",
      "2025-11-07T15:15:16,782 [INFO ] W-9005-pytorch_autoencoder_1.0 TS_METRICS - ts_queue_latency_microseconds.Microseconds:368.912|#model_name:pytorch_autoencoder,model_version:default|#hostname:statmike.c.googlers.com,timestamp:1762528516\n",
      "2025-11-07T15:15:16,782 [DEBUG] W-9005-pytorch_autoencoder_1.0 org.pytorch.serve.job.RestJob - Waiting time ns: 368912, Backend time ns: 101616203\n",
      "2025-11-07T15:15:16,783 [INFO ] W-9005-pytorch_autoencoder_1.0 TS_METRICS - QueueTime.Milliseconds:0.0|#Level:Host|#hostname:statmike.c.googlers.com,timestamp:1762528516\n",
      "2025-11-07T15:15:16,783 [INFO ] W-9005-pytorch_autoencoder_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 83\n",
      "2025-11-07T15:15:16,783 [INFO ] W-9005-pytorch_autoencoder_1.0 TS_METRICS - WorkerThreadTime.Milliseconds:19.0|#Level:Host|#hostname:statmike.c.googlers.com,timestamp:1762528516\n",
      "Response status: 200\n",
      "✅ Prediction successful!\n",
      "\n",
      "Key Results:\n",
      "  Anomaly Score (MAE): 2791.81\n",
      "  RMSE: 15284.22\n",
      "  Encoded representation (4D): [0.0, 0.0, 0.34752362966537476, 0.0]\n",
      "\n",
      "Other available fields: normalized_reconstruction, normalized_reconstruction_errors, normalized_MAE, normalized_RMSE, normalized_MSE, normalized_MSLE, denormalized_reconstruction, denormalized_reconstruction_errors, denormalized_MSE, denormalized_MSLE\n"
     ]
    }
   ],
   "source": [
    "# Create sample input data (30 features)\n",
    "sample_transaction = np.random.randn(30).astype(np.float32)\n",
    "\n",
    "print(\"Sample transaction data:\")\n",
    "print(f\"  Shape: {sample_transaction.shape}\")\n",
    "print(f\"  Data type: {sample_transaction.dtype}\")\n",
    "print(f\"  First 5 values: {sample_transaction[:5]}\")\n",
    "\n",
    "# Make prediction request\n",
    "print(f\"\\nSending prediction request to {INFERENCE_URL}...\")\n",
    "\n",
    "# TorchServe expects data as a JSON array, not wrapped in \"instances\"\n",
    "data = json.dumps(sample_transaction.tolist())\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "try:\n",
    "    response = requests.post(INFERENCE_URL, data=data, headers=headers)\n",
    "    \n",
    "    print(f\"Response status: {response.status_code}\")\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        prediction = response.json()\n",
    "        print(\"✅ Prediction successful!\")\n",
    "        \n",
    "        # Parse and display key metrics\n",
    "        if isinstance(prediction, dict):\n",
    "            print(f\"\\nKey Results:\")\n",
    "            if \"denormalized_MAE\" in prediction:\n",
    "                print(f\"  Anomaly Score (MAE): {prediction['denormalized_MAE']:.2f}\")\n",
    "            if \"denormalized_RMSE\" in prediction:\n",
    "                print(f\"  RMSE: {prediction['denormalized_RMSE']:.2f}\")\n",
    "            if \"encoded\" in prediction:\n",
    "                print(f\"  Encoded representation (4D): {prediction['encoded']}\")\n",
    "            \n",
    "            # Show what other fields are available\n",
    "            other_fields = [k for k in prediction.keys() if k not in ['denormalized_MAE', 'denormalized_RMSE', 'encoded']]\n",
    "            if other_fields:\n",
    "                print(f\"\\nOther available fields: {', '.join(other_fields)}\")\n",
    "        else:\n",
    "            print(f\"Response: {json.dumps(prediction, indent=2)[:300]}...\")\n",
    "    else:\n",
    "        print(f\"❌ Error: {response.text}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error making prediction: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mgmt",
   "metadata": {},
   "source": [
    "### Management API\n",
    "\n",
    "Explore the management API to inspect loaded models and their details.\n",
    "\n",
    "> **Note:** TorchServe access logs may appear in the output showing API requests being processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "mgmt_c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Querying Management API...\n",
      "============================================================\n",
      "2025-11-07T15:15:20,593 [INFO ] epollEventLoopGroup-3-3 ACCESS_LOG - /[0:0:0:0:0:0:0:1%0]:56840 \"GET /models HTTP/1.1\" 200 1\n",
      "2025-11-07T15:15:20,594 [INFO ] epollEventLoopGroup-3-3 TS_METRICS - Requests2XX.Count:1.0|#Level:Host|#hostname:statmike.c.googlers.com,timestamp:1762528520\n",
      "\n",
      "1. List Models: GET /models\n",
      "   Status: 200\n",
      "   Models: {\n",
      "  \"models\": [\n",
      "    {\n",
      "      \"modelName\": \"pytorch_autoencoder\",\n",
      "      \"modelUrl\": \"pytorch_autoencoder.mar\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n",
      "2. Model Details: GET /models/pytorch_autoencoder\n",
      "2025-11-07T15:15:20,626 [INFO ] epollEventLoopGroup-3-4 ACCESS_LOG - /[0:0:0:0:0:0:0:1%0]:56844 \"GET /models/pytorch_autoencoder HTTP/1.1\" 200 11\n",
      "2025-11-07T15:15:20,627 [INFO ] epollEventLoopGroup-3-4 TS_METRICS - Requests2XX.Count:1.0|#Level:Host|#hostname:statmike.c.googlers.com,timestamp:1762528520\n",
      "   Status: 200\n",
      "\n",
      "   Model Configuration:\n",
      "     Name: pytorch_autoencoder\n",
      "     Version: 1.0\n",
      "     Runtime: python\n",
      "     Workers: 8 (min) - 8 (max)\n",
      "     Batch size: 1\n",
      "     Device: cpu\n",
      "     Loaded at startup: True\n",
      "\n",
      "   Active Workers: 8\n",
      "     - ID 9000: READY (PID: 1734534)\n",
      "     - ID 9001: READY (PID: 1734524)\n",
      "     - ID 9002: READY (PID: 1734531)\n",
      "     - ... and 5 more workers\n",
      "\n",
      "   Job Queue:\n",
      "     Remaining capacity: 100\n",
      "     Pending requests: 0\n"
     ]
    }
   ],
   "source": [
    "# List all models\n",
    "print(\"Querying Management API...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Get all models\n",
    "response = requests.get(f\"{MANAGEMENT_URL}/models\")\n",
    "print(f\"\\n1. List Models: GET /models\")\n",
    "print(f\"   Status: {response.status_code}\")\n",
    "if response.status_code == 200:\n",
    "    models = response.json()\n",
    "    print(f\"   Models: {json.dumps(models, indent=2)}\")\n",
    "\n",
    "# Get detailed model information\n",
    "print(f\"\\n2. Model Details: GET /models/{MODEL_NAME}\")\n",
    "response = requests.get(f\"{MANAGEMENT_URL}/models/{MODEL_NAME}\")\n",
    "print(f\"   Status: {response.status_code}\")\n",
    "if response.status_code == 200:\n",
    "    model_info = response.json()[0]  # Returns a list with one element\n",
    "    print(f\"\\n   Model Configuration:\")\n",
    "    print(f\"     Name: {model_info['modelName']}\")\n",
    "    print(f\"     Version: {model_info['modelVersion']}\")\n",
    "    print(f\"     Runtime: {model_info['runtime']}\")\n",
    "    print(f\"     Workers: {model_info['minWorkers']} (min) - {model_info['maxWorkers']} (max)\")\n",
    "    print(f\"     Batch size: {model_info['batchSize']}\")\n",
    "    print(f\"     Device: {model_info['deviceType']}\")\n",
    "    print(f\"     Loaded at startup: {model_info['loadedAtStartup']}\")\n",
    "    \n",
    "    print(f\"\\n   Active Workers: {len(model_info['workers'])}\")\n",
    "    for worker in model_info['workers'][:3]:  # Show first 3 workers\n",
    "        print(f\"     - ID {worker['id']}: {worker['status']} (PID: {worker['pid']})\")\n",
    "    if len(model_info['workers']) > 3:\n",
    "        print(f\"     - ... and {len(model_info['workers']) - 3} more workers\")\n",
    "    \n",
    "    print(f\"\\n   Job Queue:\")\n",
    "    print(f\"     Remaining capacity: {model_info['jobQueueStatus']['remainingCapacity']}\")\n",
    "    print(f\"     Pending requests: {model_info['jobQueueStatus']['pendingRequests']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metrics",
   "metadata": {},
   "source": [
    "### Metrics\n",
    "\n",
    "Access the metrics endpoint to view performance and usage statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "metrics_c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching metrics from Prometheus endpoint...\n",
      "============================================================\n",
      "2025-11-07T15:15:45,798 [INFO ] epollEventLoopGroup-4-1 ACCESS_LOG - /[0:0:0:0:0:0:0:1%0]:48248 \"GET /metrics HTTP/1.1\" 200 0\n",
      "2025-11-07T15:15:45,799 [INFO ] epollEventLoopGroup-4-1 TS_METRICS - Requests2XX.Count:1.0|#Level:Host|#hostname:statmike.c.googlers.com,timestamp:1762528545\n",
      "Status: 200\n",
      "\n",
      "⚠️  No metrics available yet.\n",
      "   Metrics are generated after API requests are made.\n",
      "   Make a prediction first, then check metrics again.\n",
      "\n",
      "Total metrics available: 1\n",
      "View all metrics: curl http://localhost:8082/metrics\n"
     ]
    }
   ],
   "source": [
    "# Get metrics\n",
    "print(\"Fetching metrics from Prometheus endpoint...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "response = requests.get(METRICS_URL)\n",
    "print(f\"Status: {response.status_code}\")\n",
    "\n",
    "if response.status_code == 200:\n",
    "    metrics = response.text\n",
    "    metrics_lines = metrics.strip().split('\\n')\n",
    "    \n",
    "    if len(metrics_lines) > 1:\n",
    "        # Display model-specific and TorchServe metrics\n",
    "        model_metrics = [line for line in metrics_lines if MODEL_NAME in line or 'ts_inference' in line or 'ts_queue' in line]\n",
    "        host_metrics = [line for line in metrics_lines if 'CPUUtilization' in line or 'MemoryUtilization' in line]\n",
    "        \n",
    "        if model_metrics:\n",
    "            print(f\"\\nModel Metrics ({len(model_metrics)} metrics):\")\n",
    "            for line in model_metrics[:10]:  # Show first 10\n",
    "                print(f\"  {line}\")\n",
    "            if len(model_metrics) > 10:\n",
    "                print(f\"  ... and {len(model_metrics) - 10} more\")\n",
    "        \n",
    "        if host_metrics:\n",
    "            print(f\"\\nHost Metrics (sample):\")\n",
    "            for line in host_metrics[:5]:\n",
    "                print(f\"  {line}\")\n",
    "    else:\n",
    "        print(\"\\n⚠️  No metrics available yet.\")\n",
    "        print(\"   Metrics are generated after API requests are made.\")\n",
    "        print(\"   Make a prediction first, then check metrics again.\")\n",
    "    \n",
    "    print(f\"\\nTotal metrics available: {len(metrics_lines)}\")\n",
    "    print(f\"View all metrics: curl {METRICS_URL}\")\n",
    "else:\n",
    "    print(f\"❌ Error fetching metrics: {response.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stop",
   "metadata": {},
   "source": [
    "### Stop TorchServe\n",
    "\n",
    "Gracefully shutdown the TorchServe instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "stop_c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping TorchServe...\n",
      "TorchServe has stopped.\n",
      "\n",
      "TorchServe stopped.\n",
      "Confirmed: Server is not responding (stopped successfully)\n"
     ]
    }
   ],
   "source": [
    "# Stop TorchServe\n",
    "print(\"Stopping TorchServe...\")\n",
    "result = subprocess.run([\"torchserve\", \"--stop\"], capture_output=True, text=True)\n",
    "print(result.stdout)\n",
    "if result.stderr:\n",
    "    print(\"Stderr:\", result.stderr)\n",
    "\n",
    "# Wait for shutdown\n",
    "time.sleep(5)\n",
    "print(\"TorchServe stopped.\")\n",
    "\n",
    "# Verify server is stopped\n",
    "try:\n",
    "    response = requests.get(f\"{MANAGEMENT_URL}/models\", timeout=2)\n",
    "    print(\"Warning: Server may still be running\")\n",
    "except requests.exceptions.RequestException:\n",
    "    print(\"Confirmed: Server is not responding (stopped successfully)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": "---\n## Summary and Next Steps\n\n### What You Accomplished\n\nIn this notebook, you successfully:\n\n1. **Located** the model archive (.mar) created in the training notebook\n2. **Copied** the model archive to a local model store\n3. **Installed** TorchServe and its dependencies locally\n4. **Started** a local TorchServe instance with your model\n5. **Made predictions** via the REST inference API\n6. **Explored** the management API to inspect model details\n7. **Accessed** metrics for monitoring model performance\n8. **Stopped** TorchServe gracefully\n\n### Key Takeaways\n\n- **TorchServe provides three APIs**: inference (8080), management (8081), and metrics (8082)\n- **Model archives (.mar)** package the model, handler, and dependencies together\n- **Local testing** enables rapid iteration before cloud deployment\n- **REST APIs** make it easy to integrate with any client application\n- **Metrics** help you understand model performance and usage patterns\n\n### Next Steps\n\nNow that you've tested TorchServe locally, you can:\n\n1. **Deploy to Cloud Run**: Containerize TorchServe for serverless deployment\n   - See `torchserve-cloud-run.ipynb` for serverless Cloud Run deployment\n   - **Tip**: Try this first to understand containerization and troubleshooting before moving to VMs\n   \n2. **Deploy to Vertex AI**: Use managed endpoints for production serving\n   - See `vertex-ai-endpoint-prebuilt-container.ipynb` for TorchServe on Vertex AI\n   - See `vertex-ai-endpoint-custom-container.ipynb` for custom FastAPI deployment\n   \n3. **Customize the handler**: Modify preprocessing/postprocessing logic\n   - Edit the handler in the training notebook\n   - Rebuild the .mar file and test locally again\n   \n4. **Performance testing**: Load test your model locally\n   - Use tools like `apache-bench` or `locust` for load testing\n   - Adjust worker counts and batch sizes in TorchServe config\n   \n5. **Scale to production**: Deploy to production environments\n   - See `torchserve-gce.md` for Compute Engine deployment guide\n   - See `torchserve-gke.md` for Kubernetes deployment guide\n\n### Additional Resources\n\n- [TorchServe Documentation](https://pytorch.org/serve/)\n- [TorchServe REST API](https://pytorch.org/serve/rest_api.html)\n- [Custom Handlers Guide](https://pytorch.org/serve/custom_service.html)\n- [Vertex AI Custom Container Deployment](https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (frameworks-pytorch)",
   "language": "python",
   "name": "frameworks-pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}