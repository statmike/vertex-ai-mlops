{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# TorchServe Local Development and Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overview",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This notebook demonstrates how to run **TorchServe locally** for development and testing of PyTorch model deployments. This is an essential workflow for validating model serving behavior before deploying to production environments.\n",
    "\n",
    "### What This Notebook Does\n",
    "\n",
    "- Downloads a pre-built Model Archive (.mar) file from Google Cloud Storage\n",
    "- Installs TorchServe and its dependencies locally\n",
    "- Starts a local TorchServe instance with the model\n",
    "- Tests predictions via the REST API\n",
    "- Demonstrates management and metrics endpoints\n",
    "- Shows proper shutdown procedures\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "Before running this notebook, you should have:\n",
    "- Completed the `../pytorch-autoencoder.ipynb` notebook to create and export the model archive\n",
    "- A `.mar` file stored in Google Cloud Storage\n",
    "- Python 3.8+ environment\n",
    "- Sufficient local compute resources (CPU/GPU)\n",
    "\n",
    "### What is TorchServe?\n",
    "\n",
    "[TorchServe](https://pytorch.org/serve/) is PyTorch's official model serving framework that provides:\n",
    "- Production-ready REST and gRPC APIs for model inference\n",
    "- Multi-model serving with dynamic model loading\n",
    "- Built-in metrics and logging\n",
    "- Custom preprocessing and postprocessing via handlers\n",
    "- Support for batching and versioning\n",
    "\n",
    "### Benefits of Local Testing\n",
    "\n",
    "Running TorchServe locally allows you to:\n",
    "- **Validate** model behavior before cloud deployment\n",
    "- **Debug** custom handlers and preprocessing logic\n",
    "- **Test** prediction endpoints with sample data\n",
    "- **Iterate** quickly without cloud deployment overhead\n",
    "- **Understand** TorchServe configuration and APIs\n",
    "\n",
    "### Architecture\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────┐\n",
    "│         Local Development               │\n",
    "│                                         │\n",
    "│  ┌───────────────────────────────────┐ │\n",
    "│  │      TorchServe Instance          │ │\n",
    "│  │                                   │ │\n",
    "│  │  ┌─────────────────────────────┐ │ │\n",
    "│  │  │   Inference API :8080       │ │ │\n",
    "│  │  │   - POST /predictions       │ │ │\n",
    "│  │  └─────────────────────────────┘ │ │\n",
    "│  │                                   │ │\n",
    "│  │  ┌─────────────────────────────┐ │ │\n",
    "│  │  │   Management API :8081      │ │ │\n",
    "│  │  │   - GET /models             │ │ │\n",
    "│  │  └─────────────────────────────┘ │ │\n",
    "│  │                                   │ │\n",
    "│  │  ┌─────────────────────────────┐ │ │\n",
    "│  │  │   Metrics API :8082         │ │ │\n",
    "│  │  │   - GET /metrics            │ │ │\n",
    "│  │  └─────────────────────────────┘ │ │\n",
    "│  │                                   │ │\n",
    "│  │  ┌─────────────────────────────┐ │ │\n",
    "│  │  │   Model Store               │ │ │\n",
    "│  │  │   - model.mar               │ │ │\n",
    "│  │  └─────────────────────────────┘ │ │\n",
    "│  └───────────────────────────────────┘ │\n",
    "│                                         │\n",
    "│  ┌───────────────────────────────────┐ │\n",
    "│  │   Jupyter Notebook Client         │ │\n",
    "│  │   - HTTP requests to APIs         │ │\n",
    "│  └───────────────────────────────────┘ │\n",
    "└─────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "### What You Will Learn\n",
    "\n",
    "1. How to install and configure TorchServe locally\n",
    "2. How to load model archives into TorchServe\n",
    "3. How to make predictions via the REST API\n",
    "4. How to use management APIs to inspect models\n",
    "5. How to access metrics and monitoring data\n",
    "6. Best practices for local model serving development"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "env",
   "metadata": {},
   "source": [
    "---\n",
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proj",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = !gcloud config get-value project\n",
    "PROJECT_ID = PROJECT_ID[0]\n",
    "print(f\"Project ID: {PROJECT_ID}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conf",
   "metadata": {},
   "outputs": [],
   "source": [
    "SERIES = \"frameworks\"\n",
    "EXPERIMENT = \"pytorch-autoencoder\"\n",
    "REGION = \"us-central1\"\n",
    "\n",
    "# Model archive settings\n",
    "MODEL_NAME = \"pytorch_autoencoder\"\n",
    "MAR_FILE = \"model.mar\"\n",
    "GCS_PATH = f\"gs://{PROJECT_ID}/{SERIES}/{EXPERIMENT}/{MAR_FILE}\"\n",
    "\n",
    "# TorchServe settings\n",
    "INFERENCE_PORT = 8080\n",
    "MANAGEMENT_PORT = 8081\n",
    "METRICS_PORT = 8082\n",
    "\n",
    "print(f\"Series: {SERIES}\")\n",
    "print(f\"Experiment: {EXPERIMENT}\")\n",
    "print(f\"Model Archive: {GCS_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create working directory for model store\n",
    "import os\n",
    "\n",
    "MODEL_STORE_DIR = \"./model_store\"\n",
    "os.makedirs(MODEL_STORE_DIR, exist_ok=True)\n",
    "print(f\"Model store directory: {MODEL_STORE_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "py",
   "metadata": {},
   "source": [
    "---\n",
    "## Python Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imp",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import subprocess\n",
    "import time\n",
    "import requests\n",
    "import numpy as np\n",
    "from google.cloud import storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vars",
   "metadata": {},
   "outputs": [],
   "source": [
    "# API endpoints\n",
    "INFERENCE_URL = f\"http://localhost:{INFERENCE_PORT}/predictions/{MODEL_NAME}\"\n",
    "MANAGEMENT_URL = f\"http://localhost:{MANAGEMENT_PORT}\"\n",
    "METRICS_URL = f\"http://localhost:{METRICS_PORT}/metrics\"\n",
    "\n",
    "print(f\"Inference URL: {INFERENCE_URL}\")\n",
    "print(f\"Management URL: {MANAGEMENT_URL}\")\n",
    "print(f\"Metrics URL: {METRICS_URL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "download",
   "metadata": {},
   "source": [
    "---\n",
    "## Download Model Archive\n",
    "\n",
    "Download the pre-built `.mar` file from Google Cloud Storage to the local model store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "download_c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download model archive from GCS\n",
    "storage_client = storage.Client(project=PROJECT_ID)\n",
    "\n",
    "# Parse GCS path\n",
    "gcs_path_parts = GCS_PATH.replace(\"gs://\", \"\").split(\"/\")\n",
    "bucket_name = gcs_path_parts[0]\n",
    "blob_path = \"/\".join(gcs_path_parts[1:])\n",
    "\n",
    "# Download file\n",
    "bucket = storage_client.bucket(bucket_name)\n",
    "blob = bucket.blob(blob_path)\n",
    "local_mar_path = os.path.join(MODEL_STORE_DIR, MAR_FILE)\n",
    "\n",
    "print(f\"Downloading {GCS_PATH} to {local_mar_path}...\")\n",
    "blob.download_to_filename(local_mar_path)\n",
    "print(f\"Downloaded successfully!\")\n",
    "print(f\"File size: {os.path.getsize(local_mar_path) / 1024:.2f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "install",
   "metadata": {},
   "source": [
    "### Install TorchServe\n",
    "\n",
    "Install TorchServe and the model archiver tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install_c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install TorchServe and dependencies\n",
    "!pip install -q torchserve torch-model-archiver torch-workflow-archiver\n",
    "\n",
    "# Verify installation\n",
    "!torchserve --version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "start",
   "metadata": {},
   "source": [
    "---\n",
    "## Start TorchServe\n",
    "\n",
    "Start a local TorchServe instance with the downloaded model archive.\n",
    "\n",
    "**Command options:**\n",
    "- `--start`: Start the server\n",
    "- `--model-store`: Directory containing model archives\n",
    "- `--models`: Model to load at startup (format: `model_name=archive.mar`)\n",
    "- `--ncs`: No config snapshot (simplifies local development)\n",
    "\n",
    "The server will expose three APIs:\n",
    "- **Inference API** (port 8080): For predictions\n",
    "- **Management API** (port 8081): For model management\n",
    "- **Metrics API** (port 8082): For monitoring metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "start_c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start TorchServe\n",
    "start_command = [\n",
    "    \"torchserve\",\n",
    "    \"--start\",\n",
    "    \"--model-store\", MODEL_STORE_DIR,\n",
    "    \"--models\", f\"{MODEL_NAME}={MAR_FILE}\",\n",
    "    \"--ncs\"\n",
    "]\n",
    "\n",
    "print(f\"Starting TorchServe with command: {' '.join(start_command)}\")\n",
    "result = subprocess.run(start_command, capture_output=True, text=True)\n",
    "print(result.stdout)\n",
    "if result.stderr:\n",
    "    print(\"Stderr:\", result.stderr)\n",
    "\n",
    "# Wait for server to start\n",
    "print(\"\\nWaiting for TorchServe to start...\")\n",
    "time.sleep(10)\n",
    "\n",
    "# Check if server is running\n",
    "try:\n",
    "    response = requests.get(f\"{MANAGEMENT_URL}/models\")\n",
    "    if response.status_code == 200:\n",
    "        print(\"TorchServe started successfully!\")\n",
    "        print(f\"Loaded models: {response.json()}\")\n",
    "    else:\n",
    "        print(f\"Server responded with status {response.status_code}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error checking server status: {e}\")\n",
    "    print(\"Server may still be starting up...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "test",
   "metadata": {},
   "source": [
    "### Test Predictions\n",
    "\n",
    "Make test predictions using the inference API. We'll send sample transaction data with 30 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test_c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample input data (30 features)\n",
    "# Generate a sample transaction with realistic values\n",
    "sample_transaction = np.random.randn(30).astype(np.float32)\n",
    "\n",
    "print(\"Sample transaction data:\")\n",
    "print(sample_transaction)\n",
    "print(f\"\\nShape: {sample_transaction.shape}\")\n",
    "print(f\"Data type: {sample_transaction.dtype}\")\n",
    "\n",
    "# Make prediction request\n",
    "print(f\"\\nSending prediction request to {INFERENCE_URL}...\")\n",
    "\n",
    "# Prepare request data\n",
    "data = json.dumps({\"instances\": sample_transaction.tolist()})\n",
    "\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "try:\n",
    "    response = requests.post(INFERENCE_URL, data=data, headers=headers)\n",
    "    \n",
    "    print(f\"\\nResponse status: {response.status_code}\")\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        prediction = response.json()\n",
    "        print(\"\\nPrediction successful!\")\n",
    "        print(f\"Response: {json.dumps(prediction, indent=2)}\")\n",
    "        \n",
    "        # Parse and display reconstruction\n",
    "        if isinstance(prediction, dict) and \"predictions\" in prediction:\n",
    "            reconstructed = np.array(prediction[\"predictions\"])\n",
    "            print(f\"\\nReconstructed shape: {reconstructed.shape}\")\n",
    "            \n",
    "            # Calculate reconstruction error\n",
    "            mse = np.mean((sample_transaction - reconstructed.flatten())**2)\n",
    "            print(f\"Reconstruction MSE: {mse:.6f}\")\n",
    "    else:\n",
    "        print(f\"Error: {response.text}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error making prediction: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mgmt",
   "metadata": {},
   "source": [
    "### Management API\n",
    "\n",
    "Explore the management API to inspect loaded models and their details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mgmt_c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all models\n",
    "print(\"Listing all models...\")\n",
    "response = requests.get(f\"{MANAGEMENT_URL}/models\")\n",
    "print(f\"Status: {response.status_code}\")\n",
    "print(f\"Models: {json.dumps(response.json(), indent=2)}\")\n",
    "\n",
    "# Get detailed model information\n",
    "print(f\"\\n\\nGetting details for model '{MODEL_NAME}'...\")\n",
    "response = requests.get(f\"{MANAGEMENT_URL}/models/{MODEL_NAME}\")\n",
    "print(f\"Status: {response.status_code}\")\n",
    "model_info = response.json()\n",
    "print(f\"Model info: {json.dumps(model_info, indent=2)}\")\n",
    "\n",
    "# Get model workers information\n",
    "print(f\"\\n\\nGetting worker information for model '{MODEL_NAME}'...\")\n",
    "response = requests.get(f\"{MANAGEMENT_URL}/models/{MODEL_NAME}/all\")\n",
    "print(f\"Status: {response.status_code}\")\n",
    "worker_info = response.json()\n",
    "print(f\"Worker info: {json.dumps(worker_info, indent=2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metrics",
   "metadata": {},
   "source": [
    "### Metrics\n",
    "\n",
    "Access the metrics endpoint to view performance and usage statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metrics_c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get metrics\n",
    "print(\"Fetching metrics...\")\n",
    "response = requests.get(METRICS_URL)\n",
    "print(f\"Status: {response.status_code}\")\n",
    "\n",
    "if response.status_code == 200:\n",
    "    metrics = response.text\n",
    "    print(\"\\nMetrics (Prometheus format):\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Display relevant metrics\n",
    "    for line in metrics.split('\\n'):\n",
    "        # Filter for model-specific metrics\n",
    "        if MODEL_NAME in line or 'ts_' in line:\n",
    "            print(line)\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\nTotal metrics lines: {len(metrics.split(chr(10)))}\")\n",
    "else:\n",
    "    print(f\"Error fetching metrics: {response.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stop",
   "metadata": {},
   "source": [
    "### Stop TorchServe\n",
    "\n",
    "Gracefully shutdown the TorchServe instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stop_c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop TorchServe\n",
    "print(\"Stopping TorchServe...\")\n",
    "result = subprocess.run([\"torchserve\", \"--stop\"], capture_output=True, text=True)\n",
    "print(result.stdout)\n",
    "if result.stderr:\n",
    "    print(\"Stderr:\", result.stderr)\n",
    "\n",
    "# Wait for shutdown\n",
    "time.sleep(5)\n",
    "print(\"TorchServe stopped.\")\n",
    "\n",
    "# Verify server is stopped\n",
    "try:\n",
    "    response = requests.get(f\"{MANAGEMENT_URL}/models\", timeout=2)\n",
    "    print(\"Warning: Server may still be running\")\n",
    "except requests.exceptions.RequestException:\n",
    "    print(\"Confirmed: Server is not responding (stopped successfully)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary and Next Steps\n",
    "\n",
    "### What You Accomplished\n",
    "\n",
    "In this notebook, you successfully:\n",
    "\n",
    "1. **Downloaded** a model archive (.mar) from Google Cloud Storage\n",
    "2. **Installed** TorchServe and its dependencies locally\n",
    "3. **Started** a local TorchServe instance with your model\n",
    "4. **Made predictions** via the REST inference API\n",
    "5. **Explored** the management API to inspect model details\n",
    "6. **Accessed** metrics for monitoring model performance\n",
    "7. **Stopped** TorchServe gracefully\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- **TorchServe provides three APIs**: inference (8080), management (8081), and metrics (8082)\n",
    "- **Model archives (.mar)** package the model, handler, and dependencies together\n",
    "- **Local testing** enables rapid iteration before cloud deployment\n",
    "- **REST APIs** make it easy to integrate with any client application\n",
    "- **Metrics** help you understand model performance and usage patterns\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "Now that you've tested TorchServe locally, you can:\n",
    "\n",
    "1. **Deploy to Vertex AI**: Use the model archive in a custom container deployment\n",
    "   - See the `torchserve-vertex.ipynb` notebook for Vertex AI deployment\n",
    "   \n",
    "2. **Customize the handler**: Modify preprocessing/postprocessing logic\n",
    "   - Edit the handler in your model training notebook\n",
    "   - Rebuild the .mar file and test locally\n",
    "   \n",
    "3. **Performance testing**: Load test your model locally\n",
    "   - Use tools like `apache-bench` or `locust` for load testing\n",
    "   - Adjust worker counts and batch sizes\n",
    "   \n",
    "4. **Production deployment**: Deploy to production environments\n",
    "   - Use Vertex AI Prediction for managed serving\n",
    "   - Or deploy to GKE for full control\n",
    "\n",
    "### Additional Resources\n",
    "\n",
    "- [TorchServe Documentation](https://pytorch.org/serve/)\n",
    "- [TorchServe REST API](https://pytorch.org/serve/rest_api.html)\n",
    "- [Custom Handlers Guide](https://pytorch.org/serve/custom_service.html)\n",
    "- [Vertex AI Custom Container Deployment](https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
