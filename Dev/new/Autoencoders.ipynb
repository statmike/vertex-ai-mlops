{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f54c64e",
   "metadata": {},
   "source": [
    "![tracker](https://us-central1-vertex-ai-mlops-369716.cloudfunctions.net/pixel-tracking?path=statmike%2Fvertex-ai-mlops%2FDev%2Fnew&file=Autoencoders.ipynb)\n",
    "<!--- header table --->\n",
    "<table align=\"left\">\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://colab.research.google.com/github/statmike/vertex-ai-mlops/blob/main/Dev/new/Autoencoders.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Google Colaboratory logo\">\n",
    "      <br>Run in<br>Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https%3A%2F%2Fraw.githubusercontent.com%2Fstatmike%2Fvertex-ai-mlops%2Fmain%2FDev%2Fnew%2FAutoencoders.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\">\n",
    "      <br>Run in<br>Colab Enterprise\n",
    "    </a>\n",
    "  </td>      \n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://github.com/statmike/vertex-ai-mlops/blob/main/Dev/new/Autoencoders.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      <br>View on<br>GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/statmike/vertex-ai-mlops/main/Dev/new/Autoencoders.ipynb\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
    "      <br>Open in<br>Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6957802-89ac-4b76-aa6d-14c97433cb9e",
   "metadata": {},
   "source": [
    "# Autoencoders\n",
    "\n",
    "The basics of autoencoders from training to serving with application examples!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98cfb41-a5f7-4d53-ac09-583939caa0d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09c8bf6-27d1-4a81-aaaa-b08f171f9f3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887e86a9-891c-4754-90d4-4f4759925726",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146b2aba-b92c-4b35-b14f-b2a09c0edfe2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3a013c-3deb-43d0-b957-6b261dcf3372",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4197a4a3-c097-4fcc-9d7e-7efa98b1ccb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2d6353-dab2-433e-99fc-095d315c5efc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fe97d3-b38e-4565-ba78-4c115ba7c083",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbc0715-205d-47eb-904d-2d9bef3436cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1afb283a-7192-41cc-ad9e-03d42e908688",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1adb1b69-b0d7-43ef-a9f4-22769e56180f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# package import\n",
    "from tensorflow.python.framework import dtypes\n",
    "from tensorflow_io.bigquery import BigQueryClient\n",
    "import tensorflow as tf\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import aiplatform\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# import argument to local variables\n",
    "parser = argparse.ArgumentParser()\n",
    "# the passed param, dest: a name for the param, default: if absent fetch this param from the OS, type: type to convert to, help: description of argument\n",
    "parser.add_argument('--epochs', dest = 'epochs', default = 10, type = int, help = 'Number of Epochs')\n",
    "parser.add_argument('--batch_size', dest = 'batch_size', default = 32, type = int, help = 'Batch Size')\n",
    "parser.add_argument('--var_target', dest = 'var_target', type=str)\n",
    "parser.add_argument('--var_omit', dest = 'var_omit', type=str, nargs='*')\n",
    "parser.add_argument('--project_id', dest = 'project_id', type=str)\n",
    "parser.add_argument('--bq_project', dest = 'bq_project', type=str)\n",
    "parser.add_argument('--bq_dataset', dest = 'bq_dataset', type=str)\n",
    "parser.add_argument('--bq_table', dest = 'bq_table', type=str)\n",
    "parser.add_argument('--region', dest = 'region', type=str)\n",
    "parser.add_argument('--experiment', dest = 'experiment', type=str)\n",
    "parser.add_argument('--series', dest = 'series', type=str)\n",
    "parser.add_argument('--experiment_name', dest = 'experiment_name', type=str)\n",
    "parser.add_argument('--run_name', dest = 'run_name', type=str)\n",
    "args = parser.parse_args()\n",
    "\n",
    "# clients\n",
    "bq = bigquery.Client(project = args.project_id)\n",
    "aiplatform.init(project = args.project_id, location = args.region)\n",
    "\n",
    "# Vertex AI Experiment\n",
    "expRun = aiplatform.ExperimentRun.create(run_name = args.run_name, experiment = args.experiment_name)\n",
    "expRun.log_params({'experiment': args.experiment, 'series': args.series, 'project_id': args.project_id})\n",
    "\n",
    "# get schema from bigquery source\n",
    "query = f\"SELECT * FROM {args.bq_project}.{args.bq_dataset}.INFORMATION_SCHEMA.COLUMNS WHERE TABLE_NAME = '{args.bq_table}'\"\n",
    "schema = bq.query(query).to_dataframe()\n",
    "\n",
    "# get number of classes from bigquery source\n",
    "nclasses = bq.query(query = f'SELECT DISTINCT {args.var_target} FROM {args.bq_project}.{args.bq_dataset}.{args.bq_table} WHERE {args.var_target} is not null').to_dataframe()\n",
    "nclasses = nclasses.shape[0]\n",
    "expRun.log_params({'data_source': f'bq://{args.bq_project}.{args.bq_dataset}.{args.bq_table}', 'nclasses': nclasses, 'var_split': 'splits', 'var_target': args.var_target})\n",
    "\n",
    "# Make a list of columns to omit\n",
    "OMIT = args.var_omit + ['splits']\n",
    "\n",
    "# use schema to prepare a list of columns to read from BigQuery\n",
    "selected_fields = schema[~schema.column_name.isin(OMIT)].column_name.tolist()\n",
    "\n",
    "# all the columns in this data source are either float64 or int64\n",
    "output_types = [dtypes.float64 if x=='FLOAT64' else dtypes.int64 for x in schema[~schema.column_name.isin(OMIT)].data_type.tolist()]\n",
    "\n",
    "# remap input data to Tensorflow inputs of features and target\n",
    "def transTable(row_dict):\n",
    "    target = row_dict.pop(args.var_target)\n",
    "    target = tf.one_hot(tf.cast(target, tf.int64), nclasses)\n",
    "    target = tf.cast(target, tf.float32)\n",
    "    features = [tf.cast(v, tf.float32) for v in row_dict.values()]\n",
    "    features = tf.stack(features)\n",
    "    return(\n",
    "        features, \n",
    "        {\n",
    "            'logistic': target, \n",
    "            'classification': target, \n",
    "            'decoder': features}\n",
    "    )\n",
    "\n",
    "# function to setup a bigquery reader with Tensorflow I/O\n",
    "def bq_reader(split):\n",
    "    reader = BigQueryClient()\n",
    "\n",
    "    training = reader.read_session(\n",
    "        parent = f\"projects/{args.project_id}\",\n",
    "        project_id = args.bq_project,\n",
    "        table_id = args.bq_table,\n",
    "        dataset_id = args.bq_dataset,\n",
    "        selected_fields = selected_fields,\n",
    "        output_types = output_types,\n",
    "        row_restriction = f\"splits='{split}'\",\n",
    "        requested_streams = 3\n",
    "    )\n",
    "    \n",
    "    return training\n",
    "\n",
    "# setup feed for train, validate and test\n",
    "train = bq_reader('TRAIN').parallel_read_rows().prefetch(1).map(transTable).shuffle(args.batch_size*10).batch(args.batch_size)\n",
    "validate = bq_reader('VALIDATE').parallel_read_rows().prefetch(1).map(transTable).batch(args.batch_size)\n",
    "test = bq_reader('TEST').parallel_read_rows().prefetch(1).map(transTable).batch(args.batch_size)\n",
    "expRun.log_params({'training.batch_size': args.batch_size, 'training.shuffle': 10*args.batch_size, 'training.prefetch': 1})\n",
    "\n",
    "# Three targets: logistics, autoencoder, classification from encoder\n",
    "\n",
    "# inputs\n",
    "features = tf.keras.layers.Input(shape = (len(selected_fields)-1,), name = 'features')\n",
    "\n",
    "# normalize here\n",
    "normalized = tf.keras.layers.BatchNormalization(name = 'batch_normalization_layer')(features)\n",
    "\n",
    "# logistic\n",
    "logistic = tf.keras.layers.Dense(nclasses, activation = tf.nn.softmax, name = 'logistic')(normalized)#(normalized)(features)\n",
    "\n",
    "# encoder\n",
    "encode = tf.keras.layers.Dense(25, activation = tf.nn.relu)(normalized)#(normalized)(features)\n",
    "encode = tf.keras.layers.Dense(20, activation = tf.nn.relu)(encode)\n",
    "encode = tf.keras.layers.Dense(15, activation = tf.nn.relu, name = 'encoder')(encode)\n",
    "\n",
    "# classifier\n",
    "classifier = tf.keras.layers.Dense(nclasses, activation = tf.nn.softmax, name = 'classification')(encode)\n",
    "\n",
    "# decoder\n",
    "decode = tf.keras.layers.Dense(20, activation = tf.nn.relu)(encode)\n",
    "decode = tf.keras.layers.Dense(25, activation = tf.nn.relu)(decode)\n",
    "decode = tf.keras.layers.Dense(features.shape[1], activation = tf.nn.sigmoid, name = 'decoder')(decode)\n",
    "\n",
    "# the model\n",
    "model = tf.keras.Model(\n",
    "    inputs = features,\n",
    "    outputs = [logistic, classifier, decode],\n",
    "    name = args.experiment\n",
    ")\n",
    "\n",
    "# compile\n",
    "model.compile(\n",
    "    optimizer = tf.keras.optimizers.Adam(), #SGD or Adam\n",
    "    loss = {\n",
    "        'logistic': tf.keras.losses.CategoricalCrossentropy(),\n",
    "        'classification': tf.keras.losses.CategoricalCrossentropy(),\n",
    "        'decoder': tf.keras.losses.BinaryCrossentropy()\n",
    "    },\n",
    "    metrics = {\n",
    "        'logistic': ['accuracy', tf.keras.metrics.AUC(curve = 'PR', name = 'auprc')],\n",
    "        'classification': ['accuracy', tf.keras.metrics.AUC(curve = 'PR', name = 'auprc')],\n",
    "        'decoder': tf.keras.metrics.RootMeanSquaredError(name = 'rmse')\n",
    "    }\n",
    ")\n",
    "\n",
    "# setup tensorboard logs and train\n",
    "# setup tensorboard logs and train\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "    log_dir=os.environ['AIP_TENSORBOARD_LOG_DIR'],\n",
    "    histogram_freq=1\n",
    ")\n",
    "history = model.fit(\n",
    "    train, \n",
    "    epochs = args.epochs, \n",
    "    callbacks = [tensorboard_callback], \n",
    "    validation_data = validate\n",
    ")\n",
    "expRun.log_params({'training.epochs': history.params['epochs']})\n",
    "for e in range(0, history.params['epochs']):\n",
    "    expRun.log_time_series_metrics(\n",
    "        {\n",
    "            'train_loss': history.history['loss'][e],\n",
    "            'train_logistic_loss': history.history['logistic_loss'][e],\n",
    "            'train_classification_loss': history.history['classification_loss'][e],\n",
    "            'train_decoder_loss': history.history['decoder_loss'][e],\n",
    "            'train_logistic_accuracy': history.history['logistic_accuracy'][e],\n",
    "            'train_classification_accuracy': history.history['classification_accuracy'][e],\n",
    "            'train_logistic_auprc': history.history['logistic_auprc'][e],\n",
    "            'train_classification_auprc': history.history['classification_auprc'][e],\n",
    "            'train_decoder_rmse': history.history['decoder_rmse'][e],\n",
    "            'val_loss': history.history['val_loss'][e],\n",
    "            'val_logistic_loss': history.history['val_logistic_loss'][e],\n",
    "            'val_classification_loss': history.history['val_classification_loss'][e],\n",
    "            'val_decoder_loss': history.history['val_decoder_loss'][e],\n",
    "            'val_logistic_accuracy': history.history['val_logistic_accuracy'][e],\n",
    "            'val_classification_accuracy': history.history['val_classification_accuracy'][e],\n",
    "            'val_logistic_auprc': history.history['val_logistic_auprc'][e],\n",
    "            'val_classification_auprc': history.history['val_classification_auprc'][e],\n",
    "            'val_decoder_rmse': history.history['val_decoder_rmse'][e],\n",
    "        }\n",
    "    )\n",
    "\n",
    "# test evaluations:\n",
    "metrics = model.evaluate(test)\n",
    "expRun.log_metrics(\n",
    "    {\n",
    "        'test_loss': metrics[0],\n",
    "        'test_logistic_loss': metrics[1],\n",
    "        'test_classification_loss': metrics[2],\n",
    "        'test_decoder_loss': metrics[3],\n",
    "        'test_logistic_accuracy': metrics[4],\n",
    "        'test_logistic_auprc': metrics[5],\n",
    "        'test_classification_accuracy': metrics[6],\n",
    "        'test_classification_auprc': metrics[7],\n",
    "        'test_decoder_rmse': metrics[8]\n",
    "    }\n",
    ")\n",
    "\n",
    "# extract encode layer\n",
    "encode_model = tf.keras.Model(\n",
    "    inputs = model.input,\n",
    "    outputs = model.get_layer('encoder').output,\n",
    "    name = args.experiment+'_encoder'\n",
    ")\n",
    "\n",
    "# output the model save files\n",
    "encode_model.save(os.getenv(\"AIP_MODEL_DIR\")+'encoder/')\n",
    "model.save(os.getenv(\"AIP_MODEL_DIR\"))\n",
    "expRun.log_params({'model.save': os.getenv(\"AIP_MODEL_DIR\")})\n",
    "expRun.end_run()"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-root-py",
   "name": "workbench-notebooks.m113",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/workbench-notebooks:m113"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
