{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1038461a",
   "metadata": {},
   "source": [
    "![tracker](https://us-central1-vertex-ai-mlops-369716.cloudfunctions.net/pixel-tracking?path=statmike%2Fvertex-ai-mlops%2F04+-+scikit-learn&file=04+-+Vertex+AI+Custom+Model+-+scikit-learn+-+Notebook+to+Script.ipynb)\n",
    "<!--- header table --->\n",
    "<table align=\"left\">\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://colab.research.google.com/github/statmike/vertex-ai-mlops/blob/main/04%20-%20scikit-learn/04%20-%20Vertex%20AI%20Custom%20Model%20-%20scikit-learn%20-%20Notebook%20to%20Script.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Google Colaboratory logo\">\n",
    "      <br>Run in<br>Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https%3A%2F%2Fraw.githubusercontent.com%2Fstatmike%2Fvertex-ai-mlops%2Fmain%2F04%2520-%2520scikit-learn%2F04%2520-%2520Vertex%2520AI%2520Custom%2520Model%2520-%2520scikit-learn%2520-%2520Notebook%2520to%2520Script.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\">\n",
    "      <br>Run in<br>Colab Enterprise\n",
    "    </a>\n",
    "  </td>      \n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://github.com/statmike/vertex-ai-mlops/blob/main/04%20-%20scikit-learn/04%20-%20Vertex%20AI%20Custom%20Model%20-%20scikit-learn%20-%20Notebook%20to%20Script.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      <br>View on<br>GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/statmike/vertex-ai-mlops/main/04%20-%20scikit-learn/04%20-%20Vertex%20AI%20Custom%20Model%20-%20scikit-learn%20-%20Notebook%20to%20Script.ipynb\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
    "      <br>Open in<br>Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1387e49e-a52e-4aeb-9e0d-7fdf9aa72c48",
   "metadata": {},
   "source": [
    "# 04 - Vertex AI Custom Model - scikit-learn - From Notebook to Script\n",
    "\n",
    "In the notebook [04 - Vertex AI Custom Model - scikit-learn - in Notebook](./04%20-%20Vertex%20AI%20Custom%20Model%20-%20scikit-learn%20-%20in%20Notebook.ipynb) code was developed ad-hoc and run piece by piece in cells of the notebook.  As the data scales and training scales it is tempting to increase the size of compute backing the notebook environment with more CPU, Memory and/or GPU.  A better practice for scaling is running the ML training code in training jobs with specified compute.  The notebooks `04a` through `04i` show the ways to run training jobs for various purposes.  This notebook builds the script that is used by notebooks `04a` through `04f` and is the basis for the hyperparameter tuning version built in [04 - Vertex AI Custom Model - scikit-learn - Notebook to Hyperparameter Tuning Script](./04%20-%20Vertex%20AI%20Custom%20Model%20-%scikit-learn%20-%20Notebook%20to%20Hyperparameter%20Tuning%20Script.ipynb) (WIP) and used in `04g` through `04i`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d85eeb-94f2-4297-964c-4c340f3ac0f4",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bc801b-7da7-4d18-9085-3c260bc68bcf",
   "metadata": {},
   "source": [
    "inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd6da559-f60d-4a5b-9c5f-5c0af64e610f",
   "metadata": {},
   "outputs": [],
   "source": [
    "SCRIPT_PATH = './code/train.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12dca265-c411-438a-ac35-f4f16317d529",
   "metadata": {},
   "source": [
    "packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d209b693-223a-401f-a52d-00ddb08757de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from IPython.display import Markdown as md"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7087e9dc-76d6-4280-ad8e-5d3e6b257985",
   "metadata": {},
   "source": [
    "environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0115f764-a321-4572-b5bb-1cdd9c69119e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The code directory alredy exists\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists('code'):\n",
    "    print('The code directory alredy exists')\n",
    "else:\n",
    "    print('Creating the code directory')\n",
    "    os.makedirs('code')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77da047a-673b-4cd3-ae69-386b6b47d125",
   "metadata": {},
   "source": [
    "---\n",
    "## Build the ML Training Code\n",
    "\n",
    "Use the cell magic `%%writefile` to create a single ML training code file named `train.py`.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9a29fb-c56e-496f-b81d-3859ac90459e",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49bae688-6852-4e19-bb55-8248255df29a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./code/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {SCRIPT_PATH}\n",
    "\n",
    "# package import\n",
    "import sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import metrics\n",
    "\n",
    "import pickle\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud import storage\n",
    "import argparse\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8bf4fcc-7ecb-40c2-9363-19414e45940d",
   "metadata": {},
   "source": [
    "### Parse inputs and parameters\n",
    "This script uses the `argparse` package to parse input parameters.  To see more details on this method and alternatives like `docopt` and `click` as well as methods for using files to import parameters (YAML, JSON, pickle) check on this tips notebook: [Python Job Parameters](../Tips/Python%20Job%20Parameters.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "914f37bd-4db8-4084-a024-a6950a796697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to ./code/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a {SCRIPT_PATH}\n",
    "\n",
    "# import argument to local variables\n",
    "parser = argparse.ArgumentParser()\n",
    "# the passed param, dest: a name for the param, default: if absent fetch this param from the OS, type: type to convert to, help: description of argument\n",
    "parser.add_argument('--penalty', dest = 'penalty', default = 'l2', type = str, help = 'Penalty term')\n",
    "parser.add_argument('--solver', dest = 'solver', default = 'newton-cg', type = str, help = 'Logistic regression solver')\n",
    "parser.add_argument('--var_target', dest = 'var_target', type=str)\n",
    "parser.add_argument('--var_omit', dest = 'var_omit', type=str)\n",
    "parser.add_argument('--project_id', dest = 'project_id', type=str)\n",
    "parser.add_argument('--bq_project', dest = 'bq_project', type=str)\n",
    "parser.add_argument('--bq_dataset', dest = 'bq_dataset', type=str)\n",
    "parser.add_argument('--bq_table', dest = 'bq_table', type=str)\n",
    "parser.add_argument('--region', dest = 'region', type=str)\n",
    "parser.add_argument('--experiment', dest = 'experiment', type=str)\n",
    "parser.add_argument('--series', dest = 'series', type=str)\n",
    "parser.add_argument('--experiment_name', dest = 'experiment_name', type=str)\n",
    "parser.add_argument('--run_name', dest = 'run_name', type=str)\n",
    "args = parser.parse_args()\n",
    "\n",
    "# Model Training\n",
    "VAR_TARGET = str(args.var_target)\n",
    "VAR_OMIT = str(args.var_omit).split('-')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c8c592-7b2c-4580-834d-5f14d9261fb8",
   "metadata": {},
   "source": [
    "### Define Clients for Services\n",
    "This script with use clients for BigQuery and Vertex AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1e1aead-898d-4265-aca5-dbd558278719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to ./code/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a {SCRIPT_PATH}\n",
    "\n",
    "# clients\n",
    "bq = bigquery.Client(project = args.project_id)\n",
    "aiplatform.init(project = args.project_id, location = args.region)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e905551e-903e-4ace-b70b-b64b97eb9df4",
   "metadata": {},
   "source": [
    "### Setup Vertex AI Experiments\n",
    "\n",
    "Use the Vertex AI client to setup a run of the experiment (input)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c8c5108-e3e1-4688-8f45-c713b3a5d893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to ./code/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a {SCRIPT_PATH}\n",
    "\n",
    "# Vertex AI Experiment\n",
    "if args.run_name in [run.name for run in aiplatform.ExperimentRun.list(experiment = args.experiment_name)]:\n",
    "    expRun = aiplatform.ExperimentRun(run_name = args.run_name, experiment = args.experiment_name)\n",
    "else:\n",
    "    expRun = aiplatform.ExperimentRun.create(run_name = args.run_name, experiment = args.experiment_name)\n",
    "expRun.log_params({'experiment': args.experiment, 'series': args.series, 'project_id': args.project_id})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48e3053-0d5f-45ba-82c8-4a492f32a4fd",
   "metadata": {},
   "source": [
    "### Training Data\n",
    "\n",
    "Use the BigQuery client to read column information and target variable information for the source table in BigQuery.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b45b2a6-e689-49cf-9b90-366cd1570e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to ./code/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a {SCRIPT_PATH}\n",
    "\n",
    "# get schema from bigquery source\n",
    "query = f\"SELECT * FROM `{args.bq_project}.{args.bq_dataset}.INFORMATION_SCHEMA.COLUMNS` WHERE TABLE_NAME = '{args.bq_table}'\"\n",
    "schema = bq.query(query).to_dataframe()\n",
    "\n",
    "# get number of classes from bigquery source\n",
    "nclasses = bq.query(query = f'SELECT DISTINCT {VAR_TARGET} FROM `{args.bq_project}.{args.bq_dataset}.{args.bq_table}` WHERE {VAR_TARGET} is not null').to_dataframe()\n",
    "nclasses = nclasses.shape[0]\n",
    "expRun.log_params({'data_source': f'bq://{args.bq_project}.{args.bq_dataset}.{args.bq_table}', 'nclasses': nclasses, 'var_split': 'splits', 'var_target': VAR_TARGET})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2decc46-ee6d-47e4-96fd-158e59645712",
   "metadata": {},
   "source": [
    "### Read From BigQuery\n",
    "\n",
    "Create training, validation, and test datasets by reading from BigQuery into dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68dca0c8-0624-48b5-8a35-c3305a6034f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to ./code/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a {SCRIPT_PATH}\n",
    "\n",
    "train_query = f\"SELECT * FROM `{args.bq_project}.{args.bq_dataset}.{args.bq_table}` WHERE splits = 'TRAIN'\"\n",
    "train = bq.query(train_query).to_dataframe()\n",
    "X_train = train.loc[:, ~train.columns.isin(VAR_OMIT)]\n",
    "y_train = train[VAR_TARGET].astype('int')\n",
    "\n",
    "val_query = f\"SELECT * FROM `{args.bq_project}.{args.bq_dataset}.{args.bq_table}` WHERE splits = 'VALIDATE'\"\n",
    "val = bq.query(val_query).to_dataframe()\n",
    "X_val = val.loc[:, ~val.columns.isin(VAR_OMIT)]\n",
    "y_val = val[VAR_TARGET].astype('int')\n",
    "\n",
    "test_query = f\"SELECT * FROM `{args.bq_project}.{args.bq_dataset}.{args.bq_table}` WHERE splits = 'TEST'\"\n",
    "test = bq.query(test_query).to_dataframe()\n",
    "X_test = test.loc[:, ~test.columns.isin(VAR_OMIT)]\n",
    "y_test = test[VAR_TARGET].astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b10b92-8ad4-4a06-acd7-6b21e66089fe",
   "metadata": {},
   "source": [
    "### Define The Model\n",
    "\n",
    "Define a scikit-learn version of Logistic Regression using scikit-learn and a standard scaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75737d35-3352-4761-b977-4c0a385b2e02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to ./code/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a {SCRIPT_PATH}\n",
    "\n",
    "# Logistic Regression\n",
    "# instantiate the model \n",
    "logistic = LogisticRegression(solver=args.solver, penalty=args.penalty)\n",
    "\n",
    "# Define a Standard Scaler to normalize inputs\n",
    "scaler = StandardScaler()\n",
    "\n",
    "expRun.log_params({'solver': args.solver, 'penalty': args.penalty})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f3e08d-90ce-4aa3-bb17-e19cdff08550",
   "metadata": {},
   "source": [
    "### Train The Model\n",
    "Combine the scaling and model training (logistic regression) steps into a pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f11dec9e-ca0a-406d-bb65-ed1b88422328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to ./code/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a {SCRIPT_PATH}\n",
    "\n",
    "# define pipeline\n",
    "pipe = Pipeline(steps=[(\"scaler\", scaler), (\"logistic\", logistic)])\n",
    "\n",
    "# define grid search model\n",
    "model = pipe.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0600e41-576e-4848-a87a-1dbfe38362e8",
   "metadata": {},
   "source": [
    "### Evaluate The Model\n",
    "Gather metrics for the trained model on each split of the data: train, validate, and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cdaf079e-95ae-4963-a743-e57eaa4de582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to ./code/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a {SCRIPT_PATH}\n",
    "\n",
    "# test evaluations:\n",
    "y_pred = model.predict(X_test)\n",
    "test_acc = metrics.accuracy_score(y_test, y_pred) \n",
    "test_prec = metrics.precision_score(y_test, y_pred)\n",
    "test_rec = metrics.recall_score(y_test, y_pred)\n",
    "test_rocauc = metrics.roc_auc_score(y_test, y_pred)\n",
    "expRun.log_metrics({'test_accuracy': test_acc, 'test_precision': test_prec, 'test_recall': test_rec, 'test_roc_auc': test_rocauc})\n",
    "\n",
    "# val evaluations:\n",
    "y_pred_val = model.predict(X_val)\n",
    "val_acc = metrics.accuracy_score(y_val, y_pred_val) \n",
    "val_prec = metrics.precision_score(y_val, y_pred_val)\n",
    "val_rec = metrics.recall_score(y_val, y_pred_val)\n",
    "val_rocauc = metrics.roc_auc_score(y_val, y_pred_val)\n",
    "expRun.log_metrics({'validation_accuracy': val_acc, 'validation_precision': val_prec, 'validation_recall': val_rec, 'validation_roc_auc': val_rocauc})\n",
    "\n",
    "# training evaluations:\n",
    "y_pred_training = model.predict(X_train)\n",
    "training_acc = metrics.accuracy_score(y_train, y_pred_training) \n",
    "training_prec = metrics.precision_score(y_train, y_pred_training)\n",
    "training_rec = metrics.recall_score(y_train, y_pred_training)\n",
    "training_rocauc = metrics.roc_auc_score(y_train, y_pred_training)\n",
    "expRun.log_metrics({'training_accuracy': training_acc, 'training_precision':training_prec, 'training_recall': training_rec, 'training_roc_auc': training_rocauc})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91cb2214-0b18-4529-9185-7a2c6bb30541",
   "metadata": {},
   "source": [
    "### Save The Model\n",
    "Write the model save file to the GCS using the predefined environment variable `AIP_MODEL_DIR`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f63b3687-8d33-4ac8-a581-f07280089c61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to ./code/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a {SCRIPT_PATH}\n",
    "\n",
    "file_name = 'model.pkl'\n",
    "\n",
    "# Use predefined environment variable to establish model directory\n",
    "model_directory = os.environ['AIP_MODEL_DIR']\n",
    "storage_path = f'/gcs/{model_directory[5:]}' + file_name\n",
    "os.makedirs(os.path.dirname(storage_path), exist_ok=True)\n",
    "\n",
    "# output the model save files directly to GCS destination\n",
    "with open(storage_path,'wb') as f:\n",
    "    pickle.dump(model,f)\n",
    "\n",
    "expRun.log_params({'model.save': storage_path})\n",
    "expRun.end_run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7965f285-42ca-4734-851b-acb2e57a34d9",
   "metadata": {},
   "source": [
    "---\n",
    "## Review The Training Code\n",
    "Read in the completed script and print it out for review below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5bd6cd2d-7d5f-4c08-861b-637f04d3861a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "\n",
       "\n",
       "# package import\n",
       "import sklearn\n",
       "from sklearn.preprocessing import StandardScaler\n",
       "from sklearn.linear_model import LogisticRegression\n",
       "from sklearn.pipeline import Pipeline\n",
       "from sklearn import metrics\n",
       "\n",
       "import pickle\n",
       "import pandas as pd \n",
       "import numpy as np \n",
       "\n",
       "from google.cloud import bigquery\n",
       "from google.cloud import aiplatform\n",
       "from google.cloud import storage\n",
       "import argparse\n",
       "import os\n",
       "import sys\n",
       "\n",
       "# import argument to local variables\n",
       "parser = argparse.ArgumentParser()\n",
       "# the passed param, dest: a name for the param, default: if absent fetch this param from the OS, type: type to convert to, help: description of argument\n",
       "parser.add_argument('--penalty', dest = 'penalty', default = 'l2', type = str, help = 'Penalty term')\n",
       "parser.add_argument('--solver', dest = 'solver', default = 'newton-cg', type = str, help = 'Logistic regression solver')\n",
       "parser.add_argument('--var_target', dest = 'var_target', type=str)\n",
       "parser.add_argument('--var_omit', dest = 'var_omit', type=str)\n",
       "parser.add_argument('--project_id', dest = 'project_id', type=str)\n",
       "parser.add_argument('--bq_project', dest = 'bq_project', type=str)\n",
       "parser.add_argument('--bq_dataset', dest = 'bq_dataset', type=str)\n",
       "parser.add_argument('--bq_table', dest = 'bq_table', type=str)\n",
       "parser.add_argument('--region', dest = 'region', type=str)\n",
       "parser.add_argument('--experiment', dest = 'experiment', type=str)\n",
       "parser.add_argument('--series', dest = 'series', type=str)\n",
       "parser.add_argument('--experiment_name', dest = 'experiment_name', type=str)\n",
       "parser.add_argument('--run_name', dest = 'run_name', type=str)\n",
       "args = parser.parse_args()\n",
       "\n",
       "# Model Training\n",
       "VAR_TARGET = str(args.var_target)\n",
       "VAR_OMIT = str(args.var_omit).split('-')\n",
       "\n",
       "# clients\n",
       "bq = bigquery.Client(project = args.project_id)\n",
       "aiplatform.init(project = args.project_id, location = args.region)\n",
       "\n",
       "# Vertex AI Experiment\n",
       "if args.run_name in [run.name for run in aiplatform.ExperimentRun.list(experiment = args.experiment_name)]:\n",
       "    expRun = aiplatform.ExperimentRun(run_name = args.run_name, experiment = args.experiment_name)\n",
       "else:\n",
       "    expRun = aiplatform.ExperimentRun.create(run_name = args.run_name, experiment = args.experiment_name)\n",
       "expRun.log_params({'experiment': args.experiment, 'series': args.series, 'project_id': args.project_id})\n",
       "\n",
       "# get schema from bigquery source\n",
       "query = f\"SELECT * FROM `{args.bq_project}.{args.bq_dataset}.INFORMATION_SCHEMA.COLUMNS` WHERE TABLE_NAME = '{args.bq_table}'\"\n",
       "schema = bq.query(query).to_dataframe()\n",
       "\n",
       "# get number of classes from bigquery source\n",
       "nclasses = bq.query(query = f'SELECT DISTINCT {VAR_TARGET} FROM `{args.bq_project}.{args.bq_dataset}.{args.bq_table}` WHERE {VAR_TARGET} is not null').to_dataframe()\n",
       "nclasses = nclasses.shape[0]\n",
       "expRun.log_params({'data_source': f'bq://{args.bq_project}.{args.bq_dataset}.{args.bq_table}', 'nclasses': nclasses, 'var_split': 'splits', 'var_target': VAR_TARGET})\n",
       "\n",
       "train_query = f\"SELECT * FROM `{args.bq_project}.{args.bq_dataset}.{args.bq_table}` WHERE splits = 'TRAIN'\"\n",
       "train = bq.query(train_query).to_dataframe()\n",
       "X_train = train.loc[:, ~train.columns.isin(VAR_OMIT)]\n",
       "y_train = train[VAR_TARGET].astype('int')\n",
       "\n",
       "val_query = f\"SELECT * FROM `{args.bq_project}.{args.bq_dataset}.{args.bq_table}` WHERE splits = 'VALIDATE'\"\n",
       "val = bq.query(val_query).to_dataframe()\n",
       "X_val = val.loc[:, ~val.columns.isin(VAR_OMIT)]\n",
       "y_val = val[VAR_TARGET].astype('int')\n",
       "\n",
       "test_query = f\"SELECT * FROM `{args.bq_project}.{args.bq_dataset}.{args.bq_table}` WHERE splits = 'TEST'\"\n",
       "test = bq.query(test_query).to_dataframe()\n",
       "X_test = test.loc[:, ~test.columns.isin(VAR_OMIT)]\n",
       "y_test = test[VAR_TARGET].astype('int')\n",
       "\n",
       "# Logistic Regression\n",
       "# instantiate the model \n",
       "logistic = LogisticRegression(solver=args.solver, penalty=args.penalty)\n",
       "\n",
       "# Define a Standard Scaler to normalize inputs\n",
       "scaler = StandardScaler()\n",
       "\n",
       "expRun.log_params({'solver': args.solver, 'penalty': args.penalty})\n",
       "\n",
       "# define pipeline\n",
       "pipe = Pipeline(steps=[(\"scaler\", scaler), (\"logistic\", logistic)])\n",
       "\n",
       "# define grid search model\n",
       "model = pipe.fit(X_train, y_train)\n",
       "\n",
       "# test evaluations:\n",
       "y_pred = model.predict(X_test)\n",
       "test_acc = metrics.accuracy_score(y_test, y_pred) \n",
       "test_prec = metrics.precision_score(y_test, y_pred)\n",
       "test_rec = metrics.recall_score(y_test, y_pred)\n",
       "test_rocauc = metrics.roc_auc_score(y_test, y_pred)\n",
       "expRun.log_metrics({'test_accuracy': test_acc, 'test_precision': test_prec, 'test_recall': test_rec, 'test_roc_auc': test_rocauc})\n",
       "\n",
       "# val evaluations:\n",
       "y_pred_val = model.predict(X_val)\n",
       "val_acc = metrics.accuracy_score(y_val, y_pred_val) \n",
       "val_prec = metrics.precision_score(y_val, y_pred_val)\n",
       "val_rec = metrics.recall_score(y_val, y_pred_val)\n",
       "val_rocauc = metrics.roc_auc_score(y_val, y_pred_val)\n",
       "expRun.log_metrics({'validation_accuracy': val_acc, 'validation_precision': val_prec, 'validation_recall': val_rec, 'validation_roc_auc': val_rocauc})\n",
       "\n",
       "# training evaluations:\n",
       "y_pred_training = model.predict(X_train)\n",
       "training_acc = metrics.accuracy_score(y_train, y_pred_training) \n",
       "training_prec = metrics.precision_score(y_train, y_pred_training)\n",
       "training_rec = metrics.recall_score(y_train, y_pred_training)\n",
       "training_rocauc = metrics.roc_auc_score(y_train, y_pred_training)\n",
       "expRun.log_metrics({'training_accuracy': training_acc, 'training_precision':training_prec, 'training_recall': training_rec, 'training_roc_auc': training_rocauc})\n",
       "\n",
       "file_name = 'model.pkl'\n",
       "\n",
       "# Use predefined environment variable to establish model directory\n",
       "model_directory = os.environ['AIP_MODEL_DIR']\n",
       "storage_path = f'/gcs/{model_directory[5:]}' + file_name\n",
       "os.makedirs(os.path.dirname(storage_path), exist_ok=True)\n",
       "\n",
       "# output the model save files directly to GCS destination\n",
       "with open(storage_path,'wb') as f:\n",
       "    pickle.dump(model,f)\n",
       "\n",
       "expRun.log_params({'model.save': storage_path})\n",
       "expRun.end_run()\n",
       "\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(SCRIPT_PATH, 'r') as file:\n",
    "    data = file.read()\n",
    "md(f\"```python\\n\\n{data}\\n```\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33acfaf-b405-4181-ba16-1fc4c75827d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e216c0a-e007-4423-bcfe-7ade01dcea38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6ace70-4c73-4f6f-9277-87e29fbea4a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-11.m104",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-11:m104"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
