{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "416e4702",
   "metadata": {},
   "source": [
    "![tracker](https://us-central1-vertex-ai-mlops-369716.cloudfunctions.net/pixel-tracking?path=statmike%2Fvertex-ai-mlops%2F08+-+R&file=R+-+Dataproc+Serverless+Spark-R+Jobs.ipynb)\n",
    "<!--- header table --->\n",
    "<table align=\"left\">\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://colab.research.google.com/github/statmike/vertex-ai-mlops/blob/main/08%20-%20R/R%20-%20Dataproc%20Serverless%20Spark-R%20Jobs.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Google Colaboratory logo\">\n",
    "      <br>Run in<br>Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https%3A%2F%2Fraw.githubusercontent.com%2Fstatmike%2Fvertex-ai-mlops%2Fmain%2F08%2520-%2520R%2FR%2520-%2520Dataproc%2520Serverless%2520Spark-R%2520Jobs.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\">\n",
    "      <br>Run in<br>Colab Enterprise\n",
    "    </a>\n",
    "  </td>      \n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://github.com/statmike/vertex-ai-mlops/blob/main/08%20-%20R/R%20-%20Dataproc%20Serverless%20Spark-R%20Jobs.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      <br>View on<br>GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/statmike/vertex-ai-mlops/main/08%20-%20R/R%20-%20Dataproc%20Serverless%20Spark-R%20Jobs.ipynb\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
    "      <br>Open in<br>Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235b07c1-4cd3-43e3-8639-feb554954981",
   "metadata": {},
   "source": [
    "# R - Dataproc Serverless Spark-R Jobs\n",
    "\n",
    "Running an **R** script as a job using [SparkR](https://spark.apache.org/docs/latest/sparkr.html#overview).  Submit a prepared script directly to Google Cloud [Dataproc Serverless](https://cloud.google.com/dataproc-serverless/docs/overview) as a batch job.  This allows for a completely serverless Spark job with a startup time under 60s.\n",
    "\n",
    "---\n",
    "Part of the series of [**R**](https://github.com/statmike/vertex-ai-mlops/blob/main/08%20-%20R/readme.md) workflows:\n",
    "\n",
    "A series of workflows focused on using **R** in Vertex AI as well as other Google Cloud services to run R code, train models with R, and serve predictionns with R.\n",
    "\n",
    "---\n",
    "\n",
    "**Prerequisites:**\n",
    "\n",
    "- This notebook running in Vertex AI Workbench Instance as described in the series [readme](./readme.md)\n",
    "- Run the workflow: [R - Notebook Based Workflow](./R%20-%20Notebook%20Based%20Workflow.ipynb)\n",
    "    - This prepares the data source used by the custom job in this workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d419a5e6-b190-4a42-9332-748312b19013",
   "metadata": {},
   "source": [
    "---\n",
    "## Installs\n",
    "\n",
    "The list `packages` contains tuples of package import names and install names.  If the import name is not found then the install name is used to install quitely for the current user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a1765341-7542-41f1-af96-a0bd2db536f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# tuples of (import name, install name)\n",
    "packages = [\n",
    "    ('google.cloud.storage', 'google-cloud-storage'),\n",
    "    ('google.cloud.dataproc', 'google-cloud-dataproc')\n",
    "]\n",
    "\n",
    "import importlib\n",
    "install = False\n",
    "for package in packages:\n",
    "    if not importlib.util.find_spec(package[0]):\n",
    "        print(f'installing package {package[1]}')\n",
    "        install = True\n",
    "        !pip install {package[1]} -U -q --user"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c3b841-dc93-4007-84f9-4c5ce0f8b2be",
   "metadata": {},
   "source": [
    "### Enable APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "77828585-3f67-4f2a-a7da-0ecf9b3ccca8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!gcloud services enable dataproc.googleapis.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11fa28e-af69-4e98-8c45-75563d8f9ba1",
   "metadata": {},
   "source": [
    "### Restart Kernel (If Installs Occured)\n",
    "\n",
    "After a kernel restart the code submission can start with the next cell after this one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c8200f0c-add7-484e-9133-8772f91d51cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if install:\n",
    "    import IPython\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54541db6-b761-40ef-ac49-e6fb64effb93",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4092122e-9797-40f1-8a67-951f1f9b869f",
   "metadata": {},
   "source": [
    "inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "28100b98-b514-41e1-b348-6df0ec6bc669",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'statmike-mlops-349915'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project = !gcloud config get-value project\n",
    "PROJECT_ID = project[0]\n",
    "PROJECT_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3859c1e5-72a7-406d-871d-1be6039d8e3f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "REGION = 'us-central1'\n",
    "EXPERIMENT = 'dataproc-serverless'\n",
    "SERIES = 'r'\n",
    "\n",
    "# BigQuery Parameters\n",
    "BQ_PROJECT = PROJECT_ID\n",
    "BQ_DATASET = SERIES\n",
    "BQ_TABLE = 'bigquery-data'\n",
    "BQ_REGION = REGION[0:2]\n",
    "\n",
    "# GCS Parameters: Give bucket name\n",
    "GCS_BUCKET = PROJECT_ID\n",
    "\n",
    "# key columns in the data:\n",
    "VAR_TARGET = 'Class'\n",
    "VAR_OMIT = ['transaction_id', 'splits']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29ebd5d-0a58-4db9-bee2-7e87cd2d894b",
   "metadata": {},
   "source": [
    "packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "79ecebca-63f5-49e3-89c4-dc7c402b14d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "from google.cloud import dataproc_v1\n",
    "\n",
    "from IPython.display import Markdown as md\n",
    "from datetime import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc629e0b-a0c7-4f82-894a-64191acea8ca",
   "metadata": {},
   "source": [
    "parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d6dee224-d322-4e4d-9025-aa3bf195abd2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "URI = f\"gs://{GCS_BUCKET}/{SERIES}/{EXPERIMENT}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ed883b-51e1-4c3b-8805-e9efbee4d4ad",
   "metadata": {},
   "source": [
    "clients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a2ae3c14-a4d9-48f3-a07f-24a20edd7c38",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gcs = storage.Client(project = PROJECT_ID)\n",
    "dataproc_batch = dataproc_v1.BatchControllerClient(client_options = dict(api_endpoint = f\"{REGION}-dataproc.googleapis.com:443\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44740d7-b10a-4ce4-9e9d-3e36ead5effd",
   "metadata": {},
   "source": [
    "---\n",
    "## Prepare Training Code: **SparkR** Script\n",
    "\n",
    "The prior workflow in this series, [R - Notebook Based Workflow](./R%20-%20Notebook%20Based%20Workflow.ipynb), did the model training work in a notebook using an **R** kernel.  \n",
    "\n",
    "The first step is converting the workflow of the prior notebook to a script that will run with SparkR. The steps from the notebook workflow have been replicated in the **R** script included with this repository.  The cell below loads and shows this script.  \n",
    "- review directly in GitHub with [this link](https://github.com/statmike/vertex-ai-mlops/blob/main/08%20-%20R/code/sparkr.R)\n",
    "\n",
    "**Notes On Script**\n",
    "- The steps are replicated identically with the following additions:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b52d32e2-4c3b-4f74-831c-f5d238f25d04",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```R\n",
       "\n",
       "n <- 1000000  # Number of random points\n",
       "x <- runif(n, -1, 1)\n",
       "y <- runif(n, -1, 1)\n",
       "\n",
       "inside <- x^2 + y^2 <= 1  # Points within the unit circle\n",
       "pi_estimate <- 4 * sum(inside) / n \n",
       "print(pi_estimate)\n",
       "\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load a view the script:\n",
    "SCRIPT_PATH = './code/sparkr.R'\n",
    "\n",
    "with open(SCRIPT_PATH, 'r') as file:\n",
    "    data = file.read()\n",
    "md(f\"```R\\n\\n{data}\\n```\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe62cf31-1f57-4034-ba92-4d17798b9c7c",
   "metadata": {},
   "source": [
    "---\n",
    "## Create Batch SparkR Job With Dataproc Serverless"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4651ca80-efff-4761-9f27-8b4df14b66b9",
   "metadata": {
    "id": "1479c476-7755-4a7a-bddc-961e6570cea1"
   },
   "source": [
    "### Setup Dataproc\n",
    "Using Google APIs from Spark code will require the subnet to have Private Google Access enabled.\n",
    "- Network Configuration: https://cloud.google.com/dataproc-serverless/docs/concepts/network\n",
    "    - Configure Private Google Access: https://cloud.google.com/vpc/docs/configure-private-google-access#config-pga"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7cca7b-e1b8-4208-800d-594dba8e0b20",
   "metadata": {},
   "source": [
    "Current networks name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "896cdee8-ef3e-4d5a-bad5-c4ef0f592ea9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME     SUBNET_MODE  BGP_ROUTING_MODE  IPV4_RANGE  GATEWAY_IPV4\n",
      "default  AUTO         REGIONAL\n"
     ]
    }
   ],
   "source": [
    "!gcloud compute networks list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f1ae42-1489-4f4b-99fb-869d4d96d320",
   "metadata": {},
   "source": [
    "Enable the network's subnet for the region for Private Google Access:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "009233c2-8e7a-4300-b7dc-f37a1cc786f3",
   "metadata": {
    "id": "e08c83dd-d117-4e55-adef-40951f122a90",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Private Google Access is Enable = True\n"
     ]
    }
   ],
   "source": [
    "status = !gcloud compute networks subnets describe default --region={REGION} --format=\"get(privateIpGoogleAccess)\"\n",
    "if status[0] == 'False':\n",
    "  !gcloud compute networks subnets update default --region={REGION} --enable-private-ip-google-access\n",
    "  status = !gcloud compute networks subnets describe default --region={REGION} --format=\"get(privateIpGoogleAccess)\"\n",
    "print(f\"Private Google Access is Enable = {status[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d580ca-6d99-4843-90f0-09796f05223c",
   "metadata": {},
   "source": [
    "Open subnet connectivity to allow ingress communication:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "48db49a6-261c-43d5-bc6b-011a0b3b78e8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating firewall...failed.                                                    \n",
      "\u001b[1;31mERROR:\u001b[0m (gcloud.compute.firewall-rules.create) Could not fetch resource:\n",
      " - The resource 'projects/statmike-mlops-349915/global/firewalls/allow-internal-ingress' already exists\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!gcloud compute firewall-rules create allow-internal-ingress \\\n",
    "--network=default \\\n",
    "--source-ranges=10.128.0.0/9 \\\n",
    "--direction=ingress \\\n",
    "--action=allow \\\n",
    "--rules=all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671d16f0-836e-4115-8e6d-07292be77c1d",
   "metadata": {},
   "source": [
    "### Copy Script To GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "bf98d527-1742-47c4-a792-526b9a4f2980",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bucket = gcs.lookup_bucket(GCS_BUCKET)\n",
    "SOURCEPATH = f'{SERIES}/{EXPERIMENT}/models/{TIMESTAMP}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f1346f3d-369f-412f-8f0e-487da33dd924",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "blob = bucket.blob(f'{SOURCEPATH}/sparkr.R')\n",
    "blob.upload_from_filename(SCRIPT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "bc7d6e9c-002b-4731-938f-7d4267e5ed79",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'r/dataproc-serverless/models/20240129012536/sparkr.R'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob.name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b7f307-ba53-48a3-85da-5f89f4fb8024",
   "metadata": {},
   "source": [
    "### Submit Job\n",
    "\n",
    "The [script can be submitted](https://cloud.google.com/dataproc-serverless/docs/quickstarts/spark-batch#submit_a_spark_batch_workload) with Google Cloud Console, the [`gcloud` CLI](https://cloud.google.com/sdk/gcloud/reference/dataproc/batches/submit) or [one of the APIs](https://cloud.google.com/dataproc-serverless/docs/reference) including the [Python Client](https://cloud.google.com/python/docs/reference/dataproc/latest) used here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "90e0ab1b-08c0-45e4-a27c-6f38a40563e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "operation = dataproc_batch.create_batch(\n",
    "    parent = f'projects/{PROJECT_ID}/locations/{REGION}',\n",
    "    batch = dataproc_v1.Batch(\n",
    "        spark_r_batch = dataproc_v1.SparkRBatch(\n",
    "            main_r_file_uri = f'gs://{GCS_BUCKET}/{blob.name}',\n",
    "            args = ['1000']\n",
    "        )\n",
    "    ),\n",
    "    batch_id = f'{SERIES}-{EXPERIMENT}'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e81f60e7-eeb0-4b3d-910a-127a975f9400",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result = operation.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f5530426-6241-49aa-a4ab-9f04f574c98f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "name: \"projects/statmike-mlops-349915/locations/us-central1/batches/r-dataproc-serverless\"\n",
       "uuid: \"89bffc2c-ae2d-4234-863d-10d64a725c94\"\n",
       "create_time {\n",
       "  seconds: 1706491705\n",
       "  nanos: 480689000\n",
       "}\n",
       "spark_r_batch {\n",
       "  main_r_file_uri: \"gs://statmike-mlops-349915/r/dataproc-serverless/models/20240129012536/sparkr.R\"\n",
       "  args: \"1000\"\n",
       "}\n",
       "runtime_info {\n",
       "  output_uri: \"gs://dataproc-staging-us-central1-1026793852137-jnbcmtsj/google-cloud-dataproc-metainfo/15fac571-1146-4a43-93f2-b3752d108a52/jobs/srvls-batch-89bffc2c-ae2d-4234-863d-10d64a725c94/driveroutput\"\n",
       "  approximate_usage {\n",
       "    milli_dcu_seconds: 720000\n",
       "    shuffle_storage_gb_seconds: 72000\n",
       "  }\n",
       "}\n",
       "state: SUCCEEDED\n",
       "state_time {\n",
       "  seconds: 1706491765\n",
       "  nanos: 633147000\n",
       "}\n",
       "creator: \"1026793852137-compute@developer.gserviceaccount.com\"\n",
       "labels {\n",
       "  key: \"goog-dataproc-location\"\n",
       "  value: \"us-central1\"\n",
       "}\n",
       "labels {\n",
       "  key: \"goog-dataproc-batch-uuid\"\n",
       "  value: \"89bffc2c-ae2d-4234-863d-10d64a725c94\"\n",
       "}\n",
       "labels {\n",
       "  key: \"goog-dataproc-batch-id\"\n",
       "  value: \"r-dataproc-serverless\"\n",
       "}\n",
       "runtime_config {\n",
       "  version: \"2.0.55\"\n",
       "  properties {\n",
       "    key: \"spark:spark.executor.instances\"\n",
       "    value: \"2\"\n",
       "  }\n",
       "  properties {\n",
       "    key: \"spark:spark.executor.cores\"\n",
       "    value: \"4\"\n",
       "  }\n",
       "  properties {\n",
       "    key: \"spark:spark.dynamicAllocation.executorAllocationRatio\"\n",
       "    value: \"0.3\"\n",
       "  }\n",
       "  properties {\n",
       "    key: \"spark:spark.driver.cores\"\n",
       "    value: \"4\"\n",
       "  }\n",
       "  properties {\n",
       "    key: \"spark:spark.app.name\"\n",
       "    value: \"projects/statmike-mlops-349915/locations/us-central1/batches/r-dataproc-serverless\"\n",
       "  }\n",
       "}\n",
       "environment_config {\n",
       "  execution_config {\n",
       "    service_account: \"1026793852137-compute@developer.gserviceaccount.com\"\n",
       "  }\n",
       "  peripherals_config {\n",
       "    spark_history_server_config {\n",
       "    }\n",
       "  }\n",
       "}\n",
       "operation: \"projects/statmike-mlops-349915/regions/us-central1/operations/ad6b9bc7-4e41-4131-b636-385eff6efb20\"\n",
       "state_history {\n",
       "  state: PENDING\n",
       "  state_start_time {\n",
       "    seconds: 1706491705\n",
       "    nanos: 480689000\n",
       "  }\n",
       "}\n",
       "state_history {\n",
       "  state: RUNNING\n",
       "  state_start_time {\n",
       "    seconds: 1706491754\n",
       "    nanos: 463790000\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c31e979-caec-4a92-87a1-04d6e963bc61",
   "metadata": {},
   "source": [
    "### Wait On Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99003899-0e6b-4f67-b3a8-b818b1dae5ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1666b431-a872-4ac5-b5fe-a213337665b4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review job details in the console at this link:\n",
      "https://console.cloud.google.com/dataproc/batches/us-central1/r-dataproc-serverless/monitoring?project=statmike-mlops-349915\n"
     ]
    }
   ],
   "source": [
    "print(f\"Review job details in the console at this link:\\nhttps://console.cloud.google.com/dataproc/batches/us-central1/{result.name.split('/')[-1]}/monitoring?project={PROJECT_ID}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310a6fec-fd48-4667-a5ed-98185b9d5baa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac7beb5-1c0f-443f-a573-1d23128e8bb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79764f9-e5c9-49ff-9b5a-8c43bfce0563",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76ff696-6fa6-4ee6-8f7c-06ce832033c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-root-py",
   "name": "workbench-notebooks.m113",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/workbench-notebooks:m113"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
